<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">萨摩耶德（Samoyeds）：利用稀疏张量核心加速混合专家模型，实现结构化稀疏性</h2><a id="user-content-萨摩耶德samoyeds利用稀疏张量核心加速混合专家模型实现结构化稀疏性" class="anchor" aria-label="Permalink: 萨摩耶德（Samoyeds）：利用稀疏张量核心加速混合专家模型，实现结构化稀疏性" href="#萨摩耶德samoyeds利用稀疏张量核心加速混合专家模型实现结构化稀疏性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.10725v1 公告类型：新研究<br>
摘要：基于专家混合（Mixture-of-Experts, MoE）的大型语言模型（Large Language Models, LLMs）规模不断扩大，带来了显著的计算和内存挑战，迫切需要创新解决方案在不牺牲模型准确性的前提下提升效率。结构化稀疏性作为一种有前景的策略，通过利用新兴的稀疏计算硬件来应对这些挑战。以往的研究主要集中在模型参数的稀疏性上，忽视了激活中固有的稀疏模式。这种忽视可能导致与激活相关的额外计算成本，进而可能影响性能表现。</p>
<p>本文介绍了Samoyeds，一个利用稀疏张量核心（Sparse Tensor Cores, SpTCs）为MoE LLMs设计的创新加速系统。Samoyeds首次同时将稀疏性应用于激活和模型参数。它引入了一种专为MoE计算定制的稀疏数据格式，并开发了一种专门的稀疏-稀疏矩阵乘法内核。此外，Samoyeds还包含了一系列系统优化，专门针对在SpTCs上执行双面结构化稀疏MoE LLMs而设计，进一步提升了系统性能。</p>
<p>评估结果显示，Samoyeds在内核级别上比现有最先进（SOTA）工作提升了高达1.99倍，在模型级别上提升了1.58倍。同时，它还提高了内存效率，平均最大支持批量大小增加了4.41倍。此外，Samoyeds在模型准确性和硬件可移植性方面均超越了现有的SOTA结构化稀疏解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">基于张量注意力自回归变换器的KV缓存压缩极限</h2><a id="user-content-基于张量注意力自回归变换器的kv缓存压缩极限" class="anchor" aria-label="Permalink: 基于张量注意力自回归变换器的KV缓存压缩极限" href="#基于张量注意力自回归变换器的kv缓存压缩极限"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.11108v1 公告类型：新研究  摘要：自回归变换器中的键值（KV）缓存是推理过程中的一个显著瓶颈，它限制了大语言模型（LLMs）的上下文长度能力。尽管之前的工作分析了标准注意力机制中的基本空间复杂度障碍[Haris 和 Onak, 2025]，但我们的研究将空间复杂度障碍的结果推广到了张量注意力版本。我们的理论贡献基于一种新颖的通信复杂度归约，并推导出当$d = \Omega(\log n)$时，张量结构注意力机制的内存下限。在$d = o(\log n)$的低维情况下，我们也分析了空间复杂度的理论界限。总体而言，我们的工作为我们理解张量注意力机制中的压缩-表达能力权衡提供了理论基础，并为开发更节省内存的变换器架构提供了更多视角。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>