<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">SpargeAttn：精准稀疏注意力机制，加速任意模型推理</h2><a id="user-content-spargeattn精准稀疏注意力机制加速任意模型推理" class="anchor" aria-label="Permalink: SpargeAttn：精准稀疏注意力机制，加速任意模型推理" href="#spargeattn精准稀疏注意力机制加速任意模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.18137v1 公告类型：新研究  摘要：由于注意力机制具有二次时间复杂度，其高效实现对于大型模型至关重要。幸运的是，注意力机制通常表现出稀疏性，即注意力图中的许多值接近零，这使得可以省略相应的计算。许多研究已经利用这种稀疏模式来加速注意力计算。然而，现有工作大多集中于通过利用注意力图的特定稀疏模式来优化特定模型内的注意力计算。一个既能保证加速又能确保多种模型端到端性能的通用稀疏注意力机制仍未被发现。本文中，我们提出了SpargeAttn，一种适用于任何模型的通用稀疏量化注意力机制。我们的方法采用两阶段在线过滤：第一阶段，我们快速准确地预测注意力图，从而跳过注意力中的部分矩阵乘法运算；第二阶段，我们设计了一种在线软最大感知过滤器，该过滤器不会产生额外开销，并进一步跳过部分矩阵乘法运算。实验表明，我们的方法显著加速了包括语言、图像和视频生成在内的多种模型，且不牺牲端到端性能指标。代码可在<a href="https://github.com/thu-ml/SpargeAttn%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/thu-ml/SpargeAttn获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">彩票LLM假说，重新思考LLM压缩应保留哪些能力？</h2><a id="user-content-彩票llm假说重新思考llm压缩应保留哪些能力" class="anchor" aria-label="Permalink: 彩票LLM假说，重新思考LLM压缩应保留哪些能力？" href="#彩票llm假说重新思考llm压缩应保留哪些能力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.17535v1 公告类型：新摘要 摘要：为了降低大型语言模型（LLMs）的计算和存储成本，模型压缩和键值缓存（KV cache）压缩引起了研究者的广泛关注。然而，当前的方法主要侧重于保持压缩后LLMs的性能，如通过困惑度（perplexity）或常识知识问答及基础算术推理任务的简单准确率来衡量。在本博客中，我们简要回顾了LLMs在检索增强生成、多步推理、外部工具使用及计算表达能力方面的最新进展，这些进展显著提升了LLM的性能。随后，我们提出了一个“彩票LLM假设”，即对于给定的LLM和任务，存在一个较小的彩票LLM，在多步推理和外部工具的辅助下，能够产生与原始LLM相同的性能。基于对LLM当前进展的回顾，我们讨论并总结了彩票LLM和KV缓存压缩必须具备的关键能力，这些能力在现有方法中往往被忽视。</p>
<div class="markdown-heading"><h2 class="heading-element">CoKV：通过合作博弈优化键值缓存分配</h2><a id="user-content-cokv通过合作博弈优化键值缓存分配" class="anchor" aria-label="Permalink: CoKV：通过合作博弈优化键值缓存分配" href="#cokv通过合作博弈优化键值缓存分配"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.17501v1 公告类型：新研究  摘要：大型语言模型（LLMs）在人类生活的各个方面取得了显著成功。然而，部署这些模型面临的主要挑战之一是存储键值对（KV）所需的大量内存消耗，这对资源提出了重大需求。最近的研究集中于KV缓存预算分配，几种方法通过评估单个注意力头的重要性提出了头级别的预算分配方案。然而，这些方法独立评估头的重要性，忽视了它们在模型中的协同贡献，可能导致对模型性能真实影响的偏离。鉴于这一局限，我们提出了CoKV，一种新颖的方法，将模型推理中头之间的合作建模为合作博弈。通过评估每个头在合作博弈中的贡献，CoKV能够更有效地分配缓存预算。大量实验表明，CoKV在使用LLama-3-8B-Instruct和Mistral-7B模型的LongBench基准测试中实现了最先进的性能。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>