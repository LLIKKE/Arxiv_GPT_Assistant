<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 08/27/2025</h1><a id="user-content-personalized-daily-arxiv-papers-08272025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 08/27/2025" href="#personalized-daily-arxiv-papers-08272025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 25</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models</a>
<strong>Authors:</strong> Tianze Liu, Md Abu Bakr Siddique, Hongyu An</p>
</li>
<li>
<p><a href="#link1">HAEPO: History-Aggregated Exploratory Policy Optimization</a>
<strong>Authors:</strong> Gaurish Trivedi, Alakh Sharma, Kartikey Singh Bhandari, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa</p>
</li>
<li>
<p><a href="#link2">History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL</a>
<strong>Authors:</strong> Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, Haibo Chen</p>
</li>
<li>
<p><a href="#link3">VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery</a>
<strong>Authors:</strong> Wang Jiayin, Wei Yanran, Jiang Lei, Guo Xiaoyu, Zheng Ayong, Zhao Weidong, Li Zhongkui</p>
</li>
<li>
<p><a href="#link4">Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units</a>
<strong>Authors:</strong> Chao Hao, Zezheng Wang, Yanhua Huang, Ruiwen Xu, Wenzhe Niu, Xin Liu, Zitong Yu</p>
</li>
<li>
<p><a href="#link5">BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration</a>
<strong>Authors:</strong> Jun Hou, Le Wang, Xuan Wang</p>
</li>
<li>
<p><a href="#link6">FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge</a>
<strong>Authors:</strong> Gang Hu, Yinglei Teng, Pengfei Wu, Nan Wang</p>
</li>
<li>
<p><a href="#link7">MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</a>
<strong>Authors:</strong> Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang</p>
</li>
<li>
<p><a href="#link8">Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution</a>
<strong>Authors:</strong> Chunlong Wu, Zhibo Qu</p>
</li>
<li>
<p><a href="#link9">C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</a>
<strong>Authors:</strong> Wei Li, Hangjie Yuan, Zixiang Zhao, Yifan Zhu, Aojun Lu, Tao Feng, Yanan Sun</p>
</li>
<li>
<p><a href="#link10">DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction</a>
<strong>Authors:</strong> Weilin Cai, Le Qin, Shwai He, Junwei Cui, Ang Li, Jiayi Huang</p>
</li>
<li>
<p><a href="#link11">Predicting the Order of Upcoming Tokens Improves Language Modeling</a>
<strong>Authors:</strong> Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji</p>
</li>
<li>
<p><a href="#link12">Linear cost mutual information estimation and independence test of similar performance as HSIC</a>
<strong>Authors:</strong> Jarek Duda, Jagoda Bracha, Adrian Przybysz</p>
</li>
<li>
<p><a href="#link13">Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming</a>
<strong>Authors:</strong> Jiajun Li, Ran Hou, Yu Ding, Yixuan Li, Shisi Guan, Jiahui Duan, Xiongwei Han, Tao Zhong, Vincent Chau, Weiwei Wu, Wanyuan Wang</p>
</li>
<li>
<p><a href="#link14">Enabling MoE on the Edge via Importance-Driven Expert Scheduling</a>
<strong>Authors:</strong> Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang</p>
</li>
<li>
<p><a href="#link15">ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation</a>
<strong>Authors:</strong> Yang Li, Hanjie Wang, Yuanzheng Li, Jiazheng Li, Zhaoyang Dong</p>
</li>
<li>
<p><a href="#link16">Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method</a>
<strong>Authors:</strong> I. I. Priezzhev, D. A. Danko, A. V. Shubin</p>
</li>
<li>
<p><a href="#link17">MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation</a>
<strong>Authors:</strong> Ernest Lim, Yajie Vera He, Jared Joselowitz, Kate Preston, Mohita Chowdhury, Louis Williams, Aisling Higham, Katrina Mason, Mariane Melo, Tom Lawton, Yan Jia, Ibrahim Habli</p>
</li>
<li>
<p><a href="#link18">pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data</a>
<strong>Authors:</strong> Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu</p>
</li>
<li>
<p><a href="#link19">STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning</a>
<strong>Authors:</strong> Yue Jiang, Chenxi Liu, Yile Chen, Qin Chao, Shuai Liu, Gao Cong</p>
</li>
<li>
<p><a href="#link20">APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration</a>
<strong>Authors:</strong> Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang</p>
</li>
<li>
<p><a href="#link21">UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning</a>
<strong>Authors:</strong> Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, Siyuan Qiao</p>
</li>
<li>
<p><a href="#link22">Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks</a>
<strong>Authors:</strong> Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota</p>
</li>
<li>
<p><a href="#link23">Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</a>
<strong>Authors:</strong> Yuxuan Cai, Yipeng Hao, Jie Zhou, Hang Yan, Zhikai Lei, Rui Zhen, Zhenhua Han, Yutao Yang, Junsong Li, Qianjun Pan, Tianyu Huai, Qin Chen, Xin Li, Kai Chen, Bo Zhang, Xipeng Qiu, Liang He</p>
</li>
<li>
<p><a href="#link24">Efficient task and path planning for maintenance automation using a robot system</a>
<strong>Authors:</strong> Christian Friedrich, Akos Csiszar, Armin Lechler, Alexander Verl</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2508.18460" rel="nofollow">Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-mimicking-associative-learning-of-rats-via-a-neuromorphic-robot-in-open-field-maze-using-spatial-cell-models-" class="anchor" aria-label="Permalink: 0. Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models" href="#0-mimicking-associative-learning-of-rats-via-a-neuromorphic-robot-in-open-field-maze-using-spatial-cell-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18460
<strong>Authors:</strong> Tianze Liu, Md Abu Bakr Siddique, Hongyu An</p>
<p><strong>Abstract:</strong> arXiv:2508.18460v1 Announce Type: new  Abstract: Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable prowess across various cognitive tasks using extensive training data. However, the reliance on large datasets and neural networks presents challenges such as highpower consumption and limited adaptability, particularly in SWaP-constrained applications like planetary exploration. To address these issues, we propose enhancing the autonomous capabilities of intelligent robots by emulating the associative learning observed in animals. Associative learning enables animals to adapt to their environment by memorizing concurrent events. By replicating this mechanism, neuromorphic robots can navigate dynamic environments autonomously, learning from interactions to optimize performance. This paper explores the emulation of associative learning in rodents using neuromorphic robots within open-field maze environments, leveraging insights from spatial cells such as place and grid cells. By integrating these models, we aim to enable online associative learning for spatial tasks in real-time scenarios, bridging the gap between biological spatial cognition and robotics for advancements in autonomous systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2508.18884" rel="nofollow">HAEPO: History-Aggregated Exploratory Policy Optimization</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-haepo-history-aggregated-exploratory-policy-optimization-" class="anchor" aria-label="Permalink: 1. HAEPO: History-Aggregated Exploratory Policy Optimization" href="#1-haepo-history-aggregated-exploratory-policy-optimization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18884
<strong>Authors:</strong> Gaurish Trivedi, Alakh Sharma, Kartikey Singh Bhandari, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa</p>
<p><strong>Abstract:</strong> arXiv:2508.18884v1 Announce Type: new  Abstract: Exploration is essential in modern learning, from reinforcement learning environments with small neural policies to large language models (LLMs). Existing work, such as DPO, leverages full sequence log-likelihoods to capture an entire trajectory of the model's decisions, while methods like GRPO aggregate per-token ratios into a trajectory-level update. However, both often limit exploration on long-horizon tasks. We introduce History-Aggregated Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to combat these shortcomings. HAEPO compresses each trajectory into the sum of its logarithmic probabilities (a cumulative logarithmic likelihood), and applies a Plackett-Luce softmax across trajectories to obtain normalized weights proportional to their returns, thus encouraging broader exploration. We add entropy regularization to stabilize the aggressive updates to prevent premature collapse and a soft KL penalty relative to a frozen copy of the previous (reference) policy. Empirically, HAEPO converges fast, explores thoroughly, aligns closely with true rewards, and demonstrates robust learning behavior better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO provides a stable and interpretable framework by explicitly leveraging full-trajectory history while balancing exploration and stability.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2508.18588" rel="nofollow">History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-history-rhymes-accelerating-llm-reinforcement-learning-with-rhymerl-" class="anchor" aria-label="Permalink: 2. History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL" href="#2-history-rhymes-accelerating-llm-reinforcement-learning-with-rhymerl-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18588
<strong>Authors:</strong> Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, Haibo Chen</p>
<p><strong>Abstract:</strong> arXiv:2508.18588v1 Announce Type: new  Abstract: With the rapid advancement of large language models (LLMs), reinforcement learning (RL) has emerged as a pivotal methodology for enhancing the reasoning capabilities of LLMs. Unlike traditional pre-training approaches, RL encompasses multiple stages: rollout, reward, and training, which necessitates collaboration among various worker types. However, current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles. While prior solutions like asynchronous execution and truncation offer partial relief, they may compromise training accuracy for efficiency.   Our key insight stems from a previously overlooked observation: rollout responses exhibit remarkable similarity across adjacent training epochs. Based on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate RL training with two key innovations. First, to enhance rollout generation, we present HistoSpec, a speculative decoding inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts. Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers. We have evaluated RhymeRL within a real production environment, demonstrating scalability from dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2508.18937" rel="nofollow">VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-visionsafeenhanced-vpc-cautious-predictive-control-with-visibility-constraints-under-uncertainty-for-autonomous-robotic-surgery-" class="anchor" aria-label="Permalink: 3. VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery" href="#3-visionsafeenhanced-vpc-cautious-predictive-control-with-visibility-constraints-under-uncertainty-for-autonomous-robotic-surgery-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18937
<strong>Authors:</strong> Wang Jiayin, Wei Yanran, Jiang Lei, Guo Xiaoyu, Zheng Ayong, Zhao Weidong, Li Zhongkui</p>
<p><strong>Abstract:</strong> arXiv:2508.18937v1 Announce Type: new  Abstract: Autonomous control of the laparoscope in robot-assisted Minimally Invasive Surgery (MIS) has received considerable research interest due to its potential to improve surgical safety. Despite progress in pixel-level Image-Based Visual Servoing (IBVS) control, the requirement of continuous visibility and the existence of complex disturbances, such as parameterization error, measurement noise, and uncertainties of payloads, could degrade the surgeon's visual experience and compromise procedural safety. To address these limitations, this paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and uncertainty-adaptive framework for autonomous laparoscope control that guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian Process Regression (GPR) is utilized to perform hybrid (deterministic + stochastic) quantification of operational uncertainties including residual model uncertainties, stochastic uncertainties, and external disturbances. Based on uncertainty quantification, a novel safety aware trajectory optimization framework with probabilistic guarantees is proposed, where a uncertainty-adaptive safety Control Barrier Function (CBF) condition is given based on uncertainty propagation, and chance constraints are simultaneously formulated based on probabilistic approximation. This uncertainty aware formulation enables adaptive control effort allocation, minimizing unnecessary camera motion while maintaining robustness. The proposed method is validated through comparative simulations and experiments on a commercial surgical robot platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph node dissection. Compared with baseline methods, the framework maintains near-perfect target visibility (&gt;99.9%), reduces tracking e</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2508.18763" rel="nofollow">Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-dynamic-collaboration-of-multi-language-models-based-on-minimal-complete-semantic-units-" class="anchor" aria-label="Permalink: 4. Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units" href="#4-dynamic-collaboration-of-multi-language-models-based-on-minimal-complete-semantic-units-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18763
<strong>Authors:</strong> Chao Hao, Zezheng Wang, Yanhua Huang, Ruiwen Xu, Wenzhe Niu, Xin Liu, Zitong Yu</p>
<p><strong>Abstract:</strong> arXiv:2508.18763v1 Announce Type: new  Abstract: This paper investigates the enhancement of reasoning capabilities in language models through token-level multi-model collaboration. Our approach selects the optimal tokens from the next token distributions provided by multiple models to perform autoregressive reasoning. Contrary to the assumption that more models yield better results, we introduce a distribution distance-based dynamic selection strategy (DDS) to optimize the multi-model collaboration process. To address the critical challenge of vocabulary misalignment in multi-model collaboration, we propose the concept of minimal complete semantic units (MCSU), which is simple yet enables multiple language models to achieve natural alignment within the linguistic space. Experimental results across various benchmarks demonstrate the superiority of our method. The code will be available at <a href="https://github.com/Fanye12/DDS">https://github.com/Fanye12/DDS</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2508.18551" rel="nofollow">BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-btw-a-non-parametric-variance-stabilization-framework-for-multimodal-model-integration-" class="anchor" aria-label="Permalink: 5. BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration" href="#5-btw-a-non-parametric-variance-stabilization-framework-for-multimodal-model-integration-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18551
<strong>Authors:</strong> Jun Hou, Le Wang, Xuan Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.18551v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in multimodal learning by enabling modular specialization across modalities. However, their effectiveness remains unclear when additional modalities introduce more noise than complementary information. Existing approaches, such as the Partial Information Decomposition, struggle to scale beyond two modalities and lack the resolution needed for instance-level control. We propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric weighting framework that combines instance-level Kullback-Leibler (KL) divergence and modality-level mutual information (MI) to dynamically adjust modality importance during training. Our method does not require additional parameters and can be applied to an arbitrary number of modalities. Specifically, BTW computes per-example KL weights by measuring the divergence between each unimodal and the current multimodal prediction, and modality-wide MI weights by estimating global alignment between unimodal and multimodal outputs. Extensive experiments on sentiment regression and clinical classification demonstrate that our method significantly improves regression performance and multiclass classification accuracy.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2508.18663" rel="nofollow">FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-fft-moe-efficient-federated-fine-tuning-for-foundation-models-via-large-scale-sparse-moe-under-heterogeneous-edge-" class="anchor" aria-label="Permalink: 6. FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge" href="#6-fft-moe-efficient-federated-fine-tuning-for-foundation-models-via-large-scale-sparse-moe-under-heterogeneous-edge-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18663
<strong>Authors:</strong> Gang Hu, Yinglei Teng, Pengfei Wu, Nan Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.18663v1 Announce Type: new  Abstract: As FMs drive progress toward Artificial General Intelligence (AGI), fine-tuning them under privacy and resource constraints has become increasingly critical particularly when highquality training data resides on distributed edge devices. Federated Learning (FL) offers a compelling solution through Federated Fine-Tuning (FFT), which enables collaborative model adaptation without sharing raw data. Recent approaches incorporate Parameter-Efficient Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce computational overhead. However, LoRA-based FFT faces two major limitations in heterogeneous FL environments: structural incompatibility across clients with varying LoRA configurations and limited adaptability to non-IID data distributions, which hinders convergence and generalization. To address these challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight gating network to selectively activate a personalized subset of experts, enabling fine-grained adaptation to local resource budgets while preserving aggregation compatibility. To further combat the expert load imbalance caused by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary loss that dynamically regularizes the routing distribution to ensure expert diversity and balanced utilization. Extensive experiments spanning both IID and non-IID conditions demonstrate that FFT MoE consistently outperforms state of the art FFT baselines in generalization performance and training efficiency.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2508.19236" rel="nofollow">MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-memoryvla-perceptual-cognitive-memory-in-vision-language-action-models-for-robotic-manipulation-" class="anchor" aria-label="Permalink: 7. MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation" href="#7-memoryvla-perceptual-cognitive-memory-in-vision-language-action-models-for-robotic-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19236
<strong>Authors:</strong> Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang</p>
<p><strong>Abstract:</strong> arXiv:2508.19236v1 Announce Type: new  Abstract: Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: <a href="https://shihao1895.github.io/MemoryVLA" rel="nofollow">https://shihao1895.github.io/MemoryVLA</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2508.18749" rel="nofollow">Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-reflection-enhanced-meta-optimization-integrating-textgrad-style-prompt-optimization-with-memory-driven-self-evolution-" class="anchor" aria-label="Permalink: 8. Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution" href="#8-reflection-enhanced-meta-optimization-integrating-textgrad-style-prompt-optimization-with-memory-driven-self-evolution-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18749
<strong>Authors:</strong> Chunlong Wu, Zhibo Qu</p>
<p><strong>Abstract:</strong> arXiv:2508.18749v1 Announce Type: new  Abstract: Recent advances in prompt optimization, exemplified by methods such as TextGrad, enable automatic, gradient-like refinement of textual prompts to enhance the performance of large language models (LLMs) on specific downstream tasks. However, current approaches are typically stateless and operate independently across optimization runs, lacking mechanisms to preserve and leverage historical optimization experience. Furthermore, they are susceptible to overfitting, often yielding prompt updates that generalize poorly beyond the immediate task context.   To address these limitations, we propose Reflection-Enhanced Meta-Optimization (REMO), a novel framework that integrates (1) a memory-augmented Reflection Retrieval-Augmented Generation (RAG) module - structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer, implemented via an LLM-driven meta-controller that synthesizes epoch-level reflective insights to iteratively improve system-level prompting strategies. This architecture enables not only local, fine-grained prompt tuning akin to TextGrad, but also the systematic accumulation and reuse of cross-run optimization knowledge, thereby supporting continual improvement over time.   We instantiate the REMO framework using Qwen3-32B in standard inference mode - without explicit chain-of-thought prompting - and evaluate its efficacy on the GSM8K benchmark for mathematical reasoning. Experimental results demonstrate that, compared to a TextGrad baseline, REMO achieves more stable and robust generalization, albeit at the cost of increased computational overhead. We provide a detailed exposition of the algorithmic design, conduct a qualitative and quantitative analysis of optimization dynamics, and present a comprehensive ablation study to elucidate the contributions of each component.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2508.18860" rel="nofollow">C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-c-flat-towards-a-more-efficient-and-powerful-framework-for-continual-learning-" class="anchor" aria-label="Permalink: 9. C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning" href="#9-c-flat-towards-a-more-efficient-and-powerful-framework-for-continual-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18860
<strong>Authors:</strong> Wei Li, Hangjie Yuan, Zixiang Zhao, Yifan Zhu, Aojun Lu, Tao Feng, Yanan Sun</p>
<p><strong>Abstract:</strong> arXiv:2508.18860v1 Announce Type: new  Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge is crucial in continual learning (CL). Recently, sharpness-aware minimization has proven effective in transfer learning and has also been adopted in continual learning (CL) to improve memory retention and learning efficiency. However, relying on zeroth-order sharpness alone may favor sharper minima over flatter ones in certain settings, leading to less robust and potentially suboptimal solutions. In this paper, we propose \textbf{C}ontinual \textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling easy integration with minimal modifications to the code pipeline. Besides, we present a general framework that integrates C-Flat into all major CL paradigms and conduct comprehensive comparisons with loss-minima optimizers and flat-minima-based CL methods. Our results show that C-Flat consistently improves performance across a wide range of settings. In addition, we introduce C-Flat++, an efficient yet effective framework that leverages selective flatness-driven promotion, significantly reducing the update cost required by C-Flat. Extensive experiments across multiple CL methods, datasets, and scenarios demonstrate the effectiveness and efficiency of our proposed approaches. Code is available at <a href="https://github.com/WanNaa/C-Flat">https://github.com/WanNaa/C-Flat</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2508.18376" rel="nofollow">DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-dualsparse-moe-coordinating-tensorneuron-level-sparsity-with-expert-partition-and-reconstruction-" class="anchor" aria-label="Permalink: 10. DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction" href="#10-dualsparse-moe-coordinating-tensorneuron-level-sparsity-with-expert-partition-and-reconstruction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18376
<strong>Authors:</strong> Weilin Cai, Le Qin, Shwai He, Junwei Cui, Ang Li, Jiayi Huang</p>
<p><strong>Abstract:</strong> arXiv:2508.18376v1 Announce Type: new  Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building Large Language Models (LLMs) by reducing per-token computation while enabling model scaling. It can be viewed as partitioning a large Feed-Forward Network (FFN) at the tensor level into fine-grained sub-FFNs, or experts, and activating only a sparse subset for each input. While this sparsity improves efficiency, MoE still faces substantial challenges due to their massive computational scale and unpredictable activation patterns.   To enable efficient MoE deployment, we identify dual sparsity at the tensor and neuron levels in pre-trained MoE modules as a key factor for both accuracy and efficiency. Unlike prior work that increases tensor-level sparsity through finer-grained expert design during pre-training, we introduce post-training expert partitioning to induce such sparsity without retraining. This preserves the mathematical consistency of model transformations and enhances both efficiency and accuracy in subsequent fine-tuning and inference. Building upon this, we propose DualSparse-MoE, an inference system that integrates dynamic tensor-level computation dropping with static neuron-level reconstruction to deliver significant efficiency gains with minimal accuracy loss.   Experimental results show that enforcing an approximate 25% drop rate with our approach reduces average accuracy by only 0.08%-0.28% across three prevailing MoE models, while nearly all degrees of computation dropping consistently yield proportional computational speedups. Furthermore, incorporating load-imbalance awareness into expert parallelism achieves a 1.41x MoE module speedup with just 0.5% average accuracy degradation.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2508.19228" rel="nofollow">Predicting the Order of Upcoming Tokens Improves Language Modeling</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-predicting-the-order-of-upcoming-tokens-improves-language-modeling-" class="anchor" aria-label="Permalink: 11. Predicting the Order of Upcoming Tokens Improves Language Modeling" href="#11-predicting-the-order-of-upcoming-tokens-improves-language-modeling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19228
<strong>Authors:</strong> Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji</p>
<p><strong>Abstract:</strong> arXiv:2508.19228v1 Announce Type: new  Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. We argue that MTP's exact future token prediction is too difficult as an auxiliary loss. Instead, we propose Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using a learning-to-rank loss. TOP requires only a single additional unembedding layer compared to MTP's multiple transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. Our code is available at <a href="https://github.com/zaydzuhri/token-order-prediction">https://github.com/zaydzuhri/token-order-prediction</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2508.18338" rel="nofollow">Linear cost mutual information estimation and independence test of similar performance as HSIC</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-linear-cost-mutual-information-estimation-and-independence-test-of-similar-performance-as-hsic-" class="anchor" aria-label="Permalink: 12. Linear cost mutual information estimation and independence test of similar performance as HSIC" href="#12-linear-cost-mutual-information-estimation-and-independence-test-of-similar-performance-as-hsic-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18338
<strong>Authors:</strong> Jarek Duda, Jagoda Bracha, Adrian Przybysz</p>
<p><strong>Abstract:</strong> arXiv:2508.18338v1 Announce Type: new  Abstract: Evaluation of statistical dependencies between two data samples is a basic problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information Criterion)<del>\cite{HSIC} is considered the state-of-art method. However, for size $n$ data sample it requires multiplication of $n\times n$ matrices, what currently needs $\sim O(n^{2.37})$ computational complexity</del>\cite{mult}, making it impractical for large data samples. We discuss HCR (Hierarchical Correlation Reconstruction) as its linear cost practical alternative of even higher dependence sensitivity in tests, and additionally providing actual joint distribution model by description of dependencies through features being mixed moments, starting with correlation and homoscedasticity, also allowing to approximate mutual information as just sum of squares of such nontrivial mixed moments between two data samples. Such single dependence describing feature is calculated in $O(n)$ linear time. Their number to test varies with dimension $d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also consider more subtle triplewise, and so on.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2508.18742" rel="nofollow">Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-constraint-matters-multi-modal-representation-for-reducing-mixed-integer-linear-programming-" class="anchor" aria-label="Permalink: 13. Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming" href="#13-constraint-matters-multi-modal-representation-for-reducing-mixed-integer-linear-programming-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18742
<strong>Authors:</strong> Jiajun Li, Ran Hou, Yu Ding, Yixuan Li, Shisi Guan, Jiahui Duan, Xiongwei Han, Tao Zhong, Vincent Chau, Weiwei Wu, Wanyuan Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.18742v1 Announce Type: new  Abstract: Model reduction, which aims to learn a simpler model of the original mixed integer linear programming (MILP), can solve large-scale MILP problems much faster. Most existing model reduction methods are based on variable reduction, which predicts a solution value for a subset of variables. From a dual perspective, constraint reduction that transforms a subset of inequality constraints into equalities can also reduce the complexity of MILP, but has been largely ignored. Therefore, this paper proposes a novel constraint-based model reduction approach for the MILP. Constraint-based MILP reduction has two challenges: 1) which inequality constraints are critical such that reducing them can accelerate MILP solving while preserving feasibility, and 2) how to predict these critical constraints efficiently. To identify critical constraints, we first label these tight-constraints at the optimal solution as potential critical constraints and design a heuristic rule to select a subset of critical tight-constraints. To learn the critical tight-constraints, we propose a multi-modal representation technique that leverages information from both instance-level and abstract-level MILP formulations. The experimental results show that, compared to the state-of-the-art methods, our method improves the quality of the solution by over 50% and reduces the computation time by 17.47%.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2508.18983" rel="nofollow">Enabling MoE on the Edge via Importance-Driven Expert Scheduling</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-enabling-moe-on-the-edge-via-importance-driven-expert-scheduling-" class="anchor" aria-label="Permalink: 14. Enabling MoE on the Edge via Importance-Driven Expert Scheduling" href="#14-enabling-moe-on-the-edge-via-importance-driven-expert-scheduling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18983
<strong>Authors:</strong> Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.18983v1 Announce Type: new  Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2508.18318" rel="nofollow">ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-ztfed-mas2s-a-zero-trust-federated-learning-framework-with-verifiable-privacy-and-trust-aware-aggregation-for-wind-power-data-imputation-" class="anchor" aria-label="Permalink: 15. ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation" href="#15-ztfed-mas2s-a-zero-trust-federated-learning-framework-with-verifiable-privacy-and-trust-aware-aggregation-for-wind-power-data-imputation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18318
<strong>Authors:</strong> Yang Li, Hanjie Wang, Yuanzheng Li, Jiazheng Li, Zhaoyang Dong</p>
<p><strong>Abstract:</strong> arXiv:2508.18318v1 Announce Type: new  Abstract: Wind power data often suffers from missing values due to sensor faults and unstable transmission at edge sites. While federated learning enables privacy-preserving collaboration without sharing raw data, it remains vulnerable to anomalous updates and privacy leakage during parameter exchange. These challenges are amplified in open industrial environments, necessitating zero-trust mechanisms where no participant is inherently trusted. To address these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated learning framework that integrates a multi-head attention-based sequence-to-sequence imputation model. ZTFed integrates verifiable differential privacy with non-interactive zero-knowledge proofs and a confidentiality and integrity verification mechanism to ensure verifiable privacy preservation and secure model parameters transmission. A dynamic trust-aware aggregation mechanism is employed, where trust is propagated over similarity graphs to enhance robustness, and communication overhead is reduced via sparsity- and quantization-based compression. MAS2S captures long-term dependencies in wind power data for accurate imputation. Extensive experiments on real-world wind farm datasets validate the superiority of ZTFed-MAS2S in both federated learning performance and missing data imputation, demonstrating its effectiveness as a secure and efficient solution for practical applications in the energy sector.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2508.18953" rel="nofollow">Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-novel-approaches-to-artificial-intelligence-development-based-on-the-nearest-neighbor-method-" class="anchor" aria-label="Permalink: 16. Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method" href="#16-novel-approaches-to-artificial-intelligence-development-based-on-the-nearest-neighbor-method-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18953
<strong>Authors:</strong> I. I. Priezzhev, D. A. Danko, A. V. Shubin</p>
<p><strong>Abstract:</strong> arXiv:2508.18953v1 Announce Type: new  Abstract: Modern neural network technologies, including large language models, have achieved remarkable success in various applied artificial intelligence applications, however, they face a range of fundamental limitations. Among them are hallucination effects, high computational complexity of training and inference, costly fine-tuning, and catastrophic forgetting issues. These limitations significantly hinder the use of neural networks in critical areas such as medicine, industrial process management, and scientific research. This article proposes an alternative approach based on the nearest neighbors method with hierarchical clustering structures. Employing the k-nearest neighbors algorithm significantly reduces or completely eliminates hallucination effects while simplifying model expansion and fine-tuning without the need for retraining the entire network. To overcome the high computational load of the k-nearest neighbors method, the paper proposes using tree-like data structures based on Kohonen self-organizing maps, thereby greatly accelerating nearest neighbor searches. Tests conducted on handwritten digit recognition and simple subtitle translation tasks confirmed the effectiveness of the proposed approach. With only a slight reduction in accuracy, the nearest neighbor search time was reduced hundreds of times compared to exhaustive search methods. The proposed method features transparency and interpretability, closely aligns with human cognitive mechanisms, and demonstrates potential for extensive use in tasks requiring high reliability and explainable results.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2508.19163" rel="nofollow">MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-matrix-multi-agent-simulation-framework-for-safe-interactions-and-contextual-clinical-conversational-evaluation-" class="anchor" aria-label="Permalink: 17. MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation" href="#17-matrix-multi-agent-simulation-framework-for-safe-interactions-and-contextual-clinical-conversational-evaluation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19163
<strong>Authors:</strong> Ernest Lim, Yajie Vera He, Jared Joselowitz, Kate Preston, Mohita Chowdhury, Louis Williams, Aisling Higham, Katrina Mason, Mariane Melo, Tom Lawton, Yan Jia, Ibrahim Habli</p>
<p><strong>Abstract:</strong> arXiv:2508.19163v1 Announce Type: new  Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue systems, existing evaluations focus on task completion or fluency, offering little insight into the behavioral and risk management requirements essential for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation), a structured, extensible framework for safety-oriented evaluation of clinical dialogue agents.   MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical scenarios, expected system behaviors and failure modes derived through structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator for detecting safety-relevant dialogue failures, validated against expert clinician annotations; and (3) PatBot, a simulated patient agent capable of producing diverse, scenario-conditioned responses, evaluated for realism and behavioral fidelity with human factors expertise, and a patient-preference study.   Across three experiments, we show that MATRIX enables systematic, scalable safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded assessment of 240 dialogues. We also conducted one of the first realism analyses of LLM-based patient simulation, showing that PatBot reliably simulates realistic patient behavior in quantitative and qualitative evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios and 10 clinical domains.   MATRIX is the first framework to unify structured safety engineering with scalable, validated conversational AI evaluation, enabling regulator-aligned safety auditing. We release all evaluation tools, prompts, structured scenarios, and datasets.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2508.18891" rel="nofollow">pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-pyfast-a-modular-pytorch-framework-for-time-series-modeling-with-multi-source-and-sparse-data-" class="anchor" aria-label="Permalink: 18. pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data" href="#18-pyfast-a-modular-pytorch-framework-for-time-series-modeling-with-multi-source-and-sparse-data-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18891
<strong>Authors:</strong> Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu</p>
<p><strong>Abstract:</strong> arXiv:2508.18891v1 Announce Type: new  Abstract: Modern time series analysis demands frameworks that are flexible, efficient, and extensible. However, many existing Python libraries exhibit limitations in modularity and in their native support for irregular, multi-source, or sparse data. We introduce pyFAST, a research-oriented PyTorch framework that explicitly decouples data processing from model computation, fostering a cleaner separation of concerns and facilitating rapid experimentation. Its data engine is engineered for complex scenarios, supporting multi-source loading, protein sequence handling, efficient sequence- and patch-level padding, dynamic normalization, and mask-based modeling for both imputation and forecasting. pyFAST integrates LLM-inspired architectures for the alignment-free fusion of sparse data sources and offers native sparse metrics, specialized loss functions, and flexible exogenous data fusion. Training utilities include batch-based streaming aggregation for evaluation and device synergy to maximize computational efficiency. A comprehensive suite of classical and deep learning models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a modular architecture that encourages extension. Released under the MIT license at GitHub, pyFAST provides a compact yet powerful platform for advancing time series research and applications.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2508.18635" rel="nofollow">STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-strata-ts-selective-knowledge-transfer-for-urban-time-series-forecasting-with-retrieval-guided-reasoning-" class="anchor" aria-label="Permalink: 19. STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning" href="#19-strata-ts-selective-knowledge-transfer-for-urban-time-series-forecasting-with-retrieval-guided-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18635
<strong>Authors:</strong> Yue Jiang, Chenxi Liu, Yile Chen, Qin Chao, Shuai Liu, Gao Cong</p>
<p><strong>Abstract:</strong> arXiv:2508.18635v1 Announce Type: new  Abstract: Urban forecasting models often face a severe data imbalance problem: only a few cities have dense, long-span records, while many others expose short or incomplete histories. Direct transfer from data-rich to data-scarce cities is unreliable because only a limited subset of source patterns truly benefits the target domain, whereas indiscriminate transfer risks introducing noise and negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware retrieval for Time Series), a framework that combines domain-adapted retrieval with reasoning-capable large models to improve forecasting in scarce data regimes. STRATA-TS employs a patch-based temporal encoder to identify source subsequences that are semantically and dynamically aligned with the target query. These retrieved exemplars are then injected into a retrieval-guided reasoning stage, where an LLM performs structured inference over target inputs and retrieved support. To enable efficient deployment, we distill the reasoning process into a compact open model via supervised fine-tuning. Extensive experiments on three parking availability datasets across Singapore, Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms strong forecasting and transfer baselines, while providing interpretable knowledge transfer pathways.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2508.19087" rel="nofollow">APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-apt-llm-exploiting-arbitrary-precision-tensor-core-computing-for-llm-acceleration-" class="anchor" aria-label="Permalink: 20. APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration" href="#20-apt-llm-exploiting-arbitrary-precision-tensor-core-computing-for-llm-acceleration-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19087
<strong>Authors:</strong> Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.19087v1 Announce Type: new  Abstract: Large language models (LLMs) have revolutionized AI applications, yet their enormous computational demands severely limit deployment and real-time performance. Quantization methods can help reduce computational costs, however, attaining the extreme efficiency associated with ultra-low-bit quantized LLMs at arbitrary precision presents challenges on GPUs. This is primarily due to the limited support for GPU Tensor Cores, inefficient memory management, and inflexible kernel optimizations. To tackle these challenges, we propose a comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM. Firstly, we introduce a novel data format, bipolar-INT, which allows for efficient and lossless conversion with signed INT, while also being more conducive to parallel computation. We also develop a matrix multiplication (MatMul) method allowing for arbitrary precision by dismantling and reassembling matrices at the bit level. This method provides flexible precision and optimizes the utilization of GPU Tensor Cores. In addition, we propose a memory management system focused on data recovery, which strategically employs fast shared memory to substantially increase kernel execution speed and reduce memory access latency. Finally, we develop a kernel mapping method that dynamically selects the optimal configurable hyperparameters of kernels for varying matrix sizes, enabling optimal performance across different LLM architectures and precision settings. In LLM inference, APT-LLM achieves up to a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800, APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup over CUTLASS integer baselines.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2508.18756" rel="nofollow">UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-ultramemv2-memory-networks-scaling-to-120b-parameters-with-superior-long-context-learning-" class="anchor" aria-label="Permalink: 21. UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning" href="#21-ultramemv2-memory-networks-scaling-to-120b-parameters-with-superior-long-context-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18756
<strong>Authors:</strong> Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, Siyuan Qiao</p>
<p><strong>Abstract:</strong> arXiv:2508.18756v1 Announce Type: new  Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every transformer block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total sparse parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient sparse computation.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2508.18672" rel="nofollow">Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-optimal-sparsity-of-mixture-of-experts-language-models-for-reasoning-tasks-" class="anchor" aria-label="Permalink: 22. Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks" href="#22-optimal-sparsity-of-mixture-of-experts-language-models-for-reasoning-tasks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18672
<strong>Authors:</strong> Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota</p>
<p><strong>Abstract:</strong> arXiv:2508.18672v1 Announce Type: new  Abstract: Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-$k$ routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-$k$ alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at <a href="https://github.com/rioyokotalab/optimal-sparsity">https://github.com/rioyokotalab/optimal-sparsity</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2508.19005" rel="nofollow">Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-building-self-evolving-agents-via-experience-driven-lifelong-learning-a-framework-and-benchmark-" class="anchor" aria-label="Permalink: 23. Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark" href="#23-building-self-evolving-agents-via-experience-driven-lifelong-learning-a-framework-and-benchmark-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19005
<strong>Authors:</strong> Yuxuan Cai, Yipeng Hao, Jie Zhou, Hang Yan, Zhikai Lei, Rui Zhen, Zhenhua Han, Yutao Yang, Junsong Li, Qianjun Pan, Tianyu Huai, Qin Chen, Xin Li, Kai Chen, Bo Zhang, Xipeng Qiu, Liang He</p>
<p><strong>Abstract:</strong> arXiv:2508.19005v1 Announce Type: new  Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".   We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm shifts: From Passive to Proactive, From Context to Memory, and From Imitation to Learning. In this dynamic environment, agents must acquire and distill practical skills and maintain persistent memory to make decisions based on evolving state variables. StuLife provides a comprehensive platform for evaluating lifelong learning capabilities, including memory retention, skill transfer, and self-motivated behavior. Beyond evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of context engineering in advancing AGI.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2508.18400" rel="nofollow">Efficient task and path planning for maintenance automation using a robot system</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-efficient-task-and-path-planning-for-maintenance-automation-using-a-robot-system-" class="anchor" aria-label="Permalink: 24. Efficient task and path planning for maintenance automation using a robot system" href="#24-efficient-task-and-path-planning-for-maintenance-automation-using-a-robot-system-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.18400
<strong>Authors:</strong> Christian Friedrich, Akos Csiszar, Armin Lechler, Alexander Verl</p>
<p><strong>Abstract:</strong> arXiv:2508.18400v1 Announce Type: new  Abstract: The research and development of intelligent automation solutions is a ground-breaking point for the factory of the future. A promising and challenging mission is the use of autonomous robot systems to automate tasks in the field of maintenance. For this purpose, the robot system must be able to plan autonomously the different manipulation tasks and the corresponding paths. Basic requirements are the development of algorithms with a low computational complexity and the possibility to deal with environmental uncertainties. In this work, an approach is presented, which is especially suited to solve the problem of maintenance automation. For this purpose, offline data from CAD is combined with online data from an RGBD vision system via a probabilistic filter, to compensate uncertainties from offline data. For planning the different tasks, a method is explained, which use a symbolic description, founded on a novel sampling-based method to compute the disassembly space. For path planning we use global state-of-the art algorithms with a method that allows the adaption of the exploration stepsize in order to reduce the planning time. Every method is experimentally validated and discussed.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model. Furthermore, it incorporates Mixture-of-Experts (MoE) architectures to significantly decrease deployment overhead and accelerate inference speed, enabling more efficient and scalable model serving in resource-constrained environments.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>