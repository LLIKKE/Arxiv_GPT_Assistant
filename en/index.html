<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 05/27/2025</h1><a id="user-content-personalized-daily-arxiv-papers-05272025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 05/27/2025" href="#personalized-daily-arxiv-papers-05272025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 18</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</a>
<strong>Authors:</strong> Peijie Dong, Zhenheng Tang, Xiang Liu, Lujun Li, Xiaowen Chu, Bo Li</p>
</li>
<li>
<p><a href="#link1">Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs</a>
<strong>Authors:</strong> Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman</p>
</li>
<li>
<p><a href="#link2">Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?</a>
<strong>Authors:</strong> Waleed Reda, Abhinav Jangda, Krishna Chintalapudi</p>
</li>
<li>
<p><a href="#link3">$\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts</a>
<strong>Authors:</strong> Toshiaki Koike-Akino, Jing Liu, Ye Wang</p>
</li>
<li>
<p><a href="#link4">Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding</a>
<strong>Authors:</strong> Alexander Conzelmann, Robert Bamler</p>
</li>
<li>
<p><a href="#link5">Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer</a>
<strong>Authors:</strong> Guodong Du, Zitao Fang, Jing Li, Junlin Li, Runhua Jiang, Shuyang Yu, Yifei Guo, Yangneng Chen, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Honghai Liu, Min Zhang</p>
</li>
<li>
<p><a href="#link6">WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference</a>
<strong>Authors:</strong> Sihan Chen, Dan Zhao, Jongwoo Ko, Colby Banbury, Huiping Zhuang, Luming Liang, Tianyi Chen</p>
</li>
<li>
<p><a href="#link7">NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache</a>
<strong>Authors:</strong> Donghyun Son, Euntae Choi, Sungjoo Yoo</p>
</li>
<li>
<p><a href="#link8">ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning</a>
<strong>Authors:</strong> Mingkuan Feng, Jinyang Wu, Siyuan Liu, Shuai Zhang, Hongjian Fang, Ruihan Jin, Feihu Che, Pengpeng Shao, Zhengqi Wen, Jianhua Tao</p>
</li>
<li>
<p><a href="#link9">Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression</a>
<strong>Authors:</strong> Jacob Sander, David Moe, Achraf Cohen, Brent Venable, Venkat Dasari, Brian Jalaian</p>
</li>
<li>
<p><a href="#link10">LatentLLM: Attention-Aware Joint Tensor Compression</a>
<strong>Authors:</strong> Toshiaki Koike-Akino (Perry), Xiangyu Chen (Perry), Jing Liu (Perry), Ye Wang (Perry), Pu (Perry), Wang, Matthew Brand</p>
</li>
<li>
<p><a href="#link11">CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models</a>
<strong>Authors:</strong> Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen</p>
</li>
<li>
<p><a href="#link12">Communication-Efficient Multi-Device Inference Acceleration for Transformer Models</a>
<strong>Authors:</strong> Xiao Liu, Lijun Zhang, Deepak Ganesan, Hui Guan</p>
</li>
<li>
<p><a href="#link13">LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning</a>
<strong>Authors:</strong> Junyu Chen, Junzhuo Li, Zhen Peng, Wenjie Wang, Yuxiang Ren, Long Shi, Xuming Hu</p>
</li>
<li>
<p><a href="#link14">Model-Distributed Inference for Large Language Models at the Edge</a>
<strong>Authors:</strong> Davide Macario, Hulya Seferoglu, Erdem Koyuncu</p>
</li>
<li>
<p><a href="#link15">Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression</a>
<strong>Authors:</strong> Kunjun Li, Zigeng Chen, Cheng-Yen Yang, Jenq-Neng Hwang</p>
</li>
<li>
<p><a href="#link16">FP4 All the Way: Fully Quantized Training of LLMs</a>
<strong>Authors:</strong> Brian Chmiel, Maxim Fishman, Ron Banner, Daniel Soudry</p>
</li>
<li>
<p><a href="#link17">Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks</a>
<strong>Authors:</strong> Safa Hamreras, Sukhbinder Singh, Rom'an Or'us</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2505.19433" rel="nofollow">Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-can-compressed-llms-truly-act-an-empirical-evaluation-of-agentic-capabilities-in-llm-compression-" class="anchor" aria-label="Permalink: 0. Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression" href="#0-can-compressed-llms-truly-act-an-empirical-evaluation-of-agentic-capabilities-in-llm-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.19433
<strong>Authors:</strong> Peijie Dong, Zhenheng Tang, Xiang Liu, Lujun Li, Xiaowen Chu, Bo Li</p>
<p><strong>Abstract:</strong> arXiv:2505.19433v1 Announce Type: new  Abstract: Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in <a href="https://github.com/pprp/ACBench">https://github.com/pprp/ACBench</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2505.19481" rel="nofollow">Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-win-fast-or-lose-slow-balancing-speed-and-accuracy-in-latency-sensitive-decisions-of-llms-" class="anchor" aria-label="Permalink: 1. Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs" href="#1-win-fast-or-lose-slow-balancing-speed-and-accuracy-in-latency-sensitive-decisions-of-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.19481
<strong>Authors:</strong> Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman</p>
<p><strong>Abstract:</strong> arXiv:2505.19481v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency quality trade off, it remains underexplored in the context of LLM based agents. In this work, we present the first systematic study of this trade off in real time decision making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency aware evaluation and deployment strategies for LLM based agents. These results demonstrate the critical importance of latency aware evaluation and deployment strategies for real world LLM based agents. Our benchmarks are available at Latency Sensitive Benchmarks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2505.18350" rel="nofollow">Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-task-specific-pruning-with-llm-sieve-how-many-parameters-does-your-task-really-need-" class="anchor" aria-label="Permalink: 2. Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?" href="#2-task-specific-pruning-with-llm-sieve-how-many-parameters-does-your-task-really-need-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18350
<strong>Authors:</strong> Waleed Reda, Abhinav Jangda, Krishna Chintalapudi</p>
<p><strong>Abstract:</strong> arXiv:2505.18350v1 Announce Type: new  Abstract: As Large Language Models (LLMs) are increasingly being adopted for narrow tasks - such as medical question answering or sentiment analysis - and deployed in resource-constrained settings, a key question arises: how many parameters does a task actually need? In this work, we present LLM-Sieve, the first comprehensive framework for task-specific pruning of LLMs that achieves 20-75% parameter reduction with only 1-5% accuracy degradation across diverse domains. Unlike prior methods that apply uniform pruning or rely on low-rank approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns task-aware joint projections to better approximate output behavior, and (ii) employs a Genetic Algorithm to discover differentiated pruning levels for each matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization, and uniquely demonstrates strong generalization across datasets within the same task domain. Together, these results establish a practical and robust mechanism to generate smaller performant task-specific models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2505.18451" rel="nofollow">$\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-mu-moe-test-time-pruning-as-micro-grained-mixture-of-experts-" class="anchor" aria-label="Permalink: 3. $\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts" href="#3-mu-moe-test-time-pruning-as-micro-grained-mixture-of-experts-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18451
<strong>Authors:</strong> Toshiaki Koike-Akino, Jing Liu, Ye Wang</p>
<p><strong>Abstract:</strong> arXiv:2505.18451v1 Announce Type: new  Abstract: To tackle the huge computational demand of large foundation models, activation-aware compression techniques without retraining have been introduced. However, since these rely on calibration data, domain shift may arise for unknown downstream tasks. With a computationally efficient calibration, activation-aware pruning can be executed for every prompt adaptively, yet achieving reduced complexity at inference. We formulate it as a mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured sparsity on the fly.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2505.18758" rel="nofollow">Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-reducing-storage-of-pretrained-neural-networks-by-rate-constrained-quantization-and-entropy-coding-" class="anchor" aria-label="Permalink: 4. Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding" href="#4-reducing-storage-of-pretrained-neural-networks-by-rate-constrained-quantization-and-entropy-coding-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18758
<strong>Authors:</strong> Alexander Conzelmann, Robert Bamler</p>
<p><strong>Abstract:</strong> arXiv:2505.18758v1 Announce Type: new  Abstract: The ever-growing size of neural networks poses serious challenges on resource-constrained devices, such as embedded sensors. Compression algorithms that reduce their size can mitigate these problems, provided that model performance stays close to the original. We propose a novel post-training compression framework that combines rate-aware quantization with entropy coding by (1) extending the well-known layer-wise loss by a quadratic rate estimation, and (2) providing locally exact solutions to this modified objective following the Optimal Brain Surgeon (OBS) method. Our method allows for very fast decoding and is compatible with arbitrary quantization grids. We verify our results empirically by testing on various computer-vision networks, achieving a 20-40% decrease in bit rate at the same performance as the popular compression algorithm NNCodec. Our code is available at <a href="https://github.com/Conzel/cerwu">https://github.com/Conzel/cerwu</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2505.18713" rel="nofollow">Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-neural-parameter-search-for-slimmer-fine-tuned-models-and-better-transfer-" class="anchor" aria-label="Permalink: 5. Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer" href="#5-neural-parameter-search-for-slimmer-fine-tuned-models-and-better-transfer-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18713
<strong>Authors:</strong> Guodong Du, Zitao Fang, Jing Li, Junlin Li, Runhua Jiang, Shuyang Yu, Yifei Guo, Yangneng Chen, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Honghai Liu, Min Zhang</p>
<p><strong>Abstract:</strong> arXiv:2505.18713v1 Announce Type: new  Abstract: Foundation models and their checkpoints have significantly advanced deep learning, boosting performance across various applications. However, fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Recent studies suggest that combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency. In this context, developing an effective pruning strategy for fine-tuned models is crucial. Leveraging the advantages of the task vector mechanism, we preprocess fine-tuned models by calculating the differences between them and the original model. Recognizing that different task vector subspaces contribute variably to model performance, we introduce a novel method called Neural Parameter Search (NPS-Pruning) for slimming down fine-tuned models. This method enhances pruning efficiency by searching through neural parameters of task vectors within low-rank subspaces. Our method has three key applications: enhancing knowledge transfer through pairwise model interpolation, facilitating effective knowledge fusion via model merging, and enabling the deployment of compressed models that retain near-original performance while significantly reducing storage costs. Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of our approach, resulting in substantial performance gains. The code is publicly available at: <a href="https://github.com/duguodong7/NPS-Pruning">https://github.com/duguodong7/NPS-Pruning</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2505.19427" rel="nofollow">WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-wina-weight-informed-neuron-activation-for-accelerating-large-language-model-inference-" class="anchor" aria-label="Permalink: 6. WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference" href="#6-wina-weight-informed-neuron-activation-for-accelerating-large-language-model-inference-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.19427
<strong>Authors:</strong> Sihan Chen, Dan Zhao, Jongwoo Ko, Colby Banbury, Huiping Zhuang, Luming Liang, Tianyi Chen</p>
<p><strong>Abstract:</strong> arXiv:2505.19427v1 Announce Type: new  Abstract: The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise $\ell_2$-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to $2.94%$ in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at <a href="https://github.com/microsoft/wina">https://github.com/microsoft/wina</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2505.18231" rel="nofollow">NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-nsnquant-a-double-normalization-approach-for-calibration-free-low-bit-vector-quantization-of-kv-cache-" class="anchor" aria-label="Permalink: 7. NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache" href="#7-nsnquant-a-double-normalization-approach-for-calibration-free-low-bit-vector-quantization-of-kv-cache-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18231
<strong>Authors:</strong> Donghyun Son, Euntae Choi, Sungjoo Yoo</p>
<p><strong>Abstract:</strong> arXiv:2505.18231v1 Announce Type: new  Abstract: Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\times$ throughput gain over full-precision baselines.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2505.18232" rel="nofollow">ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-elder-getting-efficient-llms-through-data-driven-regularized-layer-wise-pruning-" class="anchor" aria-label="Permalink: 8. ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning" href="#8-elder-getting-efficient-llms-through-data-driven-regularized-layer-wise-pruning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18232
<strong>Authors:</strong> Mingkuan Feng, Jinyang Wu, Siyuan Liu, Shuai Zhang, Hongjian Fang, Ruihan Jin, Feihu Che, Pengpeng Shao, Zhengqi Wen, Jianhua Tao</p>
<p><strong>Abstract:</strong> arXiv:2505.18232v1 Announce Type: new  Abstract: The deployment of Large language models (LLMs) in many fields is largely hindered by their high computational and memory costs. Recent studies suggest that LLMs exhibit sparsity, which can be used for pruning. Previous pruning methods typically follow a prune-then-finetune paradigm. Since the pruned parts still contain valuable information, statically removing them without updating the remaining parameters often results in irreversible performance degradation, requiring costly recovery fine-tuning (RFT) to maintain performance. To address this, we propose a novel paradigm: first apply regularization, then prune. Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning. We multiply the output of each transformer layer by an initial weight, then we iteratively learn the weights of each transformer layer by using a small amount of data in a simple way. After that, we apply regularization to the difference between the output and input of the layers with smaller weights, forcing the information to be transferred to the remaining layers. Compared with direct pruning, ELDeR reduces the information loss caused by direct parameter removal, thus better preserving the model's language modeling ability. Experimental results show that ELDeR achieves superior performance compared with powerful layer-wise structured pruning methods, while greatly reducing RFT computational costs. Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect is obvious, making it a promising technique for efficient LLMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2505.18166" rel="nofollow">Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-constrained-edge-ai-deployment-fine-tuning-vs-distillation-for-llm-compression-" class="anchor" aria-label="Permalink: 9. Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression" href="#9-constrained-edge-ai-deployment-fine-tuning-vs-distillation-for-llm-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18166
<strong>Authors:</strong> Jacob Sander, David Moe, Achraf Cohen, Brent Venable, Venkat Dasari, Brian Jalaian</p>
<p><strong>Abstract:</strong> arXiv:2505.18166v1 Announce Type: new  Abstract: Modern foundational models are often compressed via a combination of structured pruning and re-training to meet the strict compute, memory, and connectivity constraints of edge deployments. While state-of-the-art pruning schemes target the entire Transformer, we adopt a simple, layer-wise L2-norm pruning on only the MLP blocks as a fixed baseline. Our focus is not on achieving maximal compression, but on isolating the impact of the re-training loss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires labeled data, versus (ii) Self-Distillation with KL-divergence, which leverages only teacher logits (no labels) (L2PSD). We evaluate both pipelines on the OLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied connectivity scenarios typical of edge networks. Under identical pruning schedules, KL-based distillation matches or exceeds CE fine-tuning in test accuracy, demonstrating that, even with a basic MLP-only pruning, the choice of loss function materially affects compressed model recovery in resource-constrained environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2505.18413" rel="nofollow">LatentLLM: Attention-Aware Joint Tensor Compression</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-latentllm-attention-aware-joint-tensor-compression-" class="anchor" aria-label="Permalink: 10. LatentLLM: Attention-Aware Joint Tensor Compression" href="#10-latentllm-attention-aware-joint-tensor-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18413
<strong>Authors:</strong> Toshiaki Koike-Akino (Perry), Xiangyu Chen (Perry), Jing Liu (Perry), Ye Wang (Perry), Pu (Perry), Wang, Matthew Brand</p>
<p><strong>Abstract:</strong> arXiv:2505.18413v1 Announce Type: new  Abstract: Modern foundation models such as large language models (LLMs) and large multi-modal models (LMMs) require a massive amount of computational and memory resources. We propose a new framework to convert such LLMs/LMMs into a reduced-dimension latent structure. Our method extends a local activation-aware tensor decomposition to a global attention-aware joint tensor de-composition. Our framework can significantly improve the model accuracy over the existing model compression methods when reducing the latent dimension to realize computationally/memory-efficient LLMs/LLMs. We show the benefit on several benchmark including multi-modal reasoning tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2505.19235" rel="nofollow">CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-corematching-a-co-adaptive-sparse-inference-framework-with-token-and-neuron-pruning-for-comprehensive-acceleration-of-vision-language-models-" class="anchor" aria-label="Permalink: 11. CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models" href="#11-corematching-a-co-adaptive-sparse-inference-framework-with-token-and-neuron-pruning-for-comprehensive-acceleration-of-vision-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.19235
<strong>Authors:</strong> Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen</p>
<p><strong>Abstract:</strong> arXiv:2505.19235v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at <a href="https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main">https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2505.19342" rel="nofollow">Communication-Efficient Multi-Device Inference Acceleration for Transformer Models</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-communication-efficient-multi-device-inference-acceleration-for-transformer-models-" class="anchor" aria-label="Permalink: 12. Communication-Efficient Multi-Device Inference Acceleration for Transformer Models" href="#12-communication-efficient-multi-device-inference-acceleration-for-transformer-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.19342
<strong>Authors:</strong> Xiao Liu, Lijun Zhang, Deepak Ganesan, Hui Guan</p>
<p><strong>Abstract:</strong> arXiv:2505.19342v1 Announce Type: new  Abstract: Transformer models power many AI applications but suffer from high inference latency, limiting their use in real-time settings. Multi-device inference can reduce latency by parallelizing computation. Yet, existing methods require high inter-device bandwidth, making them impractical for bandwidth-constrained environments. We propose ASTRA, a communication-efficient framework that accelerates Transformer inference through a novel integration of sequence parallelism and a Mixed-Precision Attention mechanism designed to minimize inter-device communication. ASTRA compresses non-local token embeddings via vector quantization and preserves task accuracy through two optimizations, Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X speedups over single-device inference and up to 15.25X speedups over state-of-the-art multi-device inferences, while operating under bandwidths as low as 10 Mbps. ASTRA is open-sourced at <a href="https://github.com/xl1990/Astra">https://github.com/xl1990/Astra</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2505.18724" rel="nofollow">LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-lota-qaf-lossless-ternary-adaptation-for-quantization-aware-fine-tuning-" class="anchor" aria-label="Permalink: 13. LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning" href="#13-lota-qaf-lossless-ternary-adaptation-for-quantization-aware-fine-tuning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18724
<strong>Authors:</strong> Junyu Chen, Junzhuo Li, Zhen Peng, Wenjie Wang, Yuxiang Ren, Long Shi, Xuming Hu</p>
<p><strong>Abstract:</strong> arXiv:2505.18724v1 Announce Type: new  Abstract: Quantization and fine-tuning are crucial for deploying large language models (LLMs) on resource-constrained edge devices. However, fine-tuning quantized models presents significant challenges, primarily stemming from: First, the mismatch in data types between the low-precision quantized weights (e.g., 4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch limits the computational efficiency advantage offered by quantized weights during inference. Second, potential accuracy degradation when merging these high-precision adaptation weights into the low-precision quantized weights, as the adaptation weights often necessitate approximation or truncation. Third, as far as we know, no existing methods support the lossless merging of adaptation while adjusting all quantized weights. To address these challenges, we introduce lossless ternary adaptation for quantization-aware fine-tuning (LoTA-QAF). This is a novel fine-tuning method specifically designed for quantized LLMs, enabling the lossless merging of ternary adaptation weights into quantized weights and the adjustment of all quantized weights. LoTA-QAF operates through a combination of: i) A custom-designed ternary adaptation (TA) that aligns ternary weights with the quantization grid and uses these ternary weights to adjust quantized weights. ii) A TA-based mechanism that enables the lossless merging of adaptation weights. iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and Qwen-2.5 model families and validate its effectiveness on several downstream tasks. On the MMLU benchmark, our method effectively recovers performance for quantized models, surpassing 16-bit LoRA by up to 5.14%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2505.18164" rel="nofollow">Model-Distributed Inference for Large Language Models at the Edge</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-model-distributed-inference-for-large-language-models-at-the-edge-" class="anchor" aria-label="Permalink: 14. Model-Distributed Inference for Large Language Models at the Edge" href="#14-model-distributed-inference-for-large-language-models-at-the-edge-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.18164
<strong>Authors:</strong> Davide Macario, Hulya Seferoglu, Erdem Koyuncu</p>
<p><strong>Abstract:</strong> arXiv:2505.18164v1 Announce Type: new  Abstract: We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM), a novel framework designed to facilitate the deployment of state-of-the-art large-language models (LLMs) across low-power devices at the edge. This is accomplished by dividing the model into multiple partitions, which are then assigned to different devices/nodes within the network. These nodes exchange intermediate activation vectors via device-to-device links, enabling collaborative computation. To enhance the efficiency of this process, we propose the "recurrent pipeline parallelism" technique, which reduces idle time on each device and facilitates parallel inference during the generation of multiple text sequences. By leveraging the combined computational resources of multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the memory capacity of individual devices, making it possible to perform inference on low-cost hardware. Furthermore, as the number of participating devices increases, MDI-LLM boosts token generation throughput and reduces memory consumption per device.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2505.19602" rel="nofollow">Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-memory-efficient-visual-autoregressive-modeling-with-scale-aware-kv-cache-compression-" class="anchor" aria-label="Permalink: 15. Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression" href="#15-memory-efficient-visual-autoregressive-modeling-with-scale-aware-kv-cache-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.19602
<strong>Authors:</strong> Kunjun Li, Zigeng Chen, Cheng-Yen Yang, Jenq-Neng Hwang</p>
<p><strong>Abstract:</strong> arXiv:2505.19602v1 Announce Type: new  Abstract: Visual Autoregressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction approach, which yields substantial improvements in efficiency, scalability, and zero-shot generalization. Nevertheless, the coarse-to-fine methodology inherent in VAR results in exponential growth of the KV cache during inference, causing considerable memory consumption and computational redundancy. To address these bottlenecks, we introduce ScaleKV, a novel KV cache compression framework tailored for VAR architectures. ScaleKV leverages two critical observations: varying cache demands across transformer layers and distinct attention patterns at different scales. Based on these insights, ScaleKV categorizes transformer layers into two functional groups: drafters and refiners. Drafters exhibit dispersed attention across multiple scales, thereby requiring greater cache capacity. Conversely, refiners focus attention on the current token map to process local details, consequently necessitating substantially reduced cache capacity. ScaleKV optimizes the multi-scale inference pipeline by identifying scale-specific drafters and refiners, facilitating differentiated cache management tailored to each scale. Evaluation on the state-of-the-art text-to-image VAR model family, Infinity, demonstrates that our approach effectively reduces the required KV cache memory to 10% while preserving pixel-level fidelity.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2505.19115" rel="nofollow">FP4 All the Way: Fully Quantized Training of LLMs</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-fp4-all-the-way-fully-quantized-training-of-llms-" class="anchor" aria-label="Permalink: 16. FP4 All the Way: Fully Quantized Training of LLMs" href="#16-fp4-all-the-way-fully-quantized-training-of-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.19115
<strong>Authors:</strong> Brian Chmiel, Maxim Fishman, Ron Banner, Daniel Soudry</p>
<p><strong>Abstract:</strong> arXiv:2505.19115v1 Announce Type: new  Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in <a href="https://github.com/Anonymous1252022/fp4-all-the-way">https://github.com/Anonymous1252022/fp4-all-the-way</a> .</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2505.20132" rel="nofollow">Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-tensorization-is-a-powerful-but-underexplored-tool-for-compression-and-interpretability-of-neural-networks-" class="anchor" aria-label="Permalink: 17. Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks" href="#17-tensorization-is-a-powerful-but-underexplored-tool-for-compression-and-interpretability-of-neural-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2505.20132
<strong>Authors:</strong> Safa Hamreras, Sukhbinder Singh, Rom'an Or'us</p>
<p><strong>Abstract:</strong> arXiv:2505.20132v1 Announce Type: new  Abstract: Tensorizing a neural network involves reshaping some or all of its dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions. This technique has shown promise as a model compression strategy for large-scale neural networks. However, despite encouraging empirical results, tensorized neural networks (TNNs) remain underutilized in mainstream deep learning. In this position paper, we offer a perspective on both the potential and current limitations of TNNs. We argue that TNNs represent a powerful yet underexplored framework for deep learning--one that deserves greater attention from both engineering and theoretical communities. Beyond compression, we highlight the value of TNNs as a flexible class of architectures with distinctive scaling properties and increased interpretability. A central feature of TNNs is the presence of bond indices, which introduce new latent spaces not found in conventional networks. These internal representations may provide deeper insight into the evolution of features across layers, potentially advancing the goals of mechanistic interpretability. We conclude by outlining several key research directions aimed at overcoming the practical barriers to scaling and adopting TNNs in modern deep learning workflows.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>