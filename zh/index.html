<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">LMCache：用于大规模LLM推理的高效KV缓存层</h2><a id="user-content-lmcache用于大规模llm推理的高效kv缓存层" class="anchor" aria-label="Permalink: LMCache：用于大规模LLM推理的高效KV缓存层" href="#lmcache用于大规模llm推理的高效kv缓存层"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09665v1宣布类型：新摘要：为了简单起见，当今的LLM推理系统独立处理各个引擎和查询，但这会导致资源效率严重低下。虽然有人建议通过在查询中重复使用KV缓存来避免冗余计算，并通过将单个查询分解到不同的引擎来提高图形处理器的利用率，但如果不在LLM推理引擎和查询中高效卸载和通信KV缓存，就无法实现他们的承诺。   我们介绍了LMCache，这是第一个也是迄今为止最有效的开源KV缓存解决方案，它提取和存储由现代LLM引擎（vLLM和SGLang）生成的KV缓存，并在引擎和查询之间共享KV缓存。LMCache在LLM引擎接口中公开了KV缓存，有效地将LLM引擎从单个令牌处理器转变为以KV缓存作为存储和通信媒体的引擎集合。特别是，它支持高速缓存卸载（跨查询的前置码重复使用）和预填充解码分解（跨引擎高速缓存传输）。LMCache的高性能和广泛采用源于以下贡献：高度优化的KV缓存数据移动，并进行了性能优化，包括批量数据移动操作、计算和I/O流水线;模块化的KV缓存连接器组件，将LMCache与快速发展的推理引擎脱钩;一流的控制API，例如固定、查找、清理、移动和压缩，用于跨图形处理器、中央处理器、存储和网络层灵活的缓存编排。评估表明，将LMCache与vLLM结合起来，在不同工作负载上的吞吐量可提高高达15倍。随着社区的不断壮大，LMCache被企业推理系统的采用率急剧增长，这为未来的KV缓存解决方案提供了宝贵的经验教训。LMCache的源代码位于：<a href="https://github.com/LMCache/LMCache%E3%80%82">https://github.com/LMCache/LMCache。</a></p>
<div class="markdown-heading"><h2 class="heading-element">Meta骆驼模型的演变和大型语言模型的参数高效微调：调查</h2><a id="user-content-meta骆驼模型的演变和大型语言模型的参数高效微调调查" class="anchor" aria-label="Permalink: Meta骆驼模型的演变和大型语言模型的参数高效微调：调查" href="#meta骆驼模型的演变和大型语言模型的参数高效微调调查"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12178v1宣布类型：新摘要：本评论调查了Meta AI的LLaMA（大型语言模型Meta AI）系列的快速演变-从LLaMA 1到LLaMA 4以及为这些模型开发的专业参数高效微调（PEFT）方法。我们首先描述LLaMA系列基础模型（7 B-65 B至288 B参数）、其架构（包括原生多模式和Mixtureof-Experts变体）以及关键性能特征。然后，我们描述和讨论PEFT的概念，它通过仅更新参数的一小部分来适应大型预训练模型，并回顾了已应用于LLaMA的五种PEFT方法：LoRA（低等级自适应）、LLaMA-Adaptor V1和V2、LLaMA-Excitor和QLoRA（量化LoRA）。我们讨论每种方法的机制、参数节省以及LLaMA的示例应用（例如，指令调优、多模式任务）。我们提供对模型和适配器架构、参数计数和基准结果的结构化讨论和分析（包括微调的LLaMA模型优于更大基线的示例）。最后，我们研究了成功应用基于LLaMA的模型和PEFT的现实世界用例（例如，法律和医学领域），我们讨论了持续的挑战和未来的研究方向（例如扩展到更大的环境和提高稳健性）。这篇调查论文为对LLaMA模型和高效微调策略感兴趣的ML研究人员和从业者提供了一站式资源。</p>
<div class="markdown-heading"><h2 class="heading-element">RAG-Anything：一体化RAG框架</h2><a id="user-content-rag-anything一体化rag框架" class="anchor" aria-label="Permalink: RAG-Anything：一体化RAG框架" href="#rag-anything一体化rag框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12323v1宣布类型：新摘要：检索增强生成（RAG）已成为扩展大型语言模型超越静态训练限制的基本范式。然而，当前的RAG能力与现实世界的信息环境之间存在严重的不一致。现代知识库本质上是多模式的，包含文本内容、视觉元素、结构化表格和数学表达的丰富组合。然而，现有的RAG框架仅限于文本内容，在处理多模式文档时造成了根本性的差距。我们提出了RAG-Anything，这是一个统一的框架，可以实现跨所有模式的全面知识检索。我们的方法将多模式内容重新概念化为相互关联的知识实体，而不是孤立的数据类型。该框架引入了双图结构，以在统一的表示中捕获跨模式关系和文本语义。我们开发了将结构知识导航与语义匹配相结合的跨模式混合检索。这使得对相关证据跨越多种模式的异类内容进行有效推理。RAG-Anything在具有挑战性的多模式基准测试上表现出卓越的性能，比最先进的方法实现了显着改进。在传统方法失败的长文档中，性能提升变得尤其明显。我们的框架为多模式知识访问建立了一个新的范式，消除了限制当前系统的架构碎片。我们的框架是开源的：<a href="https://github.com/HKUDS/RAG-Anything%E3%80%82">https://github.com/HKUDS/RAG-Anything。</a></p>
<div class="markdown-heading"><h2 class="heading-element">并非所有位都是相等的：推理模型的规模相关内存优化策略</h2><a id="user-content-并非所有位都是相等的推理模型的规模相关内存优化策略" class="anchor" aria-label="Permalink: 并非所有位都是相等的：推理模型的规模相关内存优化策略" href="#并非所有位都是相等的推理模型的规模相关内存优化策略"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10964v1宣布类型：新摘要：虽然4位量化已成为跨规模非推理模型和零触发任务的内存最佳选择，但我们表明，这种通用规定对于推理模型来说是失败的，因为在推理模型中，KV缓存而不是模型大小可以主导内存。通过在AIM 25和GPQA-Diamond上对1，700个推理场景进行系统实验，我们发现了一个与规模相关的权衡：有效大小低于8位4 B参数的模型通过将内存分配给更多权重而不是更长的代来实现更好的准确性，而更大的模型通过将内存分配给更长的代来实现更好的准确性。此缩放阈值还确定并行缩放何时变得具有内存效率，以及KV缓存驱逐是否优于KV量化。我们的研究结果表明，LLM的内存优化不能与规模无关，同时提供了原则性的指导方针：对于小型推理模型，优先考虑模型容量而不是测试时计算，而对于大型推理模型，最大限度地提高测试时计算。我们的结果表明，优化推理模型以进行部署需要与为非推理模型建立的策略有根本不同的策略。</p>
<div class="markdown-heading"><h2 class="heading-element">用于时空交通预测的视觉LLM</h2><a id="user-content-用于时空交通预测的视觉llm" class="anchor" aria-label="Permalink: 用于时空交通预测的视觉LLM" href="#用于时空交通预测的视觉llm"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11282v1宣布类型：新摘要：准确的时空流量预测是密集城市移动网络中主动资源管理的重要前提。虽然大型语言模型（LLM）在时间序列分析中表现出了希望，但它们本质上很难对基于网格的交通数据的复杂空间依赖性进行建模。有效地将LLM扩展到这个领域是具有挑战性的，因为表示来自密集地理网格的大量信息可能是低效的，并且会淹没模型的上下文。为了解决这些挑战，我们提出了ST-视觉-LLM，一种新的框架，将时空预测重新定义为视觉语言融合问题。我们的方法利用Vision-LLM视觉编码器将历史全球流量矩阵处理为图像序列，为模型提供全面的全球视图，为单元级预测提供信息。为了克服LLM在处理数字数据方面的效率低下，我们引入了一种高效的编码方案，该方案通过专门的词汇表将浮点值表示为单个令牌，并结合两阶段数字对齐微调过程。该模型首先使用监督微调（SFT）进行训练，然后使用组相对政策优化（GRPO）（一种内存高效的强化学习方法）进一步优化预测准确性。对现实世界移动交通数据集的评估表明，ST-Vision-LLM在长期预测准确性方面比现有方法高出15.6%，在跨域少镜头场景中比第二佳基线高出30.04%以上。我们广泛的实验验证了该模型在各种数据稀缺环境中的强大概括能力。</p>
<div class="markdown-heading"><h2 class="heading-element">重新缩放感知培训，在全负载硬件上高效部署深度学习模型</h2><a id="user-content-重新缩放感知培训在全负载硬件上高效部署深度学习模型" class="anchor" aria-label="Permalink: 重新缩放感知培训，在全负载硬件上高效部署深度学习模型" href="#重新缩放感知培训在全负载硬件上高效部署深度学习模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11484v1宣布类型：新摘要：RST AI推理显着降低了嵌入式系统中的计算复杂性。量化感知训练（QAT）有助于减轻与训练后量化相关的准确性下降，但仍然忽略了推理期间整元重新缩放的影响，这是纯整人工智能推理中一项硬件成本高昂的操作。这项工作表明，通过对重新缩放的被乘数应用更强的量化，在不损失模型质量的情况下，可以在训练后显着降低重新缩放的成本。此外，我们还引入了重新缩放感知训练，这是一种用于超低比特宽度重新缩放被乘数的微调方法。实验表明，即使重新缩放器宽度减少了8倍，也可以通过最小限度的增量再训练保持完全准确性。这使得资源受限的嵌入式系统能够实现更节能和更具成本效益的人工智能推理。</p>
<div class="markdown-heading"><h2 class="heading-element">LouisKV：针对长输入输出序列的高效KV缓存检索</h2><a id="user-content-louiskv针对长输入输出序列的高效kv缓存检索" class="anchor" aria-label="Permalink: LouisKV：针对长输入输出序列的高效KV缓存检索" href="#louiskv针对长输入输出序列的高效kv缓存检索"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11292v1宣布类型：新摘要：虽然Key-Value（KV）缓存成功地减少了自回归模型中的冗余计算，但它引入了大量的内存负担，限制了其在长序列场景中的实际部署。现有的KV检索方法通过在图形处理器上仅动态保留KV条目的子集来缓解这一问题。然而，由于按令牌检索和粗粒度的页面级KN管理，它们仍然面临显着的效率和准确性瓶颈，特别是在长输出推理场景中。随着大型推理模型的出现，有效处理此类场景变得越来越重要。为了解决这个问题，我们提出了两个关键观察结果：（1）关键KN在解码期间表现出很强的时间局部性，以及（2）这些KN在输入提示和生成的输出中表现出不同的分布模式。在这些观察的基础上，我们提出了LouisKV，这是一个针对各种长序列场景设计的高效的KV缓存检索框架。具体来说，LouisGV引入了一种语义感知检索策略，利用时间局部性仅在语义边界触发检索，从而大幅减少了计算和数据传输负担。LouisKN还设计了一个脱钩的细粒度管理方案，为输入和输出序列量身定制差异化的策略，以创建更好地匹配模型注意力模式的检索单元，从而能够精确识别关键KN。此外，为了提高效率，LouisKN结合了多项核心级优化，包括自定义Triton和CUDA核心，以加速GV集群和检索。评估表明，LouisKV比最先进的KV检索方法实现了高达4.7 $\times $的加速，同时在各种长序列任务（包括长输入短输出、短输入长输出和长输入长输出场景）中保持近乎无损的准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">PermLLM：N：M稀疏大型语言模型的可学习通道排列</h2><a id="user-content-permllmnm稀疏大型语言模型的可学习通道排列" class="anchor" aria-label="Permalink: PermLLM：N：M稀疏大型语言模型的可学习通道排列" href="#permllmnm稀疏大型语言模型的可学习通道排列"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10136v1宣布类型：新摘要：通道置换是一种强大的技术，可以通过重新排序权重矩阵的通道以优先考虑重要权重的保留来提高N：M稀疏模型的准确性。然而，传统的通道置换方法依赖于手工制作的质量指标，这通常无法准确捕捉修剪对模型性能的真正影响。为了解决这一限制，我们提出了PermLLM，这是一种新型的训练后修剪框架，它引入了针对N：M稀疏性的可学习通道置换（LCP）。LCP利用Sinkhorn正规化将离散排列矩阵转换为可微软排列矩阵，实现端到端优化。此外，PermLLM还结合了高效的逐块通道置换策略，可以显着减少可学习参数的数量和计算复杂性。PermLLM与现有的一次性修剪方法无缝集成，以自适应地优化通道排列，有效地减轻修剪引起的错误。对LLaMA系列、Qwen和OPT模型的大量实验表明，PermLLM在优化N：M稀疏模型方面实现了卓越的性能。该代码可在<a href="https://github.com/lanchengzou/PermLLM%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lanchengzou/PermLLM上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">Cache：通过有效的KV缓存重用加速RAG</h2><a id="user-content-cache通过有效的kv缓存重用加速rag" class="anchor" aria-label="Permalink: Cache：通过有效的KV缓存重用加速RAG" href="#cache通过有效的kv缓存重用加速rag"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10129v1宣布类型：新摘要：由于输入序列长，检索增强生成（RAG）系统面临严重的首个令牌时间（TTFT）瓶颈。现有的KV缓存重用方法面临着一个基本的权衡：前置缓存需要相同的前置码，而这在RAG场景中很少出现，而直接预计算则由于丢失块间注意力和重复的注意力下沉而牺牲了质量。最近的方法，例如MBE和Cache Blend，部分解决了这些问题，但仍然不足以满足强大的RAG应用程序。本文介绍了Cache，这是一种新颖的框架，可以实现快速TTFT和高生成质量。我们的主要见解是，小型辅助LLM表现出与主要LLM（生成的目标模型）类似的最后一层注意力分布，能够高效识别对于恢复块间注意力至关重要的令牌，从而显着提高跨块推理任务的响应质量。Cache集成了三种技术：（1）辅助模型引导的令牌选择，用于选择性的KV缓存重新计算，其中辅助模型经过微调以提高选择准确性，（2）共享前置码以消除多余的注意力汇，（3）分组策略以在部分KV缓存更新期间保持本地一致性。实验表明，Cache在NIAH和LongBench上保留了高达94.8%和85.0%的全注意力表现，在NIAH上的表现优于BEP和Cache Blend 25.2%和35.1%（reomp% = 20%）。与此同时，Cache将LLM推断的预填充时间提高了1.92倍，为RAG系统中的效率与质量权衡提供了实用的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">PatentVision：起草专利申请的多模式方法</h2><a id="user-content-patentvision起草专利申请的多模式方法" class="anchor" aria-label="Permalink: PatentVision：起草专利申请的多模式方法" href="#patentvision起草专利申请的多模式方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09762v1宣布类型：新摘要：由于需要详细的技术描述、法律合规性和视觉元素，专利起草很复杂。尽管大视觉语言模型（LVLM）在各种任务中表现出了前景，但其在专利撰写自动化方面的应用仍然没有得到充分的探索。在本文中，我们介绍了PatentVision，这是一个多模式框架，它集成了专利权利要求和图纸等文本和视觉输入，以生成完整的专利规范。PatentVision建立在先进的LVLM之上，通过将微调的视觉语言模型与针对专利量身定制的领域特定训练相结合来提高准确性。实验表明，它超越了纯文本方法，产生的输出具有更高的保真度并与人类书面标准保持一致。它整合的视觉数据使其能够更好地表现复杂的设计特征和功能连接，从而获得更丰富、更精确的结果。这项研究强调了多模式技术在专利自动化中的价值，提供了一种可扩展的工具来减少手动工作量并提高一致性。PatentVision不仅推进了专利起草，还为LVLM在专门领域的更广泛使用奠定了基础，从而可能改变知识产权管理和创新流程。</p>
<div class="markdown-heading"><h2 class="heading-element">通过LLM增强优化在支持无人机的低空经济网络中进行高效的机载视觉语言推理</h2><a id="user-content-通过llm增强优化在支持无人机的低空经济网络中进行高效的机载视觉语言推理" class="anchor" aria-label="Permalink: 通过LLM增强优化在支持无人机的低空经济网络中进行高效的机载视觉语言推理" href="#通过llm增强优化在支持无人机的低空经济网络中进行高效的机载视觉语言推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10028v1宣布类型：新摘要：低空经济网络（LAENets）的快速发展使各种应用成为可能，包括空中监视、环境传感和语义数据收集。为了支持这些场景，配备机载视觉语言模型（VLM）的无人机（UF）为实时多模式推理提供了一种有前途的解决方案。然而，由于有限的机载资源和动态网络条件，确保推理准确性和通信效率仍然是一个重大挑战。在本文中，我们首先提出了一个无人机启用LAENet系统模型，共同捕捉无人机的移动性，用户-无人机通信，以及机载视觉问答（VQA）管道。基于此模型，我们制定了一个混合整数非凸优化问题，以最大限度地减少用户特定的精度约束下的任务延迟和功耗。为了解决这个问题，我们设计了一个分层优化框架，包括两个部分：（i）一个交替分辨率和功率优化（ARPO）算法的资源分配精度约束下，和（ii）一个大语言模型增强的强化学习方法（LLaRA）的自适应无人机轨迹优化。大型语言模型（LLM）是以离线方式完善强化学习奖励设计的专家，不会在实时决策中引入额外的延迟。数值结果证明了我们提出的框架在动态LAENet条件下提高推理性能和通信效率方面的功效。</p>
<div class="markdown-heading"><h2 class="heading-element">空间强迫：视觉-语言-动作模型的隐式空间表示对齐</h2><a id="user-content-空间强迫视觉-语言-动作模型的隐式空间表示对齐" class="anchor" aria-label="Permalink: 空间强迫：视觉-语言-动作模型的隐式空间表示对齐" href="#空间强迫视觉-语言-动作模型的隐式空间表示对齐"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12276v1宣布类型：新摘要：视觉-语言-动作（VLA）模型最近在使机器人遵循语言指令并执行精确动作方面显示出强大的潜力。然而，大多数VGA都建立在仅在2D数据上预先训练的视觉语言模型之上，这缺乏准确的空间感知并阻碍了它们在3D物理世界中操作的能力。现有的解决方案试图合并显式的3D传感器输入，例如深度图或点云，但由于传感器噪音、硬件异类和现有数据集中不完整的深度覆盖，这些方法面临挑战。从2D图像估计3D线索的替代方法也受到深度估计器性能有限的影响。我们提出空间强迫（SF），这是一种简单而有效的对齐策略，它隐式地迫使VLA模型开发空间理解能力，而不依赖于显式的3D输入或深度估计器。SF将VLA的中间视觉嵌入与预训练的3D基础模型生成的几何表示相匹配。通过在中间层强制对齐，SF引导VGA编码更丰富的空间表示，以提高动作精度。模拟和现实环境中的大量实验表明，SF实现了最先进的结果，超越了基于2D和3D的VGA。SF将训练速度进一步提高3.8倍，并提高各种机器人任务的数据效率。项目页面位于<a href="https://spatial-forcing.github.io/" rel="nofollow">https://spatial-forcing.github.io/</a></p>
<div class="markdown-heading"><h2 class="heading-element">ERA：通过预定先验学习和在线强化学习将VLM转换为预定代理</h2><a id="user-content-era通过预定先验学习和在线强化学习将vlm转换为预定代理" class="anchor" aria-label="Permalink: ERA：通过预定先验学习和在线强化学习将VLM转换为预定代理" href="#era通过预定先验学习和在线强化学习将vlm转换为预定代理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12693v1宣布类型：新摘要：嵌入式人工智能的最新进展凸显了视觉语言模型（VLM）作为能够在复杂环境中进行感知、推理和交互的代理的潜力。然而，顶级性能的系统依赖于部署成本高昂的大规模模型，而较小的VLM缺乏成功所需的知识和技能。为了弥合这一差距，我们提出了\textit{proxed Reasoning Agent（ERA）}，这是一个两阶段框架，集成了先验知识学习和在线强化学习（RL）。第一阶段，\textit{propried Prior Learning}从三种类型的数据中提取基础知识：（1）轨迹增强先验，通过更强大的模型生成的结构化推理来丰富现有轨迹数据;（2）环境锚定先验，提供环境内知识和基础监督;（3）外部知识先验，从环境外数据集传输一般知识。在第二阶段，我们开发一个在线RL管道，该管道建立在这些先验之上，以进一步提高代理性能。为了克服代理RL中固有的挑战，包括长视野、稀疏奖励和训练不稳定性，我们引入了三个关键设计：上下文管理的自我总结、密集奖励塑造和回合级策略优化。高层规划（EB-ALFRED）和低级控制（EB-Manipulation）任务的大量实验表明，ERA-3B超越了基于预算的大型模型和之前基于训练的基线。具体来说，与GPT-4 o相比，它在EB-ALFRED上实现了8.4%的总体改进，在EB-Manipulation上实现了19.4%的总体改进，并且对未见任务表现出了很强的概括性。总体而言，ERA提供了一条通往可扩展的具体智能的实用途径，为未来的具体人工智能系统提供方法论见解。</p>
<div class="markdown-heading"><h2 class="heading-element">带宽高效边缘云推测解码的共形稀疏化</h2><a id="user-content-带宽高效边缘云推测解码的共形稀疏化" class="anchor" aria-label="Permalink: 带宽高效边缘云推测解码的共形稀疏化" href="#带宽高效边缘云推测解码的共形稀疏化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09942v1宣布类型：新摘要：边缘云推测解码（SD）通过具有基于云的大型语言模型（LLM）来加速推理，该模型验证由边缘资源受限的小型语言模型（LAM）生成的草稿令牌。一个核心瓶颈是边缘云链接的带宽有限，这需要对草稿令牌分发进行高效压缩。我们首先推导出一个信息论界限，该界限将令牌拒绝率分解为来自SL M-LLM分布不匹配和量化失真的贡献。在此分析的指导下，我们提出了稀疏量化和样本SD（SQS-SD）框架，该框架通过结构化稀疏化和基于格的量化来利用分布稀疏性。在此框架内，K-SQS应用固定的top-K截断，而C-SQS通过在线保形预测自适应地调整保留的令牌集，以确保与密集分布的有界偏差。经验结果证实，这两种方法都改善了互补操作机制中的端到端延迟和拒绝率。</p>
<div class="markdown-heading"><h2 class="heading-element">AnyBCQ：用于多精度LLM的硬件高效灵活二进制编码量化</h2><a id="user-content-anybcq用于多精度llm的硬件高效灵活二进制编码量化" class="anchor" aria-label="Permalink: AnyBCQ：用于多精度LLM的硬件高效灵活二进制编码量化" href="#anybcq用于多精度llm的硬件高效灵活二进制编码量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10467v1宣布类型：新摘要：大型语言模型（LLM）的部署越来越受到内存和延迟瓶颈的限制，这促使人们需要灵活平衡准确性和效率的量化技术。最近的工作引入了多精度模型，可以根据运行时约束在单个模型内以多个精度进行推理。为了支持这种灵活性，量化权重通常存储为位平面，当计算直接在位平面级别操作并且仅激活每个请求所需的精度时，硬件效率会提高。在这项工作中，我们介绍了AnyBCQ，这是二进制编码量化（BCQ）的硬件友好多精度扩展，支持直接位平面操作。通过将权重表示为具有相应比例因子的二进制位平面，AnyBCQ实现了位平面级计算，并自然地映射到加速器友好的位并行算法。我们的渐进精度扩展机制在重复使用之前分配的二进制代码的同时逐步细化缩放因子，从而随着启用额外的位而产生准确性的单调改进。我们进一步共同设计了一个专门的内核，该内核利用BCQ结构来支持动态按请求精确选择，而额外的费用可以忽略不计。最近LLM的实验表明，AnyBCQ显着缩小了低位机制（例如2位）中的准确度下降，在更高精度下保持竞争力，并在半精度上实现了高达3.0倍的吞吐量增长，在最先进的多精度方法上实现了1.2倍的吞吐量增长。通过将算法灵活性与硬件效率结合起来，AnyBCQ为跨不同服务级别目标的多精度LLM部署提供了实用基础。</p>
<div class="markdown-heading"><h2 class="heading-element">视觉-语言-动作模型流匹配策略的强化微调</h2><a id="user-content-视觉-语言-动作模型流匹配策略的强化微调" class="anchor" aria-label="Permalink: 视觉-语言-动作模型流匹配策略的强化微调" href="#视觉-语言-动作模型流匹配策略的强化微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09976v1宣布类型：新摘要：OpenVLA、Octo和$\pi_0 $等视觉-语言-动作（VLA）模型通过利用大规模演示显示出很强的概括性，但它们的性能仍然从根本上受到监督数据的质量和覆盖范围的限制。强化学习（RL）为通过在线交互改进和微调VLA提供了一条有希望的途径。然而，由于重要性抽样过程的棘手性，需要显式计算策略比率，传统的政策梯度方法在基于流匹配的模型的背景下在计算上是不可行的。为了克服这一限制，我们提出了流策略优化（FPO）算法，该算法通过利用条件流匹配目标中的每个样本变化来重新制定重要性抽样。此外，FPO通过集成结构感知信用分配以增强梯度效率、精简代理目标以稳定优化、鼓励多样化政策更新的多步潜在探索以及提供稳健价值估计的Q-系综机制，实现了$\pi_0 $模型的稳定且可扩展的在线强化微调。我们根据有监督的、偏好对齐的、基于扩散的、自回归的在线RL和$\pi_0 $-Fast基线，在LIBERO基准和ALOHA模拟任务上评估了FPO，观察到相对于先前的模仿和强替代方案的一致改进，在稀疏奖励下具有稳定学习。此外，对潜在空间动力学的消融研究和分析进一步强调了FPO中各个组件的贡献，验证了拟议计算模块的有效性以及在线RL期间条件流匹配目标的稳定收敛。</p>
<div class="markdown-heading"><h2 class="heading-element">分层LoRA MoE用于高效的TLR模型缩放</h2><a id="user-content-分层lora-moe用于高效的tlr模型缩放" class="anchor" aria-label="Permalink: 分层LoRA MoE用于高效的TLR模型缩放" href="#分层lora-moe用于高效的tlr模型缩放"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10432v1宣布类型：新摘要：深度模型推动了点击率（TLR）预测的重大进步。虽然通过层堆叠的垂直扩展提高了模型的表达能力，但逐层顺序计算对高效扩展提出了挑战。相反，通过混合专家（MoE）的水平扩展通过并行激活一小部分专家来实现高效扩展，但扁平MoE层可能难以捕捉推荐任务中固有的分层结构。为了突破投资回报（Investment）界限，我们探索了两个方向的互补优势，并提出HiLoMoE，这是一种分层LoRA MoE框架，可以以参数高效的方式实现整体扩展。具体来说，HiLoMoE采用轻量级排名1的专家来实现参数高效的水平扩展，并通过分层路由堆叠多个MoE层，以实现组合多样化的专家组合。与传统堆叠不同，HiLoMoE路由基于先前的层分数而不是输出，允许所有层并行执行。原则性的三阶段培训框架确保稳定的优化和专家多样性。对四个公共数据集的实验表明，HiLoMoE实现了更好的性能-效率权衡，与非MoE基线相比，实现了平均AUC0.20%的平均AUC0.20%和FLOP降低18.5%。</p>
<div class="markdown-heading"><h2 class="heading-element">ELMO：通过大输出空间中的低精度和峰值内存优化提高效率</h2><a id="user-content-elmo通过大输出空间中的低精度和峰值内存优化提高效率" class="anchor" aria-label="Permalink: ELMO：通过大输出空间中的低精度和峰值内存优化提高效率" href="#elmo通过大输出空间中的低精度和峰值内存优化提高效率"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11168v1宣布类型：新摘要：大输出空间，也称为极端多标签分类（XMC），是一种出现的设置，例如，在大规模标签和产品到产品推荐方面，其特点是标签数量从数十万到数百万不等。这意味着线性分类头（通常只占整个模型的一小部分）将成为计算和内存需求的主要驱动力。当前最先进的XMC方法主要依赖于FP 16-FP 32混合精度训练，我们表明这种训练可能不稳定，并且在内存使用和计算负担方面效率低下。与此同时，现有的低精度方法通常可以保留更高的分类层精度。在这项工作中，我们提出了ELMO，这是一个使用BFloat 16和Float 8数据类型的XMC模型的纯低精度训练框架。通过利用Kahan总和和随机舍入，我们证明XMC模型可以完全在Float 8中进行有效训练，而无需依赖单精度主权重或张量缩放。低精度训练，与我们提出的内存优化（梯度融合和分块）相结合，可以显着减少图形处理器内存的使用。例如，我们训练了一个拥有300万个标签的XMC模型，仅使用6.6 GiB的图形处理器，而优化的SOTA方法Renee所需的39.7 GiB却不影响准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">ADEPT：通过自适应扩展和动态脱钩调整进行连续预训练</h2><a id="user-content-adept通过自适应扩展和动态脱钩调整进行连续预训练" class="anchor" aria-label="Permalink: ADEPT：通过自适应扩展和动态脱钩调整进行连续预训练" href="#adept通过自适应扩展和动态脱钩调整进行连续预训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10071v1宣布类型：新摘要：用于大型语言模型（LLM）领域适应的传统连续预训练（CPD）经常遭受灾难性遗忘和有限的领域容量。现有策略采用分层扩展，引入额外的可训练参数以适应新知识。然而，统一的扩展和更新仍然纠缠在一般学习和领域学习之间，削弱了其有效性。我们的试点研究表明，LLM表现出功能专业化，其中层和单元有差异地编码一般关键能力，这表明参数扩展和优化应该具有功能感知性。然后，我们提出了ADEPT、自适应扩展和动态去耦合调优来进行连续预训练，这是一个领域自适应CPD的两阶段框架。ADEPT首先执行通用能力引导的选择性层扩展，复制对通用领域最不重要的层，以提高代表能力，同时最大限度地减少对通用知识的干扰。然后，它应用自适应单元式去耦合调优，根据扩展层中的参数单元的一般域重要性来解开参数单元，并分配不对称学习率以平衡知识注入和保留。数学和医学基准实验表明，ADEPT在一般领域比全参数CPD高出5.76%，在目标领域比全参数CPD高出5.58%，仅调整了15%的参数，训练时间少于50%。消融研究、理论分析和扩展研究进一步证明了有针对性的扩展和脱钩优化的必要性，为高效且稳健的领域自适应CPD提供了新的原则。我们的代码在<a href="https://github.com/PuppyKnightUniversity/ADEPT%E4%B8%8A%E5%BC%80%E6%BA%90">https://github.com/PuppyKnightUniversity/ADEPT上开源</a></p>
<div class="markdown-heading"><h2 class="heading-element">自我改进VLA的基于反思的任务适应</h2><a id="user-content-自我改进vla的基于反思的任务适应" class="anchor" aria-label="Permalink: 自我改进VLA的基于反思的任务适应" href="#自我改进vla的基于反思的任务适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12710v1宣布类型：新摘要：预训练的视觉-语言-动作（VLA）模型代表了通用机器人的重大飞跃，但有效地将它们适应新颖的、特定的现场任务仍然是一个重大障碍。虽然强化学习（RL）是此类适应的一种有希望的途径，但该过程往往效率低下，阻碍了任务的快速掌握。我们引入了反思性自适应，这是一个无需人类干预的快速、自主的任务适应框架。我们的框架建立了一个自我改进循环，代理从自己的经验中学习以增强策略和执行力。   我们框架的核心是解决整个适应生命周期的双路径架构。首先，失败驱动的反思RL途径通过使用VLM的因果推理来自动从失败分析中合成有针对性的密集奖励函数，从而实现快速学习。这提供了一个集中的学习信号，大大加快了政策探索。然而，优化这样的代理奖励会引入“奖励黑客”的潜在风险，即代理掌握了奖励功能，但未能完成实际任务。为了应对这一点，我们的第二条途径，以质量为导向的SFT，将政策建立在整体成功的基础上。它识别并有选择地模仿高质量的成功轨迹，确保智能体与最终任务目标保持一致。有条件的课程机制加强了这一途径，以帮助初步探索。   我们在具有挑战性的操纵任务中进行实验。结果表明，与代表性基线相比，我们的框架实现了更快的收敛和更高的最终成功率。我们的工作提供了一种强大的解决方案，用于创建能够有效、可靠地适应新环境的自我改进代理。</p>
<div class="markdown-heading"><h2 class="heading-element">消失的贡献：将神经模型平稳转换为压缩形式的统一方法</h2><a id="user-content-消失的贡献将神经模型平稳转换为压缩形式的统一方法" class="anchor" aria-label="Permalink: 消失的贡献：将神经模型平稳转换为压缩形式的统一方法" href="#消失的贡献将神经模型平稳转换为压缩形式的统一方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09696v1宣布类型：新摘要：深度神经网络规模的不断扩大导致对修剪、量化和低等级分解等压缩技术的需求不断增长。虽然这些方法在减少内存、计算和能源消耗方面非常有效，但直接应用时通常会导致严重的准确性下降。我们引入了消失贡献（VCON），这是一种将神经模型平稳过渡为压缩形式的通用方法。VCON不会直接用其压缩版本替换原始网络，而是在微调期间并行执行两者。原始（未压缩）模型的贡献逐渐减少，而压缩模型的贡献逐渐增加。这种平稳过渡使网络能够随着时间的推移进行调整，提高稳定性并减轻准确性下降。我们通过计算机视觉和自然语言处理基准并结合多种压缩策略来评估VCON。在所有场景中，VCON都带来了一致的改进：典型的收益超过3%，而某些配置的准确性提高了20%。因此，VCON提供了一种可推广的方法，可以应用于现有的压缩技术，并且有证据表明多个基准测试中的一致收益。</p>
<div class="markdown-heading"><h2 class="heading-element">ReLook：基于视觉的RL，具有针对大型Web编码的多模式LLM评论</h2><a id="user-content-relook基于视觉的rl具有针对大型web编码的多模式llm评论" class="anchor" aria-label="Permalink: ReLook：基于视觉的RL，具有针对大型Web编码的多模式LLM评论" href="#relook基于视觉的rl具有针对大型web编码的多模式llm评论"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11498v1宣布类型：新摘要：虽然大型语言模型（LLM）在算法代码生成方面表现出色，但它们在前端开发方面遇到了困难，前端开发的正确性是根据渲染像素和交互来判断的。我们提出了ReLook，这是一个基于视觉的代理强化学习框架，它使代理能够通过调用多模式LLM（MLLM）作为工具来关闭稳健的生成-诊断-细化循环。在培训过程中，代理将MLLM循环用作视觉评论家（通过屏幕截图评分代码），并作为可操作的、基于视觉的反馈的来源;针对无效的严格零奖励规则会增强可渲染性并防止奖励黑客攻击。为了防止行为崩溃，我们引入了强制优化，这是一项严格的接受规则，只允许改进的修改，从而产生单调更好的轨迹。在推断时，我们将评论者脱钩并运行一个轻量级、无评论的自编辑周期，保持与基本解码相当的延迟，同时保留大部分收益。在三个广泛使用的基准测试中，ReLook在基于视觉的前端代码生成方面始终优于强大的基线，凸显了代理感知、视觉奖励和训练-推理脱钩的好处。</p>
<div class="markdown-heading"><h2 class="heading-element">FedHybrid：通过混合张量管理打破联邦学习的记忆墙</h2><a id="user-content-fedhybrid通过混合张量管理打破联邦学习的记忆墙" class="anchor" aria-label="Permalink: FedHybrid：通过混合张量管理打破联邦学习的记忆墙" href="#fedhybrid通过混合张量管理打破联邦学习的记忆墙"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11400v1宣布类型：新摘要：联合学习（FL）作为一种新的学习范式出现，使多个设备能够协作训练共享模型，同时保护数据隐私。然而，阻碍FL在移动设备上部署的一个基本且普遍的挑战是内存限制。本文提出了\textit{FedHybrid}，这是一个新颖的框架，可以有效减少训练过程中的内存占用，同时保证模型准确性和整体训练进度。具体来说，\textit{FedHybrid}首先通过联合评估其内存预算、计算能力和数据多样性来选择每轮训练的参与设备。之后，它明智地分析计算图并为每个选择的客户端生成执行计划，以满足相应的内存预算，同时根据每个张量的特征通过采用重新计算和压缩技术的混合来最大限度地减少训练延迟。在本地训练过程中，\textit{FedHybrid}使用精心设计的激活压缩技术执行执行计划，以有效地实现内存减少，并将准确性损失降至最低。我们进行了广泛的实验，以在模拟和现货移动设备上评估\textit{FedHybrid}。实验结果表明，与基线相比，\textit{FedHybrid}在各种内存预算下实现了高达39.1%的模型准确性提高，壁挂时间减少了15.5$times $。</p>
<div class="markdown-heading"><h2 class="heading-element">用于分子性质预测的推理增强大型语言模型</h2><a id="user-content-用于分子性质预测的推理增强大型语言模型" class="anchor" aria-label="Permalink: 用于分子性质预测的推理增强大型语言模型" href="#用于分子性质预测的推理增强大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10248v1公告类型：新摘要：分子性质预测对于药物发现和材料科学至关重要，但现有方法存在可解释性有限、跨任务泛化能力差以及缺乏化学推理能力等问题。传统的机器学习模型难以实现任务的可转移性，而专门的分子语言模型几乎无法深入了解其决策过程。为了解决这些限制，我们提出了\textBF{MPPReasoner}，这是一种多模式大型语言模型，它结合了分子性质预测的化学推理。我们的方法基于Qwen 2.5-BL-7 B-Direct，将分子图像与SMILES字符串集成在一起，以实现全面的分子理解。我们开发了两阶段训练策略：使用通过专家知识和多个教师模型生成的16，000个高质量推理轨迹进行监督微调（SFT），然后是原则引导奖励的强化学习（WLPGR）。WLPGR采用可验证的、基于规则的奖励，通过计算验证系统地评估化学原理应用、分子结构分析和逻辑一致性。针对8个数据集的广泛实验表明，MPPReasoner在分发内和分发外任务上的性能分别比最佳基线高出7.91%和4.53%。MPPReasoner表现出出色的跨任务概括性，并生成化学上合理的推理路径，为分子性质分析提供有价值的见解，极大地增强了化学家的可解释性和实际实用性。代码可在<a href="https://anonymous.4open.science/r/MPPReasoner-12687%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82" rel="nofollow">https://anonymous.4open.science/r/MPPReasoner-12687上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">记忆即行动：长期抽象任务的自主上下文治愈</h2><a id="user-content-记忆即行动长期抽象任务的自主上下文治愈" class="anchor" aria-label="Permalink: 记忆即行动：长期抽象任务的自主上下文治愈" href="#记忆即行动长期抽象任务的自主上下文治愈"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12635v1宣布类型：新摘要：大型语言模型在长期代理任务中面临挑战，因为它们有限的记忆很容易被分散注意力或不相关的上下文所淹没。现有的工作记忆方法通常依赖于与代理的核心策略脱钩的外部启发式机制。在这项工作中，我们将工作记忆管理重新定义为一种可学习的内在能力。我们提出了一种新颖的框架，即行动记忆，其中代理通过作为统一策略的一部分执行显式编辑操作来主动管理其工作记忆。这种公式允许通过强化学习训练的代理在给定资源限制下平衡记忆策展与长期任务目标。然而，这种记忆编辑动作打破了LLM相互作用中不断增长的前置符的标准假设，从而导致我们所谓的轨迹断裂。这些非前置变化扰乱了标准政策梯度方法所需的因果连续性，使这些方法不适用。为了解决这个问题，我们提出了一种新算法，即动态上下文策略优化，该算法通过分割记忆动作点的轨迹并将智能级优势应用于生成的动作片段来实现稳定的端到端强化学习。我们的结果表明，以端到端的方式联合优化任务推理和内存管理不仅减少了总体计算消耗，而且还提高了任务性能，这是由针对模型内在能力定制的自适应上下文策展策略驱动的。</p>
<div class="markdown-heading"><h2 class="heading-element">QeRL：超越效率--针对LLM的量化增强强化学习</h2><a id="user-content-qerl超越效率--针对llm的量化增强强化学习" class="anchor" aria-label="Permalink: QeRL：超越效率--针对LLM的量化增强强化学习" href="#qerl超越效率--针对llm的量化增强强化学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11696v1宣布类型：新摘要：我们提出了QeRL，这是一个用于大型语言模型（LLM）的量化增强强化学习框架。虽然RL对于LLM的推理能力至关重要，但它是资源密集型的，需要大量的图形处理器内存和较长的推出持续时间。QeRL通过将NVFP 4量化与低等级自适应（LoRA）相结合来解决这些问题，加速RL的推出阶段，同时减少内存负担。除了效率之外，我们的研究结果还表明，量化噪音还能增加策略熵，增强探索，并能够在RL期间发现更好的策略。为了进一步优化探索，QeRL引入了自适应量化噪音（AQN）机制，可以在训练期间动态调整噪音。实验表明，QeRL在推出阶段提供了超过1.5倍的加速。此外，这是第一个在单个H100 80 GB图形处理器上支持32 B LLM的RL训练的框架，同时为RL训练提供总体加速。它还实现了比16位LoRA和QLoRA更快的奖励增长和更高的最终准确性，同时与7 B模型中GSM 8K（90.8%）和MAT 500（77.4%）等数学基准上全参数微调的性能相匹配。这些结果确立了QeRL作为LLM RL培训的高效且有效的框架。</p>
<div class="markdown-heading"><h2 class="heading-element">MC#：适用于专家混合大型型号的混合压缩机</h2><a id="user-content-mc适用于专家混合大型型号的混合压缩机" class="anchor" aria-label="Permalink: MC#：适用于专家混合大型型号的混合压缩机" href="#mc适用于专家混合大型型号的混合压缩机"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10962v1宣布类型：新摘要：专家混合（MoE）通过稀疏激活增加容量，有效地扩展大型语言模型（LLM）和视觉语言模型（VLM）。然而，将所有专家预加载到内存中并在每个输入中激活多个专家会带来大量的计算和内存负担，使专家模块成为模型大小和推理成本的主要贡献者。为了解决这个问题，我们提出了MC#（Mixture-Compressor-Sharp），这是一个结合静态量化和动态专家修剪的框架，通过利用专家和令牌的重要性来积极压缩MoE-LLM/VLM。为了降低存储和加载成本，我们引入了预加载混合精度量化（PMQ），它通过线性规划优化位分配，平衡专家重要性和量化误差，以在大小和性能之间实现帕累托最优权衡。为了减少运行时计算，Online Top-any Pruning（OTC）使用Gumbel-Softmax采样来动态选择每个令牌的专家子集，从而实现对激活的细粒度控制。通过将PMQ的静态比特宽度优化与OTC的动态路由相结合，MC#以最小的准确性损失实现了极限压缩。在DeepSeek-VL 2上，MC#实现了6.2倍的重量减少，平均比特数为2.57，而五个多模式基准的准确性仅下降了1.7%。此外，OTC还将专家激活率降低了20%以上，性能下降幅度不到1%，展示了基于MoE的高效模型部署的强大潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">语言模型的神经权重压缩</h2><a id="user-content-语言模型的神经权重压缩" class="anchor" aria-label="Permalink: 语言模型的神经权重压缩" href="#语言模型的神经权重压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11234v1宣布类型：新摘要：随着语言模型权重的规模和采用不断增长，语言模型权重的高效存储和传输变得越来越重要。然而，由于我们对这种新数据形态的理解有限，因此为语言模型权重设计良好的压缩算法严重依赖于手动、试错方法。在本文中，我们提出了一个学习压缩框架，可以直接从预训练的语言模型权重训练神经编解码器。与传统数据不同（例如，图像），语言模型权重提出了独特的挑战：权重张量的大小和形状差异显着，重建质量必须通过下游模型预测而不是原始的SSE损失来判断。为了解决这个问题，我们引入了神经权重压缩（NWC），这是一种新型的基于自动编码器的神经编解码器，专为模型权重压缩而定制。所提出的方法继承了基于自动编码器的编解码器的优点，同时结合了三个技术组件：（1）逐列张量分块和正规化;（2）重要性感知训练损失;（3）由模型输出引导的推断时错误补偿机制。开权语言模型的实验表明，NWC实现了有竞争力或最先进的准确性-压缩权衡，在4-6位精度下取得了特别强劲的结果，其中准确性几乎与FP 16模型相当。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>