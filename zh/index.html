<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">人工智能模型是否会跨模式执行类人抽象推理？</h2><a id="user-content-人工智能模型是否会跨模式执行类人抽象推理" class="anchor" aria-label="Permalink: 人工智能模型是否会跨模式执行类人抽象推理？" href="#人工智能模型是否会跨模式执行类人抽象推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02125v1宣布类型：新摘要：OpenAI的o3预览推理模型在ARC-AGI基准上超过了人类的准确性，但这是否意味着最先进的模型能够识别任务创建者想要的抽象并进行推理？我们在ConceptARC上研究模型的抽象能力。我们在输入方式（文本与视觉）不同的设置下评估模型，该设置是否允许模型使用外部Python工具，以及对于推理模型，推理工作量。除了测量输出准确性外，我们还对模型生成以解释其解决方案的自然语言规则进行细粒度评估。这种双重评估使我们能够评估模型是否使用ConceptARC旨在引出的抽象来解决任务，而不是依赖于表面层面的模式。我们的结果表明，虽然一些使用基于文本的表示的模型与人类的输出准确性相匹配，但最好的模型的规则通常基于表面层面的“捷径”，并且捕获预期抽象的频率远低于人类。因此，仅基于准确性的评估可能会高估它们的一般抽象推理能力。在视觉形态中，人工智能模型的输出准确性急剧下降，但我们的规则级分析表明，模型可能被低估，因为它们仍然表现出很大一部分捕获预期抽象的规则，但通常无法正确应用这些规则。简而言之，我们的结果表明，模型在抽象推理方面仍然落后于人类，并且仅使用准确性来评估类ARC任务的抽象推理可能会高估文本模式中的抽象推理能力，而低估视觉模式中的抽象推理能力。我们相信，我们的评估框架提供了更忠实的多模式模型抽象推理能力的描述，以及一种更有原则的方式来跟踪类人、以抽象为中心的智能的进展。</p>
<div class="markdown-heading"><h2 class="heading-element">故障安全：视觉-语言-动作模型中的推理和故障恢复</h2><a id="user-content-故障安全视觉-语言-动作模型中的推理和故障恢复" class="anchor" aria-label="Permalink: 故障安全：视觉-语言-动作模型中的推理和故障恢复" href="#故障安全视觉-语言-动作模型中的推理和故障恢复"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01642v1宣布类型：新摘要：机器人操作领域的最新进展已将低级机器人控制集成到视觉语言模型（VLM）中，并将其扩展到视觉语言动作（VLA）模型中。尽管最先进的VGA在大规模众包机器人训练数据的支持下在下游机器人应用中实现了强劲的性能，但它们在执行过程中仍然不可避免地会遇到失败。使机器人能够推理并从不可预测和突然的故障中恢复仍然是一个严峻的挑战。现有的机器人操纵数据集在模拟或现实世界中收集，主要仅提供地面真相轨迹，导致机器人一旦发生故障就无法恢复。此外，解决故障检测的少数数据集通常仅提供文本解释，这很难在VLA模型中直接利用。为了解决这一差距，我们引入了故障安全，这是一种新型的故障生成和恢复系统，可以自动生成不同的故障案例，并与可执行的恢复操作相结合。故障安全可以无缝应用于任何模拟器中的任何操纵任务，从而实现可扩展的故障动作数据创建。为了证明其有效性，我们对LLaVa-OneVision-7 B（LLaVa-Ov-7 B）进行了微调以构建FireSafe-VLM。实验结果表明，失败安全-VLM成功帮助机械臂检测潜在故障并从潜在故障中恢复，将三种最先进的VLA模型pi 0-Fast、OpenVLA、OpenVLA-OFT）的性能在Maniskill的多项任务中平均提高了22.6%。此外，失败安全-VLM可以在不同的空间配置、相机视角和机器人实施例上进行概括。我们计划向社区发布故障安全代码。</p>
<div class="markdown-heading"><h2 class="heading-element">xLSTM缩放定律：具有线性时间复杂性的竞争性能</h2><a id="user-content-xlstm缩放定律具有线性时间复杂性的竞争性能" class="anchor" aria-label="Permalink: xLSTM缩放定律：具有线性时间复杂性的竞争性能" href="#xlstm缩放定律具有线性时间复杂性的竞争性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02228v1宣布类型：新摘要：缩放定律在大型语言模型（LLM）的成功中发挥着核心作用，可以在训练之前预测模型性能相对于计算预算。虽然Transformers一直是主导架构，但xLSTM等最近的替代方案提供了上下文长度方面的线性复杂性，同时在十亿参数机制中保持竞争力。我们按照以下方式对Transformers和xLSTM的扩展行为进行了比较研究，为指导未来的模型设计和部署提供见解。首先，我们在各种模型大小（80 M-7 B）和训练令牌数量（2B-2 T）上使用IsoFLOP和参数匹配方法研究了计算最优和过度训练方案中xLSTM的缩放行为。其次，我们研究了最佳模型大小对上下文长度的依赖性，这是之前的工作中很大程度上被忽视的一个关键方面。最后，我们分析了推理时缩放特性。我们的研究结果表明，在典型的LLM训练和推理场景中，xLSTM的扩展性优于Transformers。重要的是，随着训练和推理上下文的增长，xLSTM的优势也会扩大。</p>
<div class="markdown-heading"><h2 class="heading-element">RSAVQ：大型语言模型的Riemann敏感性感知的载体量化</h2><a id="user-content-rsavq大型语言模型的riemann敏感性感知的载体量化" class="anchor" aria-label="Permalink: RSAVQ：大型语言模型的Riemann敏感性感知的载体量化" href="#rsavq大型语言模型的riemann敏感性感知的载体量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01240v1公告类型：新摘要：大型语言模型（LLM）在广泛的自然语言处理任务中表现出卓越的性能。然而，它们呈指数级增长的参数对在资源受限设备上的部署提出了重大挑战。载体量化（VQ）显示出低位量化的巨大前景（例如，2至4位），但现有工作面临两个关键挑战：不受约束的方向误差和次优位分配。在本文中，我们提出了RSAVQ，这是一种新型VQ框架，用于增强LLM的极低位量化。RSAVQ引入了两项几何驱动的创新，可以有效地减轻上述限制：（1）误差方向灵敏度指导（EDSG），它利用费舍尔信息矩阵（RST）诱导的Riemann度量将量化误差投影到参数空间中的低灵敏度方向上。具体来说，该投影是沿着负自然梯度方向进行的，有效地抑制了误差扩大。(2)加权通道敏感度指导（WCGM），通过跨导曲线分析构建通道敏感度指标，以动态指导比特资源分配。该方法促进了规定位约束内的全局最优量化解决方案。实验表明，RSAVQ优于LLM的现有方法。例如，在LLaMA-3 8 B的2位量化中，RSAVQ在困惑度（PPL）方面领先VPTQ和QuIP#等基线0.4，在零激发准确度方面领先1.5。这项工作为受约束环境提供了实用的解决方案，并在信息几何和神经网络量化之间架起了理论桥梁，促进了高效的深度学习。</p>
<div class="markdown-heading"><h2 class="heading-element">揭开LLM层在检索、知识和推理中的作用的神秘面纱</h2><a id="user-content-揭开llm层在检索知识和推理中的作用的神秘面纱" class="anchor" aria-label="Permalink: 揭开LLM层在检索、知识和推理中的作用的神秘面纱" href="#揭开llm层在检索知识和推理中的作用的神秘面纱"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02091v1宣布类型：新摘要：最近的研究表明，大型语言模型（LLM）的更深层对表示学习贡献不大，并且通常可以删除而不会出现显着的性能损失。然而，此类主张通常来自狭隘的评估，并且可能忽视了模型行为的重要方面。在这项工作中，我们对不同维度的深度利用进行了系统研究，包括评估协议、任务类别和模型架构。我们的分析证实，非常深的层通常不如早期的层有效，但它们的贡献因评估环境而存在很大差异。在没有生成的基于可能性的指标下，修剪大多数层可以保留性能，只有最初的少数层是关键的。相比之下，基于代际的评估揭示了中层和深层在促进推理和保持长期一致性方面不可或缺的作用。我们进一步发现，知识和检索集中在浅层部分，而推理准确性严重依赖于更深层次，但可以通过蒸馏重塑。这些结果凸显了LLM中的深度使用高度异类且依赖于上下文，这凸显了在解释和压缩大型模型时对任务、指标和模型感知角度的需求。</p>
<div class="markdown-heading"><h2 class="heading-element">UpSafe$^\circ$C：大型语言模型中可控安全性的升级</h2><a id="user-content-upsafecircc大型语言模型中可控安全性的升级" class="anchor" aria-label="Permalink: UpSafe$^\circ$C：大型语言模型中可控安全性的升级" href="#upsafecircc大型语言模型中可控安全性的升级"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02194v1宣布类型：新摘要：大型语言模型（LLM）在广泛的任务中取得了显着进展，但仍然容易受到有害内容生成和越狱攻击等安全风险的影响。现有的安全技术--包括外部护栏、推断时间引导和训练后对齐--在平衡安全性、实用性和可控性方面都面临着局限性。在这项工作中，我们提出了UpSafe$^\circ$C，这是一个通过安全意识升级来增强LLM安全性的统一框架。我们的方法首先识别安全关键层，并将它们升级为稀疏的专家混合（MoE）结构，其中路由器充当软护栏，可以选择性地激活原始MLP和添加的安全专家。我们进一步引入两阶段SFT策略，以加强安全歧视，同时保留一般能力。为了在推理时实现灵活控制，我们引入了安全温度机制，允许动态调整安全性和实用性之间的权衡。跨多个基准测试、基本模型和模型规模的实验表明，UpSafe$^\circ$C针对有害和越狱输入实现了稳健的安全改进，同时在一般任务上保持竞争力的性能。此外，分析表明，安全温度提供了细粒度的推断时间控制，实现了实用性和安全性之间的帕累托最优边界。我们的结果凸显了LLM安全的新方向：从静态对齐转向动态、模块化和推理感知控制。</p>
<div class="markdown-heading"><h2 class="heading-element">KaVaa：通过压缩KV缓存蒸馏的潜在推理</h2><a id="user-content-kavaa通过压缩kv缓存蒸馏的潜在推理" class="anchor" aria-label="Permalink: KaVaa：通过压缩KV缓存蒸馏的潜在推理" href="#kavaa通过压缩kv缓存蒸馏的潜在推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02312v1宣布类型：新摘要：大型语言模型（LLM）擅长处理具有显式思想链（CoT）的多步推理问题，但冗长的跟踪会带来巨大的计算成本和内存负担，并且通常携带冗余的风格文物。潜在推理已成为一种内化思维过程的有效替代方案，但它严重缺乏监督，限制了其对复杂的自然语言推理痕迹的有效性。在这项工作中，我们提出了KaVA，这是第一个弥合这一差距的框架，通过自我提炼将知识直接从教师的压缩GV缓存中提炼到潜在推理学生中，利用连续潜在令牌的代表灵活性来对齐逐步的KV轨迹。我们表明，压缩的KV缓存中缺乏直接令牌对应关系的抽象、非结构化知识可以作为潜在推理学生的丰富监督信号。从经验上看，该方法始终优于强潜在基线，从纯方程到自然语言痕迹的退化明显较小，并且在保持效率的同时扩展到更大的主干。这些结果将压缩的KV缓存蒸馏建立为潜在推理的可扩展监督信号，将CoT培训教师的准确性与潜在推理的效率和可部署性相结合。</p>
<div class="markdown-heading"><h2 class="heading-element">ThinKV：用于高效推理模型的自适应KV缓存压缩</h2><a id="user-content-thinkv用于高效推理模型的自适应kv缓存压缩" class="anchor" aria-label="Permalink: ThinKV：用于高效推理模型的自适应KV缓存压缩" href="#thinkv用于高效推理模型的自适应kv缓存压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01290v1宣布类型：新摘要：大型推理模型的长输出上下文生成可以扩展思维链（CoT），但也会推动key-Value（KV）缓存的快速增长，迅速压倒图形处理器内存。为了应对这一挑战，我们提出了ThinKV，这是一个思想自适应的KV缓存压缩框架。ThinKV基于这样的观察：注意力稀疏性揭示了CoT内重要性不同的独特思维类型。它应用了混合量化-驱逐策略，通过思想重要性分配代币精确度，并随着推理轨迹的发展逐渐将代币从不那么批判的思想中驱逐。此外，为了实现ThinKV，我们设计了一个扩展PagedAttention的内核，以实现对被驱逐令牌的内存插槽的有效重复使用，从而消除压缩管理费用。在数学和编码基准上对DeepSeek-R1-Distill、GPT-OSS和NVIDIA AceReason进行的广泛实验表明，ThinKV以不到原始KV缓存的5%的速度实现了近乎无损的准确性，同时通过比最先进的基线高出5.8倍的推理吞吐量来提高性能。</p>
<div class="markdown-heading"><h2 class="heading-element">具有变分Bayesian最后一层的微调LLM，以实现多维Bayesian优化</h2><a id="user-content-具有变分bayesian最后一层的微调llm以实现多维bayesian优化" class="anchor" aria-label="Permalink: 具有变分Bayesian最后一层的微调LLM，以实现多维Bayesian优化" href="#具有变分bayesian最后一层的微调llm以实现多维bayesian优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01471v1宣布类型：新摘要：大量的应用需要解决评估成本高的黑匣子优化问题，包括药物发现、材料设计以及超参数调整。为了以样本效率找到此类黑匣子优化问题的全局最优值，Bayesian优化（BO）是一个理论上优雅的框架，它依赖于概率代理模型，以便通过良好平衡的探索-利用权衡迭代选择查询点。高斯过程（GP）作为代理建模的事实选择，已经为具有低维连续变量的vanilla BO取得了令人信服的性能。然而，全科医生在应对具有{\it unique}变量的多维对应者方面表现不佳（例如，类别、顺序等）。为了缓解这一问题，人们探索了基于神经网络的代理人。受LLM强大功能的启发，我们采用LLM作为替代品来建模从多维输入变量到目标函数的映射。为了适应当前的问题，我们利用低等级自适应（LoRA）通过变分Bayesian Last Layer（WBLL）框架微调LLM参数以及线性回归头部的后验。与现有的替代方案相比，最终的LoRA-VB LL不仅计算量轻，而且还允许迭代更新。为了自动化LoRA等级以及其他超参数的关键选择，设计了LoRA-VB LL代理的加权集合（ANS），它进一步适应通过迭代Bayes对每个模型权重和单个LoRA-VB LL参数的持续更新。大量的实验结果证明了所提出的（ANS-）LoRA-VB LL方法在各种多维基准和现实世界的分子优化任务上具有令人信服的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">稀疏查询注意力（SQA）：一种具有查询头减少的计算高效注意力机制</h2><a id="user-content-稀疏查询注意力sqa一种具有查询头减少的计算高效注意力机制" class="anchor" aria-label="Permalink: 稀疏查询注意力（SQA）：一种具有查询头减少的计算高效注意力机制" href="#稀疏查询注意力sqa一种具有查询头减少的计算高效注意力机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01817v1宣布类型：新摘要：以多头注意力（MHA）机制为基础的Transformer架构已成为人工智能领域最先进模型的事实标准。然而，MHA相对于序列长度的二次计算复杂性给扩展带来了重大障碍，特别是对于涉及长上下文的应用程序。流行的解决方案，例如多查询注意力（MQA）和分组查询注意力（GQA），通过共享Key和Value预测，有效地解决了主导自回归推理延迟的内存带宽瓶颈。虽然这些方法非常成功，但并没有减少注意力分数计算所需的浮点运算（FLOPs）的基本数量，这仍然是训练和全序列处理的关键瓶颈。本文介绍了稀疏查询注意力（SQA），这是一种新颖的注意力架构，追求替代且补充的优化路径。SQA不是减少Key/Value头部，而是减少查询头部的数量。这种架构修改直接降低了注意力机制的计算复杂性，其比例与查询头的减少成比例，从而降低了总体FLOP。这项工作介绍了SQA的理论基础、其数学公式以及一系列架构变体。长序列（32 k-200 k个令牌）的经验基准表明，SQA可以在模型预训练、微调和基于编码器的任务等计算绑定场景中实现高达3倍的吞吐量显着提高，而对初步小规模实验中模型质量的影响很小。SQA是在即将推出的Reactive Transformer架构的开发过程中偶然发现的，这表明它有潜力作为构建更高效和可扩展模型的强大工具</p>
<div class="markdown-heading"><h2 class="heading-element">揭开LLM预培训中合成数据的神秘面纱：缩放定律、好处和陷阱的系统研究</h2><a id="user-content-揭开llm预培训中合成数据的神秘面纱缩放定律好处和陷阱的系统研究" class="anchor" aria-label="Permalink: 揭开LLM预培训中合成数据的神秘面纱：缩放定律、好处和陷阱的系统研究" href="#揭开llm预培训中合成数据的神秘面纱缩放定律好处和陷阱的系统研究"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01631v1宣布类型：新摘要：训练数据在大型语言模型（LLM）扩展中发挥着至关重要的作用，但高质量的数据供应有限。合成数据技术提供了一条规避这些限制的潜在途径。我们使用统一协议和缩放定律进行大规模实证调查（&gt;1000个LLM，&gt; 100 k个图形处理器小时），比较自然网络数据、不同的合成类型（重新措辞的文本、生成的教科书）以及自然和合成数据的混合。具体来说，我们发现对重新措辞的合成数据\textit{alone}进行预训练并不比对自然网络文本进行预训练快;而对1/3重新措辞的合成数据与2/3自然网络文本混合进行预训练可以在更大的数据预算下加速5- 10倍（达到相同的验证损失）。对教科书风格的合成数据\textit{alone}进行预训练会导致许多下游域的损失明显更高，尤其是在数据预算较小的情况下。训练数据混合物中合成数据的“良好”比例取决于模型大小和数据预算，根据经验，重新表述的合成数据会收敛到<del>30%。更大的生成器模型不一定比</del>8B-param模型产生更好的预训练数据。这些结果为合成数据的大规模单轮（n=1）模型训练期间的“模型崩溃”提供了混杂的证据--重新措辞的合成数据的训练显示在可预见的范围内性能没有下降，而在教科书式纯生成的合成数据的混合物上的训练显示出由“模型崩溃”预测的模式。我们的工作揭开了预训练中的合成数据的神秘面纱，验证了其条件效益，并提供了实践指导。</p>
<div class="markdown-heading"><h2 class="heading-element">LangGrasp：利用微调的LLM进行语言交互机器人抓取，具有模糊指令</h2><a id="user-content-langgrasp利用微调的llm进行语言交互机器人抓取具有模糊指令" class="anchor" aria-label="Permalink: LangGrasp：利用微调的LLM进行语言交互机器人抓取，具有模糊指令" href="#langgrasp利用微调的llm进行语言交互机器人抓取具有模糊指令"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02104v1宣布类型：新摘要：现有的语言驱动的抓取方法很难完全处理包含隐含意图的模糊指令。为了应对这一挑战，我们提出了LangGrasp，这是一种新型的语言交互式机器人抓取框架。该框架集成了微调的大型语言模型（LLM），以利用其强大的常识理解和环境感知能力，从而从语言指令中推导出隐含的意图，并澄清任务要求以及目标操作对象。此外，我们设计的点云定位模块，引导下的2D部分分割，使部分点云定位在场景中，从而扩展抓取操作从粗粒度的对象级到细粒度的部分级操作。实验结果表明，LangGrasp框架准确地解决了模糊指令中的隐含意图，识别出关键操作和目标信息，这些操作和目标信息对于任务完成来说是不可或缺的。此外，它动态地选择最佳的抓取姿势，通过整合环境信息。这可以实现从对象级到零件级操纵的高精度抓取，显着增强机器人在非结构化环境中的适应性和任务执行效率。更多信息和代码请访问：<a href="https://github.com/wu467/LangGrasp%E3%80%82">https://github.com/wu467/LangGrasp。</a></p>
<div class="markdown-heading"><h2 class="heading-element">抽象拼图交互学习，增强视觉语言模型中的视觉感知和推理</h2><a id="user-content-抽象拼图交互学习增强视觉语言模型中的视觉感知和推理" class="anchor" aria-label="Permalink: 抽象拼图交互学习，增强视觉语言模型中的视觉感知和推理" href="#抽象拼图交互学习增强视觉语言模型中的视觉感知和推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01304v1宣布类型：新摘要：尽管当前大型视觉语言模型（VLM）在多模式理解和推理方面取得了进步，但它们的基本感知和推理能力仍然有限。具体来说，即使在简单的拼图任务中，现有的VLM也几乎随机执行，暴露了核心感知和推理能力的缺陷。虽然高质量的视觉语言数据可以增强这些能力，但其稀缺性和有限的可扩展性带来了显着的限制。为了解决这个问题，我们提出了AGILE，这是一种大型jiGsaw交互学习，用于增强VLM中的视觉感知和推理。AGILE将拼图解决方案制定为一个交互过程，使模型能够逐步融入环境。在每一步，模型都会生成可执行代码以根据当前状态执行动作，而环境则提供细粒度的视觉反馈来指导任务完成。通过这种观察和交互的迭代循环，模型通过探索和反馈逐步提高其感知和推理能力。实验结果表明，AGILE不仅大大提高了不同复杂度的拼图任务的性能（例如，在2 $\times $2设置下，准确率从9.5%提高到82.8%），但在9项一般视觉任务中也表现出很强的泛化能力，平均提高了3.1%。这些结果表明，知觉和推理能力显着增强。这项工作为推进多模式模型的推理和概括开辟了一条新途径，并为多模式强化学习数据的稀缺性提供了一种高效、可扩展的解决方案。代码和数据集可在<a href="https://github.com/yuzeng0-0/AGILE%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yuzeng0-0/AGILE上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">LLM和诱导小代理的故事：知识挖掘的可扩展代理</h2><a id="user-content-llm和诱导小代理的故事知识挖掘的可扩展代理" class="anchor" aria-label="Permalink: LLM和诱导小代理的故事：知识挖掘的可扩展代理" href="#llm和诱导小代理的故事知识挖掘的可扩展代理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01427v1公告类型：新摘要：深度研究的核心是知识挖掘，即根据用户指令从大量非结构化文本中提取结构化信息的任务。大型语言模型（LLM）擅长解释这些指令，但大规模部署成本高得令人望而却步，而传统的分类器和提取器管道仍然高效但脆弱，无法推广到新任务。我们引入Falconer，这是一个协作框架，将LLM的代理推理与轻量级代理模型相结合，以实现可扩展的知识挖掘。在Falconer中，LLM充当规划者，将用户指令分解为可执行管道，并充当注释器，产生监督以训练小型代理。该框架将分类和提取统一为两个原子操作：获取标签和获取跨度，使单个描述遵循模型能够替换多个特定于任务的组件。为了评估Falconer孵化的代理模型与人类和大型模型提供的注释之间的一致性，我们构建了涵盖规划和端到端执行的新基准。实验表明，Falconer在描述跟踪准确性方面与最先进的LLM密切匹配，同时将推理成本降低高达90%，并将大规模知识挖掘加速20倍以上，为深度研究提供高效且可扩展的基础。</p>
<div class="markdown-heading"><h2 class="heading-element">支持基础：超越有限范围的快速关注</h2><a id="user-content-支持基础超越有限范围的快速关注" class="anchor" aria-label="Permalink: 支持基础：超越有限范围的快速关注" href="#支持基础超越有限范围的快速关注"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01643v1宣布类型：新摘要：softmax注意力的二次复杂性仍然是扩展大型语言模型（LLM）的核心瓶颈。[Alman和Song，NeurIPS 2023]提出了一种次二次注意力逼近算法，但它仅在限制性有界入口假设下有效。由于这一假设在实践中很少成立，因此其对现代LLM的适用性有限。   在本文中，我们引入了支持基分解，这是一种超越有界条目的高效注意力逼近的新框架。我们经验证明，查询和关键矩阵的条目表现出亚高斯行为。我们的方法使用这一属性来拆分大大小小的条目，从而能够对稀疏分量进行精确计算，并对密集分量进行多项逼近。我们建立了严格的理论保证，证明了次二次运行时，并将该方法扩展到消除所有分布假设的多阈值设置。此外，我们为多元关注的经验成功提供了第一个理论依据[Kacham、Mirrokni和Zhong，ICML 2024]，表明softmax注意力可以通过多个多元关注与草图的组合来接近。</p>
<div class="markdown-heading"><h2 class="heading-element">通过基础分解加速注意力</h2><a id="user-content-通过基础分解加速注意力" class="anchor" aria-label="Permalink: 通过基础分解加速注意力" href="#通过基础分解加速注意力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01718v1宣布类型：新摘要：注意力是大型语言模型（LLM）和视觉语言模型（VLM）的核心操作。我们介绍了BD Attention（BDA），这是第一个注意力的无损算法重建。BDA由基础分解（BD）的简单矩阵单位支持，该单位将多头投影重组为紧凑形式，同时保留精确的输出。与Flash Attention等I/O感知系统优化不同，BDA提供了数学上有保证的加速，且与架构无关。在DeepSeek-V2-Lite（16 B，FP 16）上，BDA仅需要4秒的离线准备，无需再培训，并且在现代图形处理器上，键/值预测速度提高了32%，权重降低了25%，同时端到端困惑度（PPL）仅提高了0.02%（FP 16）或0.0004%（FP 32），对模型性能的影响可以忽略不计。这些结果将BDA定位为第一个理论上精确的无损注意力加速方法，该方法是对现有工程级优化的补充。我们的代码可在<a href="https://github.com/abcbdf/basis-decomposition-official%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/abcbdf/basis-decomposition-official上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">看不见的前沿：用无代理人的ADMM突破LLM稀缺性的极限</h2><a id="user-content-看不见的前沿用无代理人的admm突破llm稀缺性的极限" class="anchor" aria-label="Permalink: 看不见的前沿：用无代理人的ADMM突破LLM稀缺性的极限" href="#看不见的前沿用无代理人的admm突破llm稀缺性的极限"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01650v1宣布类型：新摘要：神经网络修剪是一种有前途的技术，可以减轻大型语言模型（LLM）的过度计算和内存需求。然而，尽管有希望，但该领域的进展已经减弱，因为传统方法似乎无法在不严重降低模型准确性的情况下超过中等稀疏度水平（50-60%）。这项工作打破了当前的僵局，提出了一种名为$\textttt {Elsa}$的原则性且有效的方法，该方法在保持高模型保真度的同时实现了高达90%的极端稀疏度水平。这是通过识别当前实践中的几个局限性来实现的，所有这些局限性都可以追溯到他们对替代目标公式的依赖。$\textttt {Elsa}$通过基于ADMM的标准且成熟的约束优化技术直接有效地解决了这个问题。我们在各种模型和规模上进行的广泛实验表明，$\textttt {Elsa}$比现有方法实现了实质性改进;例如，在90%稀疏度下，它比LLaMA-2- 7 B上的最佳现有方法减少了7.8 $\x $。此外，我们还提出了$\textttt {Elsa}_{\text{-L}}$，这是一个可扩展到超大模型（27 B）的量化变体，并建立了其理论收敛保证。这些结果凸显了在推进LLM稀疏性前沿方面取得的有意义的进展，同时承诺在迄今为止吸引有限探索的方向上可能仍然存在进一步进步的重要机会。</p>
<div class="markdown-heading"><h2 class="heading-element">视觉-语言-动作模型的对比表示正规化</h2><a id="user-content-视觉-语言-动作模型的对比表示正规化" class="anchor" aria-label="Permalink: 视觉-语言-动作模型的对比表示正规化" href="#视觉-语言-动作模型的对比表示正规化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01711v1宣布类型：新摘要：视觉语言动作（VLA）模型通过利用预训练的视觉语言模型（VLM）的丰富表示，展示了其在机器人操纵方面的能力。然而，它们的表现可以说仍然是次优的，对控制动作和主体感受状态等机器人信号缺乏敏感性。为了解决这个问题，我们引入了机器人状态感知对比损失（RS-CL），这是一种简单有效的VLA模型表示正则化，旨在弥合VLM表示和机器人信号之间的差距。特别是，RS-CL对齐表示更紧密地与机器人的本体感受状态，通过使用状态之间的相对距离作为软监督。作为对原始动作预测目标的补充，RS-CL有效地增强了控制相关的表示学习，同时是轻量级的，并与标准VLA训练管道完全兼容。我们的实证结果表明，RS-CL大大提高了最先进的VLA模型的操作性能;通过在抓取和放置过程中更准确的定位，它将RoboCasa-Kitchen中的拾取和放置任务的现有技术从30.8%提升到41.5%，并将具有挑战性的真实机器人操作任务的成功率从45.0%提升到58.3%。</p>
<div class="markdown-heading"><h2 class="heading-element">正确思考：学会通过自适应、专注的压缩来减轻过度思考</h2><a id="user-content-正确思考学会通过自适应专注的压缩来减轻过度思考" class="anchor" aria-label="Permalink: 正确思考：学会通过自适应、专注的压缩来减轻过度思考" href="#正确思考学会通过自适应专注的压缩来减轻过度思考"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01581v1宣布类型：新摘要：最近的思维模型通过扩展测试时计算来解决复杂的推理任务，但这种扩展必须根据任务难度进行分配。一方面，简短的推理（思考不足）会导致在需要扩展推理步骤的更困难问题上出现错误;但是，过长的推理（思考过度）可能会导致效率低下，即使在达到正确的中间解决方案后也会产生不必要的步骤。我们将其称为适应不足，即模型未能在困难程度不同的问题的情况下适当调整其响应长度。为了解决适应不足问题并在思考不足和过度思考之间取得平衡，我们提出了TRAAC（通过自适应、专注压缩思考正确），这是一种在线训练后RL方法，它利用模型在长推理轨迹上的自我注意力来识别重要步骤并删除多余的步骤。TRAAC还估计难度并将其纳入培训奖励，从而学习分配与示例难度相称的推理预算。与基本模型和其他RL基线相比，我们的方法提高了准确性，减少了推理步骤，并实现了自适应思维。在各种任务（AIME、AMC、GPQA-D、BBEH）中，TRAAC（Qwen 3 - 4 B）实现了8.4%的平均绝对准确率提高，与基本模型相比推理长度相对减少了36.8%，与最佳RL基线相比，准确率提高了7.9%，长度下降了29.4%。TRAAC还表现出很强的概括性：尽管我们的模型是在数学数据集上训练的，但它们在GPQA-D、BBEH和OptimalThinkingBench等非分布非数学数据集上显示出准确性和效率的提高。我们的分析进一步验证了TRAAC根据难度对思维预算进行细粒度调整，并且任务难度校准和基于注意力的压缩相结合可以在不同任务中产生收益。</p>
<div class="markdown-heading"><h2 class="heading-element">早期疾病检测的多模式基础模型</h2><a id="user-content-早期疾病检测的多模式基础模型" class="anchor" aria-label="Permalink: 早期疾病检测的多模式基础模型" href="#早期疾病检测的多模式基础模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01899v1公告类型：新摘要：医疗保健产生各种数据流，包括电子健康记录（EHR），医学成像，遗传学和可穿戴设备的持续监控。传统的诊断模型经常孤立地分析这些来源，这限制了它们识别早期疾病诊断所必需的跨模态相关性的能力。我们的研究提出了一种多模式基础模型，该模型通过基于注意力的Transformer框架整合不同的患者数据。首先，专用编码器将每个模式放入共享的潜在空间中。然后，他们使用多头注意力和残余正规化将它们组合起来。该架构专为许多任务的预训练而设计，这使得无需额外工作即可轻松适应新疾病和数据集。我们提供了一种实验策略，使用肿瘤学、心脏病学和神经病学中的基准数据集，目标是测试早期检测任务。除了技术性能外，该框架还包括数据治理和模型管理工具，以提高透明度、可靠性和临床可解释性。所建议的方法致力于建立一个精确诊断的单一基础模型，这可以提高预测的准确性并帮助医生做出决策。</p>
<div class="markdown-heading"><h2 class="heading-element">VaPR --推理的视觉语言偏好对齐</h2><a id="user-content-vapr---推理的视觉语言偏好对齐" class="anchor" aria-label="Permalink: VaPR --推理的视觉语言偏好对齐" href="#vapr---推理的视觉语言偏好对齐"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01700v1宣布类型：新摘要：具有人工智能生成反馈的直接偏好优化（DPO）等偏好微调方法在将大型视觉语言模型（LVLM）与人类偏好相匹配方面表现出了希望。然而，现有技术忽视了合成偏好注释中以风格和长度偏差形式存在的噪音的普遍性。为此，我们引入了一个基于LLM引导的响应编辑的硬消极响应生成框架，该框架生成带有针对性错误的拒绝响应，并与已接受的响应保持文体和长度相似性。使用该框架，我们开发了由30 K高质量样本组成的VaPR数据集，以微调三个LVLM系列：LLaBA-V1.5、Qwen 2 VL &amp; Qwen 2.5VL（2B-13 B尺寸）。我们的VaPR模型在十个基准测试中实现了显着的性能改进，平均提高了6.5%（LLaVA）、4.0%（Qwen 2 VL）和1.5%（Qwen 2.5VL），并且在推理任务上有显着的改进。扩展分析表明，性能会随着数据规模的增加而不断提高，LLaVA模型即使在较小的规模下也会受益。此外，VaPR减少了在二元问题中回答“是”的倾向--解决了LLaVA等LVLM中的常见故障模式。最后，我们表明该框架将开源LLM推广为编辑器，在VaPR-OS上训练的模型的性能达到了在\Name上训练的模型（使用GPT-4 o合成）的~99%。我们的数据、模型和代码可以在项目页面<a href="https://vap-r.github.io%E4%B8%8A%E6%89%BE%E5%88%B0" rel="nofollow">https://vap-r.github.io上找到</a></p>
<div class="markdown-heading"><h2 class="heading-element">用于高效大型语言模型训练的随机梯度子空间</h2><a id="user-content-用于高效大型语言模型训练的随机梯度子空间" class="anchor" aria-label="Permalink: 用于高效大型语言模型训练的随机梯度子空间" href="#用于高效大型语言模型训练的随机梯度子空间"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01878v1宣布类型：新摘要：训练大型语言模型（LLM）通常会受到极端内存需求的限制，优化器状态占据主导地位。最近的作品通过使用复杂的更新策略将梯度投影到低维度子空间中来减轻了这一成本。本文分析了梯度空间及其底层子空间的动力学。我们发现，虽然一个小的子空间捕获了大部分梯度能量，但很大一部分仍然存在于剩余体积中;此外，核心子空间的影响随着时间的推移和更深的层中逐渐减弱。我们还观察到梯度空间表现出近乎平坦的弯曲，这需要显式地考虑这种几何形状的算法。受这些见解的启发，我们引入了一套随机算法GrassWalk和GrassJump，它们利用子空间并实现最先进的内存节省，同时提高LLaMA-1B和LLaMA-7 B预训练的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">Nav-EE：导航引导的早期退出，以实现自动驾驶中的高效视觉语言模型</h2><a id="user-content-nav-ee导航引导的早期退出以实现自动驾驶中的高效视觉语言模型" class="anchor" aria-label="Permalink: Nav-EE：导航引导的早期退出，以实现自动驾驶中的高效视觉语言模型" href="#nav-ee导航引导的早期退出以实现自动驾驶中的高效视觉语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01795v1宣布类型：新摘要：视觉语言模型（VLM）越来越多地应用于自动驾驶中，以实现统一感知和推理，但高推理延迟阻碍了实时部署。提前退出通过终止中间层的推理来减少延迟，但其任务依赖性限制了在不同场景中的概括。我们观察到这种限制与自动驾驶一致：导航系统可以预测即将到来的上下文（例如，十字路口、交通灯），指示需要哪些任务。我们提出Nav-EE，这是一个导航引导的早期退出框架，它离线预先计算特定于任务的退出层，并根据导航先验在线动态应用它们。CODA、Waymo和BOsch上的实验表明，Nav-EE的准确性与完全推理相当，同时将延迟降低高达63.9%。实车与Autoware Universe的集成进一步证明了推理延迟（600 ms至300 ms）的减少，支持复杂场景中更快的决策。这些结果表明，将导航预见与提前退出相结合为在自主系统中高效部署大型模型提供了一条可行的途径。代码和数据可在我们的匿名存储库中获取：<a href="https://anonymous.4open.science/r/Nav-EE-BBC4" rel="nofollow">https://anonymous.4open.science/r/Nav-EE-BBC4</a></p>
<div class="markdown-heading"><h2 class="heading-element">推理时间优化的最优停止与最优N</h2><a id="user-content-推理时间优化的最优停止与最优n" class="anchor" aria-label="Permalink: 推理时间优化的最优停止与最优N" href="#推理时间优化的最优停止与最优n"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01394v1宣布类型：新摘要：大型语言模型（LLM）生成通常需要平衡输出质量与推理成本，尤其是在使用多代时。我们基于经典的潘多拉盒子问题引入了一个新的推理时优化框架。我们将每一代视为打开一个带有随机奖励的昂贵“盒子”，我们开发了算法，在不知道潜在奖励分布的情况下决定何时停止生成。我们的第一个贡献是UCB风格的Pandora ' s Box算法，该算法的性能与Weitzman算法接近，这是已知分布时的最佳策略。我们通过Bradley-Terry启发的转换解决提示之间的奖励缩放问题，进一步将这种方法调整到实际的LLM设置中。这导致了一种自适应推断时优化方法，该方法将奖励标准化并动态学习停止阈值。使用多个LLM奖励模型对在AlpacaFarm和HH-RL HF数据集上进行的实验表明，我们的自适应策略可以获得与非自适应N最佳抽样相同的性能，同时平均需要减少15- 35%的世代。我们的结果在最佳停止理论和推理时缩放之间建立了一座原则性的桥梁，为LLM部署提供了理论性能界限和实际效率收益。</p>
<div class="markdown-heading"><h2 class="heading-element">预留广播：神经网络效率的活动相关修剪规则</h2><a id="user-content-预留广播神经网络效率的活动相关修剪规则" class="anchor" aria-label="Permalink: 预留广播：神经网络效率的活动相关修剪规则" href="#预留广播神经网络效率的活动相关修剪规则"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01263v1宣布类型：新摘要：大多数修剪方法都会删除按损失影响排名的参数（例如，幅度或梯度）。我们提出了临时广播（BB），它为每个单元提供本地流量预算（其长期在线速率$a_i$和散开$k_i$的积）。受约束的信息量分析表明，在全球流量预算下最大化编码信息量会产生选择性与受众的平衡，$\log\fRAC{1-a_i}{a_i}=\Beta k_i$。BB通过简单的本地致动器来实现这种平衡，这些致动器可以修剪扇入（以降低活动）或扇出（以减少广播）。在实践中，BB增加了编码熵和去相关性，并提高了用于ASB的Transformers、用于面部识别的ResNets和用于突触预测的3D U-Net之间匹配稀疏度的准确性，有时会超过密集基线。在电子显微镜图像上，根据我们的评估方案，它获得了最先进的F1和PR-AUC。BB易于集成，并为学习更多样化和更高效的表示提供了一条途径。</p>
<div class="markdown-heading"><h2 class="heading-element">VOGUE：用视觉不确定性指导探索改进多模式推理</h2><a id="user-content-vogue用视觉不确定性指导探索改进多模式推理" class="anchor" aria-label="Permalink: VOGUE：用视觉不确定性指导探索改进多模式推理" href="#vogue用视觉不确定性指导探索改进多模式推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01444v1宣布类型：新摘要：具有可验证奖励的强化学习（WLVR）改进了大型语言模型（LLM）中的推理，但在探索方面遇到困难，这对于多模式LLM（MLLM）来说仍然存在这个问题。当前的方法将视觉输入视为固定的、确定性的条件，忽视了模糊性的关键来源，并努力构建对合理的视觉变化稳健的政策。我们引入了$\textBF{VOGUE（视觉不确定性引导探索）}$，这是一种将探索从输出（文本）转移到输入（视觉）空间的新颖方法。通过将图像视为随机上下文，VOGUE使用“原始”和“有噪”分支之间的对称KL偏差量化政策对视觉扰动的敏感性，为不确定性感知探索创建直接信号。该信号通过不确定性比例奖金塑造了学习目标，该奖金与标记性的信息量奖金和经过调整的采样时间表相结合，有效地平衡了探索和利用。VOGUE在GRPO中以两种模型规模（Qwen 2.5-DL-3B/7 B）实施，在三个视觉数学基准测试上将pass@1准确性平均提高2.6%，在三个通用域推理基准测试上平均提高3.7%，同时提高pass@4性能并减轻RL微调中常见的探索衰退。我们的工作表明，以视觉输入固有的不确定性为基础的探索是改善多模式推理的有效策略。</p>
<div class="markdown-heading"><h2 class="heading-element">INSIGHT：在视觉-语言-动作模型中生成帮助触发器的推理时序列内省</h2><a id="user-content-insight在视觉-语言-动作模型中生成帮助触发器的推理时序列内省" class="anchor" aria-label="Permalink: INSIGHT：在视觉-语言-动作模型中生成帮助触发器的推理时序列内省" href="#insight在视觉-语言-动作模型中生成帮助触发器的推理时序列内省"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01389v1宣布类型：新摘要：最近的视觉-语言-动作（VLA）模型显示出强大的概括能力，但它们缺乏预测失败和向人类主管请求帮助的内省机制。我们介绍了\textBF{INSIGHT}，这是一个学习框架，用于利用标记级不确定性信号来预测VLA何时应该请求帮助。使用$\pi_0$-FAST作为底层模型，我们提取每个标记的\n {熵}、\n {对数概率}和基于狄利克雷的\n {任意和认知不确定性}估计，并训练紧凑的Transformer分类器来映射这些序列以帮助触发。我们探索强监管或弱监管的监管制度，并在分配内和分配外任务中对其进行广泛比较。我们的结果显示了一个权衡：强标签使模型能够捕捉细粒度的不确定性动态，以实现可靠的帮助检测，而弱标签尽管噪音更大，但在训练和评估一致时仍然支持竞争性内省，在密集注释不切实际时提供了可扩展的路径。至关重要的是，我们发现用变换器对标记级不确定性信号的时间演变进行建模，比静态序列级分数提供了更大的预测能力。这项研究首次对VLA中基于不确定性的内省进行了系统评估，为主动学习和通过选择性人类干预实现实时错误缓解开辟了未来的途径。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>