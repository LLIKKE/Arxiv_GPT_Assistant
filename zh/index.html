<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">压缩后的大语言模型真能行动吗？LLM压缩中代理能力的实证评估</h2><a id="user-content-压缩后的大语言模型真能行动吗llm压缩中代理能力的实证评估" class="anchor" aria-label="Permalink: 压缩后的大语言模型真能行动吗？LLM压缩中代理能力的实证评估" href="#压缩后的大语言模型真能行动吗llm压缩中代理能力的实证评估"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.19433v1 公告类型：新研究<br>
摘要：训练后压缩技术能降低大语言模型（LLMs）的计算与内存开销，实现资源高效部署。然而现有压缩基准仅关注语言建模（如困惑度）和自然语言理解任务（如GLUE准确率），忽视了智能体核心能力——工作流生成、工具使用/函数调用、长上下文理解及实际应用。我们推出首个综合性智能体压缩基准ACBench，系统评估压缩对LLMs智能体能力的影响。该基准涵盖：(1) 4大类能力下的12项任务（如工作流生成的WorfBench、长上下文检索的Needle-in-Haystack）；(2) 量化（GPTQ、AWQ）与剪枝（Wanda、SparseGPT）方法；(3) 15个模型，包括小规模（Gemma-2B）、标准（Qwen2.5 7B-32B）和蒸馏推理LLMs（DeepSeek-R1-Distill）。实验揭示压缩的权衡：4比特量化能保持工作流生成与工具使用能力（仅下降1%-3%），但会使实际应用准确率降低10%-15%。我们提出ERank、Top-k排序相关性与能量指标以系统化分析。ACBench为智能体场景下的LLM压缩优化提供了可操作的洞见，代码详见<a href="https://github.com/pprp/ACBench%E3%80%82">https://github.com/pprp/ACBench。</a></p>
<p>（注：根据技术文献翻译规范，对以下术语进行了特殊处理：</p>
<ol>
<li>"agentic capabilities"译为"智能体核心能力"以突出主体性</li>
<li>"WorfBench/Needle-in-Haystack"保留原名+中文说明</li>
<li>"4-bit quantization"译为"4比特量化"符合计算机领域表述习惯</li>
<li>"ERank"等新指标保留原名确保学术严谨性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">赢快或输慢：在LLM延迟敏感决策中平衡速度与准确性</h2><a id="user-content-赢快或输慢在llm延迟敏感决策中平衡速度与准确性" class="anchor" aria-label="Permalink: 赢快或输慢：在LLM延迟敏感决策中平衡速度与准确性" href="#赢快或输慢在llm延迟敏感决策中平衡速度与准确性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.19481v1 公告类型：新研究<br>
摘要：大语言模型（LLM）在多样化推理与生成任务中展现出卓越性能，并日益作为智能体被部署于代码生成、推荐系统等动态环境中。然而，高频交易、实时竞技游戏等现实应用场景要求决策必须满足严格延迟约束——更快的响应速度直接转化为更高收益。尽管这种延迟与质量的权衡至关重要，但在基于LLM的智能体领域仍缺乏深入探索。本研究首次系统性地考察了实时决策任务中的这种权衡关系。</p>
<p>为支撑研究，我们引入两个新基准测试：HFTBench（高频交易模拟器）与StreetFighter（竞技游戏平台）。分析表明，最优延迟-质量平衡因任务而异，适当牺牲质量换取更低延迟能显著提升下游表现。为此，我们提出FPX自适应框架，可根据实时需求动态选择模型规模与量化等级。该方法在两项基准测试中均取得最佳表现：在《街头霸王》中获胜率最高提升80%，在交易场景中日收益率最高提升26.52%，有力印证了基于LLM的智能体需要延迟感知的评估与部署策略。</p>
<p>这些成果揭示了延迟敏感策略对于现实世界LLM智能体的关键意义。我们的基准测试已发布于Latency Sensitive Benchmarks平台。</p>
<p>（注：根据学术规范与技术文档特点，翻译时进行了以下处理：</p>
<ol>
<li>专业术语如"quantization level"译为"量化等级"</li>
<li>保留"FPX"等框架名称不译以保持技术准确性</li>
<li>游戏名《Street Fighter》采用官方中文译名《街头霸王》</li>
<li>长复合句拆解为符合中文表达习惯的短句结构</li>
<li>关键数据（80%/26.52%）严格对应原文确保精确性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">任务特定剪枝与LLM筛法：你的任务究竟需要多少参数？</h2><a id="user-content-任务特定剪枝与llm筛法你的任务究竟需要多少参数" class="anchor" aria-label="Permalink: 任务特定剪枝与LLM筛法：你的任务究竟需要多少参数？" href="#任务特定剪枝与llm筛法你的任务究竟需要多少参数"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18350v1 公告类型：新研究<br>
摘要：随着大语言模型（LLM）越来越多地被应用于特定任务（如医疗问答或情感分析）并部署在资源受限的环境中，一个核心问题随之浮现：一项任务究竟需要多少参数量？本研究提出LLM-Sieve——首个面向任务定制的LLM剪枝综合框架，能在各类领域实现20-75%的参数削减，同时仅造成1-5%的准确率下降。与现有方法采用均匀剪枝或仅依赖权重矩阵/输入的独立低秩近似不同，LLM-Sieve具有两大创新：(i) 通过任务感知的联合投影学习更精准地逼近输出行为；(ii) 运用遗传算法为每个矩阵发现差异化的剪枝强度。该框架完全兼容LoRA微调与量化技术，并独特地展现出同一任务领域内跨数据集的强泛化能力。这些成果共同构建了一个实用且鲁棒的机制，可生成更精简的任务专用高性能模型。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了如下处理：</p>
<ol>
<li>"parameter reduction"译为"参数削减"以体现计算效率提升</li>
<li>"Genetic Algorithm"保留专业术语"遗传算法"</li>
<li>"LoRA"作为专有名词未翻译</li>
<li>长难句拆分为符合中文阅读习惯的短句结构</li>
<li>被动语态"are deployed"转化为主动式"部署"）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">$\mu$-MoE：测试时剪枝作为微粒度专家混合模型</h2><a id="user-content-mu-moe测试时剪枝作为微粒度专家混合模型" class="anchor" aria-label="Permalink: $\mu$-MoE：测试时剪枝作为微粒度专家混合模型" href="#mu-moe测试时剪枝作为微粒度专家混合模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：根据技术术语惯例，"$\mu$"通常音译为"微"，代表"micro"；"MoE"是"Mixture-of-Experts"的缩写，在中文领域常直译为"专家混合"；"Test-Time Pruning"采用"测试时剪枝"这一通用译法；"Micro-Grained"译为"微粒度"以准确表达细颗粒度的技术概念。整个标题采用技术文献常见的破折号连接形式，保持学术文本的简洁性和专业性。）</p>
<p>arXiv:2505.18451v1 公告类型：新研究<br>
摘要：为应对大型基础模型巨大的计算需求，无需重新训练的激活感知压缩技术应运而生。然而，由于这些技术依赖校准数据，面对未知下游任务时可能出现领域偏移。通过计算高效的校准过程，激活感知剪枝能够针对每个提示词进行自适应调整，同时实现推理阶段的复杂度降低。我们将该方法构建为微型专家混合系统（$\mu$-MoE）。多项实验表明，$\mu$-MoE能实时动态适应任务/提示词相关的结构化稀疏性。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语处理："foundation models"译为"基础模型"符合ML领域共识；"structured sparsity"译为"结构化稀疏性"是计算机专业术语</li>
<li>技术概念转化："mixture of micro-experts"译为"微型专家混合系统"既保留原意又符合中文表达习惯</li>
<li>句式重构：将英文被动语态"can be executed"转化为中文主动式"能够进行"，符合中文表达规范</li>
<li>数学符号保留：$\mu$-MoE保持原格式，确保学术严谨性</li>
<li>动态表述："on the fly"译为"实时动态"准确传达技术特性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">通过率约束量化和熵编码减少预训练神经网络的存储需求</h2><a id="user-content-通过率约束量化和熵编码减少预训练神经网络的存储需求" class="anchor" aria-label="Permalink: 通过率约束量化和熵编码减少预训练神经网络的存储需求" href="#通过率约束量化和熵编码减少预训练神经网络的存储需求"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18758v1 公告类型：新研究<br>
摘要：神经网络规模的持续增长对嵌入式传感器等资源受限设备提出了严峻挑战。压缩算法若能缩减模型尺寸且保持性能接近原始水平，将有效缓解这些问题。我们提出了一种新颖的训练后压缩框架，通过以下方式将速率感知量化与熵编码相结合：（1）在著名的逐层损失函数中引入二次速率估计项；（2）遵循最优脑外科医生（OBS）方法，为这一改进目标函数提供局部精确解。该方法支持极速解码，且兼容任意量化网格。我们通过在多种计算机视觉网络上进行实证验证，结果表明在保持与流行压缩算法NNCodec相同性能的前提下，比特率降低了20-40%。代码已开源：<a href="https://github.com/Conzel/cerwu%E3%80%82">https://github.com/Conzel/cerwu。</a></p>
<p>（注：根据学术文献翻译规范，对技术术语进行了标准化处理：</p>
<ol>
<li>"rate-aware quantization"译为"速率感知量化"</li>
<li>"entropy coding"译为"熵编码"（信息论标准译法）</li>
<li>"Optimal Brain Surgeon"保留专业缩写OBS并标注全称"最优脑外科医生"方法</li>
<li>"bit rate"译为"比特率"（通信领域通用译法）</li>
<li>被动语态转换为中文主动表述（如"can mitigate"→"将有效缓解"）</li>
<li>长难句拆分重组（如原文最后一句拆分为结果陈述+代码声明两独立句））</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">神经参数搜索：打造更精简的微调模型与更优的迁移效果</h2><a id="user-content-神经参数搜索打造更精简的微调模型与更优的迁移效果" class="anchor" aria-label="Permalink: 神经参数搜索：打造更精简的微调模型与更优的迁移效果" href="#神经参数搜索打造更精简的微调模型与更优的迁移效果"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18713v1 公告类型：新研究<br>
摘要：基础模型及其检查点极大推动了深度学习发展，显著提升了各类应用的性能。然而，经过微调的模型往往局限于特定领域且存在大量冗余。最新研究表明，将剪枝后的微调模型与原始预训练模型结合，既能缓解灾难性遗忘问题，又可降低跨任务合并模型参数时的干扰，同时提升压缩效率。在此背景下，开发高效的微调模型剪枝策略至关重要。我们利用任务向量机制的优势，通过计算微调模型与原始模型的参数差异进行预处理。鉴于不同任务向量子空间对模型性能贡献度存在差异，本文提出了一种名为"神经参数搜索剪枝法"（NPS-Pruning）的创新方法，通过在低秩子空间内搜索任务向量的神经参数来提升剪枝效率。该方法具有三大核心应用价值：通过双模型插值增强知识迁移能力，借助模型合并实现高效知识融合，以及部署压缩模型——在保持接近原始性能的同时显著降低存储成本。在视觉、自然语言处理和多模态基准测试中的大量实验证明，该方法具有卓越的有效性和鲁棒性，能带来显著的性能提升。代码已开源：<a href="https://github.com/duguodong7/NPS-Pruning%E3%80%82">https://github.com/duguodong7/NPS-Pruning。</a></p>
<p>（翻译说明：</p>
<ol>
<li>专业术语处理："Foundation models"译为"基础模型"符合ML领域共识，"fine-tuned"统一译为"微调"</li>
<li>技术概念转译："task vector mechanism"译为"任务向量机制"保持准确性，"low-rank subspaces"译为"低秩子空间"符合线性代数术语</li>
<li>长句拆分：将原文复合长句按中文表达习惯拆分为多个短句，如将方法描述部分分为三个应用场景</li>
<li>被动语态转换："it is crucial"转为主动句式"至关重要"</li>
<li>学术风格保持：使用"鉴于""本文""显著"等符合学术论文表述的词汇</li>
<li>链接处理：保留原始arXiv编号和GitHub链接格式，确保可追溯性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">WINA：基于权重感知的神经元激活加速大语言模型推理</h2><a id="user-content-wina基于权重感知的神经元激活加速大语言模型推理" class="anchor" aria-label="Permalink: WINA：基于权重感知的神经元激活加速大语言模型推理" href="#wina基于权重感知的神经元激活加速大语言模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.19427v1 公告类型：新研究<br>
摘要：随着大语言模型（LLM）计算需求的日益增长，高效的推理与激活策略变得愈发关键。虽然近期研究（如混合专家模型MoE）通过选择性激活提升效率，但这类方法需专门训练。相比之下，无需训练的稀疏激活方法凭借即插即用设计，具有更广泛的适用性和卓越的资源效率。然而，现有方法大多仅依赖隐藏状态幅值决定激活，导致高近似误差和次优推理精度。为突破这些局限，我们提出WINA（权重感知神经元激活）——一种新颖、简洁且无需训练的稀疏激活框架，其同时考量隐藏状态幅值与权重矩阵列向$\ell_2$范数。理论证明该稀疏化策略能获得最优近似误差界，其理论保障严格优于现有技术。实证表明，在相同稀疏度下，WINA在多样化LLM架构和数据集上的平均性能最高可超越前沿方法（如TEAL）2.94%。这些成果使WINA成为LLM推理中无需训练稀疏激活的新性能标杆，推动了该领域发展，并为高效推理建立了强基线。源代码已发布于<a href="https://github.com/microsoft/wina%E3%80%82">https://github.com/microsoft/wina。</a></p>
<p>（注：根据技术论文翻译规范，关键术语如"sparse activation"统一译为"稀疏激活"，数学符号$\ell_2$-norms保留原格式，理论性表述采用"最优近似误差界"等严谨措辞，同时保持长句的汉语意合特征进行合理切分。）</p>
<div class="markdown-heading"><h2 class="heading-element">NSNQuant：一种免校准低比特键值缓存向量量化的双重归一化方法</h2><a id="user-content-nsnquant一种免校准低比特键值缓存向量量化的双重归一化方法" class="anchor" aria-label="Permalink: NSNQuant：一种免校准低比特键值缓存向量量化的双重归一化方法" href="#nsnquant一种免校准低比特键值缓存向量量化的双重归一化方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18231v1 公告类型：新研究<br>
摘要：大语言模型（LLM）推理通常具有较高的内存需求，尤其是在处理大批量数据和长序列时，关键值缓存（KV cache）的庞大容量是主要原因。虽然近期采用向量量化（VQ）技术缓解了这一问题，但我们发现现有方法因依赖校准数据集而容易受到分布偏移的影响。为此，我们提出了NSNQuant——一种免校准的向量量化技术，专为KV cache的低比特压缩而设计。该方法通过三步变换（1）词元级归一化（Normalize）、（2）通道级中心化（Shift）、（3）二次词元级归一化（Normalize）结合Hadamard变换，将词元分布有效对齐标准正态分布。这种对齐方式使得仅需单个可复用码本即可实现鲁棒的免校准向量量化。大量实验表明，NSNQuant在1比特和2比特设置下均显著优于现有方法，不仅具有强泛化性，还能实现较全精度基线最高3倍的吞吐量提升。</p>
<p>（注：根据学术文献翻译规范，术语如"Hadamard transform"保留专业译名"Hadamard变换"；"throughput gain"译为"吞吐量提升"以符合计算机领域表述习惯；数学符号"3×"转换为"3倍"并保留原文的$\times$排版格式；技术名词如"token-wise normalization"采用"词元级归一化"的译法以保持与NLP领域术语一致性。）</p>
<div class="markdown-heading"><h2 class="heading-element">ELDeR：通过数据驱动的逐层正则化剪枝实现高效大型语言模型</h2><a id="user-content-elder通过数据驱动的逐层正则化剪枝实现高效大型语言模型" class="anchor" aria-label="Permalink: ELDeR：通过数据驱动的逐层正则化剪枝实现高效大型语言模型" href="#elder通过数据驱动的逐层正则化剪枝实现高效大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18232v1 公告类型：新研究<br>
摘要：大型语言模型（LLM）在许多领域的应用正因其高昂的计算与内存成本而受到制约。近期研究表明，LLM具有稀疏性特征，可用于模型剪枝。传统剪枝方法通常遵循"先剪枝后微调"的范式，但由于被剪枝部分仍包含有价值信息，静态移除这些参数而不更新剩余参数往往会导致不可逆的性能下降，需要耗费大量计算资源进行恢复性微调（RFT）以维持模型性能。为解决这一问题，我们提出创新范式：先正则化，再剪枝。基于此范式，我们开发了ELDeR方法——通过数据驱动的正则化分层剪枝构建高效LLM。该方法首先为每个Transformer层的输出乘以初始权重，随后利用少量数据以简单方式迭代学习各层权重。之后对权重较小层的输入输出差异施加正则化约束，迫使信息转移至保留层中。相较于直接剪枝，ELDeR有效减少了参数直接移除造成的信息损失，从而更好地保留了模型的语言建模能力。实验结果表明，ELDeR在显著降低RFT计算成本的同时，其性能优于现有的强分层结构化剪枝方法。由于ELDeR采用分层剪枝策略，其端到端加速效果显著，为构建高效LLM提供了极具前景的技术路径。</p>
<p>（注：根据学术论文摘要的文体特征，译文在保持专业术语准确性的同时，通过以下处理增强专业性：</p>
<ol>
<li>技术概念统一："fine-tuning"译为"微调"，"regularization"译为"正则化"</li>
<li>被动语态转化：将英文被动结构转换为中文主动表述</li>
<li>长句拆分：将原文复合长句按中文表达习惯分解为逻辑连贯的短句</li>
<li>专业表述："irreversible performance degradation"译为"不可逆的性能下降"</li>
<li>机构名称保留：ELDeR作为方法名称保持原文大写形式）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">受限边缘AI部署：大语言模型压缩中的微调与蒸馏对比</h2><a id="user-content-受限边缘ai部署大语言模型压缩中的微调与蒸馏对比" class="anchor" aria-label="Permalink: 受限边缘AI部署：大语言模型压缩中的微调与蒸馏对比" href="#受限边缘ai部署大语言模型压缩中的微调与蒸馏对比"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18166v1 公告类型：新论文<br>
摘要：现代基础模型通常通过结构化剪枝与再训练的联合手段进行压缩，以满足边缘部署场景下对计算能力、存储空间和连接性的严苛限制。虽然最先进的剪枝方案针对整个Transformer架构，但我们采用了一种简单的层间L2范数剪枝方法，仅对MLP模块进行固定基线处理。本研究重点不在于实现最大压缩率，而在于分离再训练损失函数的影响：(i) 使用交叉熵进行微调（L2PFT）需要标注数据；(ii) 利用KL散度进行自蒸馏（L2PSD）仅需教师模型输出的逻辑值（无需标注）。我们在适用于边缘网络典型间歇性或拒绝连接场景的OLMo2-7B-SFT模型（面向CommonsenseQA任务）上评估两种流程。在相同剪枝进度下，基于KL散度的蒸馏在测试准确率上达到或超越交叉熵微调效果，这表明即便采用基础的仅MLP剪枝策略，损失函数的选择仍会显著影响资源受限环境下压缩模型的恢复效果。</p>
<p>（注：根据学术规范，技术术语保持英文缩写如"MLP"、"KL"等；"arXiv"作为专有名词保留原格式；长句按中文表达习惯拆分；被动语态转换为主动表述；专业概念如"Self-Distillation"采用学界通用译法"自蒸馏"）</p>
<div class="markdown-heading"><h2 class="heading-element">潜在语言模型（LatentLLM）：注意力感知的联合张量压缩</h2><a id="user-content-潜在语言模型latentllm注意力感知的联合张量压缩" class="anchor" aria-label="Permalink: 潜在语言模型（LatentLLM）：注意力感知的联合张量压缩" href="#潜在语言模型latentllm注意力感知的联合张量压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18413v1 公告类型：新研究<br>
摘要：现代基础模型（如大语言模型LLMs和大规模多模态模型LMMs）需要消耗巨大的计算与内存资源。我们提出了一种新框架，可将此类LLMs/LMMs转化为降维的潜在结构。该方法将局部激活感知的张量分解扩展为全局注意力感知的联合张量分解。当降低潜在维度以实现计算/内存高效的LLMs/LMMs时，我们的框架能显著提升模型精度，超越现有模型压缩方法。我们在包括多模态推理任务在内的多个基准测试中验证了其优势。</p>
<p>（注：根据学术文献翻译规范，专业术语保持英文缩写形式；"latent structure"译为"潜在结构"以保持机器学习领域的术语一致性；"attention-aware"采用"注意力感知"这一通用译法；通过拆分英文长句为中文短句结构，如将"when reducing..."处理为条件状语分句；"benchmark"根据语境译为"基准测试"而非字面直译）</p>
<div class="markdown-heading"><h2 class="heading-element">核心匹配：一种协同自适应的稀疏推理框架，结合令牌与神经元剪枝技术，全面加速视觉-语言模型</h2><a id="user-content-核心匹配一种协同自适应的稀疏推理框架结合令牌与神经元剪枝技术全面加速视觉-语言模型" class="anchor" aria-label="Permalink: 核心匹配：一种协同自适应的稀疏推理框架，结合令牌与神经元剪枝技术，全面加速视觉-语言模型" href="#核心匹配一种协同自适应的稀疏推理框架结合令牌与神经元剪枝技术全面加速视觉-语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.19235v1 公告类型：新研究<br>
摘要：视觉语言模型（VLMs）在多样化任务中表现卓越，但存在推理时的高时间与内存成本问题。令牌稀疏化通过优化令牌使用效率缓解计算冗余，而神经元稀疏化则能降低高维运算开销，二者均为提升效率提供了可行方案。近期这两种稀疏化范式基本平行发展，学界普遍假设它们独立运作。然而一个根本性却未被充分探索的问题始终存在：它们是否真正相互隔离？抑或存在尚未揭示的深层关联机制？本文首次对该问题展开系统性研究。通过引入并分析核心神经元与核心令牌的匹配机制，我们发现推理过程中的关键神经元与令牌会相互影响并形成正向强化。基于这一发现，我们提出CoreMatching——一个协同自适应的稀疏推理框架，该框架通过利用令牌与神经元稀疏化的协同效应来提升推理效率。理论分析与效率评估表明，该方法在十项图像理解任务和三种硬件设备上均超越现有最优基线。在NVIDIA Titan Xp上尤为显著，实现了5倍浮点运算量削减和10倍整体加速。代码已发布于<a href="https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main%E3%80%82">https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main。</a></p>
<p>（注：根据学术文献翻译规范，对部分术语进行了统一处理：</p>
<ol>
<li>"token"译为"令牌"而非"标记"，以保持计算机领域术语一致性</li>
<li>"neuron"译为"神经元"而非"神经单元"，符合神经网络领域惯例</li>
<li>"FLOPs"保留英文缩写形式，中文领域普遍采用此写法</li>
<li>长难句按中文表达习惯进行了分句重组，如将原文条件状语从句转换为更符合中文逻辑的因果表述）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">通信高效的多设备推理加速用于Transformer模型</h2><a id="user-content-通信高效的多设备推理加速用于transformer模型" class="anchor" aria-label="Permalink: 通信高效的多设备推理加速用于Transformer模型" href="#通信高效的多设备推理加速用于transformer模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.19342v1 公告类型：新成果<br>
摘要：Transformer模型虽驱动着众多AI应用，却因推理延迟较高而难以适用于实时场景。多设备推理可通过并行计算降低延迟，但现有方法依赖高带宽的跨设备通信，在带宽受限环境中实用性不足。我们提出ASTRA框架——通过创新性地整合序列并行与混合精度注意力机制（专为最小化跨设备通信设计），实现了高效的通信优化推理加速。ASTRA采用向量量化压缩非局部词元嵌入，并通过"噪声增强量化"和"分布式分类标记"两项优化保持任务精度。在视觉与自然语言处理任务中对ViT和GPT2的实验表明：ASTRA在低至10 Mbps带宽下，相比单设备推理可实现最高2.64倍加速，较现有最优多设备推理方案更获得最高15.25倍加速。项目已开源于<a href="https://github.com/xl1990/Astra%E3%80%82">https://github.com/xl1990/Astra。</a></p>
<p>（翻译说明：</p>
<ol>
<li>技术术语统一处理："vector quantization"译为"向量量化"，"Mixed-Precision Attention"保留核心概念译为"混合精度注意力机制"</li>
<li>被动语态转化："are limited"转为主动句式"难以适用"，符合中文表达习惯</li>
<li>长句拆分：将原文复合长句分解为多个短句，如将通信优化策略拆分为框架介绍+技术手段</li>
<li>数字规范：保留精确数值表达，X倍速统一为中文"倍"字表述</li>
<li>链接保留：GitHub地址完整保留，符合学术文献惯例</li>
<li>专业表述："state-of-the-art"译为"现有最优方案"既准确又简洁）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">LoTA-QAF：面向量化感知微调的无损三元自适应</h2><a id="user-content-lota-qaf面向量化感知微调的无损三元自适应" class="anchor" aria-label="Permalink: LoTA-QAF：面向量化感知微调的无损三元自适应" href="#lota-qaf面向量化感知微调的无损三元自适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18724v1 公告类型：新论文<br>
摘要：量化和微调对于在资源受限的边缘设备上部署大语言模型（LLM）至关重要。然而，对量化模型进行微调存在显著挑战，主要源于以下三点：首先，低精度量化权重（如4位）与高精度适配权重（如16位）之间的数据类型不匹配，这限制了量化权重在推理过程中本应提供的计算效率优势；其次，当将这些高精度适配权重合并到低精度量化权重时，由于适配权重往往需要近似或截断处理，可能导致精度下降；第三，据我们所知，现有方法均不支持在调整所有量化权重的同时无损合并适配权重。为应对这些挑战，我们提出了面向量化感知微调的无损三元适配方法（LoTA-QAF）。这是一种专为量化大语言模型设计的新型微调方法，能够将三元适配权重无损合并到量化权重中，并调整所有量化权重。LoTA-QAF通过以下机制实现：i）定制设计的三元适配（TA），将三元权重与量化网格对齐，并利用这些三元权重调整量化权重；ii）基于TA的机制实现适配权重的无损合并；iii）采用三元符号梯度下降（t-SignSGD）更新TA权重。我们将LoTA-QAF应用于Llama-3.1/3.3和Qwen-2.5模型系列，并在多个下游任务上验证其有效性。在MMLU基准测试中，我们的方法有效恢复了量化模型的性能，较16位LoRA最高提升5.14%；在任务特定微调场景下，16位LoRA虽能取得更优结果，但LoTA-QAF仍优于其他方法。</p>
<p>（注：根据学术规范，技术术语保持原文大小写格式如"LoTA-QAF"、"t-SignSGD"等；模型名称"Llama-3.1/3.3"和"Qwen-2.5"保留数字编号；百分数"5.14%"采用中文排版规范；长句按中文表达习惯拆分为短句，保持技术准确性同时符合中文科技论文语体。）</p>
<div class="markdown-heading"><h2 class="heading-element">边缘端大规模语言模型的分布式推理</h2><a id="user-content-边缘端大规模语言模型的分布式推理" class="anchor" aria-label="Permalink: 边缘端大规模语言模型的分布式推理" href="#边缘端大规模语言模型的分布式推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.18164v1 公告类型：新成果<br>
摘要：本文提出面向大语言模型的模型分布式推理框架（MDI-LLM），该创新架构旨在将尖端大语言模型（LLM）部署于边缘低功耗设备群。其核心实现方式是将模型划分为多个分区，并分配到网络中的不同设备/节点上。这些节点通过设备间直连链路交换中间激活向量，从而实现协同计算。为提升计算效率，我们首创"循环流水线并行"技术，既能减少各设备空闲时间，又能在生成多组文本序列时实现并行推理。通过聚合多台边缘设备的联合计算资源，MDI-LLM使得部署超越单设备内存容量的大模型成为可能，让低成本硬件也能执行推理任务。此外，随着参与设备数量增加，MDI-LLM能显著提升令牌生成吞吐量，同时降低单台设备的内存占用。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语处理：MDI-LLM采用中文全称+括号标注原缩写的形式，符合学术文献惯例</li>
<li>技术概念转化："device-to-device links"译为"设备间直连链路"准确传达技术特征</li>
<li>长句拆分：将原文复合长句拆分为符合中文表达习惯的短句结构</li>
<li>动态对等："low-power devices at the edge"译为"边缘低功耗设备群"通过量词增强可读性</li>
<li>被动语态转化："are then assigned"译为主动态的"分配到"更符合中文表达</li>
<li>技术隐喻保留："pipeline parallelism"保留"流水线"核心意象同时添加"循环"前缀准确传达创新性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">高效记忆视觉自回归建模：基于尺度感知的键值缓存压缩技术</h2><a id="user-content-高效记忆视觉自回归建模基于尺度感知的键值缓存压缩技术" class="anchor" aria-label="Permalink: 高效记忆视觉自回归建模：基于尺度感知的键值缓存压缩技术" href="#高效记忆视觉自回归建模基于尺度感知的键值缓存压缩技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译时做了以下处理：</p>
<ol>
<li>"Memory-Efficient"译为"高效记忆"而非直译"内存高效"，更符合中文技术文献表述习惯</li>
<li>"Scale-Aware"译为"尺度感知"，准确传达多尺度特征处理的含义</li>
<li>"KV Cache Compression"采用专业术语直译"键值缓存压缩"，保留技术准确性</li>
<li>整体采用"技术"作为后缀，符合中文论文标题常用形式</li>
<li>使用冒号替代原介词"with"，更符合中文标题语法结构）</li>
</ol>
<p>arXiv:2505.19602v1 公告类型：新研究<br>
摘要：视觉自回归（VAR）建模因其创新的"下一尺度预测"方法获得广泛关注，该方法在效率、可扩展性和零样本泛化能力上实现了显著提升。然而，VAR固有的"由粗到细"方法会导致推理过程中键值缓存（KV cache）呈指数级增长，引发巨大的内存消耗和计算冗余。针对这些瓶颈问题，我们提出了ScaleKV——一个专为VAR架构设计的创新键值缓存压缩框架。ScaleKV基于两个关键发现：不同Transformer层对缓存的需求存在差异，且不同尺度下的注意力模式具有显著区别。基于这些洞察，ScaleKV将Transformer层划分为两大功能组：草图生成层（drafters）和精修层（refiners）。草图生成层的注意力分散于多个尺度，因此需要更大的缓存容量；而精修层则聚焦于当前令牌映射以处理局部细节，因而所需缓存容量大幅降低。ScaleKV通过识别尺度特定的草图生成层与精修层，为每个尺度定制差异化缓存管理策略，从而优化多尺度推理流程。在顶尖文本生成图像VAR模型家族Infinity上的评估表明，我们的方法在保持像素级保真度的同时，能将键值缓存内存需求有效降低至原需求的10%。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了专业化处理：</p>
<ol>
<li>"KV cache"保留技术缩写并添加中文注释</li>
<li>"drafters/refiners"采用"草图生成层/精修层"的意译以体现功能差异</li>
<li>"pixel-level fidelity"译为"像素级保真度"符合计算机视觉领域表述习惯</li>
<li>长难句按中文表达习惯进行了分句处理，如将"which yields..."独立成句译为"该方法..."）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">FP4 全程护航：大语言模型的全量化训练</h2><a id="user-content-fp4-全程护航大语言模型的全量化训练" class="anchor" aria-label="Permalink: FP4 全程护航：大语言模型的全量化训练" href="#fp4-全程护航大语言模型的全量化训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.19115v1 公告类型：新研究<br>
摘要：我们首次实现了大型语言模型（LLM）的全量化训练（FQT），在高达2000亿标记的数据集上，主要使用4位浮点（FP4）精度对权重、激活值和梯度进行量化。我们深入研究了FP4的关键设计选择，包括块大小、缩放格式和舍入方法。分析表明，NVFP4格式（即每16个E2M1格式的FP4数值共享一个E4M3格式的缩放因子）能提供最优结果。我们采用随机舍入处理反向传播和参数更新，前向传播时使用就近舍入以增强稳定性。此外，我们发现量化训练有效性的理论及实证阈值：当梯度范数低于量化噪声的约$\sqrt{3}$倍时，量化训练效果会下降。基于这些发现，我们在256个英特尔Gaudi2加速器上成功训练了一个70亿参数的模型。最终FP4训练的模型在下游任务中表现与标准BF16基线相当，证实FP4训练是大规模LLM训练中实用且高效的方法。参考实现详见<a href="https://github.com/Anonymous1252022/fp4-all-the-way">https://github.com/Anonymous1252022/fp4-all-the-way</a> 。</p>
<p>（注：根据学术惯例，"E2M1"等浮点格式标注保留英文缩写；技术术语如"stochastic rounding"译为"随机舍入"；超链接保留原格式；数学符号$\sqrt{3}$直接沿用；机构名"Intel Gaudi2"不作翻译）</p>
<div class="markdown-heading"><h2 class="heading-element">张量化是一种强大但尚未被充分探索的工具，用于神经网络的压缩与可解释性提升。</h2><a id="user-content-张量化是一种强大但尚未被充分探索的工具用于神经网络的压缩与可解释性提升" class="anchor" aria-label="Permalink: 张量化是一种强大但尚未被充分探索的工具，用于神经网络的压缩与可解释性提升。" href="#张量化是一种强大但尚未被充分探索的工具用于神经网络的压缩与可解释性提升"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.20132v1 公告类型：新论文<br>
摘要：神经网络张量化是指将其部分或全部稠密权重矩阵重塑为高阶张量，并利用低秩张量网络分解进行近似。该技术已展现出作为大规模神经网络模型压缩策略的潜力。然而，尽管实证结果令人鼓舞，张量化神经网络（TNNs）在主流深度学习中仍未得到充分利用。在这篇立场论文中，我们针对TNNs的潜力与当前局限提出见解。我们认为TNNs代表了一个强大但尚未充分探索的深度学习框架——这一框架值得工程界和理论界给予更多关注。除压缩优势外，我们强调TNNs作为一类具有独特扩展特性和更高可解释性的灵活架构的价值。TNNs的核心特征在于键合索引的存在，这些索引引入了传统网络所没有的新潜在空间。此类内部表征或能更深入地揭示跨层特征的演化过程，有望推动机械可解释性研究的发展。最后，我们提出了若干关键研究方向，旨在克服现代深度学习工作流中扩展和应用TNNs的实际障碍。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>