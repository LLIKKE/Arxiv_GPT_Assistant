<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">SpecOffload：释放潜在GPU能力，助力资源受限设备上的LLM推理</h2><a id="user-content-specoffload释放潜在gpu能力助力资源受限设备上的llm推理" class="anchor" aria-label="Permalink: SpecOffload：释放潜在GPU能力，助力资源受限设备上的LLM推理" href="#specoffload释放潜在gpu能力助力资源受限设备上的llm推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在资源受限设备上实现高效的大语言模型推理面临着计算与内存利用方面的重大挑战。由于GPU内存有限，现有系统需将模型权重卸载至CPU内存，导致CPU与GPU间产生大量I/O开销。这引发两大效率瓶颈：（1）GPU核心利用率低下，常因等待数据加载而处于闲置状态；（2）GPU内存对性能影响微弱，即使缩减其容量对整体吞吐量也几无影响。本文提出SpecOffload——一种将推测式解码嵌入卸载流程的高吞吐推理引擎。其核心思想是释放GPU潜藏资源来存储和执行用于推测式解码的草稿模型，从而以近乎零额外成本加速推理。为此，我们精心编排了卸载流水线中目标模型与草稿模型在推测式解码时的交错执行，并提出管理张量放置及优化参数选择的规划器。相较最佳基线方案，SpecOffload将GPU核心利用率提升4.49倍，推理吞吐量提高2.54倍。代码已开源：<a href="https://github.com/MobiSense/SpecOffload">https://github.com/MobiSense/SpecOffload</a></p>
<div class="markdown-heading"><h2 class="heading-element">规模越大，乐趣越多？无线边缘网络中高效大型AI模型推理</h2><a id="user-content-规模越大乐趣越多无线边缘网络中高效大型ai模型推理" class="anchor" aria-label="Permalink: 规模越大，乐趣越多？无线边缘网络中高效大型AI模型推理" href="#规模越大乐趣越多无线边缘网络中高效大型ai模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译采用意译结合直译的方式处理标题中的设问句式。"The Larger the Merrier"套用英语谚语"The more the merrier"（人多热闹）的变体，结合上下文转化为中文常见的设问表达。"Wireless Edge Networks"采用行业通用译法"无线边缘网络"，"Efficient Large AI Model Inference"通过语序调整译为"高效大型AI模型推理"，既保持专业术语准确性又符合中文技术文献表述习惯。问号后的副标题通过增加"中"字完善语法结构，使整个标题形成"设问主标+解释性副标"的标准学术标题格式。）</p>
<p>大型人工智能模型（LAIM）服务需求的快速增长正推动着从传统云端推理向边缘推理的范式转变，以满足低延迟和隐私保护的应用需求。其中，边缘设备协同推理——将LAIM分割部署于终端设备与服务器之间——已成为无线网络中资源高效执行LAIM的重要策略。本文研究了一种剪枝感知的LAIM协同推理方案：通过剪枝预训练模型并将其分割为终端子模型与服务器子模型进行部署。在理论分析层面，我们首先证明了模型输出失真度受限于其参数失真度的上界，继而基于率失真理论推导出参数失真度的下界，从而解析剪枝率与协同推理性能的关系。基于此，我们建立了在系统延迟、能耗及资源约束下联合优化剪枝率、传输功率与计算频率的LAIM协同推理失真边界最小化问题，并提出高效算法求解这一高度非凸问题。仿真实验验证了所提方案的有效性：参数失真度被证实能可靠约束输出失真度；相比全终端推理或全服务器推理等基准方案，联合剪枝与资源管理设计在推理性能、系统延迟与能耗的权衡中表现更优。研究还表明，在异构资源受限的边缘环境中，模型分割点对系统性能优化具有关键影响。</p>
<div class="markdown-heading"><h2 class="heading-element">模拟基础模型</h2><a id="user-content-模拟基础模型" class="anchor" aria-label="Permalink: 模拟基础模型" href="#模拟基础模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>模拟内存计算（AIMC）是一种突破传统冯·诺依曼架构限制、提升神经网络推理速度与能效的新型计算范式。然而该技术也带来了根本性挑战，包括计算噪声问题以及对输入输出量化的严格约束。受限于这些因素，现有大型语言模型（LLM）在基于AIMC的硬件上运行时，无法实现4比特级别的性能表现。尽管先前研究已尝试在小型视觉模型上弥补这一精度差距，但尚未出现适用于基于万亿级语料预训练LLM的通用解决方案。</p>
<p>本研究提出了一种可扩展的通用方法，能使LLM在存在噪声的低精度模拟硬件上稳定运行。我们的方法使前沿模型——包括Phi-3-mini-4k-instruct和Llama-3.2-1B-Instruct——在模拟噪声和量化约束下，仍能保持与4比特权重、8比特激活基线相当的性能。值得注意的是，该训练方法的副产品是：模拟基础模型可被量化用于低精度数字硬件的推理。实验还表明，相较于采用4比特权重和8比特静态输入量化的模型，我们的模型在测试时计算扩展方面表现更优。</p>
<p>这项工作弥合了大容量LLM与高效模拟硬件之间的鸿沟，为构建高能效基础模型开辟了新路径。相关代码已开源：<a href="https://github.com/IBM/analog-foundation-models">https://github.com/IBM/analog-foundation-models</a></p>
<p>（注：技术术语处理说明：</p>
<ol>
<li>"von Neumann-based architectures"译为"冯·诺依曼架构"，采用计算机领域通用译法</li>
<li>"4-bit-level performance"译为"4比特级别性能"，保持计量单位统一性</li>
<li>"test-time compute scaling"译为"测试时计算扩展"，准确传达动态计算资源调整的语境</li>
<li>模型名称保留英文原名，符合AI领域惯例）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">BINGO：一种缩减神经网络规模的新型剪枝机制</h2><a id="user-content-bingo一种缩减神经网络规模的新型剪枝机制" class="anchor" aria-label="Permalink: BINGO：一种缩减神经网络规模的新型剪枝机制" href="#bingo一种缩减神经网络规模的新型剪枝机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>过去十年间，机器学习的应用呈现指数级增长。模型复杂度远超以往，规模膨胀至惊人程度，承载着数百万个权重参数。但遗憾的是，大型模型成为行业标杆的同时，也意味着其训练和运营成本往往高达数百万美元。这种高昂开支不仅令企业不堪重负，更将非富裕群体阻挡在技术创新的门槛之外，最终迫使消费者为人工智能服务支付更高溢价。当前采用的模型剪枝方法（如迭代幅度剪枝）虽能保持较高精度，但需要消耗大量计算资源的迭代训练流程，对环境造成沉重负担。为此，我们提出BINGO解决方案。该技术在训练过程中逐次分析神经网络的特定子集，精准评估每个权重参数对模型准确性的贡献程度。训练完成后，BINGO会为每个权重生成重要性评分，从而实现一次性剪枝冗余参数。相比现有方法，BINGO以更低计算成本实现了精度无损的模型压缩，为人工智能发展开辟了新路径——技术进步不再必然伴随模型体量的膨胀。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>