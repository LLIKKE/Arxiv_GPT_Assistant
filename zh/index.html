<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">大语言模型量化的几何视角：GPTQ即巴拜最近平面算法</h2><a id="user-content-大语言模型量化的几何视角gptq即巴拜最近平面算法" class="anchor" aria-label="Permalink: 大语言模型量化的几何视角：GPTQ即巴拜最近平面算法" href="#大语言模型量化的几何视角gptq即巴拜最近平面算法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>将大型语言模型（LLM）的权重从16位量化至更低比特宽度，是将海量Transformer模型部署到经济型加速器上的实际解决方案。GPTQ作为LLM规模单次训练后量化的标准方法之一崭露头角，但其内部机制被描述为一系列临时性代数更新步骤，掩盖了几何意义与最坏情况保障。本研究表明：当对线性层执行从后向前（即从末维到首维）的量化时，GPTQ在数学上等同于Babai最近平面算法——该算法用于求解由输入特征海森矩阵定义的格上经典最近向量问题（CVP）。这一等价性基于严密的数学论证，并产生两个分析性推论：(i) GPTQ误差传播步骤获得直观的几何解释；(ii) 在无剪裁条件下，GPTQ继承了Babai算法的误差上界。这些发现不仅为GPTQ奠定了坚实的理论基础，更开启了将格算法数十年研究进展引入亿级参数模型量化算法设计的新篇章。</p>
<div class="markdown-heading"><h2 class="heading-element">大型语言模型量化技术的综合评估</h2><a id="user-content-大型语言模型量化技术的综合评估" class="anchor" aria-label="Permalink: 大型语言模型量化技术的综合评估" href="#大型语言模型量化技术的综合评估"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>对于大型语言模型（LLM）而言，训练后量化（PTQ）能显著降低内存占用和计算开销。模型量化是一个快速发展的研究领域，尽管许多论文报道了突破性性能，但由于量化方法通常包含多个组件，这些研究可能未在相同实验条件下进行对比。此外，深入理解现有方法之间的理论关联至关重要。为弥合这些差距，我们对前沿方法进行了系统性梳理，并在统一实验框架下开展全面评估以确保公平比较。据我们所知，这种公平而全面的调研仍具有关键意义却尚未充分探索。</p>
<p>为解析理论关联，我们将已发表的量化方法解耦为两个步骤：预量化变换与量化误差补偿。前者定义为量化前的预处理步骤，通过减少异常值影响使数据分布更平缓，从而提升量化适用性；后者则指抵消量化过程中引入误差的技术手段，以增强模型性能。我们评估并量化了各方法组件的实际影响，同时针对新兴的MXFP4数据格式及其性能表现进行分析。</p>
<p>实验结果表明：在预量化变换阶段，优化旋转与缩放策略能带来最佳性能；而在误差补偿环节，低秩补偿与GPTQ的组合使用偶尔能超越单独使用GPTQ的效果。此外，我们对MXFP4量化的潜力进行探索，发现INT4最优的预量化变换策略并不能直接推广至MXFP4，这一发现为后续研究提供了新思路。</p>
<div class="markdown-heading"><h2 class="heading-element">Squeeze10-LLM：通过分阶段混合精度量化方法将大语言模型权重压缩10倍</h2><a id="user-content-squeeze10-llm通过分阶段混合精度量化方法将大语言模型权重压缩10倍" class="anchor" aria-label="Permalink: Squeeze10-LLM：通过分阶段混合精度量化方法将大语言模型权重压缩10倍" href="#squeeze10-llm通过分阶段混合精度量化方法将大语言模型权重压缩10倍"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>部署大型语言模型（LLM）面临巨大参数量和高计算成本的挑战。超低位量化能显著降低存储需求并加速推理，但极端压缩（即平均位宽≤2比特）通常会导致性能严重下降。为此，我们提出Squeeze10-LLM方案，将16比特LLM的权重有效"压缩"10倍。具体而言，该方案采用分阶段混合精度训练后量化（PTQ）框架，通过将80%权重量化为1比特、20%量化为4比特，实现平均1.6比特/权重的突破。我们创新性地引入两项核心技术：后二值化激活鲁棒性（PBAR）和全信息激活监督（FIAS）。PBAR作为精细化权重重要性度量标准，能评估量化对激活值的影响，从而提升低位宽环境下的精度；FIAS则通过在量化过程中保留完整的激活信息，有效抑制误差在层级间的累积传播。基于LLaMA和LLaMA2的实验表明，Squeeze10-LLM在2比特以下权重专用量化领域实现最先进性能，在六个零样本分类任务上平均准确率从43%提升至56%，较现有PTQ方法取得显著突破。相关代码将在论文发表时开源。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>