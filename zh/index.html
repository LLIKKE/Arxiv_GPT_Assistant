<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">xLSTM 7B：一款用于快速高效推理的循环大语言模型</h2><a id="user-content-xlstm-7b一款用于快速高效推理的循环大语言模型" class="anchor" aria-label="Permalink: xLSTM 7B：一款用于快速高效推理的循环大语言模型" href="#xlstm-7b一款用于快速高效推理的循环大语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.13427v1 公告类型：新研究  摘要：近期，在利用大型语言模型（LLMs）解决推理、数学和编码问题方面取得的突破，得益于在推理阶段投入的大量计算资源。因此，推理速度成为LLM架构中最关键的特性之一，市场对高效且推理迅速的LLM需求日益增长。最近，基于xLSTM架构构建的LLM作为Transformer的有力替代方案崭露头角，它们提供了随序列长度线性增长的计算扩展性和恒定的内存使用量，这两点都是高效推理极为理想的特性。然而，这类基于xLSTM的LLM尚未被扩展到更大规模模型，并在推理速度与效率方面进行充分评估和比较。本研究中，我们推出了xLSTM 7B，一个拥有70亿参数的LLM，它结合了xLSTM架构的优势，并针对快速高效推理进行了专门优化。实验结果显示，xLSTM 7B在下游任务上的表现与同类规模LLM相当，同时相较于基于Llama和Mamba的LLM，提供了显著更快的推理速度和更高的效率。这些成果确立了xLSTM 7B作为当前最快、最高效的70亿参数LLM的地位，为需要大量测试时计算的任务提供了解决方案。我们的工作凸显了xLSTM作为依赖大量LLM推理方法基础架构的潜力。我们的模型权重、模型代码及训练代码均已开源。</p>
<div class="markdown-heading"><h2 class="heading-element">ZO2：针对GPU内存有限的超大规模语言模型的可扩展零阶微调方法</h2><a id="user-content-zo2针对gpu内存有限的超大规模语言模型的可扩展零阶微调方法" class="anchor" aria-label="Permalink: ZO2：针对GPU内存有限的超大规模语言模型的可扩展零阶微调方法" href="#zo2针对gpu内存有限的超大规模语言模型的可扩展零阶微调方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.12668v1 公告类型：新研究  摘要：微调大型预训练语言模型（LLMs）通常需要大量的GPU内存。随着模型规模的扩大，传统的一阶优化器如SGD在存储前向和后向阶段中的激活值和梯度时面临显著的内存需求增加问题。相比之下，零阶（ZO）技术仅通过前向操作即可计算梯度，无需存储激活值。此外，通过利用CPU的能力，可以增强单个GPU的内存和处理能力。我们提出了一个新颖的框架，ZO2（零阶卸载），用于在有限的GPU内存下高效地进行LLMs的零阶微调。我们的框架根据需要动态地在CPU和GPU之间转移模型参数，优化计算流程，并通过最小化停机时间最大化GPU的使用率。这种参数调整与ZO的双前向操作的结合减少了不必要的数据移动，提高了微调效率。此外，我们的框架支持在AMP模式下采用创新的低比特精度方法，以简化CPU和GPU之间的数据交换。采用这种方法，我们能够在仅18GB的GPU上微调超大规模模型，如拥有超过1750亿参数的OPT-175B——这是传统方法无法企及的成就。而且，与标准的零阶方法相比，我们的框架在几乎没有额外时间开销的情况下实现了这些成果，且完全没有精度损失。ZO2的代码已在<a href="https://github.com/liangyuwang/zo2%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/liangyuwang/zo2开源。</a></p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>