<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">高层注意力剪枝与重缩放</h2><a id="user-content-高层注意力剪枝与重缩放" class="anchor" aria-label="Permalink: 高层注意力剪枝与重缩放" href="#高层注意力剪枝与重缩放"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>剪枝是一种高效压缩大语言模型（LLM）的方法，能显著降低推理延迟。然而，传统免训练的结构化剪枝方法通常采用启发式度量标准，不加区分地移除所有剪枝层中的部分注意力头，而忽略了这些注意力头在网络架构中的位置特性。本研究提出了一种新颖的剪枝算法，其核心策略是优先修剪模型高层中的注意力头。鉴于移除注意力头会改变词元表征的幅度，我们引入了自适应重缩放参数，通过校准剪枝后的表征尺度来抵消这种影响。我们在包括LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B和Gemma2-9B在内的多种大语言模型上进行了全面实验，评估范围涵盖27个数据集的生成式与判别式任务。实验结果一致表明，我们的方法优于现有结构化剪枝技术，尤其在生成任务中较现有基线模型展现出显著优势。</p>
<p>（注：LLaMA3.1-8B等模型名称保留不译符合技术文献惯例；"token representations"译为"词元表征"既保持术语准确性又符合NLP领域中文表述习惯；"adaptive rescaling parameter"采用"自适应重缩放参数"的译法准确传达技术概念；长难句通过拆分与语序调整确保中文可读性，如将原文最后复合句拆分为两个递进短句，突出方法优势）</p>
<div class="markdown-heading"><h2 class="heading-element">OmniDraft：一款跨词汇表、在线自适应草拟器，用于设备端推测性解码</h2><a id="user-content-omnidraft一款跨词汇表在线自适应草拟器用于设备端推测性解码" class="anchor" aria-label="Permalink: OmniDraft：一款跨词汇表、在线自适应草拟器，用于设备端推测性解码" href="#omnidraft一款跨词汇表在线自适应草拟器用于设备端推测性解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>推测性解码技术通常要求配备一个小型高效的草稿模型，该模型需经过预训练或通过离线蒸馏适配特定目标模型系列（例如Llama或Qwen模型）。但在实际在线部署场景中，存在两大核心挑战：1）目标模型与草稿模型的兼容性问题；2）对延迟优化效果需随使用时长持续提升的预期。本研究提出OmniDraft统一框架，通过单个草稿模型实现与任意目标模型的协同工作，并能动态适应用户数据。我们创新性地采用混合蒸馏微调与在线n-gram缓存机制，解决草稿模型与目标模型间的跨词表失配问题；同时结合自适应推测技术进一步提升解码速度。该框架特别适合模型成本、运行效率与用户定制需求成为核心矛盾的设备端大语言模型应用场景，由此凸显了突破上述挑战的必要性，并推动了"通用草稿模型"范式的发展。我们通过在数学推理、代码生成及文本创作任务上的在线学习验证了OmniDraft的卓越性能：仅需单个Llama-68M模型，即可与Vicuna-7B、Qwen2-7B及Llama3-8B等不同目标模型进行推测性解码协同，最高可实现1.5-2倍的加速效果。</p>
<div class="markdown-heading"><h2 class="heading-element">EdgeLoRA：边缘设备上的高效多租户大语言模型服务系统</h2><a id="user-content-edgelora边缘设备上的高效多租户大语言模型服务系统" class="anchor" aria-label="Permalink: EdgeLoRA：边缘设备上的高效多租户大语言模型服务系统" href="#edgelora边缘设备上的高效多租户大语言模型服务系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>大型语言模型（LLMs）因其在广泛应用场景中的多功能性而备受关注。通过参数高效适配器（如低秩自适应LoRA）对LLMs进行微调，可使模型无需全面重训练即可高效适配下游任务。将微调后的LLMs部署于多租户边缘设备能带来显著优势，包括降低延迟、增强隐私保护和提供个性化响应。然而在资源受限的边缘设备上高效服务LLMs仍面临关键挑战：不同任务的适配器选择复杂度高，频繁切换适配器导致内存开销激增。此外，多租户场景下的并发请求若采用串行处理，会导致计算资源利用率不足与延迟增加。本文提出EdgeLoRA系统，为多租户边缘环境下的LLMs服务提供高效解决方案，其三大创新在于：（1）自适应适配器选择机制，简化配置流程；（2）异构内存管理，通过智能缓存与共享池降低内存操作开销；（3）批量LoRA推理，显著减少计算延迟。基于Llama3.1-8B模型的全面评估表明，EdgeLoRA在延迟和吞吐量上均显著超越现有方案（如llama.cpp），吞吐量最高提升4倍，更可同时支持数量级增长的适配器并行服务。这些成果彰显了EdgeLoRA在多租户场景下变革LLMs边缘部署的潜力，为资源受限环境提供了可扩展的高效解决方案。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>