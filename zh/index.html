<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">NestQuant：面向设备端深度神经网络的后训练整数嵌套量化技术</h2><a id="user-content-nestquant面向设备端深度神经网络的后训练整数嵌套量化技术" class="anchor" aria-label="Permalink: NestQuant：面向设备端深度神经网络的后训练整数嵌套量化技术" href="#nestquant面向设备端深度神经网络的后训练整数嵌套量化技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"NestQuant"作为专有技术名词保留不译，以保持技术一致性</li>
<li>"Integer-Nesting"译为"整数嵌套"，准确表达嵌套式整数量化的技术特征</li>
<li>"Post-Training"译为"后训练"，符合深度学习领域对模型训练阶段的通用表述</li>
<li>"On-Device DNN"译为"设备端深度神经网络"，突出在边缘设备部署的特性</li>
<li>整体采用技术文档的简洁命名风格，同时通过"技术"二字明确量化方法的属性）</li>
</ol>
<p>arXiv:2506.17870v1 公告类型：新研究<br>
摘要：在泛在物联网（IoT）设备上部署具备资源自适应能力的量化深度神经网络（DNN）模型，既能发挥模型压缩优势，又可满足多场景资源需求。然而现有动态/混合精度量化需重新训练或专用硬件支持，而训练后量化（PTQ）在资源自适应方面存在两大局限：（i）当前最优PTQ方法仅提供单一固定位宽模型，难以适应IoT设备的动态资源变化；（ii）部署多个不同位宽的PTQ模型会占用大量存储资源并产生切换开销。为此，本文提出一种资源友好的训练后整数嵌套量化方法NestQuant，支持IoT设备端量化模型切换。该方法通过整数权重分解将量化权重按比特位拆分为高比特与低比特整型权重，并采用分解权重嵌套机制，通过自适应舍入优化高比特权重后将其嵌套回原始量化权重中。实际部署时仅需传输存储单个NestQuant模型，通过动态加载/卸载低比特权重即可实现全比特/部分比特模型切换，既能适应资源波动又可降低消耗。ImageNet-1K预训练DNN的实验表明，NestQuant模型在top-1准确率上表现优异，同时显著降低了数据传输量、存储消耗和切换开销。以ResNet-101为例，INT8嵌套INT6模型的全比特/部分比特准确率分别达78.1%和77.9%，相比多位宽PTQ模型可降低约78.1%的切换开销。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"ubiquitous Internet of Things (IoT) devices"译为"泛在物联网设备"以体现普适性</li>
<li>"bit-wise splits"译为"按比特位拆分"以明确操作粒度</li>
<li>"paging in/out"译为"动态加载/卸载"更符合嵌入式系统术语</li>
<li>保持INT8/INT6等量化位宽标准写法</li>
<li>长句按中文习惯切分为短句，如嵌套机制描述部分）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">SlimMoE：通过专家精简与蒸馏实现大型混合专家模型的结构化压缩</h2><a id="user-content-slimmoe通过专家精简与蒸馏实现大型混合专家模型的结构化压缩" class="anchor" aria-label="Permalink: SlimMoE：通过专家精简与蒸馏实现大型混合专家模型的结构化压缩" href="#slimmoe通过专家精简与蒸馏实现大型混合专家模型的结构化压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.18349v1 公告类型：新研究<br>
摘要：混合专家（MoE）架构已成为扩展大语言模型（LLM）规模同时保持推理效率的强大范式。然而，其巨大的内存需求使得在资源受限环境中进行微调或部署成本极高。为解决这一挑战，我们提出SlimMoE——一个多阶段压缩框架，能将大型MoE模型转化为更小、更高效的变体，而无需承担从头训练的巨额成本。该方法通过精简专家模块和分阶段知识迁移，系统性地减少参数量，有效缓解了一刀切剪枝方法常见的性能下降问题。</p>
<p>利用该框架，我们仅使用4000亿token（不足原模型训练数据的10%），将Phi 3.5-MoE（总计419亿/激活66亿参数）压缩为Phi-mini-MoE（总计76亿/激活24亿参数）和Phi-tiny-MoE（总计38亿/激活11亿参数）。这些压缩模型可在单张GPU（Phi-mini-MoE需A100，Phi-tiny-MoE需A6000）上完成微调，非常适合学术和资源有限场景。实验表明，这些压缩模型不仅优于同规模模型，还能与更大模型竞争：Phi-mini-MoE仅用2/3的激活参数就达到甚至超越Phi-3-mini的性能，其MMLU分数与Llama 3.1 8B相当，同时延迟显著更低。</p>
<p>我们的研究证明，结构化剪枝与分阶段蒸馏相结合，能有效创建高质量紧凑型MoE模型，为MoE架构的广泛采用铺平道路。模型已开源：<br>
<a href="https://huggingface.co/microsoft/Phi-mini-MoE-instruct" rel="nofollow">https://huggingface.co/microsoft/Phi-mini-MoE-instruct</a><br>
<a href="https://huggingface.co/microsoft/Phi-tiny-MoE-instruct" rel="nofollow">https://huggingface.co/microsoft/Phi-tiny-MoE-instruct</a></p>
<p>（注：根据学术文献翻译规范，技术术语如"token"、"MMLU"保留英文形式；长数字采用"亿"为单位符合中文表达习惯；模型名称保留英文品牌标识；超链接格式完整保留以便实际使用）</p>
<div class="markdown-heading"><h2 class="heading-element">对数正态乘法动态：实现大型网络稳定低精度训练的方法</h2><a id="user-content-对数正态乘法动态实现大型网络稳定低精度训练的方法" class="anchor" aria-label="Permalink: 对数正态乘法动态：实现大型网络稳定低精度训练的方法" href="#对数正态乘法动态实现大型网络稳定低精度训练的方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.17768v1 公告类型：新研究<br>
摘要：神经科学研究表明，生物突触遵循对数正态分布，其状态转换可通过噪声乘性动力学解释。即使在由不可靠突触传递导致的动态波动条件下，生物神经网络仍能保持稳定运作。本文探讨：能否在人工神经网络中设计类似的乘性训练机制？为此，我们推导出贝叶斯学习规则，假设权重服从对数正态后验分布，从而提出新型对数正态乘性动力学（LMD）算法。该算法采用乘性更新机制，噪声与正则化均以乘性方式施加。其实现简易度与Adam优化器相当，仅需额外存储一个向量。实验表明，LMD在视觉Transformer和GPT-2的低精度前向运算中实现了稳定且准确的从零训练。这些结果表明，乘性动力学这一生物特征可能为未来高能效硬件实现稳定的低精度推理与学习提供支持。</p>
<p>（注：根据学术文献翻译规范，关键术语保持一致性处理：</p>
<ol>
<li>"log-normal distribution"统一译为"对数正态分布"</li>
<li>"multiplicative dynamics"译为"乘性动力学"（区别于"乘法动力学"以强调数学特性）</li>
<li>"Bayesian learning rule"译为"贝叶斯学习规则"</li>
<li>保留Adam/ViT/GPT-2等专有名词不译</li>
<li>采用"从零训练"而非"从头训练"以符合机器学习领域术语习惯）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">EQuARX：面向分布式机器学习加速的XLA高效量化AllReduce技术</h2><a id="user-content-equarx面向分布式机器学习加速的xla高效量化allreduce技术" class="anchor" aria-label="Permalink: EQuARX：面向分布式机器学习加速的XLA高效量化AllReduce技术" href="#equarx面向分布式机器学习加速的xla高效量化allreduce技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明如下）</p>
<ol>
<li>保持首字母缩写"EQuARX"不变，符合技术术语惯例</li>
<li>"Efficient Quantized AllReduce"译为"高效量化AllReduce"，其中：
<ul>
<li>Quantized采用计算机领域通用译法"量化"</li>
<li>AllReduce作为MPI操作术语保留英文</li>
</ul>
</li>
<li>"in XLA"处理为"面向...的XLA"，通过语序调整使中文更流畅</li>
<li>"Distributed Machine Learning Acceleration"译为"分布式机器学习加速"，采用行业标准译法</li>
<li>整体采用"技术"作为结尾词，符合中文技术名称命名习惯，比直译"加速"更准确</li>
</ol>
<p>arXiv:2506.17615v1 公告类型：新研究<br>
摘要：尽管大语言模型（LLMs）已产生深远影响，但其庞大规模带来了显著的部署挑战。高效部署这些模型通常需要将其分布式部署在多个加速器设备上，而设备间通信（集合操作）会引入显著的性能开销。虽然模型量化技术已广泛用于降低LLM权重和激活函数的内存与计算需求（且对质量影响极小），但由于涉及设备间求和操作，直接对AllReduce等集合操作进行量化存在固有困难，可能导致数值不稳定或严重误差累积。本研究提出了一种在TPU的XLA编译器内原生实现的动态分块高效量化AllReduce方法（EQuARX）。通过采用TPU友好的量化技术并深度流水线化通信与计算流程，使用int8精度的EQuARX在不同网络拓扑下相比基准BF16 AllReduce实现了1.8倍加速。此外，EQuARX将Gemma 3 27B模型的预填充阶段提速1.25倍，Gemma 3 12B模型提速1.1倍，且对模型质量的影响微乎其微。</p>
<div class="markdown-heading"><h2 class="heading-element">超速写大语言模型：基于显著性的超低比特大模型压缩速写技术</h2><a id="user-content-超速写大语言模型基于显著性的超低比特大模型压缩速写技术" class="anchor" aria-label="Permalink: 超速写大语言模型：基于显著性的超低比特大模型压缩速写技术" href="#超速写大语言模型基于显著性的超低比特大模型压缩速写技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.17255v1 公告类型：新研究<br>
摘要：大型语言模型（LLMs）的快速发展已超越边缘设备的内存限制，亟需突破1比特极限的极致权重压缩技术。现有量化方法虽能缩减模型体积，但其本质仍受限于每权重1比特的底线。当前的多对一压缩方案要么依赖映射表（导致内存开销），要么因随机权重分组引发严重精度损失。我们提出UltraSketchLLM——一种基于数据素描技术的无索引框架，在实现超低比特压缩（最低至每权重0.5比特）的同时保持模型性能。该框架创新性地运用流式计算中的亚线性表示技术，通过有界误差将多个权重映射为单一数值。具体实现包含：采用低估式AbsMaxMin素描算法以减小小权重的相对误差，基于重要性感知的空间分配机制优先处理关键权重，以及通过直通估计器实现压缩感知微调。Llama-3.2-1B上的实验表明，该方法在实现0.5比特压缩的同时保持竞争力困惑度，且延迟开销处于可接受范围。UltraSketchLLM为资源受限环境下的LLM部署提供了实用解决方案。</p>
<p>（注：根据学术文献翻译规范，对以下术语进行了专业处理：</p>
<ol>
<li>"sketch"译为"素描"（计算机领域标准译法）</li>
<li>"perplexity"译为"困惑度"（NLP领域标准评估指标）</li>
<li>"straight-through estimator"译为"直通估计器"（深度学习领域通用译法）</li>
<li>保留Llama-3.2-1B等模型名称原文以方便学术检索）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">RLRC：基于强化学习的压缩视觉-语言-动作模型恢复方法</h2><a id="user-content-rlrc基于强化学习的压缩视觉-语言-动作模型恢复方法" class="anchor" aria-label="Permalink: RLRC：基于强化学习的压缩视觉-语言-动作模型恢复方法" href="#rlrc基于强化学习的压缩视觉-语言-动作模型恢复方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译时采用了技术文献常见的"方法"作为隐性后缀，将"Recovery"译为"恢复"以准确体现算法功能，"Reinforcement Learning-based"采用"基于强化学习的"的标准译法，"Compressed Vision-Language-Action Models"使用"压缩视觉-语言-动作模型"保持术语一致性，整体结构符合中文技术命名习惯）</p>
<p>arXiv:2506.17639v1 公告类型：新研究<br>
摘要：视觉-语言-动作模型（VLA）在解决复杂机器人操作任务中展现出卓越能力和广阔前景，但其庞大的参数量和高推理延迟对实际部署（尤其在资源受限的机器人平台）构成了重大挑战。为此，我们首先开展系统性实证研究，探索模型压缩技术在VLA上的应用效果。基于这些初步实验的启示，我们提出RLRC——一种三阶段压缩模型恢复方法，包括结构化剪枝、基于监督微调（SFT）和强化学习（RL）的性能恢复，以及进一步量化。该方法实现了高达8倍的内存占用缩减和2.3倍的推理吞吐量提升，同时保持甚至超越原始VLA的任务成功率。大量实验表明，RLRC始终优于现有压缩基线，展现出VLA在终端设备部署的强劲潜力。项目网站：<a href="https://rlrc-vla.github.io" rel="nofollow">https://rlrc-vla.github.io</a></p>
<p>（注：根据学术文献翻译规范，术语处理如下：</p>
<ol>
<li>"Vision-Language-Action models" 采用学界通用译法"视觉-语言-动作模型"并保留缩写VLA</li>
<li>"structured pruning" 译为"结构化剪枝"（计算机领域标准译法）</li>
<li>"SFT"首次出现时译为"监督微调"并标注英文缩写，符合中文论文惯例</li>
<li>技术指标"8x reduction"等采用中文数字规范表述为"8倍缩减"</li>
<li>长难句按中文表达习惯拆分重组，如将"particularly on..."处理为括号补充说明）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>