<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">雷达：适用于任何Transformer的快速长上下文解码技术</h2><a id="user-content-雷达适用于任何transformer的快速长上下文解码技术" class="anchor" aria-label="Permalink: 雷达：适用于任何Transformer的快速长上下文解码技术" href="#雷达适用于任何transformer的快速长上下文解码技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Transformer模型在众多应用领域展现了卓越的性能。尽管点积注意力机制构成了Transformer模型的基础，但它对长上下文数据的处理效率并不理想，因为其时间需求随上下文长度呈二次方增长。本文提出了一种名为Radar的无训练方法，通过动态搜索最重要的上下文标记来加速推理过程。对于任何预训练的Transformer模型，Radar都能在不进行额外训练或启发式剔除标记的情况下，降低解码的时间复杂度。此外，我们为该方法提供了理论依据，证明Radar能够以高概率可靠地识别出最重要的标记。我们在多种任务上与前人方法进行了广泛比较，结果表明，Radar在不同架构上均实现了最先进的性能，同时降低了时间复杂度，为Transformer模型的高效长上下文处理提供了一个实用的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">OpenAI Whisper模型的量化：一项比较分析</h2><a id="user-content-openai-whisper模型的量化一项比较分析" class="anchor" aria-label="Permalink: OpenAI Whisper模型的量化：一项比较分析" href="#openai-whisper模型的量化一项比较分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>自动语音识别（ASR）模型在字幕生成、语音翻译和实时转录等应用中日益重要。本文研究了Whisper及其两种变体模型：一种针对实时语音流优化，另一种则专为离线转录设计。值得注意的是，这些模型被发现会产生幻觉内容，降低了转录的可靠性。此外，较大规模的模型变体表现出更高的延迟，对资源受限设备的部署提出了挑战。本研究分析了三种Whisper模型之间的异同，定性探讨了它们各自的能力。接着，量化了模型量化对延迟的影响，并评估了其在边缘部署中的可行性。利用开源LibriSpeech数据集，本文评估了单词错误率（WER），并通过三种量化方法（INT4、INT5、INT8）对whispercpp进行了延迟分析。结果显示，量化使延迟减少了19%，模型大小缩减了45%，同时保持了转录的准确性。这些发现为不同Whisper模型的最佳使用场景及边缘设备部署的可能性提供了洞见。所有代码、数据集及实现细节均可在公共GitHub仓库获取：<a href="https://github.com/allisonandreyev/WhisperQuantization.git%E3%80%82">https://github.com/allisonandreyev/WhisperQuantization.git。</a></p>
<div class="markdown-heading"><h2 class="heading-element">有时痛苦但肯定充满希望：边缘设备上语言模型推理的可行性及权衡</h2><a id="user-content-有时痛苦但肯定充满希望边缘设备上语言模型推理的可行性及权衡" class="anchor" aria-label="Permalink: 有时痛苦但肯定充满希望：边缘设备上语言模型推理的可行性及权衡" href="#有时痛苦但肯定充满希望边缘设备上语言模型推理的可行性及权衡"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>语言模型（LMs）的迅速崛起极大地扩展了自然语言处理的能力，推动了从文本生成到复杂决策的各种应用。尽管最先进的LMs通常拥有数千亿参数，并主要部署在数据中心，但最近的趋势显示，人们越来越关注通过量化和其他模型压缩技术实现的紧凑模型——通常参数少于100亿。这一转变为边缘设备上的LMs铺平了道路，提供了增强隐私、降低延迟和提高数据主权等潜在优势。然而，即使是这些较小的模型，其固有的复杂性加上边缘硬件有限的计算资源，也引发了关于在云端之外执行LM推理的实际权衡的关键问题。为了应对这些挑战，我们对基于CPU和GPU加速的代表性边缘设备上的生成式LM推理进行了全面评估。我们的研究测量了各种设备配置下的关键性能指标，包括内存使用、推理速度和能耗。此外，我们还考察了吞吐量与能耗的权衡、成本考虑和可用性，同时对模型性能进行了定性评估。虽然量化有助于缓解内存开销，但它并不能完全消除资源瓶颈，特别是对于较大的模型。我们的研究结果量化了实际部署中必须考虑的内存和能量限制，为模型大小、推理性能和效率之间的权衡提供了具体见解。边缘LMs的探索仍处于早期阶段。我们希望这项研究能为未来的研究奠定基础，指导模型的优化、推理效率的提升以及以边缘为中心的AI系统的进步。</p>
<div class="markdown-heading"><h2 class="heading-element">协作式推测推理：提升大型语言模型推理服务效率</h2><a id="user-content-协作式推测推理提升大型语言模型推理服务效率" class="anchor" aria-label="Permalink: 协作式推测推理：提升大型语言模型推理服务效率" href="#协作式推测推理提升大型语言模型推理服务效率"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>推测推理是一种颇具前景的范式，它利用小型推测模型（SSMs）作为起草者生成草稿令牌，随后由目标大型语言模型（LLM）并行验证这些令牌。这种方法通过减少LLM推理延迟和成本，同时保持生成质量，从而提高了推理服务的效率。然而，现有的推测方法面临关键挑战，包括资源利用效率低下和草稿接受率有限，这些因素限制了其可扩展性和整体有效性。为了克服这些障碍，我们提出了CoSine，一种新颖的推测推理系统，它将顺序推测解码与并行验证解耦，实现了多个节点之间的高效协作。具体而言，CoSine根据起草者的专长将推理请求路由到专门的起草者，并采用基于置信度的令牌融合机制，综合来自协作起草者的输出，确保高质量的草稿生成。此外，CoSine以流水线方式动态编排推测解码和验证的执行，采用批量调度选择性地分组请求，并通过自适应推测控制最小化空闲时间。通过异构节点协作优化并行工作流，CoSine实时平衡草稿生成和验证吞吐量，从而最大化资源利用率。实验结果表明，与最先进的推测方法相比，CoSine实现了卓越的性能。值得注意的是，在资源成本相同的情况下，与基线方法相比，CoSine实现了高达23.2%的延迟降低和32.5%的吞吐量提升。</p>
<div class="markdown-heading"><h2 class="heading-element">KV-Distill：近乎无损的可学习上下文压缩技术应用于大语言模型</h2><a id="user-content-kv-distill近乎无损的可学习上下文压缩技术应用于大语言模型" class="anchor" aria-label="Permalink: KV-Distill：近乎无损的可学习上下文压缩技术应用于大语言模型" href="#kv-distill近乎无损的可学习上下文压缩技术应用于大语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>序列到序列任务往往受益于长上下文环境，但标准Transformer中自注意力机制的二次复杂度使得这一优势难以轻易实现。在生成过程中，临时表示——存储在所谓的KV缓存中——占据了GPU内存使用的大部分，并且随着上下文长度的增加而线性增长。我们引入了KV-Distill，这是一个Transformer压缩框架，它以与问题无关的方式将长上下文的KV缓存蒸馏成显著更短的表示。KV-Distill可以作为预训练模型的高效参数适配器进行训练，并能在保持预训练模型能力的同时，压缩上下文的任意部分。我们将压缩与未压缩的缓存视为师生配对，并应用KL型散度来匹配生成的输出。在最坏情况的抽取任务中，KV-Distill优于其他压缩技术，并在长上下文问答和摘要任务中接近未压缩的性能，且可以在特定领域的上下文中进行微调，将长度减少高达99%，同时保持下游性能。我们展示了KV-Distill在不同模型规模和架构上的通用性。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>