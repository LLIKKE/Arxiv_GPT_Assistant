<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">超低量化：通过1.58位编码实现高效嵌入搜索</h2><a id="user-content-超低量化通过158位编码实现高效嵌入搜索" class="anchor" aria-label="Permalink: 超低量化：通过1.58位编码实现高效嵌入搜索" href="#超低量化通过158位编码实现高效嵌入搜索"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.00528v1 公告类型：新研究<br>
摘要：许多现代搜索领域涉及源自神经网络的高维浮点数向量，即嵌入向量。典型嵌入向量的维度范围从数百到数千不等，这使得嵌入向量的大小及比较速度成为关键问题。</p>
<p>量化是一类用更小表示形式（例如短整数）替代浮点数值的机制。这种机制通过近似嵌入空间来换取更小的数据表示和更快的比较函数。</p>
<p>本文将此理念推向极致：我们展示了如何用元素取自集合{-1,0,1}的向量替代任意精度浮点数值向量。这种方法在保持相似性测量强相关性的同时，实现了存储空间和度量计算成本的显著降低。</p>
<p>这一成果的数学基础源于高维空间中一类凸多胞体。本文概述了这些几何对象的特性，并演示了如何以其为基础实现如此极端的量化，同时仍能保持惊人的准确性。</p>
<p>（注：根据学术规范，专业术语如"arXiv"、"embedding"、"quantisation"等保留英文形式；"polytopes"译为"多胞体"符合数学领域习惯；长句按中文表达习惯拆分为短句；被动语态转换为主动表述；抽象概念如"strong correlation"译为"强相关性"以保持精确性。）</p>
<div class="markdown-heading"><h2 class="heading-element">压缩表示的统一缩放定律</h2><a id="user-content-压缩表示的统一缩放定律" class="anchor" aria-label="Permalink: 压缩表示的统一缩放定律" href="#压缩表示的统一缩放定律"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.01863v1 公告类型：新论文<br>
摘要：<br>
缩放定律通过建立模型性能与模型规模、计算量和数据量之间的可预测关系，推动了机器学习领域的最新进展。与此同时，人工智能计算成本的激增催生了模型压缩技术，特别是量化和稀疏化方法，这些技术旨在缓解大规模训练和推理所需的巨额计算需求。本文研究了缩放定律与压缩格式之间的相互作用，探讨当训练过程基于各种压缩表示（如稀疏格式、标量量化、稀疏-量化混合甚至向量量化格式）时，能否通过统一的缩放框架准确预测模型性能。我们的核心贡献包括验证了一个通用缩放定律公式，并证明该公式不仅适用于单一压缩类型，还能跨压缩类型组合使用。基于此，我们通过理论与实证研究表明：存在一个基于"表示能力"（即该表示方法拟合随机高斯数据的能力）的简单"容量"指标，可以稳健预测多种压缩表示下的参数效率。在实践层面，我们扩展了这一公式以直接比较不同压缩格式的精度潜力，并推导出针对稀疏-量化混合格式的更优训练算法。</p>
<p>（注：根据学术论文摘要的翻译规范，对部分专业术语进行了如下处理：</p>
<ol>
<li>"scaling laws"译为"缩放定律"，这是机器学习领域的标准译法</li>
<li>"vector-quantized"译为"向量量化"，保持与信号处理领域术语的一致性</li>
<li>"parameter efficiency"译为"参数效率"，准确传达原文指代模型参数利用率的含义</li>
<li>长难句采用拆分策略，如将原文"exploring whether..."复杂从句转化为中文的"探讨当...时，能否..."的流水句结构</li>
<li>保留"arXiv"等专有名词不译，符合学术惯例）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">Uni-LoRA：一向量足矣</h2><a id="user-content-uni-lora一向量足矣" class="anchor" aria-label="Permalink: Uni-LoRA：一向量足矣" href="#uni-lora一向量足矣"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.00799v1 公告类型：新研究<br>
摘要：低秩自适应（LoRA）通过将权重更新约束在低秩矩阵上，已成为大语言模型（LLM）参数高效微调（PEFT）的事实标准方法。近期研究如Tied-LoRA、VeRA和VB-LoRA通过引入额外约束进一步压缩可训练参数量来提升效率。本文提出，这些LoRA变体的参数空间压缩策略可统一于Uni-LoRA框架中——将LoRA参数空间展平为高维向量空间$R^D$后，均可通过从子空间$R^d$（$d \ll D$）的投影重构。我们发现不同LoRA方法的本质区别在于投影矩阵$P \in R^{D \times d}$的选择：现有方法多采用逐层或结构特定的投影，限制了跨层参数共享从而影响效率。基于此，我们提出一种高效且理论完备的等距投影矩阵设计，支持全局参数共享并降低计算开销。在Uni-LoRA的统一视角下，该方案仅需单个可训练向量即可重构整个LLM的LoRA参数，使Uni-LoRA兼具统一框架与"单向量解"特性。在GLUE基准、数学推理和指令微调任务上的实验表明，Uni-LoRA在参数效率上达到最先进水平，同时预测性能优于或匹配现有方法。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"de facto"译为"事实标准"以符合技术语境</li>
<li>"parameter-efficient fine-tuning"统一译为"参数高效微调"</li>
<li>数学符号保留原文格式确保准确性</li>
<li>复杂长句按中文习惯拆分为短句，如将"where"从句转换为破折号说明</li>
<li>技术概念如"isometric projection"译为"等距投影"保持学科一致性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">TAH-QUANT：慢速网络环境下流水线并行中的高效激活量化</h2><a id="user-content-tah-quant慢速网络环境下流水线并行中的高效激活量化" class="anchor" aria-label="Permalink: TAH-QUANT：慢速网络环境下流水线并行中的高效激活量化" href="#tah-quant慢速网络环境下流水线并行中的高效激活量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.01352v1 公告类型：新研究<br>
摘要：大型语言模型的去中心化训练为跨地域分布式参与者整合计算资源提供了可能，但面临着显著的网络通信瓶颈，尤其在流水线并行场景中。虽然流水线并行通过将模型层划分到不同设备来处理大规模模型，但需要频繁传递中间激活值，这在网络带宽受限时带来巨大挑战。现有激活压缩方法（如AQ-SGD）通过误差补偿缓解量化误差，却因需存储历史激活值而产生过高内存开销。为此，我们提出TAH-Quant（分块自适应哈达玛量化）——专为流水线并行设计的创新激活量化框架。该方法融合三大核心技术：通过细粒度分块量化实现精确控制，基于熵的令牌级自适应比特分配优化比特利用率，以及结合枢轴元素交换的哈达玛变换有效抑制量化异常值。我们进一步给出理论证明：配备TAH-Quant的流水线并行训练保持$\mathcal{O}(1/\sqrt{T})$收敛速率，与标准随机梯度下降相当。在多类LLM任务上的实验表明，TAH-Quant实现了激进的激活压缩（3-4比特），在保证训练收敛性的前提下获得最高4.3$\times$的端到端加速，性能媲美现有最优方法，不产生额外内存开销，且能泛化至不同训练场景。</p>
<div class="markdown-heading"><h2 class="heading-element">CPU环境下大型语言模型的内存访问特性及其潜在影响</h2><a id="user-content-cpu环境下大型语言模型的内存访问特性及其潜在影响" class="anchor" aria-label="Permalink: CPU环境下大型语言模型的内存访问特性及其潜在影响" href="#cpu环境下大型语言模型的内存访问特性及其潜在影响"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.01827v1 公告类型：新研究<br>
摘要：随着机器学习算法被证明是一种日益重要的工具，对其访问需求也相应增长。通常，在没有加速器的情况下运行大型模型的推理是不可行的，而在存在能耗、安全性或成本等限制的环境中，加速器可能无法使用。为了提高这些模型的可用性，我们旨在通过修改缓存架构来提升纯CPU环境下的LLM推理速度。为确定可能的改进方向，我们使用Llama.cpp和QWEN模型进行了两项实验：运行不同缓存配置并评估其性能，以及输出内存占用的追踪数据。通过这些实验，我们研究了内存访问模式和性能特征，以识别潜在的优化机会。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"accelerator"译为"加速器"而非"加速设备"，符合计算机领域术语</li>
<li>"memory footprint"译为"内存占用"而非"内存足迹"，采用更通用的技术表述</li>
<li>"trace"在此语境下译为"追踪数据"而非"痕迹"，更准确表达调试数据的含义</li>
<li>保留了"Llama.cpp"和"QWEN"等专有名词的原始拼写</li>
<li>将长句拆分为符合中文阅读习惯的短句结构，如将原文最后一句分译为两个分句）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">训练优质模型需以优质模型为基：广义高斯先验助力优化大语言模型</h2><a id="user-content-训练优质模型需以优质模型为基广义高斯先验助力优化大语言模型" class="anchor" aria-label="Permalink: 训练优质模型需以优质模型为基：广义高斯先验助力优化大语言模型" href="#训练优质模型需以优质模型为基广义高斯先验助力优化大语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>注：翻译要点说明：</p>
<ol>
<li>主标题采用对仗式处理，"It Takes a Good Model to Train a Good Model"译为"训练优质模型需以优质模型为基"，既保持原文回环结构，又符合中文表达习惯</li>
<li>"Generalized Gaussian Priors"专业术语译为"广义高斯先验"，准确体现统计学概念</li>
<li>"Optimized LLMs"采用行业通用译法"优化大语言模型"，其中LLMs（Large Language Models）使用中文领域通用简称"大语言模型"</li>
<li>整体采用学术论文标题的简洁风格，通过冒号分层保持原标题结构，同时使用"助力"一词体现技术方案的支撑作用，比直译"为了"更符合中文技术文献表达惯例</li>
</ol>
<p>arXiv:2506.00486v1 公告类型：新研究<br>
摘要：尽管大语言模型（LLMs）的研究与部署进展迅猛，但模型参数的统计分布及其对初始化、训练动态和下游效率的影响却鲜少受到关注。近期一项研究提出了训练时压缩算法BackSlash，首次证明预训练LLM参数更符合广义高斯分布（GGDs）。通过在训练过程中优化广义高斯先验，BackSlash能在性能损失极小的情况下将参数减少高达90%。基于这一关键发现，我们提出一个基于GG模型的统一端到端LLM优化框架，贡献包括：（1）符合训练模型统计结构的GG初始化方案，可加速收敛并提升精度；（2）DeepShape后训练正则化方法，通过重塑权重分布匹配GG轮廓，在最小化性能损失的同时提升压缩潜力；（3）专为GG分布初始化BackSlash训练设计的RF8——一种紧凑且硬件高效的8位浮点格式，可在保持精度的前提下实现低成本推理。跨多种模型架构的实验表明，该框架始终能生成更小更快的模型，其性能持平或超越标准训练基线。通过将LLM开发建立在严谨的统计建模基础上，本研究为高效、可扩展且硬件感知的AI系统开辟了新路径。代码已发布于项目页：<a href="https://huggingface.co/spaces/shifeng3711/gg_prior" rel="nofollow">https://huggingface.co/spaces/shifeng3711/gg_prior</a></p>
<p>（注：根据学术文献翻译规范，专业术语如"generalized Gaussian distributions"采用"广义高斯分布"标准译法；技术概念如"end-to-end"译为"端到端"；机构名"HuggingFace"保留英文品牌名；长难句按中文表达习惯拆分重组；URL链接保留原格式以确保可访问性）</p>
<div class="markdown-heading"><h2 class="heading-element">MLorc：面向大语言模型适配的动量低秩压缩技术</h2><a id="user-content-mlorc面向大语言模型适配的动量低秩压缩技术" class="anchor" aria-label="Permalink: MLorc：面向大语言模型适配的动量低秩压缩技术" href="#mlorc面向大语言模型适配的动量低秩压缩技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>保留技术术语"MLorc"作为专有名词不译</li>
<li>"Momentum"译为"动量"，遵循物理学/优化算法领域的既定译法</li>
<li>"Low-rank Compression"译为"低秩压缩"，采用线性代数术语的标准翻译</li>
<li>"Adaptation"译为"适配"而非"适应"，更符合计算机领域的术语使用习惯</li>
<li>整体采用"技术"作为隐性中心词，符合中文技术名称的简洁性要求</li>
<li>使用"面向...的"结构准确传达"for"的功能性含义</li>
<li>通过四字格"动量低秩"保持术语的紧凑性，同时确保专业准确性）</li>
</ol>
<p>arXiv:2506.01897v1 公告类型：新研究<br>
摘要：随着大语言模型（LLMs）规模的不断扩大，全参数微调对内存的需求急剧增加。为缓解这一问题，我们提出了一种新型内存高效训练范式——动量低秩压缩（MLorc）。该方法通过直接压缩并重建动量（而非梯度），避免了在权重更新矩阵上施加固定秩约束，从而更好地保持了全参数微调的动态特性，这与LoRA、GaLore等现有低秩方法形成鲜明对比。实验表明，MLorc在内存高效训练方法中持续领先，即使采用极小秩数（如$r=4$）也能匹配甚至超越全微调性能，且在不同优化器间展现出优异泛化能力——同时完全不牺牲时间或内存效率。此外，我们在合理假设下为其收敛性提供了理论保证。</p>
<p>（注：根据学术规范，专业术语如"momentum"在优化算法领域通常译为"动量"；"low-rank compression"译为"低秩压缩"是标准译法；"full-parameter fine-tuning"译为"全参数微调"符合NLP领域惯例；数学符号$r=4$保留原文格式；被动语态"is proposed"按中文习惯转化为主动句式"我们提出"）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>