<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">只需一个额外的RMSNorm即可实现1.58位微调</h2><a id="user-content-只需一个额外的rmsnorm即可实现158位微调" class="anchor" aria-label="Permalink: 只需一个额外的RMSNorm即可实现1.58位微调" href="#只需一个额外的rmsnorm即可实现158位微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.08823v1 公告类型：新研究<br>
摘要：大语言模型（LLM）虽已革新自然语言处理领域，但其规模导致实际部署成本高昂。训练后量化技术虽能降低内存与计算需求，却常伴随精度损失；而量化感知训练虽可恢复性能，但需额外训练开销。将量化推至三元（2比特）领域能实现更大节省，但该过程 notoriously 不稳定。基于近期研究表明——采用无偏置的RMS归一化Transformer配合直通估计法可达1.58比特精度，我们证明：只需在所有线性投影前插入RMS归一化，并采用渐进式分层量化策略，即可稳定地将全精度检查点微调为三元大语言模型。在标准语言建模基准测试中，我们的方法无需增加模型复杂度，即可达到或超越复杂知识蒸馏流程的效果。这些结果表明，仅凭精心设计的归一化就能大幅缩小三元与全精度LLM之间的精度差距，使超低位推理具备实用价值。</p>
<p>（注：根据学术规范保留"arXiv"原始标识；"notoriously"译为"众所周知"并调整语序为"但该过程 notoriously 不稳定"以符合中文表达；"straight-through estimation"采用计算机领域通用译法"直通估计法"；"knowledge-distillation pipelines"译为"知识蒸馏流程"以保持技术术语一致性；通过拆分英文长句为中文短句结构，如将"Building on..."处理为因果关系的分句；"ultra-low-bit inference practical"译为"超低位推理具备实用价值"以准确传达技术可行性。）</p>
<div class="markdown-heading"><h2 class="heading-element">越大越好？无线边缘网络中高效的大型AI模型推理</h2><a id="user-content-越大越好无线边缘网络中高效的大型ai模型推理" class="anchor" aria-label="Permalink: 越大越好？无线边缘网络中高效的大型AI模型推理" href="#越大越好无线边缘网络中高效的大型ai模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.09214v1 公告类型：新研究<br>
摘要：大规模人工智能模型（LAIM）服务需求的增长正推动从传统云端推理向边缘推理的范式转变，以满足低延迟和隐私保护的应用需求。其中，边缘-设备协同推理策略——将LAIM分割部署于边缘设备与服务器——已成为无线网络中资源高效执行LAIM的有前景方案。本文研究一种剪枝感知的LAIM协同推理机制：预训练LAIM经过剪枝后分割为设备端与服务器端子模型进行部署。在分析层面，我们首先证明LAIM输出失真度受其参数失真度的上限约束，进而通过率失真理论推导出参数失真度的下界，从理论上量化剪枝率与协同推理性能的关系。基于此，我们建立了一个联合优化剪枝率、发射功率和计算频率的LAIM协同推理失真边界最小化问题，并纳入系统延迟、能耗及资源约束。针对这一高度非凸问题，提出了一种高效求解算法。最终，大量仿真验证了所提设计的有效性：模型参数失真度被证实能可靠约束输出失真度；与全设备端/服务器端推理等基准方案相比，所提出的剪枝率与资源联合管理设计在推理性能、系统延迟和能耗的权衡上表现更优。研究还发现，在异构资源受限的边缘环境中，模型分割点对系统性能优化具有关键影响。</p>
<p>（注：翻译过程中对技术术语进行了标准化处理，如"pruning-aware"译为"剪枝感知"，"co-inference"译为"协同推理"；长难句按中文表达习惯拆分重组；关键理论概念如"rate-distortion theory"保留专业译法"率失真理论"；通过增补"研究还发现"等过渡词提升行文连贯性。）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>