<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">DBellQuant：通过双贝尔变换突破大语言模型训练后二值化的贝尔限制</h2><a id="user-content-dbellquant通过双贝尔变换突破大语言模型训练后二值化的贝尔限制" class="anchor" aria-label="Permalink: DBellQuant：通过双贝尔变换突破大语言模型训练后二值化的贝尔限制" href="#dbellquant通过双贝尔变换突破大语言模型训练后二值化的贝尔限制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.01027v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）展现出卓越性能，但面临巨大的计算和内存挑战，限制了其实际应用。量化技术已成为一种有前景的解决方案，但其效果常受限于权重分布不适宜量化及激活值异常值导致的量化误差。为解决这些问题，我们提出DBellQuant——一种创新的训练后量化（PTQ）框架，能以极小的性能损失实现近1比特权重压缩和6比特激活量化。该框架采用"双钟形可学习变换"（LTDB）算法，将单钟形权重分布转换为双钟形以降低二值化误差，并通过逆变换平滑激活值。DBellQuant在激进权重与激活量化条件下仍能保持优异模型性能，创造了新标杆。例如在Wikitext2数据集上，DBellQuant对LLaMA2-13B模型进行6比特激活量化时获得14.39的困惑度，显著优于未进行激活量化的BiLLM模型（21.35），彰显了其在现实场景中压缩大语言模型的巨大潜力。</p>
<p>（注：根据学术文献翻译规范，关键术语保持英文缩写形式如LLMs/PTQ，创新方法名称LTDB保留原称谓，"Dual-Bell"译为"双钟形"以准确传达统计学分布特征，性能数据严格对应原文数值。被动语态转换为中文主动表述，如"are limited by"处理为"常受限于"。长难句拆解为符合中文阅读习惯的短句结构，如将原文包含因果关系的复合句分译为两个递进分句。）</p>
<div class="markdown-heading"><h2 class="heading-element">基于图神经网络与大语言模型的推荐系统低延迟推理与训练效率优化研究</h2><a id="user-content-基于图神经网络与大语言模型的推荐系统低延迟推理与训练效率优化研究" class="anchor" aria-label="Permalink: 基于图神经网络与大语言模型的推荐系统低延迟推理与训练效率优化研究" href="#基于图神经网络与大语言模型的推荐系统低延迟推理与训练效率优化研究"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.01035v1 公告类型：新论文<br>
摘要：在线服务的持续涌现对推荐系统（ReS）提出了高速高效的要求，这类系统需在实时响应的同时处理极其复杂的用户-项目交互。为此，本研究针对基于图神经网络（GNN）与大型语言模型（LLM）的混合推荐系统中存在的计算瓶颈问题，重点优化其推理延迟与训练效率。研究采用了综合性方法：混合GNN-LLM集成架构优化策略（量化、LoRA、蒸馏）——硬件加速方案（FPGA、DeepSpeed）——所有实验均在R 4.4.2环境下进行。优化效果显著，最佳混合架构+FPGA+DeepSpeed配置在40-60毫秒延迟下实现了13.6%的准确率提升（NDCG@10：0.75），而LoRA技术相比未优化基线将训练时间缩短了66%（3.8小时）。无论在准确性还是效率维度，硬件-软件协同设计与参数高效调优都证明混合模型能超越独立实施的GNN或LLM方案。研究推荐采用FPGA与LoRA技术实现实时部署。未来工作应结合联邦学习与高级融合架构，以增强系统扩展性与隐私保护能力。本研究成果为平衡低延迟响应与前沿个性化需求的下一代推荐系统奠定了理论基础。</p>
<p>（注：根据学术规范，专业术语保留英文缩写并首次出现时标注全称；R 4.4.2作为软件环境不作翻译；技术术语如"quantization"译为"量化"、"distillation"译为"蒸馏"符合计算机领域惯例；被动语态转换为中文主动表述；长句按中文习惯拆分为短句群）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>