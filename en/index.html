<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 08/28/2025</h1><a id="user-content-personalized-daily-arxiv-papers-08282025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 08/28/2025" href="#personalized-daily-arxiv-papers-08282025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 19</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</a>
<strong>Authors:</strong> Hejia Liu, Mochen Yang, Gediminas Adomavicius</p>
</li>
<li>
<p><a href="#link1">Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation</a>
<strong>Authors:</strong> Ziniu Zhang, Zhenshuo Zhang, Dongyue Li, Lu Wang, Jennifer Dy, Hongyang R. Zhang</p>
</li>
<li>
<p><a href="#link2">Fast 3D Diffusion for Scalable Granular Media Synthesis</a>
<strong>Authors:</strong> Muhammad Moeeze Hassan, R'egis Cottereau, Filippo Gatti, Patryk Dec</p>
</li>
<li>
<p><a href="#link3">Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization</a>
<strong>Authors:</strong> Paimon Goulart, Shaan Pakala, Evangelos Papalexakis</p>
</li>
<li>
<p><a href="#link4">FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control</a>
<strong>Authors:</strong> Tan Jing, Shiting Chen, Yangfan Li, Weisheng Xu, Renjing Xu</p>
</li>
<li>
<p><a href="#link5">Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models</a>
<strong>Authors:</strong> Yuhang Liu, Tao Li, Zhehao Huang, Zuopeng Yang, Xiaolin Huang</p>
</li>
<li>
<p><a href="#link6">SCAR: A Characterization Scheme for Multi-Modal Dataset</a>
<strong>Authors:</strong> Ri Su, Zhao Chen, Caleb Chen Cao, Nan Tang, Lei Chen</p>
</li>
<li>
<p><a href="#link7">Pruning Strategies for Backdoor Defense in LLMs</a>
<strong>Authors:</strong> Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</p>
</li>
<li>
<p><a href="#link8">Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats</a>
<strong>Authors:</strong> Anat Heilper, Doron Singer</p>
</li>
<li>
<p><a href="#link9">Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence</a>
<strong>Authors:</strong> Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi</p>
</li>
<li>
<p><a href="#link10">Efficient Multi-Source Knowledge Transfer by Model Merging</a>
<strong>Authors:</strong> Marcin Osial, Bartosz W'ojcik, Bartosz Zieli'nski, Sebastian Cygert</p>
</li>
<li>
<p><a href="#link11">Quantum latent distributions in deep generative models</a>
<strong>Authors:</strong> Omar Bacarreza, Thorin Farnsworth, Alexander Makarovskiy, Hugo Wallner, Tessa Hicks, Santiago Sempere-Llagostera, John Price, Robert J. A. Francis-Jones, William R. Clements</p>
</li>
<li>
<p><a href="#link12">Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling</a>
<strong>Authors:</strong> Felix N"utzel, Mischa Dombrowski, Bernhard Kainz</p>
</li>
<li>
<p><a href="#link13">Memorization in Graph Neural Networks</a>
<strong>Authors:</strong> Adarsh Jamadandi, Jing Xu, Adam Dziedzic, Franziska Boenisch</p>
</li>
<li>
<p><a href="#link14">Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning</a>
<strong>Authors:</strong> Sheryl Mathew, N Harshit</p>
</li>
<li>
<p><a href="#link15">MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification</a>
<strong>Authors:</strong> Yifan Dou, Adam Khadre, Ruben C Petreaca, Golrokh Mirzaei</p>
</li>
<li>
<p><a href="#link16">Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs</a>
<strong>Authors:</strong> Yao Fu, Xianxuan Long, Runchao Li, Haotian Yu, Mu Sheng, Xiaotian Han, Yu Yin, Pan Li</p>
</li>
<li>
<p><a href="#link17">Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal</a>
<strong>Authors:</strong> Yunlong Lin, Chao Lu, Tongshuai Wu, Xiaocong Zhao, Guodong Du, Yanwei Sun, Zirui Li, Jianwei Gong</p>
</li>
<li>
<p><a href="#link18">Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation</a>
<strong>Authors:</strong> Yiguo Fan, Pengxiang Ding, Shuanghao Bai, Xinyang Tong, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, Zhaoxin Fan, Badong Chen, Donglin Wang</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2508.19563" rel="nofollow">Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-just-because-you-can-doesnt-mean-you-should-llms-for-data-fitting-" class="anchor" aria-label="Permalink: 0. Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting" href="#0-just-because-you-can-doesnt-mean-you-should-llms-for-data-fitting-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19563
<strong>Authors:</strong> Hejia Liu, Mochen Yang, Gediminas Adomavicius</p>
<p><strong>Abstract:</strong> arXiv:2508.19563v1 Announce Type: new  Abstract: Large Language Models (LLMs) are being applied in a wide array of settings, well beyond the typical language-oriented use cases. In particular, LLMs are increasingly used as a plug-and-play method for fitting data and generating predictions. Prior work has shown that LLMs, via in-context learning or supervised fine-tuning, can perform competitively with many tabular supervised learning techniques in terms of predictive performance. However, we identify a critical vulnerability of using LLMs for data fitting -- making changes to data representation that are completely irrelevant to the underlying learning task can drastically alter LLMs' predictions on the same data. For example, simply changing variable names can sway the size of prediction error by as much as 82% in certain settings. Such prediction sensitivity with respect to task-irrelevant variations manifests under both in-context learning and supervised fine-tuning, for both close-weight and open-weight general-purpose LLMs. Moreover, by examining the attention scores of an open-weight LLM, we discover a non-uniform attention pattern: training examples and variable names/values which happen to occupy certain positions in the prompt receive more attention when output tokens are generated, even though different positions are expected to receive roughly the same attention. This partially explains the sensitivity in the presence of task-irrelevant variations. We also consider a state-of-the-art tabular foundation model (TabPFN) trained specifically for data fitting. Despite being explicitly designed to achieve prediction robustness, TabPFN is still not immune to task-irrelevant variations. Overall, despite LLMs' impressive predictive capabilities, currently they lack even the basic level of robustness to be used as a principled data-fitting tool.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2508.19999" rel="nofollow">Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-linear-time-demonstration-selection-for-in-context-learning-via-gradient-estimation-" class="anchor" aria-label="Permalink: 1. Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation" href="#1-linear-time-demonstration-selection-for-in-context-learning-via-gradient-estimation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19999
<strong>Authors:</strong> Ziniu Zhang, Zhenshuo Zhang, Dongyue Li, Lu Wang, Jennifer Dy, Hongyang R. Zhang</p>
<p><strong>Abstract:</strong> arXiv:2508.19999v1 Announce Type: new  Abstract: This paper introduces an algorithm to select demonstration examples for in-context learning of a query set. Given a set of $n$ examples, how can we quickly select $k$ out of $n$ to best serve as the conditioning for downstream inference? This problem has broad applications in prompt tuning and chain-of-thought reasoning. Since model weights remain fixed during in-context learning, previous work has sought to design methods based on the similarity of token embeddings. This work proposes a new approach based on gradients of the output taken in the input embedding space. Our approach estimates model outputs through a first-order approximation using the gradients. Then, we apply this estimation to multiple randomly sampled subsets. Finally, we aggregate the sampled subset outcomes to form an influence score for each demonstration, and select $k$ most relevant examples. This procedure only requires pre-computing model outputs and gradients once, resulting in a linear-time algorithm relative to model and training set sizes. Extensive experiments across various models and datasets validate the efficiency of our approach. We show that the gradient estimation procedure yields approximations of full inference with less than $\mathbf{1}%$ error across six datasets. This allows us to scale up subset selection that would otherwise run full inference by up to $\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and outperform existing selection methods based on input embeddings by $\mathbf{11}%$ on average.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2508.19752" rel="nofollow">Fast 3D Diffusion for Scalable Granular Media Synthesis</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-fast-3d-diffusion-for-scalable-granular-media-synthesis-" class="anchor" aria-label="Permalink: 2. Fast 3D Diffusion for Scalable Granular Media Synthesis" href="#2-fast-3d-diffusion-for-scalable-granular-media-synthesis-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19752
<strong>Authors:</strong> Muhammad Moeeze Hassan, R'egis Cottereau, Filippo Gatti, Patryk Dec</p>
<p><strong>Abstract:</strong> arXiv:2508.19752v1 Announce Type: new  Abstract: Simulating granular media, using Discrete Element Method is a computationally intensive task. This is especially true during initialization phase, which dominates total simulation time because of large displacements involved and associated kinetic energy. We overcome this bottleneck with a novel generative pipeline based on 3D diffusion models that directly synthesizes arbitrarily large granular assemblies in their final and physically realistic configurations. The approach frames the problem as a 3D generative modeling task, consisting of a two-stage pipeline. First a diffusion model is trained to generate independent 3D voxel grids representing granular media. Second, a 3D inpainting model, adapted from 2D inpainting techniques using masked inputs, stitches these grids together seamlessly, enabling synthesis of large samples with physically realistic structure. The inpainting model explores several masking strategies for the inputs to the underlying UNets by training the network to infer missing portions of voxel grids from a concatenation of noised tensors, masks, and masked tensors as input channels. The model also adapts a 2D repainting technique of re-injecting noise scheduler output with ground truth to provide a strong guidance to the 3D model. This along with weighted losses ensures long-term coherence over generation of masked regions. Both models are trained on the same binarized 3D occupancy grids extracted from small-scale DEM simulations, achieving linear scaling of computational time with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track synthesis equivalent to a 3-hour DEM simulation, was completed under 20 seconds. The generated voxel grids can also be post-processed to extract grain geometries for DEM-compatibility as well, enabling physically coherent, real-time, scalable granular media synthesis for industrial applications.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2508.19443" rel="nofollow">Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-efficiently-generating-multidimensional-calorimeter-data-with-tensor-decomposition-parameterization-" class="anchor" aria-label="Permalink: 3. Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization" href="#3-efficiently-generating-multidimensional-calorimeter-data-with-tensor-decomposition-parameterization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19443
<strong>Authors:</strong> Paimon Goulart, Shaan Pakala, Evangelos Papalexakis</p>
<p><strong>Abstract:</strong> arXiv:2508.19443v1 Announce Type: new  Abstract: Producing large complex simulation datasets can often be a time and resource consuming task. Especially when these experiments are very expensive, it is becoming more reasonable to generate synthetic data for downstream tasks. Recently, these methods may include using generative machine learning models such as Generative Adversarial Networks or diffusion models. As these generative models improve efficiency in producing useful data, we introduce an internal tensor decomposition to these generative models to even further reduce costs. More specifically, for multidimensional data, or tensors, we generate the smaller tensor factors instead of the full tensor, in order to significantly reduce the model's output and overall parameters. This reduces the costs of generating complex simulation data, and our experiments show the generated data remains useful. As a result, tensor decomposition has the potential to improve efficiency in generative models, especially when generating multidimensional data, or tensors.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2508.19926" rel="nofollow">FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-farm-frame-accelerated-augmentation-and-residual-mixture-of-experts-for-physics-based-high-dynamic-humanoid-control-" class="anchor" aria-label="Permalink: 4. FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control" href="#4-farm-frame-accelerated-augmentation-and-residual-mixture-of-experts-for-physics-based-high-dynamic-humanoid-control-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19926
<strong>Authors:</strong> Tan Jing, Shiting Chen, Yangfan Li, Weisheng Xu, Renjing Xu</p>
<p><strong>Abstract:</strong> arXiv:2508.19926v1 Announce Type: new  Abstract: Unified physics-based humanoid controllers are pivotal for robotics and character animation, yet models that excel on gentle, everyday motions still stumble on explosive actions, hampering real-world deployment. We bridge this gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts), an end-to-end framework composed of frame-accelerated augmentation, a robust base controller, and a residual mixture-of-experts (MoE). Frame-accelerated augmentation exposes the model to high-velocity pose changes by widening inter-frame gaps. The base controller reliably tracks everyday low-dynamic motions, while the residual MoE adaptively allocates additional network capacity to handle challenging high-dynamic actions, significantly enhancing tracking accuracy. In the absence of a public benchmark, we curate the High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8% and lowers global mean per-joint position error by 14.6% relative to the baseline, while preserving near-perfect accuracy on low-dynamic motions. These results establish FARM as a new baseline for high-dynamic humanoid control and introduce the first open benchmark dedicated to this challenge. The code and dataset will be released at <a href="https://github.com/Colin-Jing/FARM">https://github.com/Colin-Jing/FARM</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2508.19564" rel="nofollow">Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-bi-lora-efficient-sharpness-aware-minimization-for-fine-tuning-large-scale-models-" class="anchor" aria-label="Permalink: 5. Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models" href="#5-bi-lora-efficient-sharpness-aware-minimization-for-fine-tuning-large-scale-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19564
<strong>Authors:</strong> Yuhang Liu, Tao Li, Zhehao Huang, Zuopeng Yang, Xiaolin Huang</p>
<p><strong>Abstract:</strong> arXiv:2508.19564v1 Announce Type: new  Abstract: Fine-tuning large-scale pre-trained models with limited data presents significant challenges for generalization. While Sharpness-Aware Minimization (SAM) has proven effective in improving generalization by seeking flat minima, its substantial extra memory and computation overhead make it impractical for large models. Integrating SAM with parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) is a promising direction. However, we find that directly applying SAM to LoRA parameters limits the sharpness optimization to a restricted subspace, hindering its effectiveness. To address this limitation, we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an auxiliary LoRA module to model SAM's adversarial weight perturbations. It decouples SAM's weight perturbations from LoRA optimization: the primary LoRA module adapts to specific tasks via standard gradient descent, while the auxiliary module captures the sharpness of the loss landscape through gradient ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness for achieving flatter minima while remaining memory-efficient. Another important benefit is that the dual design allows for simultaneous optimization and perturbation, eliminating SAM's doubled training costs. Extensive experiments across diverse tasks and architectures demonstrate Bi-LoRA's efficiency and effectiveness in enhancing generalization.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2508.19659" rel="nofollow">SCAR: A Characterization Scheme for Multi-Modal Dataset</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-scar-a-characterization-scheme-for-multi-modal-dataset-" class="anchor" aria-label="Permalink: 6. SCAR: A Characterization Scheme for Multi-Modal Dataset" href="#6-scar-a-characterization-scheme-for-multi-modal-dataset-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19659
<strong>Authors:</strong> Ri Su, Zhao Chen, Caleb Chen Cao, Nan Tang, Lei Chen</p>
<p><strong>Abstract:</strong> arXiv:2508.19659v1 Announce Type: new  Abstract: Foundation models exhibit remarkable generalization across diverse tasks, largely driven by the characteristics of their training data. Recent data-centric methods like pruning and compression aim to optimize training but offer limited theoretical insight into how data properties affect generalization, especially the data characteristics in sample scaling. Traditional perspectives further constrain progress by focusing predominantly on data quantity and training efficiency, often overlooking structural aspects of data quality. In this study, we introduce SCAR, a principled scheme for characterizing the intrinsic structural properties of datasets across four key measures: Scale, Coverage, Authenticity, and Richness. Unlike prior data-centric measures, SCAR captures stable characteristics that remain invariant under dataset scaling, providing a robust and general foundation for data understanding. Leveraging these structural properties, we introduce Foundation Data-a minimal subset that preserves the generalization behavior of the full dataset without requiring model-specific retraining. We model single-modality tasks as step functions and estimate the distribution of the foundation data size to capture step-wise generalization bias across modalities in the target multi-modal dataset. Finally, we develop a SCAR-guided data completion strategy based on this generalization bias, which enables efficient, modality-aware expansion of modality-specific characteristics in multimodal datasets. Experiments across diverse multi-modal datasets and model architectures validate the effectiveness of SCAR in predicting data utility and guiding data acquisition. Code is available at <a href="https://github.com/McAloma/SCAR">https://github.com/McAloma/SCAR</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2508.20032" rel="nofollow">Pruning Strategies for Backdoor Defense in LLMs</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-pruning-strategies-for-backdoor-defense-in-llms-" class="anchor" aria-label="Permalink: 7. Pruning Strategies for Backdoor Defense in LLMs" href="#7-pruning-strategies-for-backdoor-defense-in-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20032
<strong>Authors:</strong> Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</p>
<p><strong>Abstract:</strong> arXiv:2508.20032v1 Announce Type: new  Abstract: Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2508.19263" rel="nofollow">Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-lossless-compression-of-neural-network-components-weights-checkpoints-and-kv-caches-in-low-precision-formats-" class="anchor" aria-label="Permalink: 8. Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats" href="#8-lossless-compression-of-neural-network-components-weights-checkpoints-and-kv-caches-in-low-precision-formats-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19263
<strong>Authors:</strong> Anat Heilper, Doron Singer</p>
<p><strong>Abstract:</strong> arXiv:2508.19263v1 Announce Type: new  Abstract: As deep learning models grow and deployment becomes more widespread, reducing the storage and transmission costs of neural network weights has become increasingly important. While prior work such as ZipNN has shown that lossless compression methods - particularly those based on Huffman encoding floating-point exponents can significantly reduce model sizes, these techniques have primarily been applied to higher-precision formats such as FP32 and BF16. In this work, we extend the ZipNN approach to lower-precision floating-point formats, specifically FP8 and FP4, which are gaining popularity for efficient inference. We design a compression method that separates and compresses the exponent and mantissa components independently using entropy coding. Our evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also investigate the compressibility of key-value (K/V) cache tensors used in large language models (LLMs), finding that they, too, exhibit compressible patterns, enabling memory savings during deployment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2508.20019" rel="nofollow">Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-symphony-a-decentralized-multi-agent-framework-for-scalable-collective-intelligence-" class="anchor" aria-label="Permalink: 9. Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence" href="#9-symphony-a-decentralized-multi-agent-framework-for-scalable-collective-intelligence-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20019
<strong>Authors:</strong> Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi</p>
<p><strong>Abstract:</strong> arXiv:2508.20019v1 Announce Type: new  Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on centralized orchestration, incurring high deployment costs, rigid communication topologies, and limited adaptability. To address these challenges, we introduce Symphony, a decentralized multi-agent system which enables lightweight LLMs on consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms: (1) a decentralized ledger that records capabilities, (2) a Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on CoTs. This design forms a privacy-saving, scalable, and fault-tolerant orchestration with low overhead. Empirically, Symphony outperforms existing baselines on reasoning benchmarks, achieving substantial accuracy gains and demonstrating robustness across models of varying capacities.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2508.19353" rel="nofollow">Efficient Multi-Source Knowledge Transfer by Model Merging</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-efficient-multi-source-knowledge-transfer-by-model-merging-" class="anchor" aria-label="Permalink: 10. Efficient Multi-Source Knowledge Transfer by Model Merging" href="#10-efficient-multi-source-knowledge-transfer-by-model-merging-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19353
<strong>Authors:</strong> Marcin Osial, Bartosz W'ojcik, Bartosz Zieli'nski, Sebastian Cygert</p>
<p><strong>Abstract:</strong> arXiv:2508.19353v1 Announce Type: new  Abstract: While transfer learning is an advantageous strategy, it overlooks the opportunity to leverage knowledge from numerous available models online. Addressing this multi-source transfer learning problem is a promising path to boost adaptability and cut re-training costs. However, existing approaches are inherently coarse-grained, lacking the necessary precision for granular knowledge extraction and the aggregation efficiency required to fuse knowledge from either a large number of source models or those with high parameter counts. We address these limitations by leveraging Singular Value Decomposition (SVD) to first decompose each source model into its elementary, rank-one components. A subsequent aggregation stage then selects only the most salient components from all sources, thereby overcoming the previous efficiency and precision limitations. To best preserve and leverage the synthesized knowledge base, our method adapts to the target task by fine-tuning only the principal singular values of the merged matrix. In essence, this process only recalibrates the importance of top SVD components. The proposed framework allows for efficient transfer learning, is robust to perturbations both at the input level and in the parameter space (e.g., noisy or pruned sources), and scales well computationally.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2508.19857" rel="nofollow">Quantum latent distributions in deep generative models</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-quantum-latent-distributions-in-deep-generative-models-" class="anchor" aria-label="Permalink: 11. Quantum latent distributions in deep generative models" href="#11-quantum-latent-distributions-in-deep-generative-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19857
<strong>Authors:</strong> Omar Bacarreza, Thorin Farnsworth, Alexander Makarovskiy, Hugo Wallner, Tessa Hicks, Santiago Sempere-Llagostera, John Price, Robert J. A. Francis-Jones, William R. Clements</p>
<p><strong>Abstract:</strong> arXiv:2508.19857v1 Announce Type: new  Abstract: Many successful families of generative models leverage a low-dimensional latent distribution that is mapped to a data distribution. Though simple latent distributions are commonly used, it has been shown that more sophisticated distributions can improve performance. For instance, recent work has explored using the distributions produced by quantum processors and found empirical improvements. However, when latent space distributions produced by quantum processors can be expected to improve performance, and whether these improvements are reproducible, are open questions that we investigate in this work. We prove that, under certain conditions, these "quantum latent distributions" enable generative models to produce data distributions that classical latent distributions cannot efficiently produce. We also provide actionable intuitions to identify when such quantum advantages may arise in real-world settings. We perform benchmarking experiments on both a synthetic quantum dataset and the QM9 molecular dataset, using both simulated and real photonic quantum processors. Our results demonstrate that quantum latent distributions can lead to improved generative performance in GANs compared to a range of classical baselines. We also explore diffusion and flow matching models, identifying architectures compatible with quantum latent distributions. This work confirms that near-term quantum processors can expand the capabilities of deep generative models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2508.19915" rel="nofollow">Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-ontology-based-concept-distillation-for-radiology-report-retrieval-and-labeling-" class="anchor" aria-label="Permalink: 12. Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling" href="#12-ontology-based-concept-distillation-for-radiology-report-retrieval-and-labeling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19915
<strong>Authors:</strong> Felix N"utzel, Mischa Dombrowski, Bernhard Kainz</p>
<p><strong>Abstract:</strong> arXiv:2508.19915v1 Announce Type: new  Abstract: Retrieval-augmented learning based on radiology reports has emerged as a promising direction to improve performance on long-tail medical imaging tasks, such as rare disease detection in chest X-rays. Most existing methods rely on comparing high-dimensional text embeddings from models like CLIP or CXR-BERT, which are often difficult to interpret, computationally expensive, and not well-aligned with the structured nature of medical knowledge. We propose a novel, ontology-driven alternative for comparing radiology report texts based on clinically grounded concepts from the Unified Medical Language System (UMLS). Our method extracts standardised medical entities from free-text reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These entities are linked to UMLS concepts (CUIs), enabling a transparent, interpretable set-based representation of each report. We then define a task-adaptive similarity measure based on a modified and weighted version of the Tversky Index that accounts for synonymy, negation, and hierarchical relationships between medical entities. This allows efficient and semantically meaningful similarity comparisons between reports. We demonstrate that our approach outperforms state-of-the-art embedding-based retrieval methods in a radiograph classification task on MIMIC-CXR, particularly in long-tail settings. Additionally, we use our pipeline to generate ontology-backed disease labels for MIMIC-CXR, offering a valuable new resource for downstream learning tasks. Our work provides more explainable, reliable, and task-specific retrieval strategies in clinical AI systems, especially when interpretability and domain knowledge integration are essential. Our code is available at <a href="https://github.com/Felix-012/ontology-concept-distillation">https://github.com/Felix-012/ontology-concept-distillation</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2508.19352" rel="nofollow">Memorization in Graph Neural Networks</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-memorization-in-graph-neural-networks-" class="anchor" aria-label="Permalink: 13. Memorization in Graph Neural Networks" href="#13-memorization-in-graph-neural-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19352
<strong>Authors:</strong> Adarsh Jamadandi, Jing Xu, Adam Dziedzic, Franziska Boenisch</p>
<p><strong>Abstract:</strong> arXiv:2508.19352v1 Announce Type: new  Abstract: Deep neural networks (DNNs) have been shown to memorize their training data, yet similar analyses for graph neural networks (GNNs) remain largely under-explored. We introduce NCMemo (Node Classification Memorization), the first framework to quantify label memorization in semi-supervised node classification. We first establish an inverse relationship between memorization and graph homophily, i.e., the property that connected nodes share similar labels/features. We find that lower homophily significantly increases memorization, indicating that GNNs rely on memorization to learn less homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the increased memorization in low homophily graphs is tightly coupled to the GNNs' implicit bias on using graph structure during learning. In low homophily regimes, this structure is less informative, hence inducing memorization of the node labels to minimize training loss. Finally, we show that nodes with higher label inconsistency in their feature-space neighborhood are significantly more prone to memorization. Building on our insights into the link between graph homophily and memorization, we investigate graph rewiring as a means to mitigate memorization. Our results demonstrate that this approach effectively reduces memorization without compromising model performance. Moreover, we show that it lowers the privacy risk for previously memorized data points in practice. Thus, our work not only advances understanding of GNN learning but also supports more privacy-preserving GNN deployment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2508.19567" rel="nofollow">Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-counterfactual-reward-model-training-for-bias-mitigation-in-multimodal-reinforcement-learning-" class="anchor" aria-label="Permalink: 14. Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning" href="#14-counterfactual-reward-model-training-for-bias-mitigation-in-multimodal-reinforcement-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19567
<strong>Authors:</strong> Sheryl Mathew, N Harshit</p>
<p><strong>Abstract:</strong> arXiv:2508.19567v1 Announce Type: new  Abstract: In reinforcement learning with human feedback (RLHF), reward models can efficiently learn and amplify latent biases within multimodal datasets, which can lead to imperfect policy optimization through flawed reward signals and decreased fairness. Bias mitigation studies have often applied passive constraints, which can fail under causal confounding. Here, we present a counterfactual reward model that introduces causal inference with multimodal representation learning to provide an unsupervised, bias-resilient reward signal. The heart of our contribution is the Counterfactual Trust Score, an aggregated score consisting of four components: (1) counterfactual shifts that decompose political framing bias from topical bias; (2) reconstruction uncertainty during counterfactual perturbations; (3) demonstrable violations of fairness rules for each protected attribute; and (4) temporal reward shifts aligned with dynamic trust measures. We evaluated the framework on a multimodal fake versus true news dataset, which exhibits framing bias, class imbalance, and distributional drift. Following methodologies similar to unsupervised drift detection from representation-based distances [1] and temporal robustness benchmarking in language models [2], we also inject synthetic bias across sequential batches to test robustness. The resulting system achieved an accuracy of 89.12% in fake news detection, outperforming the baseline reward models. More importantly, it reduced spurious correlations and unfair reinforcement signals. This pipeline outlines a robust and interpretable approach to fairness-aware RLHF, offering tunable bias reduction thresholds and increasing reliability in dynamic real-time policy making.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2508.19424" rel="nofollow">MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-ms-contab-multi-scale-contrastive-learning-of-mutation-signatures-for-pan-cancer-representation-and-stratification-" class="anchor" aria-label="Permalink: 15. MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification" href="#15-ms-contab-multi-scale-contrastive-learning-of-mutation-signatures-for-pan-cancer-representation-and-stratification-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19424
<strong>Authors:</strong> Yifan Dou, Adam Khadre, Ruben C Petreaca, Golrokh Mirzaei</p>
<p><strong>Abstract:</strong> arXiv:2508.19424v1 Announce Type: new  Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical insights into the molecular mechanisms underlying tumorigenesis. While patient-level machine learning techniques have been widely employed to identify tumor subtypes, cohort-level clustering, where entire cancer types are grouped based on shared molecular features, has largely relied on classical statistical methods.   Results. In this study, we introduce a novel unsupervised contrastive learning framework to cluster 43 cancer types based on coding mutation data derived from the COSMIC database. For each cancer type, we construct two complementary mutation signatures: a gene-level profile capturing nucleotide substitution patterns across the most frequently mutated genes, and a chromosome-level profile representing normalized substitution frequencies across chromosomes. These dual views are encoded using TabNet encoders and optimized via a multi-scale contrastive learning objective (NT-Xent loss) to learn unified cancer-type embeddings. We demonstrate that the resulting latent representations yield biologically meaningful clusters of cancer types, aligning with known mutational processes and tissue origins. Our work represents the first application of contrastive learning to cohort-level cancer clustering, offering a scalable and interpretable framework for mutation-driven cancer subtyping.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2508.19432" rel="nofollow">Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-quantized-but-deceptive-a-multi-dimensional-truthfulness-evaluation-of-quantized-llms-" class="anchor" aria-label="Permalink: 16. Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs" href="#16-quantized-but-deceptive-a-multi-dimensional-truthfulness-evaluation-of-quantized-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19432
<strong>Authors:</strong> Yao Fu, Xianxuan Long, Runchao Li, Haotian Yu, Mu Sheng, Xiaotian Han, Yu Yin, Pan Li</p>
<p><strong>Abstract:</strong> arXiv:2508.19432v1 Announce Type: new  Abstract: Quantization enables efficient deployment of large language models (LLMs) in resource-constrained environments by significantly reducing memory and computation costs. While quantized LLMs often maintain performance on perplexity and zero-shot tasks, their impact on truthfulness-whether generating truthful or deceptive responses-remains largely unexplored. In this work, we introduce TruthfulnessEval, a comprehensive evaluation framework for assessing the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on Imitative Falsehoods. Using this framework, we examine mainstream quantization techniques (ranging from 4-bit to extreme 2-bit) across several open-source LLMs. Surprisingly, we find that while quantized models retain internally truthful representations, they are more susceptible to producing false outputs under misleading prompts. To probe this vulnerability, we test 15 rephrased variants of "honest", "neutral" and "deceptive" prompts and observe that "deceptive" prompts can override truth-consistent behavior, whereas "honest" and "neutral" prompts maintain stable outputs. Further, we reveal that quantized models "know" the truth internally yet still produce false outputs when guided by "deceptive" prompts via layer-wise probing and PCA visualizations. Our findings provide insights into future designs of quantization-aware alignment and truthfulness interventions.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2508.19571" rel="nofollow">Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-escaping-stability-plasticity-dilemma-in-online-continual-learning-for-motion-forecasting-via-synergetic-memory-rehearsal-" class="anchor" aria-label="Permalink: 17. Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal" href="#17-escaping-stability-plasticity-dilemma-in-online-continual-learning-for-motion-forecasting-via-synergetic-memory-rehearsal-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19571
<strong>Authors:</strong> Yunlong Lin, Chao Lu, Tongshuai Wu, Xiaocong Zhao, Guodong Du, Yanwei Sun, Zirui Li, Jianwei Gong</p>
<p><strong>Abstract:</strong> arXiv:2508.19571v1 Announce Type: new  Abstract: Deep neural networks (DNN) have achieved remarkable success in motion forecasting. However, most DNN-based methods suffer from catastrophic forgetting and fail to maintain their performance in previously learned scenarios after adapting to new data. Recent continual learning (CL) studies aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the ability to retain learned knowledge. Yet, excessive emphasis on the memory stability often impairs learning plasticity, i.e., the capacity of DNN to acquire new information effectively. To address such stability-plasticity dilemma, this study proposes a novel CL method, synergetic memory rehearsal (SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory buffer to represent learned knowledge. To ensure memory stability, it employs an inequality constraint that limits increments in the average loss over the memory buffer. Synergistically, a selective memory rehearsal mechanism is designed to enhance learning plasticity by selecting samples from the memory buffer that are most similar to recently observed data. This selection is based on an online-measured cosine similarity of loss gradients, ensuring targeted memory rehearsal. Since replayed samples originate from learned scenarios, this memory rehearsal mechanism avoids compromising memory stability. We validate SyReM under an online CL paradigm where training samples from diverse scenarios arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM significantly mitigates catastrophic forgetting in past scenarios while improving forecasting accuracy in new ones. The implementation is publicly available at <a href="https://github.com/BIT-Jack/SyReM">https://github.com/BIT-Jack/SyReM</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2508.19958" rel="nofollow">Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-long-vla-unleashing-long-horizon-capability-of-vision-language-action-model-for-robot-manipulation-" class="anchor" aria-label="Permalink: 18. Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation" href="#18-long-vla-unleashing-long-horizon-capability-of-vision-language-action-model-for-robot-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19958
<strong>Authors:</strong> Yiguo Fan, Pengxiang Ding, Shuanghao Bai, Xinyang Tong, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, Zhaoxin Fan, Badong Chen, Donglin Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.19958v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model. Furthermore, it incorporates Mixture-of-Experts (MoE) architectures to significantly decrease deployment overhead and accelerate inference speed, enabling more efficient and scalable model serving in resource-constrained environments.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>