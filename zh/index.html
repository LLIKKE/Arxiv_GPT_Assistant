<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">高效编辑混合专家模型与压缩专家</h2><a id="user-content-高效编辑混合专家模型与压缩专家" class="anchor" aria-label="Permalink: 高效编辑混合专家模型与压缩专家" href="#高效编辑混合专家模型与压缩专家"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.00634v1 公告类型：新研究<br>
摘要：专家混合（Mixture-of-Experts, MoE）模型已成为高效扩展大型语言模型的关键方法，通过在训练和推理过程中仅激活部分专家来实现。通常，激活专家的数量存在一个权衡：较少的专家能降低计算成本，而更多的专家则能提升性能。最近的研究表明，并非所有被激活的专家对模型性能的贡献均等，有些专家提供的效用微乎其微，尤其是在针对特定下游任务微调预训练MoE模型时。专家中重要参数与冗余参数的并存，为我们提供了在保持模型性能的同时减少激活专家数量的机会。在本研究中，我们提出了压缩专家的概念，即作为完整专家紧凑表示的轻量级模块。我们的方法保留了最重要的专家，同时用压缩专家替换其他辅助激活的专家。通过减少活跃参数，显著降低了推理成本，同时实现了可比的性能。在包括Phi-MoE和OLMoE在内的模型上进行的大量实验表明，压缩专家在各种任务中恢复了超过90%的完整专家性能，同时减少了超过30%的活跃参数，并节省了20%的推理成本。这一方法使得MoE模型能够在资源受限的环境中高效部署，并有助于以可控的开销扩展至更大模型。我们的代码可在<a href="https://github.com/yifei-he/Compressed-Experts%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yifei-he/Compressed-Experts获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">精英KV：通过RoPE频率选择与联合低秩投影实现可扩展的键值缓存压缩</h2><a id="user-content-精英kv通过rope频率选择与联合低秩投影实现可扩展的键值缓存压缩" class="anchor" aria-label="Permalink: 精英KV：通过RoPE频率选择与联合低秩投影实现可扩展的键值缓存压缩" href="#精英kv通过rope频率选择与联合低秩投影实现可扩展的键值缓存压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.01586v1 公告类型：新研究  摘要：旋转位置嵌入（RoPE）使每个注意力头能够沿序列维度捕获多频信息，并广泛应用于基础模型中。然而，RoPE引入的非线性使得基于RoPE的注意力机制中键值（KV）缓存的关键状态优化变得复杂。现有的KV缓存压缩方法通常存储旋转前的键状态，并在解码过程中应用变换，这引入了额外的计算开销。本文介绍了EliteKV，一个支持可变KV缓存压缩比的基于RoPE模型的灵活修改框架。EliteKV首先使用RoPElite识别每个头的内在频率偏好，在注意力计算中选择性恢复键的某些维度的线性。在此基础上，键和值的联合低秩压缩实现了部分缓存共享。实验结果表明，仅使用原始训练数据的$0.6%$进行最小限度的再训练，基于RoPE的模型在保持性能几乎不变的情况下，实现了KV缓存大小减少$75%$。此外，EliteKV在同一系列的不同规模模型中表现一致良好。</p>
<div class="markdown-heading"><h2 class="heading-element">DeRS：迈向极其高效的升级版专家混合模型</h2><a id="user-content-ders迈向极其高效的升级版专家混合模型" class="anchor" aria-label="Permalink: DeRS：迈向极其高效的升级版专家混合模型" href="#ders迈向极其高效的升级版专家混合模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.01359v1 公告类型：新研究 摘要：升级版的专家混合模型（MoE）通过将预训练密集模型中的前馈网络（FFN）层转换为MoE层，已在多种任务中展现出巨大潜力。然而，由于引入了多个专家，这些模型仍存在显著的参数效率低下问题。在本研究中，我们提出了一种新颖的DeRS（分解、替换与合成）范式，以克服这一不足，这一范式的灵感来源于我们对升级版MoE专家独特冗余机制的观察。具体而言，DeRS将专家分解为一个专家共享的基础权重和多个专家特定的增量权重，并随后以轻量级形式表示这些增量权重。我们提出的DeRS范式可应用于提升两种不同场景下的参数效率，包括：1）用于推理阶段的DeRS压缩，采用稀疏化或量化技术压缩普通升级版MoE模型；以及2）用于训练阶段的DeRS升级，利用轻量级稀疏或低秩矩阵高效地将密集模型升级为MoE模型。跨三个不同任务的大量实验表明，所提出的方法能在保持升级版MoE模型训练和压缩性能的同时，实现极致的参数效率。</p>
<div class="markdown-heading"><h2 class="heading-element">RSQ：从关键令牌中学习，引领量化大型语言模型更上一层楼</h2><a id="user-content-rsq从关键令牌中学习引领量化大型语言模型更上一层楼" class="anchor" aria-label="Permalink: RSQ：从关键令牌中学习，引领量化大型语言模型更上一层楼" href="#rsq从关键令牌中学习引领量化大型语言模型更上一层楼"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.01820v1 公告类型：新研究  摘要：分层量化是一种无需昂贵再训练即可高效压缩大型模型的关键技术。以往的方法通常通过“均匀”优化所有输出标记的层重建损失来量化每层的权重。然而，本文中我们证明，通过优先学习重要标记（例如具有较大注意力分数的标记），可以获得更好的量化模型。基于这一发现，我们提出了RSQ（旋转、缩放，然后量化），该方法（1）对模型应用旋转（正交变换）以减少异常值（具有异常大数值的标记），（2）根据标记的重要性缩放其特征，以及（3）使用GPTQ框架和由缩放标记计算的二阶统计量对模型进行量化。为了计算标记的重要性，我们探索了启发式和动态策略。通过对所有方法的深入分析，我们采用注意力集中度，即使用每个标记的注意力分数作为其重要性，作为最佳方法。我们展示了RSQ在多个下游任务和三个模型家族（LLaMA3、Mistral和Qwen2.5）中始终优于基线方法。此外，使用RSQ量化的模型在长上下文任务中表现出色，进一步凸显了其有效性。最后，RSQ展示了在各种设置下的通用性，包括不同的模型大小、校准数据集、比特精度和量化方法。</p>
<div class="markdown-heading"><h2 class="heading-element">KurTail：基于峰度的LLM量化</h2><a id="user-content-kurtail基于峰度的llm量化" class="anchor" aria-label="Permalink: KurTail：基于峰度的LLM量化" href="#kurtail基于峰度的llm量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.01483v1 公告类型：新  摘要：量化大型语言模型（LLM）的挑战之一在于异常值的存在。异常值往往使得均匀量化方案效果不佳，尤其是在极端情况下，如4位量化。我们引入了KurTail，一种新的训练后量化（PTQ）方案，它利用基于峰度的旋转来减轻LLM激活中的异常值。我们的方法优化峰度作为尾重性的度量。这一方法使得权重、激活和KV缓存能够在4位下进行量化。我们采用分层优化，确保内存效率。KurTail在现有量化方法中表现优异，与QuaRot相比，MMLU准确率提升了13.3%，Wiki困惑度降低了15.5%。同时，它超越了SpinQuant，MMLU增益达到2.6%，困惑度减少2.9%，且降低了训练成本。作为对比，使用SpinQuant学习Llama3-70B的旋转至少需要四块NVIDIA H100 80GB GPU，而我们的方法仅需一块GPU，为消费级GPU提供了更易实现的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">渐进式稀疏注意力：算法与系统协同设计，优化大语言模型服务中的注意力机制效率</h2><a id="user-content-渐进式稀疏注意力算法与系统协同设计优化大语言模型服务中的注意力机制效率" class="anchor" aria-label="Permalink: 渐进式稀疏注意力：算法与系统协同设计，优化大语言模型服务中的注意力机制效率" href="#渐进式稀疏注意力算法与系统协同设计优化大语言模型服务中的注意力机制效率"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.00392v1 公告类型：新研究<br>
摘要：处理长上下文已成为现代大型语言模型（LLMs）的关键能力。然而，由于键值（KV）缓存的高内存开销，服务长上下文LLMs带来了显著的推理成本。现有工作利用动态稀疏注意力算法（DSAes）来减轻KV缓存开销，但这些算法依赖于top-$k$ KV缓存选择，导致在准确性和效率之间做出权衡。较大的$k$提高准确性但降低效率，而较小的$k$提升效率但牺牲准确性。为了克服这一权衡，本文提出了PSA，一种$\underline{P}$rogressive $\underline{S}$parse $\underline{A}$ttention机制，它将算法创新与系统协同设计相结合，以实现LLM服务中的高推理准确性和提升的效率。PSA算法根据实际注意力权重分布自适应地调整不同令牌和层的KV缓存预算，而不是依赖固定的预算$k$。这实现了高准确性，同时最小化了KV缓存的使用。为了进一步提高执行效率，我们引入了一种流水线迭代方案，减少了PSA计算期间的CPU-GPU交错和同步开销。此外，我们实现了统一的GPU内存管理，通过考虑不同模型层之间不均匀的内存需求来优化PSA的内存利用率。大量实验结果表明，与最先进的DSAes和没有稀疏注意力的系统相比，PSA将注意力计算的KV缓存使用量减少了最多2.4$\times$和8.8$\times$，并将端到端服务吞吐量提高了最多1.4$\times$和2.0$\times$。</p>
<div class="markdown-heading"><h2 class="heading-element">CoSMoEs：紧凑稀疏专家混合体</h2><a id="user-content-cosmoes紧凑稀疏专家混合体" class="anchor" aria-label="Permalink: CoSMoEs：紧凑稀疏专家混合体" href="#cosmoes紧凑稀疏专家混合体"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.00245v1 公告类型：新研究  摘要：稀疏专家混合模型（Sparse Mixture of Expert, MoE）在大规模应用中作为基础架构广受欢迎，但在较小规模下却鲜有探索。本文展示了如何实现紧凑型稀疏专家混合模型（Compact Sparse Mixture of Experts, CoSMoEs）以支持设备端推理。具体而言，我们针对设备端的三大维度——质量、内存和延迟——进行了优化。在质量维度上，通过公平评估（排除混杂因素），我们证明了MoE架构在设备端规模上优于计算量（FLOP）对齐的密集模型。我们引入了权重分解专家机制，进一步提升了MoE模型的性能。关于模型内存和延迟，我们显著提高了模型卸载效率，从而降低了模型推理延迟。</p>
<div class="markdown-heading"><h2 class="heading-element">困境：边缘计算系统中的联合大语言模型量化与分布式大语言模型推理</h2><a id="user-content-困境边缘计算系统中的联合大语言模型量化与分布式大语言模型推理" class="anchor" aria-label="Permalink: 困境：边缘计算系统中的联合大语言模型量化与分布式大语言模型推理" href="#困境边缘计算系统中的联合大语言模型量化与分布式大语言模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.01704v1 公告类型：新摘要 摘要：随着近期将大型语言模型（LLMs）应用于智慧城市中不同应用的趋势，有必要将这些模型推向网络边缘，同时保持其性能。边缘计算（EC）作为更接近终端用户的物理计算资源，能够帮助减少为依赖LLM的服务处理用户任务时的通信延迟。然而，EC服务器在通信、计算和存储能力方面存在限制。本文介绍了DILEMMA，一个新颖的框架，通过联合优化EC系统中的层级放置和层级量化，解决了在EC系统中部署LLMs的挑战。DILEMMA构建了一个整数线性规划问题，旨在最小化总推理延迟，同时确保LLM性能处于可接受水平，利用层级量化和知识蒸馏来控制LLM性能。使用SQuAD数据集对OPT-350模型进行的实验评估表明，DILEMMA在保持模型损失的同时，实现了高达12.75%的量化比率，凸显了其在资源受限环境中的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">基于正则化的量化、故障与变异性感知训练框架</h2><a id="user-content-基于正则化的量化故障与变异性感知训练框架" class="anchor" aria-label="Permalink: 基于正则化的量化、故障与变异性感知训练框架" href="#基于正则化的量化故障与变异性感知训练框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.01297v1 公告类型：新研究 摘要：高效推理对于在边缘AI设备上部署深度学习模型至关重要。采用定点算术的低比特量化（如3位和4位）提升了效率，而模拟非易失性存储器等低功耗内存技术则进一步增强了性能。然而，这些方法引入了非理想的硬件行为，包括比特故障和设备间差异。我们提出了一种基于正则化的量化感知训练（QAT）框架，该框架支持固定、可学习步长以及可学习的非均匀量化，在CIFAR-10和ImageNet上取得了有竞争力的成果。我们的方法还扩展到了脉冲神经网络（SNNs），在CIFAR10-DVS和N-Caltech 101的4位网络上展示了强劲性能。除了量化之外，我们的框架还支持故障和差异感知的微调，能够缓解卡在固定权重比特的故障和设备电阻差异。与之前的故障感知训练相比，我们的方法在高达20%的比特故障率和40%的设备间差异下显著提升了性能恢复能力。我们的研究成果建立了一个可推广的量化与鲁棒性训练框架，增强了低功耗、非理想硬件中的效率和可靠性。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>