<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">《Squeeze10-LLM：通过分阶段混合精度量化方法将大语言模型权重压缩十倍》</h2><a id="user-content-squeeze10-llm通过分阶段混合精度量化方法将大语言模型权重压缩十倍" class="anchor" aria-label="Permalink: 《Squeeze10-LLM：通过分阶段混合精度量化方法将大语言模型权重压缩十倍》" href="#squeeze10-llm通过分阶段混合精度量化方法将大语言模型权重压缩十倍"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>注：译文采用技术文档常见的标题命名规范，保留英文模型名称"Squeeze10-LLM"保持专业性，通过冒号分层解释技术原理。"Staged Mixed-Precision Quantization Method"译为"分阶段混合精度量化方法"准确体现算法特征，其中：</p>
<ol>
<li>"Staged"译为"分阶段"强调流程化特性</li>
<li>"Mixed-precision"采用行业通用译法"混合精度"</li>
<li>量词"10 Times"转换符合中文习惯的"十倍"表达</li>
<li>动词"Squeezing"意译为"压缩"更符合中文技术语境</li>
</ol>
<p>部署大型语言模型（LLMs）面临巨大参数量和高计算成本的挑战。超低位数量化技术能显著降低存储需求并加速推理，但极端压缩（即平均位宽≤2比特）往往导致性能严重下降。为此，我们提出Squeeze10-LLM方案，将16比特LLM的权重高效"压缩"至原体积的1/10。具体而言，该框架采用分阶段混合精度训练后量化（PTQ）策略，通过将80%权重量化为1比特、20%量化为4比特，实现平均1.6比特/权重的突破性成果。</p>
<p>Squeeze10-LLM的核心创新在于两项关键技术：后二值化激活鲁棒性（PBAR）和全信息激活监督（FIAS）。PBAR作为改进的权重重要性度量标准，量化时兼顾对激活值的影响，有效提升低位宽环境下的精度；FIAS策略则在量化过程中完整保留激活信息，缓解误差跨层累积传播问题。在LLaMA和LLaMA2上的实验表明，该方案在2比特以下权重专用量化领域达到最先进水平——在六个零样本分类任务中平均准确率从43%提升至56%，较现有PTQ方法实现显著突破。相关代码将在论文发表时开源。</p>
<div class="markdown-heading"><h2 class="heading-element">大语言模型量化的几何视角：GPTQ即巴拜最近平面算法</h2><a id="user-content-大语言模型量化的几何视角gptq即巴拜最近平面算法" class="anchor" aria-label="Permalink: 大语言模型量化的几何视角：GPTQ即巴拜最近平面算法" href="#大语言模型量化的几何视角gptq即巴拜最近平面算法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>将大型语言模型（LLM）的权重从16位量化至更低比特宽度，是将海量Transformer模型部署到经济型加速器上的实际解决方案。GPTQ作为LLM规模下单次训练后量化的标准方法之一崭露头角，但其内部机制被描述为一系列临时性代数更新步骤，掩盖了几何意义与最坏情况保障。本研究揭示：当对线性层执行从后向前（即从末维到首维）的量化时，GPTQ在数学上等同于Babai最近平面算法——该算法用于求解由输入海森矩阵定义的格上经典最近向量问题（CVP）。这一等价性基于严密的数学论证，并产生两项分析性推论：(i) GPTQ误差传播步骤获得了直观的几何解释；(ii) 在无剪裁条件下，GPTQ继承了Babai算法的误差上界。这些发现为GPTQ奠定了坚实的理论基础，并为将格算法领域数十年的进展引入未来百亿参数模型的量化算法设计开启了新篇章。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>