<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">再论模型插值的高效推理</h2><a id="user-content-再论模型插值的高效推理" class="anchor" aria-label="Permalink: 再论模型插值的高效推理" href="#再论模型插值的高效推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10977v1宣布类型：新摘要：模型合并（通常是指令和思维模型）在高效推理方面表现出了出色的性能。在本文中，我们系统地回顾了直接内插两个权重的最简单合并方法。特别是，我们观察到模型插值遵循三阶段进化范式，推理轨迹上有不同的行为。这些动态为处理性能成本权衡提供了原则性指南。经验结果表明，战略插值模型在效率和有效性方面出人意料地超过了复杂的模型合并基线。我们通过对模型层、模块和解码策略的广泛消融研究进一步验证了我们的发现。最终，这项工作揭开了模型插值的神秘面纱，并为制作具有精确定向推理能力的模型提供了一个实用框架。代码可在\href{https：//github.com/wutaiqiang/MI}{Github}上获取。</p>
<div class="markdown-heading"><h2 class="heading-element">通过多智能体强化学习和语义融合协作文本到图像生成</h2><a id="user-content-通过多智能体强化学习和语义融合协作文本到图像生成" class="anchor" aria-label="Permalink: 通过多智能体强化学习和语义融合协作文本到图像生成" href="#通过多智能体强化学习和语义融合协作文本到图像生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10633v1宣布类型：新摘要：多模式文本到图像的生成仍然受到在不同视觉领域维护语义对齐和专业级别细节的困难的限制。我们提出了一个多代理强化学习框架，该框架协调领域专业化代理（例如，专注于建筑、肖像和风景图像）位于两个耦合的子系统中：文本增强模块和图像生成模块，每个模块都通过多模式集成组件进行增强。在平衡语义相似性、语言视觉质量和内容多样性的复合奖励函数下，使用近端策略优化（PPO）来训练代理。跨模式对齐是通过对比学习、双向注意和文本和图像之间的迭代反馈来实现的。在六个实验环境中，我们的系统显着丰富了生成的内容（字数增加了1614%），同时将ROUGE-1分数降低了69.7%。在融合方法中，尽管偶尔存在稳定性问题，但基于Transformer的策略获得了最高的综合评分（0.521）。多模式集合具有中等的一致性（范围从0.444到0.481），反映了跨模式语义基础的持续挑战。这些发现强调了协作、专业化驱动的架构对于推进可靠的多模式生成系统的前景。</p>
<div class="markdown-heading"><h2 class="heading-element">通过功能性注意力控制缓解多模式推理中的幻觉</h2><a id="user-content-通过功能性注意力控制缓解多模式推理中的幻觉" class="anchor" aria-label="Permalink: 通过功能性注意力控制缓解多模式推理中的幻觉" href="#通过功能性注意力控制缓解多模式推理中的幻觉"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10285v1宣布类型：新摘要：多模式大型推理模型（MLRM）正在迅速推进视觉语言推理，并正在成为跨模式智能的基础。幻觉仍然是一种持续存在的失败模式，表现为错误的推理链和对视觉内容的误解。在这项研究中，我们观察到注意力头表现出阶段性的划分：浅头主要服务于感知，而深头则转向符号推理，揭示了幻觉的两个主要原因，即感知偏见和推理漂移。为了解决这些问题，我们提出了一个轻量级且可解释的两步插件：功能头部识别和类条件重新缩放，它可以定位面向感知和推理的头部，并在无需再培训的情况下调节它们的贡献。对三个现实世界的MLRM（Kimi-BL、Ocean-R1、R1-Onevision）、三个领域的六个基准测试和四个基线的评估表明，我们的插件实现了5%和最高15%的平均改进，只有&lt;1%的额外计算和9%的基线延迟。我们的方法完全与模型无关，显着增强了现成MLRM的可靠性和可解释性，从而使其能够在高风险应用中安全部署。我们的代码可在<a href="https://anonymous.4open.science/r/Functional-Attention-Control%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82" rel="nofollow">https://anonymous.4open.science/r/Functional-Attention-Control上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">RoVer：机器人奖励模型作为视觉-语言-动作模型的测试时验证器</h2><a id="user-content-rover机器人奖励模型作为视觉-语言-动作模型的测试时验证器" class="anchor" aria-label="Permalink: RoVer：机器人奖励模型作为视觉-语言-动作模型的测试时验证器" href="#rover机器人奖励模型作为视觉-语言-动作模型的测试时验证器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10975v1宣布类型：新摘要：视觉-语言-动作（VLA）模型已成为体现智能的重要范式，但进一步的性能改进通常依赖于扩大训练数据和模型大小--这种方法对于机器人来说成本过高，并且从根本上受到数据收集成本的限制。我们通过$\mathBF{RoVer}$来解决这一限制，一个具体的测试时扩展框架，使用$\mathBF{Ro}$bot Process Reward模型（PRM）作为测试时$\mathBF{Ver}$tifier来增强现有VLA模型的能力，而无需修改其架构或权重。具体来说，RoVer（i）分配基于规模的流程奖励以评估候选动作的可靠性，并（ii）预测候选扩展/细化的动作空间方向。在推理过程中，RoVer从基本策略同时生成多个候选动作，沿着PRM预测的方向扩展它们，然后用PRM对所有候选进行评分，以选择最佳动作来执行。值得注意的是，通过缓存共享的感知特征，它可以摊销感知成本并在相同的测试时计算预算下评估更多候选人。从本质上讲，我们的方法有效地将可用的计算资源转化为更好的行动决策，在无需额外训练费用的情况下实现测试时间扩展的好处。我们的贡献有三重：（1）针对VLA的通用、即插即用的测试时扩展框架;（2）PRM，联合提供纯量过程奖励和指导探索的动作空间方向;（3）高效的方向引导采样策略，利用共享的感知缓存来实现可扩展的候选生成和选择推理期间。</p>
<div class="markdown-heading"><h2 class="heading-element">超越AlphaEarth：通过兴趣点引导的对比学习实现以人为本的空间表示</h2><a id="user-content-超越alphaearth通过兴趣点引导的对比学习实现以人为本的空间表示" class="anchor" aria-label="Permalink: 超越AlphaEarth：通过兴趣点引导的对比学习实现以人为本的空间表示" href="#超越alphaearth通过兴趣点引导的对比学习实现以人为本的空间表示"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09894v1宣布类型：新摘要：通用空间表示对于构建可转移的地理空间基础模型（GFM）至关重要。其中，AlphaEarth基金会（AE）代表着朝着全球统一表示地球表面迈出的重要一步，从多源地球观测（EO）数据中学习10米嵌入，这些数据捕捉了不同景观中丰富的物理和环境模式。然而，这种EO驱动的表示在捕捉城市的功能和社会经济方面仍然有限，因为它们主要编码物理和光谱模式，而不是人类活动或空间功能。我们提出了AETHER（AlphaEarth-POI Enriched Representation Learning），这是一个轻量级框架，通过兴趣点（POI）引导的多模态对齐，使AlphaEarth适应以人为本的城市分析。AETHER将AE嵌入与POI的文本表示相结合，通过有关城市功能和社会经济背景的语义线索丰富物理基础EO特征。在大伦敦地区，AETHER在AE基线上实现了一致的收益，土地利用分类F1相对改善了7.2%，社会经济制图的Kullback-Leibler分歧相对减少了23.6%。AETHER基于预训练的AE，利用轻量级多模式对齐来丰富以人为本的语义，同时保持计算效率和可扩展性，以适应城市应用。通过将EO与以人为本的语义相结合，它将地理空间基础模型推向集成物理形式和功能意义的通用城市表示。</p>
<div class="markdown-heading"><h2 class="heading-element">乳液：平滑量化训练的优化环境</h2><a id="user-content-乳液平滑量化训练的优化环境" class="anchor" aria-label="Permalink: 乳液：平滑量化训练的优化环境" href="#乳液平滑量化训练的优化环境"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08757v1宣布类型：新摘要：为量化目标优化神经网络从根本上来说具有挑战性，因为量化器是逐段不变的，除了衍生物未定义的量化阈值之外，在任何地方都产生零梯度。大多数现有方法通过使用直通估计器（STE）等技术放松梯度计算来处理这个问题，并且不提供任何收敛保证。在这项工作中，从Nesterov平滑的灵感，我们近似的量化损失表面与连续的损失表面。特别是，我们引入了LOTION，通过s\textbf{T}随机-非\textbf{I}se sm\textbf {O}othi\textbf{N}g进行低精度优化，这是一个原则性的平滑框架，可以在无偏随机舍入噪声下用其期望值替换原始量化损失。在这个框架中，标准优化器保证收敛到损失曲面的局部最小值。此外，当使用来自随机舍入的噪声时，我们表明，原始量化损失的全局最小值被保留。我们的经验表明，这种方法优于标准的QAT合成测试平台和150 M和300 M参数的语言模型。</p>
<div class="markdown-heading"><h2 class="heading-element">SyncLipMAE：用于视听说话面部表示的对比掩蔽预训练</h2><a id="user-content-synclipmae用于视听说话面部表示的对比掩蔽预训练" class="anchor" aria-label="Permalink: SyncLipMAE：用于视听说话面部表示的对比掩蔽预训练" href="#synclipmae用于视听说话面部表示的对比掩蔽预训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10069v1宣布类型：新摘要：我们引入了SyncLipMAE，这是一个针对说话面部视频的自我监督预训练框架，可以从未标记的视听流中学习同步感知且可转移的面部动态。我们的方法将视觉建模与跨模式对比对齐结合起来，并采用三个每帧提示令牌，这些令牌明确编码说话面部框架的基本因素-身份、声乐运动（语音同步面部动态）和环境运动（音频不可知运动，例如眨眼和头部姿势）。对比目标使用时间对齐的声音运动和音频令牌作为积极的，使用未对齐的对作为消极的，将两种模式驱动到共享的嵌入空间中并产生令牌级视听流同步。预训练后，对齐的音频令牌与视觉提示令牌（身份、声乐运动、环境运动）一起形成四种不同下游设置的统一界面：（i）视听流同步;（ii）面部情感和头部/面部动作识别;（iii）视觉语音识别;和（iv）视觉配音，为此我们在单个模型内实现了难以区分的音频或视频驱动控制。在四个需要不同能力的任务系列中，NSO LipMAE实现了最先进的结果，强调了同步感知、因子分解自我监督预训练的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">Logits Replay + Molip：稳定、低成本的训练后，忘记最少</h2><a id="user-content-logits-replay--molip稳定低成本的训练后忘记最少" class="anchor" aria-label="Permalink: Logits Replay + Molip：稳定、低成本的训练后，忘记最少" href="#logits-replay--molip稳定低成本的训练后忘记最少"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09152v1宣布类型：新摘要：大型语言模型（LLM）在训练后常常面临权衡：专业领域的改进常常以牺牲通用能力为代价。现有的解决方案试图通过正规化、选择性参数更新或以数据为中心的重播来缓解这种紧张关系，但每种解决方案都在计算、数据访问或适应性方面带来了巨大的成本。最近的工作表明，训练信号可以被压缩到logit的子集，而不会出现严重的准确性损失，这为高效适应提供了一条途径。然而，天真截断会破坏优化的稳定并加剧遗忘。   我们引入了Logits Replay + MoTrap，这是一个两阶段框架，可以压缩logit空间中的监督并稳定更新级别的优化。在第0阶段，我们记录覆盖概率阈值的动态Top-K代币子集，始终包括黄金标签。在第1阶段，我们重播这些紧凑子集来计算精确的重正化损失，避免完全softmax计算和隐式正规化。为了确保稳定性，我们设计了MoTrap，这是一个优化器，可以限制梯度动量旋转并应用基于arctan 2的更新重新缩放。从经验上看，我们的方法提高了通信技术（CT）和NL 2SQL任务的领域性能，同时减少了一般基准测试（MMLU、BBH、GPQA、MATH）的遗忘，并将训练成本降低了40%以上。这些贡献共同为LLM的领域适应提供了一条可扩展的、架构不可知的路径，而不会牺牲通用性。</p>
<div class="markdown-heading"><h2 class="heading-element">DixitWorld：使用多智能体分散游戏玩法评估视觉语言模型中的多模式外展推理</h2><a id="user-content-dixitworld使用多智能体分散游戏玩法评估视觉语言模型中的多模式外展推理" class="anchor" aria-label="Permalink: DixitWorld：使用多智能体分散游戏玩法评估视觉语言模型中的多模式外展推理" href="#dixitworld使用多智能体分散游戏玩法评估视觉语言模型中的多模式外展推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10117v1宣布类型：新摘要：多模式回溯推理--从部分观察中生成和选择解释性假设--是智力的基石。目前对视觉语言模型（VLM）中这种能力的评估主要局限于静态的单智能体任务。受Diplomacy的启发，我们推出了DixitWorld，这是一个旨在解构这一挑战的综合评估套件。DIXITWORLD具有两个核心组件：DixitArena，一个动态的多代理环境，在不完美的信息下评估假设生成（“讲故事的人”制作神秘的线索）和假设选择（“听众”从诱饵中选择目标图像）;和DixitBench，一个静态的QA基准，隔离听众的任务，以进行有效的，受控的评估。DixitArena的结果揭示了不同的角色依赖行为：较小的开源模型通常擅长创造性的讲故事者，产生富有想象力但不那么有区别的线索，而较大的专有模型表现出更好的整体性能，特别是作为听众。DixitBench上的性能与DixitArena中的监听结果密切相关，验证了它作为假设选择的可靠代理。我们的研究结果揭示了多模式回溯推理中生成性创造力和辨别性理解之间的关键权衡，这是开发更平衡、更有能力的视觉语言代理人的核心挑战。</p>
<div class="markdown-heading"><h2 class="heading-element">OmniVideoBench：Omni MLLM的视听理解评估</h2><a id="user-content-omnivideobenchomni-mllm的视听理解评估" class="anchor" aria-label="Permalink: OmniVideoBench：Omni MLLM的视听理解评估" href="#omnivideobenchomni-mllm的视听理解评估"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10689v1宣布类型：新摘要：多模式大型语言模型（MLLM）的最新进展已经证明了视频理解方面的巨大潜力。然而，现有的基准无法全面评估音频和视觉模式之间的协同推理能力，通常忽视其中一种模式或以逻辑上不一致的方式集成它们。为了弥合这一差距，我们引入了OmniVideoBench，这是一个大规模、严格设计的基准，致力于评估协同视听理解，并高度强调模式互补性和逻辑一致性。具体来说，OmniVideoBench由1000个高质量的问答（QA）对组成，每个对都带有分步推理痕迹，这些痕迹来自628个不同的视频，范围从几秒到30分钟，并经过手动验证以保证完全正确性和唯一性。此外，OmniVideoBench包含13种精心设计的问题类型，涵盖时间推理、空间定位、计数、因果推理、总结等，从而捕捉视频理解的基本挑战。OmniVideoBench上对多个MLLM的评估揭示了模型性能和人类推理之间存在明显差距，开源模型明显落后于其封闭源模型，凸显了真正视听推理的固有困难。我们将发布OmniVideoBench，以促进具有更强大、更通用推理能力的MLLM的开发。</p>
<div class="markdown-heading"><h2 class="heading-element">X-VLA：作为可扩展跨实施例视觉-语言-动作模型的软嵌入式Transformer</h2><a id="user-content-x-vla作为可扩展跨实施例视觉-语言-动作模型的软嵌入式transformer" class="anchor" aria-label="Permalink: X-VLA：作为可扩展跨实施例视觉-语言-动作模型的软嵌入式Transformer" href="#x-vla作为可扩展跨实施例视觉-语言-动作模型的软嵌入式transformer"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10274v1宣布类型：新摘要：成功的通才视觉-语言-动作（VLA）模型依赖于跨具有大规模、跨实施、异类数据集的不同机器人平台的有效训练。为了促进和利用丰富、多样化的机器人数据源中的多样性，我们提出了一种具有最少添加参数的新型软提示方法，通过将提示学习概念注入到跨实施机器人学习中，并为每个不同的数据源引入单独的可学习嵌入集。这些嵌入充当特定于描述的提示，它们统一地使VLA模型能够有效利用不同的跨实施特征。我们的新X-VLA是一种简洁的基于流匹配的VLA架构，完全依赖于软提示的标准Transformer编码器，具有可扩展性和简单性。经过6个模拟和3个现实世界机器人的评估，我们的0.9B实例化-X-VLA-0.9B在一系列基准测试中同时实现了SOTA性能，在广泛的能力方面展示了卓越的结果，从灵活的灵活性到跨实施例、环境和任务的快速适应。网站：<a href="https://thu-air-dream.github.io/X-VLA/" rel="nofollow">https://thu-air-dream.github.io/X-VLA/</a></p>
<div class="markdown-heading"><h2 class="heading-element">多模式提示优化：为什么不利用多模式实现MLLM</h2><a id="user-content-多模式提示优化为什么不利用多模式实现mllm" class="anchor" aria-label="Permalink: 多模式提示优化：为什么不利用多模式实现MLLM" href="#多模式提示优化为什么不利用多模式实现mllm"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09201v1宣布类型：新摘要：大型语言模型（LLM）取得了显着的成功，其多模式扩展（MLLM）进一步释放了跨越图像、视频和文本以外的其他形式的功能。然而，尽管发生了这一转变，旨在减少手动提示制作负担同时最大化性能的提示优化方法仍然局限于文本，最终限制了MLLM的全部潜力。受此差距的启发，我们引入了多模式提示优化的新问题，该问题将提示优化的先前定义扩展到由文本和非文本提示对定义的多模式空间。为了解决这个问题，我们随后提出了多模式提示优化器（MPO），这是一个统一的框架，不仅通过保留印象的更新来执行多模式提示的联合优化，而且还通过利用早期评估作为先验来指导候选提示的选择过程基于Bayesian的选择策略。通过跨越文本（例如图像、视频甚至分子）的各种形式的广泛实验，我们证明MPO优于领先的纯文本优化方法，将多模式提示优化确立为实现MLLM潜力的关键一步。</p>
<div class="markdown-heading"><h2 class="heading-element">资源高效神经网络训练的自动进化优化</h2><a id="user-content-资源高效神经网络训练的自动进化优化" class="anchor" aria-label="Permalink: 资源高效神经网络训练的自动进化优化" href="#资源高效神经网络训练的自动进化优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09566v1宣布类型：新摘要：优化神经网络模型存在许多关键挑战，包括分布式计算、压缩技术和高效训练，无论它们是否应用于特定任务。解决此类问题至关重要，因为对可扩展和资源高效模型的需求正在增加。为了应对这些挑战，我们开发了一个新的自动化机器学习（AutoML）框架，即具有鲁棒自动化的参数高效训练（PETRA）。它将进化优化应用于模型架构和训练策略。PETRA包括修剪、量化和损失正规化。对具有金融事件序列以及图像和时间序列（基准）的现实世界数据的实验研究证明了PETRA改进神经模型性能和可扩展性的能力--即模型大小（高达75%）和延迟（高达33%）显着减少，吞吐量（13%）增加（13%）而目标指标没有明显下降。</p>
<div class="markdown-heading"><h2 class="heading-element">通过子空间优化对视觉变形器进行资源限制的高效训练</h2><a id="user-content-通过子空间优化对视觉变形器进行资源限制的高效训练" class="anchor" aria-label="Permalink: 通过子空间优化对视觉变形器进行资源限制的高效训练" href="#通过子空间优化对视觉变形器进行资源限制的高效训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09160v1宣布类型：新摘要：随着人工智能日益塑造日常生活，能源消耗和数据隐私已成为紧迫的问题。设备上学习直接在边缘设备上训练模型，从而减少能源消耗并保护数据隐私。然而，现代神经网络规模的不断扩大给设备上训练带来了重大障碍。虽然之前的工作集中在紧凑的卷积架构上，但我们将基于子空间的训练应用于Transformer模型。受模型的基本信息位于固定子空间中这一想法的启发，我们引入了权重激活子空间迭代（WATI），这是一种通过将训练限制在这个子空间来缓解反向传播的内存瓶颈并提高Transformer模型中的推理效率的方法。我们的结果表明，WISI保持了与普通训练相当的准确性，同时将内存使用量减少了高达62美元，计算成本（FLOPs）减少了高达2美元。在Raspberry Pi 5上，WASH的训练和推理速度比普通训练快约1.5倍。</p>
<div class="markdown-heading"><h2 class="heading-element">SQS：通过稀疏量化子分布的Bayesian DNN压缩</h2><a id="user-content-sqs通过稀疏量化子分布的bayesian-dnn压缩" class="anchor" aria-label="Permalink: SQS：通过稀疏量化子分布的Bayesian DNN压缩" href="#sqs通过稀疏量化子分布的bayesian-dnn压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08999v1宣布类型：新摘要：压缩大规模神经网络对于在资源受限的设备上部署模型至关重要。大多数现有方法单独采用权重修剪或低位量化，通常会导致次优压缩率以保持可接受的性能下降。我们引入了一个通过Bayesian变分学习（SQS）同时修剪和低位量化的统一框架，该框架可以实现比之前基线更高的压缩率，同时保持相当的性能。关键想法是在诱导稀疏性之前采用尖峰和板，并使用高斯混合模型（GSYS）对量化权重进行建模，以实现低位精度。理论上，我们为稀疏和量化的深度神经网络提供了我们提出的变分方法的一致结果。压缩ResNet、BERT-base、Llama 3和Qwen 2.5模型的大量实验表明，我们的方法比一系列现有方法实现了更高的压缩率，但性能下降相当。</p>
<div class="markdown-heading"><h2 class="heading-element">时间感知特征选择：用于稳定稀疏自动编码器训练的自适应时态掩蔽</h2><a id="user-content-时间感知特征选择用于稳定稀疏自动编码器训练的自适应时态掩蔽" class="anchor" aria-label="Permalink: 时间感知特征选择：用于稳定稀疏自动编码器训练的自适应时态掩蔽" href="#时间感知特征选择用于稳定稀疏自动编码器训练的自适应时态掩蔽"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08855v1宣布类型：新摘要：了解大型语言模型的内部表示对于确保其可靠性和安全性至关重要，稀疏自动编码器（SAEs）正在成为一种有前途的可解释性方法。然而，当前的严重不良事件训练方法面临特征吸收，即特征（或神经元）相互吸收以最小化$L_1$惩罚，从而难以一致地识别和分析模型行为。我们引入了自适应时间掩蔽（ATM），这是一种新颖的训练方法，通过跟踪激活幅度、频率和重建贡献来动态调整特征选择，以计算随时间变化的重要性分数。ATM应用基于这些重要性分数的统计阈值的概率掩蔽机制，创建了更自然的特征选择过程。通过对Gemma-2-2b模型的广泛实验，我们证明，与TopK和DropReLU SAEs等现有方法相比，ATM的吸收评分低得多，同时保持出色的重建质量。这些结果将ATM确立为学习神经网络中稳定、可解释特征的原则性解决方案，为更可靠的模型分析提供了基础。</p>
<div class="markdown-heading"><h2 class="heading-element">Video-TR：用关系图增强视频时空推理中的MLLM</h2><a id="user-content-video-tr用关系图增强视频时空推理中的mllm" class="anchor" aria-label="Permalink: Video-TR：用关系图增强视频时空推理中的MLLM" href="#video-tr用关系图增强视频时空推理中的mllm"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10976v1宣布类型：新摘要：多模式大型语言模型（MLLM）的最新进展已经表现出强大的语义理解能力，但难以执行精确的时空理解。现有的时空方法主要关注视频本身，而忽略视频中的物理信息，例如多对象布局和运动。这些限制限制了MLLM在要求高精度的下游应用中的使用，包括嵌入式智能和VR。为了解决这个问题，我们提出了Video-BAR，这是一种用于精确视频时空推理的新型基于图的增强方法。基于具有可验证奖励的强化学习（RLVR）提高模型能力的能力，我们引入了一种使用基于图的群体相对政策优化（GRPO）方法的推理机制，以指导模型在思维过程中推断场景的底层时空布局。为了解决时空训练数据的缺乏问题，我们构建了包含205 k个问答对的STV-205 k数据集，覆盖室内和室外环境中的动态多对象场景，以支持模型训练。实验表明，Video-GR在各种基准测试上实现了最先进的结果，在STI-Bench上比基本模型高出13%，并证明了我们的方法和数据集的有效性。代码、模型和数据将被发布。</p>
<div class="markdown-heading"><h2 class="heading-element">FLToP CTC：通过相对阈值进行帧级令牌修剪，以在不同平台上进行高效且节省内存的解码</h2><a id="user-content-fltop-ctc通过相对阈值进行帧级令牌修剪以在不同平台上进行高效且节省内存的解码" class="anchor" aria-label="Permalink: FLToP CTC：通过相对阈值进行帧级令牌修剪，以在不同平台上进行高效且节省内存的解码" href="#fltop-ctc通过相对阈值进行帧级令牌修剪以在不同平台上进行高效且节省内存的解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09085v1宣布类型：新摘要：基于ATC的ASB系统在资源有限的环境中面临计算和内存瓶颈。传统的CTC解码器，需要系统中高达90%的处理时间（例如，wav 2 vec 2-L4图形处理器上的大型），由于详尽的代币级操作而面临效率低下的问题。本文介绍了连接主义时态分类的帧级令牌修剪（FLToP CTC），这是一种新型解码算法，采用相对阈值概率指导的帧级令牌修剪。通过动态消除每帧的低概率令牌，FLToP CTC减少了计算和内存需求，同时保持可忽略的WER降级。在LibriSpeech上，FLToP CTC与标准CTC解码器相比实现了10.5倍的运行时加速和2.78倍的内存减少。其简单性使得跨平台（中央处理器、图形处理器等）无缝集成到CTC解码器中。FLToP CTC解决了CTC瓶颈，为资源有限的环境和实时应用程序提供可扩展性，增强语音识别的可访问性和效率。</p>
<div class="markdown-heading"><h2 class="heading-element">用于减轻变压器中的极端令牌现象的值状态门控注意</h2><a id="user-content-用于减轻变压器中的极端令牌现象的值状态门控注意" class="anchor" aria-label="Permalink: 用于减轻变压器中的极端令牌现象的值状态门控注意" href="#用于减轻变压器中的极端令牌现象的值状态门控注意"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09017v1宣布类型：新摘要：基于Transformer架构的大型模型容易受到极端令牌现象的影响，例如注意力下沉和值状态流失。这些会降低模型性能、量化保真度和可解释性的问题源于有问题的相互强化机制，其中模型通过将注意力集中在具有接近零值状态的代币上来学习低效的“无操作”行为。在本文中，我们提出了价值状态门控注意力（VGA），这是一种简单、专用且稳定的架构机制，通过直接打破这个循环来有效地执行“无操作”注意力。VGA引入了一个可学习的、数据相关的门，直接根据值载体（V）计算，来调制输出。通过对基础梯度的理论分析，我们表明，用自身的函数来门控价值状态在脱钩价值和注意力分数更新方面比之前门控输入嵌入的方法更有效。这创建了一个直接的监管途径，允许模型根据代币的新兴价值表示抑制代币的贡献。我们的实验表明，VGA显着减轻了注意力下沉的形成并稳定了价值状态规范，从而提高了性能、稳健的量化保真度和增强的模型解释性。</p>
<div class="markdown-heading"><h2 class="heading-element">ManiAgent：通用机器人操纵的抽象框架</h2><a id="user-content-maniagent通用机器人操纵的抽象框架" class="anchor" aria-label="Permalink: ManiAgent：通用机器人操纵的抽象框架" href="#maniagent通用机器人操纵的抽象框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11660v1宣布类型：新摘要：虽然视觉-语言-动作（VLA）模型在机器人操作方面表现出了令人印象深刻的能力，但它们在复杂推理和长期任务规划方面的表现受到数据稀缺和模型容量的限制。为了解决这个问题，我们引入ManiAgent，一个代理架构的一般操作任务，实现端到端的输出任务描述和环境输入机器人操作动作。在这个框架中，多个代理涉及代理间的通信，以执行环境感知，子任务分解和动作生成，使复杂的操作场景的有效处理。评估显示，ManiAgent在SimplerEnv基准上的成功率为86.8%，在现实世界的拾取和放置任务上的成功率为95.8%，实现了高效的数据收集，产生的VLA模型的性能与在人类注释数据集上训练的模型相当。该项目网页可访问<a href="https://yi-yang929.github.io/ManiAgent/%E3%80%82" rel="nofollow">https://yi-yang929.github.io/ManiAgent/。</a></p>
<div class="markdown-heading"><h2 class="heading-element">AdaPM：LLM培训的部分动量算法</h2><a id="user-content-adapmllm培训的部分动量算法" class="anchor" aria-label="Permalink: AdaPM：LLM培训的部分动量算法" href="#adapmllm培训的部分动量算法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.09103v1宣布类型：新摘要：在大型语言模型的训练中，动量被广泛使用，并且经常被证明可以实现显着的加速。然而，储存动量通常会带来记忆方面的挑战。在本文中，我们提出了AdaPM，这是一种自适应训练策略，利用部分动量来实现内存高效优化器。为此，AdaPM采用非均匀动量设计：对于大多数块来说，完全动量并不需要保持优化的性能。在AdaPM的动量设计中，为了减轻部分动量造成的偏差和性能损失，我们通过偏差修正技术增强部分动量。从经验上看，我们验证了我们的方法在动量上减少了超过90%美元的内存，同时保持预训练60 M至1.5B的各种语言模型以及监督式微调和RL HF的效率和性能。通过结合二阶统计数据的内存高效技术，AdaPM可以在优化器状态中进一步减少高达95美元的内存，从而为预训练GPT-2 1.5B节省超过30美元的GPU小时。</p>
<div class="markdown-heading"><h2 class="heading-element">CompassNav：从路径模仿转向导航中的决策理解</h2><a id="user-content-compassnav从路径模仿转向导航中的决策理解" class="anchor" aria-label="Permalink: CompassNav：从路径模仿转向导航中的决策理解" href="#compassnav从路径模仿转向导航中的决策理解"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.10154v1宣布类型：新摘要：导航中训练大型视觉语言模型（LVLM）的主要范式依赖于模仿专家轨迹。这种方法将复杂的导航任务简化为单个正确路径的序列到序列复制，从根本上限制了代理的探索和概括能力。在这项工作中，我们主张并引入了一种新的范式：从路径模仿到决策理解的转变。该范式的目标是构建不仅遵循，而且真正了解如何导航的代理。我们通过两个核心贡献来实现这一目标：首先，我们引入了Compass-Data-22 k，这是一种新型的22 k轨迹数据集。其强化微调（RFT）子集通过用A* 测地距离注释所有可行的动作，提供了决策格局的全景视图。其次，我们设计了一种新颖的差距感知混合奖励函数，该函数动态调整其反馈以适应决策确定性，在最佳行动的决定性信号和鼓励探索的细致入微的分数之间切换。我们的CompassNav代理集成到SFT然后RFT食谱中，经过训练，不是记住静态路线，而是开发一个内部“指南针”，通过评估所有可能动作的相对质量，不断直觉地判断目标的方向。这种方法使我们的7 B代理能够在目标导航基准上设置新的最先进水平，其性能优于更大的专有模型，并在物理机器人上实现强大的现实世界目标导航。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>