<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">KompeteAI：面向机器学习问题的端到端流程生成加速型自主多智能体系统</h2><a id="user-content-kompeteai面向机器学习问题的端到端流程生成加速型自主多智能体系统" class="anchor" aria-label="Permalink: KompeteAI：面向机器学习问题的端到端流程生成加速型自主多智能体系统" href="#kompeteai面向机器学习问题的端到端流程生成加速型自主多智能体系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.10177v1 公告类型：新研究<br>
摘要：当前基于大语言模型（LLM）的自动机器学习（AutoML）系统虽展现出强大能力，但仍存在两大核心局限：探索策略受限与执行效率瓶颈。传统探索方式受限于缺乏多样性的单次生成方法，以及蒙特卡洛树搜索（MCTS）策略无法有效重组优质局部解；而冗长的代码验证周期则严重阻碍了迭代优化。为此，我们提出创新性AutoML框架KompeteAI，其突破性在于：</p>
<ol>
<li>
<strong>动态解空间探索</strong>：首创"候选方案融合"机制，通过组合最优子方案突破传统MCTS的孤立评估模式；</li>
<li>
<strong>知识增强假设空间</strong>：集成检索增强生成（RAG）技术，从Kaggle竞赛方案和arXiv论文中提取真实世界策略；</li>
<li>
<strong>执行加速引擎</strong>：
<ul>
<li>预测性评分模型：基于早期指标预判方案潜力，避免全量执行</li>
<li>快速调试方法：将管道评估速度提升6.9倍</li>
</ul>
</li>
</ol>
<p>在主流AutoML基准测试MLE-Bench中，KompeteAI以平均3%的优势超越RD-agent、AIDE等领先方法。针对MLE-Bench的固有缺陷，我们同步提出Kompete-bench新基准，KompeteAI在该基准上同样达到最先进水平。</p>
<p>（注：译文采用技术报告特有的信息密度结构，通过分项列举突出创新点；专业术语如"RAG"保留英文缩写但首次出现标注全称；"pipeline"译为"管道"符合ML领域习惯；"state-of-the-art"采用"最先进水平"的通用译法；长难句拆解为符合中文阅读习惯的短句群）</p>
<div class="markdown-heading"><h2 class="heading-element">嵌套式ReFT：通过离策略展开实现大规模语言模型微调的高效强化学习</h2><a id="user-content-嵌套式reft通过离策略展开实现大规模语言模型微调的高效强化学习" class="anchor" aria-label="Permalink: 嵌套式ReFT：通过离策略展开实现大规模语言模型微调的高效强化学习" href="#嵌套式reft通过离策略展开实现大规模语言模型微调的高效强化学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.10123v1 公告类型：新研究<br>
摘要：针对大语言模型（LLMs）在数学推理等复杂领域的高级推理能力，可采用基于可验证奖励的强化微调方法（ReFT）进行提升。传统ReFT框架中，行为模型需为每个问题生成多个含答案的补全结果，再由奖励函数对答案进行评分。虽然这类RL后训练方法在复杂推理领域展现出显著性能提升，但由于训练过程中需通过多次推理步骤生成补全结果，其计算成本使得训练开销不容忽视。</p>
<p>为解决这一问题，我们受离策略RL和推测解码技术启发，提出新型ReFT框架——Nested-ReFT。该框架以目标模型的部分层作为行为模型，在训练期间生成离策略补全结果。通过动态层跳过配置，行为模型在每批次训练中可降低推理成本。理论分析表明，Nested-ReFT能提供方差可控的无偏梯度估计。实证研究显示，该方法在多个数学推理基准测试和不同规模模型中均实现了以"令牌/秒"为衡量的计算效率提升。</p>
<p>此外，我们探索了三种偏差缓解变体，通过最小化梯度更新中的策略偏离度，使模型性能与基线ReFT保持相当。</p>
<p>（注：根据学术文献翻译规范，关键术语处理如下：</p>
<ol>
<li>"off-policy RL"译为"离策略强化学习"（强化学习领域标准译法）</li>
<li>"speculative decoding"译为"推测解码"（参照NLP领域新兴技术译法）</li>
<li>"dynamic layer skipping"译为"动态层跳过"（保持技术描述准确性）</li>
<li>保留"ReFT"作为专业缩写在首次出现时标注全称，后续直接使用）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">XQuant：通过KV缓存重计算突破大语言模型推理的内存壁垒</h2><a id="user-content-xquant通过kv缓存重计算突破大语言模型推理的内存壁垒" class="anchor" aria-label="Permalink: XQuant：通过KV缓存重计算突破大语言模型推理的内存壁垒" href="#xquant通过kv缓存重计算突破大语言模型推理的内存壁垒"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.10395v1 公告类型：新研究<br>
摘要：尽管大语言模型（LLM）推理已成为众多下游应用的关键工作负载，但由于其巨大的内存占用和带宽需求，高效推理LLM仍面临挑战。与此同时，过去几十年间计算能力的增长速度持续超越内存容量和带宽的提升，这一趋势在现代GPU硬件中依然显著，进一步加剧了LLM推理的难度。为此，新兴算法开始通过增加计算量来减少内存操作。基于此，我们提出XQuant方法，利用这一趋势实现内存消耗的阶跃式降低——通过低比特量化技术，相比最先进的KV缓存量化方法，在显著提升精度的同时将内存占用减少一个数量级。</p>
<p>我们的核心创新在于：不再使用标准KV缓存，而是对层输入激活值X进行量化缓存，并在推理过程中动态重构键值（Keys/Values）。这一方案相比KV缓存可立即实现2倍内存节省。应用XQuant后，与FP16基线相比，我们实现了最高约7.7倍的内存节省且困惑度（perplexity）上升小于0.1。此外，该方法利用X值在模型各层间具有相似性的特点，进一步提出XQuant-CL方案，通过跨层X嵌入的相似性实现极致压缩。在不同模型测试中，XQuant-CL相较FP16基线实现了：<br>
• 10倍内存节省（困惑度仅上升0.01）<br>
• 12.5倍内存节省（困惑度仅上升0.1）</p>
<p>XQuant通过充分挖掘硬件平台快速提升的计算能力，成功消除了内存瓶颈，其性能超越现有最先进的KV缓存量化方法，在各类模型上均达到接近FP16的精度水平。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>