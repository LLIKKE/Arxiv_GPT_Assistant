<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">资源高效的语言模型：量化技术助力快速且易用的推理</h2><a id="user-content-资源高效的语言模型量化技术助力快速且易用的推理" class="anchor" aria-label="Permalink: 资源高效的语言模型：量化技术助力快速且易用的推理" href="#资源高效的语言模型量化技术助力快速且易用的推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.08620v1 公告类型：新论文<br>
摘要：大语言模型虽显著推动了自然语言处理的发展，但其巨大的资源需求对硬件可及性与能源消耗提出了严峻挑战。本文针对终端用户优化大语言模型推理效率的训练后量化（PTQ）技术，从量化方案、粒度层级与效能权衡等维度展开聚焦性高层级综述。通过系统梳理不同量化策略的理论基础与工程实践，旨在为训练后量化技术的理论研究与实际应用提供平衡的全局视角。</p>
<p>（注：根据学术论文摘要的文体特点，翻译时采取了以下处理：</p>
<ol>
<li>将"heavy resource demands"译为"巨大的资源需求"，通过四字格增强专业文本的简洁性</li>
<li>"post-training quantization"统一译为"训练后量化"，符合《人工智能术语》国家标准</li>
<li>采用"维度展开聚焦性高层级综述"的表述，既保留原文"focused and high-level review"的双重含义，又符合中文综述论文的惯用表达</li>
<li>末句"balanced overview"译为"平衡的全局视角"，通过"全局"补足英文抽象名词的隐含语义）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">高效非结构化剪枝：面向资源受限环境的Mamba状态空间模型优化</h2><a id="user-content-高效非结构化剪枝面向资源受限环境的mamba状态空间模型优化" class="anchor" aria-label="Permalink: 高效非结构化剪枝：面向资源受限环境的Mamba状态空间模型优化" href="#高效非结构化剪枝面向资源受限环境的mamba状态空间模型优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.08299v1 公告类型：新研究<br>
摘要：状态空间模型（SSMs），尤其是Mamba架构，已成为序列建模中Transformer的强大替代方案，其线性时间复杂度和跨任务竞争性能表现突出。然而庞大的参数量为资源受限环境中的部署带来严峻挑战。我们提出了一种专为Mamba模型设计的非结构化剪枝框架，可实现高达70%的参数削减，同时保留原模型95%以上的性能。该方案融合三大创新：(1) 梯度感知幅度剪枝技术，通过综合权重幅值与梯度信息识别次要参数；(2) 渐进式剪枝调度策略，逐步提升稀疏度以维持模型稳定性；(3) 全局剪枝方法，实现跨模型整体的参数优化分配。基于WikiText-103、长程竞技场（Long Range Arena）和ETT时间序列基准的广泛实验表明，该方法能以极小性能损失换取显著效率提升。通过对Mamba组件剪枝效应的分析，我们揭示了该架构冗余性与鲁棒性的关键特征，使其既能适配资源受限场景，又拓宽了应用边界。</p>
<div class="markdown-heading"><h2 class="heading-element">《持续学习中基于压缩知识迁移的低复杂度推理方法》</h2><a id="user-content-持续学习中基于压缩知识迁移的低复杂度推理方法" class="anchor" aria-label="Permalink: 《持续学习中基于压缩知识迁移的低复杂度推理方法》" href="#持续学习中基于压缩知识迁移的低复杂度推理方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.08327v1 公告类型：新研究<br>
摘要：持续学习（Continual Learning, CL）旨在训练能够按序学习多个任务且不遗忘已获知识的模型。其核心挑战在于平衡稳定性（保持旧任务性能）与可塑性（适应新任务）。近年来，基于大型预训练模型的持续学习方法因兼具强大新任务泛化能力和抗遗忘性而被广泛采用。然而，这类模型在推理时的高计算成本制约了其在现实场景中的应用，尤其对低延迟或高能效需求的应用而言。</p>
<p>为解决此问题，我们探索了模型压缩技术（包括剪枝与知识蒸馏），并针对类别增量学习（Class-Incremental Learning, CIL）这一任务身份未知的挑战性CL场景，提出两种高效框架。基于剪枝的框架包含预剪枝与后剪枝策略，分别在不同训练阶段实施压缩；基于知识蒸馏的框架采用师生架构，通过大型预训练教师模型向轻量学生模型传递下游任务相关知识。在多个CIL基准测试上的实验表明，所提框架能更优地平衡精度与推理复杂度，性能持续超越强基线模型。我们进一步分析两种框架在精度与效率上的权衡，为不同场景下的应用提供实践指导。</p>
<p>（注：根据学术文本翻译规范，关键术语如"pruning"统一译为"剪枝"而非"修剪"；"knowledge distillation"采用通用译名"知识蒸馏"；长难句按中文表达习惯拆分重组；被动语态转为主动表述；专业缩写首次出现时标注全称）</p>
<div class="markdown-heading"><h2 class="heading-element">动态低秩压缩神经网络及其在对抗攻击下的鲁棒性</h2><a id="user-content-动态低秩压缩神经网络及其在对抗攻击下的鲁棒性" class="anchor" aria-label="Permalink: 动态低秩压缩神经网络及其在对抗攻击下的鲁棒性" href="#动态低秩压缩神经网络及其在对抗攻击下的鲁棒性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.08022v1 公告类型：新研究<br>
摘要：在资源受限设备上部署神经网络需要模型兼具紧凑性和对抗输入鲁棒性。然而模型压缩与对抗鲁棒性往往相互矛盾。本研究提出了一种动态低秩训练方案，通过新型谱正则器控制每层低秩核心的条件数，在不牺牲干净样本准确率的前提下，有效缓解压缩模型对对抗扰动的敏感性。该方法具有模型无关性、数据普适性和计算高效性，支持秩自适应机制实现网络自动压缩。在多种标准架构、数据集和对抗攻击下的实验表明，经正则化的网络在实现94%以上压缩率的同时，其对抗准确率可恢复甚至超越未压缩基准模型。</p>
<p>（说明：翻译过程中做了以下技术处理：</p>
<ol>
<li>"resource-constrained devices"译为"资源受限设备"符合计算机领域术语</li>
<li>"condition number"译为专业术语"条件数"</li>
<li>"clean accuracy"意译为"干净样本准确率"以区分对抗样本场景</li>
<li>"rank adaptivity"译为"秩自适应机制"体现算法特性</li>
<li>长难句拆分重组，如将原文最后复合句拆分为两个中文短句，符合科技汉语表达习惯）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>