<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">PARQ：分段仿射正则化量化</h2><a id="user-content-parq分段仿射正则化量化" class="anchor" aria-label="Permalink: PARQ：分段仿射正则化量化" href="#parq分段仿射正则化量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>我们开发了一种原则性的方法，用于大规模机器学习模型的量化感知训练（QAT）。具体而言，我们展示了凸的、分段仿射正则化（PAR）能有效引导模型参数向离散值聚集。我们采用聚合近端随机梯度法（AProx）最小化PAR正则化的损失函数，并证明了其具有最终迭代收敛性。我们的方法为直通估计器（STE）——一种广泛使用的QAT启发式方法——提供了一种解释，即将其视为PARQ的渐近形式。通过实验，我们证明了PARQ在基于卷积和Transformer的视觉任务上取得了具有竞争力的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">探索视觉自回归变换器中KV缓存压缩的极限</h2><a id="user-content-探索视觉自回归变换器中kv缓存压缩的极限" class="anchor" aria-label="Permalink: 探索视觉自回归变换器中KV缓存压缩的极限" href="#探索视觉自回归变换器中kv缓存压缩的极限"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>视觉自回归模型中的一个基本挑战是在推理过程中需要大量内存开销来存储先前生成的表示。尽管通过各种压缩技术尝试缓解这一问题，但之前的工作并未在此背景下明确形式化KV缓存压缩问题。在本研究中，我们迈出了正式定义视觉自回归变换器中KV缓存压缩问题的第一步。随后，我们建立了一个基本的否定结果，证明了在基于注意力的架构下，任何用于顺序生成视觉标记的机制，当$d = \Omega(\log n)$时，必须至少使用$\Omega(n^2 d)$的内存，其中$n$是生成的标记数量，$d$是嵌入维度。这一结果表明，在没有额外结构约束的情况下，实现真正次二次方的内存使用是不可能的。我们的证明通过从一个计算下界问题归约构建，利用了受降维原理启发的随机嵌入技术。最后，我们讨论了视觉表示上的稀疏性先验如何影响内存效率，既展示了不可能性结果，也提出了缓解内存开销的潜在方向。</p>
<div class="markdown-heading"><h2 class="heading-element">专家查找混合体</h2><a id="user-content-专家查找混合体" class="anchor" aria-label="Permalink: 专家查找混合体" href="#专家查找混合体"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>专家混合模型（Mixture-of-Experts, MoE）在推理过程中仅激活一部分专家，使得模型即便在参数规模扩大时也能保持较低的推理浮点运算次数（FLOPs）和延迟。然而，由于MoE动态选择专家，所有专家都需要加载到视频随机存取存储器（VRAM）中。专家庞大的参数规模仍限制了部署，而按需加载专家到VRAM的卸载策略，则显著增加了推理延迟。为解决这一问题，我们提出了一种新的MoE架构——查找专家混合模型（Mixture of Lookup Experts, MoLE），它在通信和VRAM使用上均表现出高效性。在MoLE中，训练阶段的专家是前馈网络（Feed-Forward Networks, FFNs），它们以嵌入层的输出作为输入。在推理前，这些专家可被重新参数化为查找表（Lookup Tables, LUTs），根据输入ID检索专家输出，并卸载至存储设备。因此，在推理过程中无需执行专家计算，而是直接根据输入ID检索专家的计算结果并加载到VRAM中，由此产生的通信开销微乎其微。实验表明，在相同的FLOPs和VRAM使用条件下，MoLE实现了与密集模型相当的推理速度，且显著快于采用专家卸载的MoE，同时保持了与MoE相当的性能表现。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>