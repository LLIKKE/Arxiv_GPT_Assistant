<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/17/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10172025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/17/2025" href="#personalized-daily-arxiv-papers-10172025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 23</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training</a>
<strong>Authors:</strong> Gyudong Kim, Hyukju Na, Jin Hyeon Kim, Hyunsung Jang, Jaemin Park, Jaegi Hwang, Namkoo Ha, Seungryong Kim, Young Geun Kim</p>
</li>
<li>
<p><a href="#link1">Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning</a>
<strong>Authors:</strong> Weijie Shen, Yitian Liu, Yuhao Wu, Zhixuan Liang, Sijia Gu, Dehui Wang, Tian Nian, Lei Xu, Yusen Qin, Jiangmiao Pang, Xinping Guan, Xiaokang Yang, Yao Mu</p>
</li>
<li>
<p><a href="#link2">A Free Lunch in LLM Compression: Revisiting Retraining after Pruning</a>
<strong>Authors:</strong> Moritz Wagner, Christophe Roux, Max Zimmer, Sebastian Pokutta</p>
</li>
<li>
<p><a href="#link3">BitNet Distillation</a>
<strong>Authors:</strong> Xun Wu, Shaohan Huang, Wenhui Wang, Ting Song, Li Dong, Yan Xia, Furu Wei</p>
</li>
<li>
<p><a href="#link4">Revisit Modality Imbalance at the Decision Layer</a>
<strong>Authors:</strong> Xiaoyu Ma, Hao Chen</p>
</li>
<li>
<p><a href="#link5">Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging</a>
<strong>Authors:</strong> Bang An, Yibo Yang, Philip Torr, Bernard Ghanem</p>
</li>
<li>
<p><a href="#link6">Towards Reversible Model Merging For Low-rank Weights</a>
<strong>Authors:</strong> Mohammadsajad Alipour, Mohammad Mohammadi Amiri</p>
</li>
<li>
<p><a href="#link7">FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients</a>
<strong>Authors:</strong> Fatih Ilhan, Selim Furkan Tekin, Tiansheng Huang, Gaowen Liu, Ramana Kompella, Greg Eisenhauer, Yingyan Celine Lin, Calton Pu, Ling Liu</p>
</li>
<li>
<p><a href="#link8">Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction</a>
<strong>Authors:</strong> Penglong Zhai, Jie Li, Fanyi Di, Yue Liu, Yifang Yuan, Jie Huang, Peng Wu, Sicong Wang, Mingyang Yin, Tingting Hu, Yao Xu, Xin Li</p>
</li>
<li>
<p><a href="#link9">Large Language Models for Real-World IoT Device Identification</a>
<strong>Authors:</strong> Rameen Mahmood, Tousif Ahmed, Sai Teja Peddinti, Danny Yuxing Huang</p>
</li>
<li>
<p><a href="#link10">SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</a>
<strong>Authors:</strong> Xiaobei Zhao, Xingqi Lyu, Xiang Li</p>
</li>
<li>
<p><a href="#link11">Efficient Dynamic Structured Sparse Training with Learned Shuffles</a>
<strong>Authors:</strong> Abhishek Tyagi, Arjun Iyer, Liam Young, William H Renninger, Christopher Kanan, Yuhao Zhu</p>
</li>
<li>
<p><a href="#link12">MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving</a>
<strong>Authors:</strong> Jungi Lee, Junyong Park, Soohyun Cha, Jaehoon Cho, Jaewoong Sim</p>
</li>
<li>
<p><a href="#link13">Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval</a>
<strong>Authors:</strong> Rashmi R, Vidyadhar Upadhya</p>
</li>
<li>
<p><a href="#link14">MergeMoE: Efficient Compression of MoE Models via Expert Output Merging</a>
<strong>Authors:</strong> Ruijie Miao, Yilun Yao, Zihan Wang, Zhiming Wang, Bairen Yi, LingJun Liu, Yikai Zhao, Tong Yang</p>
</li>
<li>
<p><a href="#link15">A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space</a>
<strong>Authors:</strong> Bingjie Zhang, Yibo Yang, Renzhe, Dandan Guo, Jindong Gu, Philip Torr, Bernard Ghanem</p>
</li>
<li>
<p><a href="#link16">CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks</a>
<strong>Authors:</strong> Munsif Ali, Leonardo Rossi, Massimo Bertozzi</p>
</li>
<li>
<p><a href="#link17">A Multimodal Approach to Heritage Preservation in the Context of Climate Change</a>
<strong>Authors:</strong> David Roqui, Ad`ele Cormier, nistor Grozavu, Ann Bourges</p>
</li>
<li>
<p><a href="#link18">Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?</a>
<strong>Authors:</strong> Yijie Hu, Zihao Zhou, Kaizhu Huang, Xiaowei Huang, Qiufeng Wang</p>
</li>
<li>
<p><a href="#link19">VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation</a>
<strong>Authors:</strong> Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang</p>
</li>
<li>
<p><a href="#link20">Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References</a>
<strong>Authors:</strong> Hongzheng Chen, Bin Fan, Alexander Collins, Bastian Hagedorn, Evghenii Gaburov, Masahiro Masuda, Matthew Brookhart, Chris Sullivan, Jason Knight, Zhiru Zhang, Vinod Grover</p>
</li>
<li>
<p><a href="#link21">Agentic NL2SQL to Reduce Computational Costs</a>
<strong>Authors:</strong> Dominik Jehle, Lennart Purucker, Frank Hutter</p>
</li>
<li>
<p><a href="#link22">Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control</a>
<strong>Authors:</strong> Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.14614" rel="nofollow">First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-first-attentions-last-better-exploiting-first-attentions-for-efficient-transformer-training-" class="anchor" aria-label="Permalink: 0. First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training" href="#0-first-attentions-last-better-exploiting-first-attentions-for-efficient-transformer-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14614
<strong>Authors:</strong> Gyudong Kim, Hyukju Na, Jin Hyeon Kim, Hyunsung Jang, Jaemin Park, Jaegi Hwang, Namkoo Ha, Seungryong Kim, Young Geun Kim</p>
<p><strong>Abstract:</strong> arXiv:2510.14614v1 Announce Type: new  Abstract: As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block's MHA-MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18x, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.14300" rel="nofollow">Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-expertise-need-not-monopolize-action-specialized-mixture-of-experts-for-vision-language-action-learning-" class="anchor" aria-label="Permalink: 1. Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning" href="#1-expertise-need-not-monopolize-action-specialized-mixture-of-experts-for-vision-language-action-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14300
<strong>Authors:</strong> Weijie Shen, Yitian Liu, Yuhao Wu, Zhixuan Liang, Sijia Gu, Dehui Wang, Tian Nian, Lei Xu, Yusen Qin, Jiangmiao Pang, Xinping Guan, Xiaokang Yang, Yao Mu</p>
<p><strong>Abstract:</strong> arXiv:2510.14300v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.14444" rel="nofollow">A Free Lunch in LLM Compression: Revisiting Retraining after Pruning</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-a-free-lunch-in-llm-compression-revisiting-retraining-after-pruning-" class="anchor" aria-label="Permalink: 2. A Free Lunch in LLM Compression: Revisiting Retraining after Pruning" href="#2-a-free-lunch-in-llm-compression-revisiting-retraining-after-pruning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14444
<strong>Authors:</strong> Moritz Wagner, Christophe Roux, Max Zimmer, Sebastian Pokutta</p>
<p><strong>Abstract:</strong> arXiv:2510.14444v1 Announce Type: new  Abstract: While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.13998" rel="nofollow">BitNet Distillation</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-bitnet-distillation-" class="anchor" aria-label="Permalink: 3. BitNet Distillation" href="#3-bitnet-distillation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.13998
<strong>Authors:</strong> Xun Wu, Shaohan Huang, Wenhui Wang, Ting Song, Li Dong, Yan Xia, Furu Wei</p>
<p><strong>Abstract:</strong> arXiv:2510.13998v1 Announce Type: new  Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation, based on MiniLM; and continual pre-training, which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the full-precision counterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Code is available at <a href="https://github.com/microsoft/BitNet">https://github.com/microsoft/BitNet</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.14411" rel="nofollow">Revisit Modality Imbalance at the Decision Layer</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-revisit-modality-imbalance-at-the-decision-layer-" class="anchor" aria-label="Permalink: 4. Revisit Modality Imbalance at the Decision Layer" href="#4-revisit-modality-imbalance-at-the-decision-layer-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14411
<strong>Authors:</strong> Xiaoyu Ma, Hao Chen</p>
<p><strong>Abstract:</strong> arXiv:2510.14411v1 Announce Type: new  Abstract: Multimodal learning integrates information from different modalities to enhance model performance, yet it often suffers from modality imbalance, where dominant modalities overshadow weaker ones during joint optimization. This paper reveals that such an imbalance not only occurs during representation learning but also manifests significantly at the decision layer. Experiments on audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after extensive pretraining and balanced optimization, models still exhibit systematic bias toward certain modalities, such as audio. Further analysis demonstrates that this bias originates from intrinsic disparities in feature-space and decision-weight distributions rather than from optimization dynamics alone. We argue that aggregating uncalibrated modality outputs at the fusion stage leads to biased decision-layer weighting, hindering weaker modalities from contributing effectively. To address this, we propose that future multimodal systems should focus more on incorporate adaptive weight allocation mechanisms at the decision layer, enabling relative balanced according to the capabilities of each modality.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.14697" rel="nofollow">Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-purifying-task-vectors-in-knowledge-aware-subspace-for-model-merging-" class="anchor" aria-label="Permalink: 5. Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging" href="#5-purifying-task-vectors-in-knowledge-aware-subspace-for-model-merging-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14697
<strong>Authors:</strong> Bang An, Yibo Yang, Philip Torr, Bernard Ghanem</p>
<p><strong>Abstract:</strong> arXiv:2510.14697v1 Announce Type: new  Abstract: Model merging aims to integrate task-specific abilities from individually fine-tuned models into a single model without extra training. In recent model merging methods, task vector has become a fundamental building block, as it can encapsulate the residual information from finetuning. However, the merged model often suffers from notable performance degradation due to the conflicts caused by task-irrelevant redundancy in task vectors. Existing efforts in overcoming redundancy by randomly dropping elements in the parameter space involves randomness and lacks knowledge awareness. To address these challenges, in this study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace. Concretely, we sample some training examples from each task, and feed them into their corresponding fine-tuned models to acquire the covariance matrices before linear layers. We then perform a context-oriented singular value decomposition, which accentuates the weight components most relevant to the target knowledge. As a result, we can split fine-tuned model weights into task-relevant and redundant components in the knowledge-aware subspace, and purify the task vector by pruning the redundant components. To induce fair pruning efforts across models, we further introduce a spectral rank allocation strategy by optimizing a normalized activated pruning error. The task vector purification by our method as a plug-and-play scheme is applicable across various task vector-based merging methods to improve their performance. In experiments, we demonstrate the effectiveness of PAVE across a diverse set of merging methods, tasks, and model architectures.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.14163" rel="nofollow">Towards Reversible Model Merging For Low-rank Weights</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-towards-reversible-model-merging-for-low-rank-weights-" class="anchor" aria-label="Permalink: 6. Towards Reversible Model Merging For Low-rank Weights" href="#6-towards-reversible-model-merging-for-low-rank-weights-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14163
<strong>Authors:</strong> Mohammadsajad Alipour, Mohammad Mohammadi Amiri</p>
<p><strong>Abstract:</strong> arXiv:2510.14163v1 Announce Type: new  Abstract: Model merging aims to combine multiple fine-tuned models into a single set of weights that performs well across all source tasks. While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations, either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). We first demonstrate that applying conventional merging methods to low-rank weights leads to severe performance degradation in the merged model. Motivated by this phenomenon, we propose a fundamentally different approach: instead of collapsing all adapters into one set of weights, we construct a compact basis (e.g., an equivalent of holding two or more models) from which original task-specific models can be recovered via linear combination. This reframes merging as generating a reconstruction-capable model space rather than producing a single merged model. Crucially, this allows us to ``revert'' to each individual model when needed, recognizing that no merged model can consistently outperform one specialized for its task. Building on this insight, we introduce our method, Reversible Model Merging (RMM), an efficient, data-free, and flexible method that provides a closed-form solution for selecting the optimal basis of model weights and task-specific coefficients for linear combination. Extensive experiments across diverse datasets and model scales demonstrate that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.14054" rel="nofollow">FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-fedhft-efficient-federated-finetuning-with-heterogeneous-edge-clients-" class="anchor" aria-label="Permalink: 7. FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients" href="#7-fedhft-efficient-federated-finetuning-with-heterogeneous-edge-clients-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14054
<strong>Authors:</strong> Fatih Ilhan, Selim Furkan Tekin, Tiansheng Huang, Gaowen Liu, Ramana Kompella, Greg Eisenhauer, Yingyan Celine Lin, Calton Pu, Ling Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.14054v1 Announce Type: new  Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common practice for personalized natural language understanding (NLU) applications on downstream tasks and domain-specific datasets. However, there are two main challenges: (i) limited and/or heterogeneous data for fine-tuning due to proprietary data confidentiality or privacy requirements, and (ii) varying computation resources available across participating clients such as edge devices. This paper presents FedHFT - an efficient and personalized federated fine-tuning framework to address both challenges. First, we introduce a mixture of masked adapters to handle resource heterogeneity across participating clients, enabling high-performance collaborative fine-tuning of pre-trained language model(s) across multiple clients in a distributed setting, while keeping proprietary data local. Second, we introduce a bi-level optimization approach to handle non-iid data distribution based on masked personalization and client clustering. Extensive experiments demonstrate significant performance and efficiency improvements over various natural language understanding tasks under data and resource heterogeneity compared to representative heterogeneous federated learning methods.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.14702" rel="nofollow">Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-cognitive-aligned-spatio-temporal-large-language-models-for-next-point-of-interest-prediction-" class="anchor" aria-label="Permalink: 8. Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction" href="#8-cognitive-aligned-spatio-temporal-large-language-models-for-next-point-of-interest-prediction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14702
<strong>Authors:</strong> Penglong Zhai, Jie Li, Fanyi Di, Yue Liu, Yifang Yuan, Jie Huang, Peng Wu, Sicong Wang, Mingyang Yin, Tingting Hu, Yao Xu, Xin Li</p>
<p><strong>Abstract:</strong> arXiv:2510.14702v1 Announce Type: new  Abstract: The next point-of-interest (POI) recommendation task aims to predict the users' immediate next destinations based on their preferences and historical check-ins, holding significant value in location-based services. Recently, large language models (LLMs) have shown great potential in recommender systems, which treat the next POI prediction in a generative manner. However, these LLMs, pretrained primarily on vast corpora of unstructured text, lack the native understanding of structured geographical entities and sequential mobility patterns required for next POI prediction tasks. Moreover, in industrial-scale POI prediction applications, incorporating world knowledge and alignment of human cognition, such as seasons, weather conditions, holidays, and users' profiles (such as habits, occupation, and preferences), can enhance the user experience while improving recommendation performance. To address these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a framework employing natural language as an interface, allowing for the incorporation of world knowledge, spatio-temporal trajectory patterns, profiles, and situational information. Specifically, CoAST mainly comprises of 2 stages: (1) Recommendation Knowledge Acquisition through continued pretraining on the enriched spatial-temporal trajectory data of the desensitized users; (2) Cognitive Alignment to align cognitive judgments with human preferences using enriched training data through Supervised Fine-Tuning (SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline experiments on various real-world datasets and online experiments deployed in "Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of CoAST.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.13817" rel="nofollow">Large Language Models for Real-World IoT Device Identification</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-large-language-models-for-real-world-iot-device-identification-" class="anchor" aria-label="Permalink: 9. Large Language Models for Real-World IoT Device Identification" href="#9-large-language-models-for-real-world-iot-device-identification-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.13817
<strong>Authors:</strong> Rameen Mahmood, Tousif Ahmed, Sai Teja Peddinti, Danny Yuxing Huang</p>
<p><strong>Abstract:</strong> arXiv:2510.13817v1 Announce Type: new  Abstract: The rapid expansion of IoT devices has outpaced current identification methods, creating significant risks for security, privacy, and network accountability. These challenges are heightened in open-world environments, where traffic metadata is often incomplete, noisy, or intentionally obfuscated. We introduce a semantic inference pipeline that reframes device identification as a language modeling task over heterogeneous network metadata. To construct reliable supervision, we generate high-fidelity vendor labels for the IoT Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble of large language models guided by mutual-information and entropy-based stability scores. We then instruction-tune a quantized LLaMA3.18B model with curriculum learning to support generalization under sparsity and long-tail vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro accuracy across 2,015 vendors while maintaining resilience to missing fields, protocol drift, and adversarial manipulation. Evaluation on an independent IoT testbed, coupled with explanation quality and adversarial stress tests, demonstrates that instruction-tuned LLMs provide a scalable and interpretable foundation for real-world device identification at scale.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.14357" rel="nofollow">SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-sum-agrivln-spatial-understanding-memory-for-agricultural-vision-and-language-navigation-" class="anchor" aria-label="Permalink: 10. SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation" href="#10-sum-agrivln-spatial-understanding-memory-for-agricultural-vision-and-language-navigation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14357
<strong>Authors:</strong> Xiaobei Zhao, Xingqi Lyu, Xiang Li</p>
<p><strong>Abstract:</strong> arXiv:2510.14357v1 Announce Type: new  Abstract: Agricultural robots are emerging as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or fixed rail systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling robots to navigate to the target positions following the natural language instructions. In practical agricultural scenarios, navigation instructions often repeatedly occur, yet AgriVLN treat each instruction as an independent episode, overlooking the potential of past experiences to provide spatial context for subsequent ones. To bridge this gap, we propose the method of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation (SUM-AgriVLN), in which the SUM module employs spatial understanding and save spatial memory through 3D reconstruction and representation. When evaluated on the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47 to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m, demonstrating the state-of-the-art performance in the agricultural domain. Code: <a href="https://github.com/AlexTraveling/SUM-AgriVLN">https://github.com/AlexTraveling/SUM-AgriVLN</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.14812" rel="nofollow">Efficient Dynamic Structured Sparse Training with Learned Shuffles</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-efficient-dynamic-structured-sparse-training-with-learned-shuffles-" class="anchor" aria-label="Permalink: 11. Efficient Dynamic Structured Sparse Training with Learned Shuffles" href="#11-efficient-dynamic-structured-sparse-training-with-learned-shuffles-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14812
<strong>Authors:</strong> Abhishek Tyagi, Arjun Iyer, Liam Young, William H Renninger, Christopher Kanan, Yuhao Zhu</p>
<p><strong>Abstract:</strong> arXiv:2510.14812v1 Announce Type: new  Abstract: Structured sparsity accelerates training and inference on modern GPUs, yet it still trails unstructured dynamic sparse training (DST) in accuracy. The shortfall stems from a loss of expressivity: whereas a dense layer can realize every possible mask obtained by choosing any $w$ active weights out of $n$, a fixed block or N:M layout explores only a subset of those possibilities. We propose to close this gap by learning, for each layer, a single permutation matrix jointly with the structured weight matrix. Applied to three canonical structures -- block, N:M, and diagonals -- we show that permutation-augmented DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95% sparsity on ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$ and infers up to $2.9\times$ faster. The results position structure + learned permutation as a sweet spot between accuracy and efficiency.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.14557" rel="nofollow">MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-mx-pushing-the-limits-of-microscaling-formats-for-efficient-large-language-model-serving-" class="anchor" aria-label="Permalink: 12. MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving" href="#12-mx-pushing-the-limits-of-microscaling-formats-for-efficient-large-language-model-serving-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14557
<strong>Authors:</strong> Jungi Lee, Junyong Park, Soohyun Cha, Jaehoon Cho, Jaewoong Sim</p>
<p><strong>Abstract:</strong> arXiv:2510.14557v1 Announce Type: new  Abstract: Reduced-precision data formats are crucial for cost-effective serving of large language models (LLMs). While numerous reduced-precision formats have been introduced thus far, they often require intrusive modifications to the software frameworks or are rather unconventional for widespread adoption across hardware vendors. In this paper, we instead focus on recent industry-driven variants of block floating-point (BFP) formats and conduct a comprehensive analysis to push their limits for efficient LLM serving. Our analysis shows that existing ultra low-bit BFP variants struggle to provide reasonable language model performance due to outlier values in blocks. To address the outliers with BFPs, we propose MX+, a cost-effective and non-intrusive extension designed for seamless integration into the microscaling (MX) formats. MX+ builds on the key insight that the outlier does not need to use its exponent field in the element data type, which allows us to repurpose the exponent field as an extended mantissa to increase the precision of the outlier element. Our evaluation shows that MX+ achieves significantly higher model performance compared to the 4-bit MX format (MXFP4) with negligible storage overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6 for efficient LLM inference.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.14592" rel="nofollow">Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-multimodal-rag-for-unstructured-dataleveraging-modality-aware-knowledge-graphs-with-hybrid-retrieval-" class="anchor" aria-label="Permalink: 13. Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval" href="#13-multimodal-rag-for-unstructured-dataleveraging-modality-aware-knowledge-graphs-with-hybrid-retrieval-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14592
<strong>Authors:</strong> Rashmi R, Vidyadhar Upadhya</p>
<p><strong>Abstract:</strong> arXiv:2510.14592v1 Announce Type: new  Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on unimodal textual data, limiting their effectiveness on unstructured multimodal documents. Such documents often combine text, images, tables, equations, and graphs, each contributing unique information. In this work, we present a Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for multimodal question answering with reasoning through a modality-aware knowledge graph. MAHA integrates dense vector retrieval with structured graph traversal, where the knowledge graph encodes cross-modal semantics and relationships. This design enables both semantically rich and context-aware retrieval across diverse modalities. Evaluations on multiple benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486, providing complete modality coverage. These results highlight MAHA's ability to combine embeddings with explicit document structure, enabling effective multimodal retrieval. Our work establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.14436" rel="nofollow">MergeMoE: Efficient Compression of MoE Models via Expert Output Merging</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-mergemoe-efficient-compression-of-moe-models-via-expert-output-merging-" class="anchor" aria-label="Permalink: 14. MergeMoE: Efficient Compression of MoE Models via Expert Output Merging" href="#14-mergemoe-efficient-compression-of-moe-models-via-expert-output-merging-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14436
<strong>Authors:</strong> Ruijie Miao, Yilun Yao, Zihan Wang, Zhiming Wang, Bairen Yi, LingJun Liu, Yikai Zhao, Tong Yang</p>
<p><strong>Abstract:</strong> arXiv:2510.14436v1 Announce Type: new  Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution to efficiently scale the model size, which has been widely applied in recent LLM advancements. However, the substantial memory overhead of MoE models has made their compression an important research direction. In this work, we provide a theoretical analysis of expert merging, a recently proposed technique for compressing MoE models. Rather than interpreting expert merging from the conventional perspective of parameter aggregation, we approach it from the perspective of merging experts' outputs. Our key insight is that the merging process can be interpreted as inserting additional matrices into the forward computation, which naturally leads to an optimization formulation. Building on this analysis, we introduce MergeMoE, a method that leverages mathematical optimization to construct the compression matrices. We evaluate MergeMoE on multiple MoE models and show that our algorithm consistently outperforms the baselines with the same compression ratios.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2510.14301" rel="nofollow">A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-a-guardrail-for-safety-preservation-when-safety-sensitive-subspace-meets-harmful-resistant-null-space-" class="anchor" aria-label="Permalink: 15. A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space" href="#15-a-guardrail-for-safety-preservation-when-safety-sensitive-subspace-meets-harmful-resistant-null-space-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14301
<strong>Authors:</strong> Bingjie Zhang, Yibo Yang, Renzhe, Dandan Guo, Jindong Gu, Philip Torr, Bernard Ghanem</p>
<p><strong>Abstract:</strong> arXiv:2510.14301v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable success in diverse tasks, yet their safety alignment remains fragile during adaptation. Even when fine-tuning on benign data or with low-rank adaptation, pre-trained safety behaviors are easily degraded, leading to harmful responses in the fine-tuned models. To address this challenge, we propose GuardSpace, a guardrail framework for preserving safety alignment throughout fine-tuning, composed of two key components: a safety-sensitive subspace and a harmful-resistant null space. First, we explicitly decompose pre-trained weights into safety-relevant and safety-irrelevant components using covariance-preconditioned singular value decomposition, and initialize low-rank adapters from the safety-irrelevant ones, while freezing safety-relevant components to preserve their associated safety mechanism. Second, we construct a null space projector that restricts adapter updates from altering safe outputs on harmful prompts, thereby maintaining the original refusal behavior. Experiments with various pre-trained models on multiple downstream tasks demonstrate that GuardSpace achieves superior performance over existing methods. Notably, for Llama-2-7B-Chat fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT, reducing the average harmful score from 14.4% to 3.6%, while improving the accuracy from from 26.0% to 28.0%.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2510.13869" rel="nofollow">CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-color-gan-continual-few-shot-learning-with-low-rank-adaptation-in-generative-adversarial-networks-" class="anchor" aria-label="Permalink: 16. CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks" href="#16-color-gan-continual-few-shot-learning-with-low-rank-adaptation-in-generative-adversarial-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.13869
<strong>Authors:</strong> Munsif Ali, Leonardo Rossi, Massimo Bertozzi</p>
<p><strong>Abstract:</strong> arXiv:2510.13869v1 Announce Type: new  Abstract: Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \textcolor{red}{\textbf{\underline{c}}}ontinual few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with \textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \href{<a href="https://github.com/munsifali11/CoLoR-GAN%7D%7BGithub">https://github.com/munsifali11/CoLoR-GAN}{Github</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2510.14136" rel="nofollow">A Multimodal Approach to Heritage Preservation in the Context of Climate Change</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-a-multimodal-approach-to-heritage-preservation-in-the-context-of-climate-change-" class="anchor" aria-label="Permalink: 17. A Multimodal Approach to Heritage Preservation in the Context of Climate Change" href="#17-a-multimodal-approach-to-heritage-preservation-in-the-context-of-climate-change-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14136
<strong>Authors:</strong> David Roqui, Ad`ele Cormier, nistor Grozavu, Ann Bourges</p>
<p><strong>Abstract:</strong> arXiv:2510.14136v1 Announce Type: new  Abstract: Cultural heritage sites face accelerating degradation due to climate change, yet tradi- tional monitoring relies on unimodal analysis (visual inspection or environmental sen- sors alone) that fails to capture the complex interplay between environmental stres- sors and material deterioration. We propose a lightweight multimodal architecture that fuses sensor data (temperature, humidity) with visual imagery to predict degradation severity at heritage sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified encoders (64D latent space) that prevent overfitting on small datasets (n=37 training samples), and (2) Adaptive Barlow Twins loss that encourages modality complementarity rather than redundancy. On data from Strasbourg Cathedral, our model achieves 76.9% accu- racy, a 43% improvement over standard multimodal architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO. Ablation studies reveal that sensor-only achieves 61.5% while image-only reaches 46.2%, confirming successful multimodal synergy. A systematic hyperparameter study identifies an optimal moderate correlation target ({\tau} =0.3) that balances align- ment and complementarity, achieving 69.2% accuracy compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9: 61.5%). This work demonstrates that architectural sim- plicity combined with contrastive regularization enables effective multimodal learning in data-scarce heritage monitoring contexts, providing a foundation for AI-driven con- servation decision support systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2510.14387" rel="nofollow">Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-can-mllms-absorb-math-reasoning-abilities-from-llms-as-free-lunch-" class="anchor" aria-label="Permalink: 18. Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?" href="#18-can-mllms-absorb-math-reasoning-abilities-from-llms-as-free-lunch-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14387
<strong>Authors:</strong> Yijie Hu, Zihao Zhou, Kaizhu Huang, Xiaowei Huang, Qiufeng Wang</p>
<p><strong>Abstract:</strong> arXiv:2510.14387v1 Announce Type: new  Abstract: Math reasoning has been one crucial ability of large language models (LLMs), where significant advancements have been achieved in recent years. However, most efforts focus on LLMs by curating high-quality annotation data and intricate training (or inference) paradigms, while the math reasoning performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM typically consists of an LLM and a vision block, we wonder: Can MLLMs directly absorb math reasoning abilities from off-the-shelf math LLMs without tuning? Recent model-merging approaches may offer insights into this question. However, they overlook the alignment between the MLLM and LLM, where we find that there is a large gap between their parameter spaces, resulting in lower performance. Our empirical evidence reveals two key factors behind this issue: the identification of crucial reasoning-associated layers in the model and the mitigation of the gaps in parameter space. Based on the empirical insights, we propose IP-Merging that first identifies the reasoning-associated parameters in both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to maintain the alignment, and finally merges parameters in this subspace. IP-Merging is a tuning-free approach since parameters are directly adjusted. Extensive experiments demonstrate that our IP-Merging method can enhance the math reasoning ability of MLLMs directly from Math LLMs without compromising their other capabilities.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2510.14902" rel="nofollow">VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-vla2-empowering-vision-language-action-models-with-an-agentic-framework-for-unseen-concept-manipulation-" class="anchor" aria-label="Permalink: 19. VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation" href="#19-vla2-empowering-vision-language-action-models-with-an-agentic-framework-for-unseen-concept-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14902
<strong>Authors:</strong> Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang</p>
<p><strong>Abstract:</strong> arXiv:2510.14902v1 Announce Type: new  Abstract: Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: <a href="https://vla-2.github.io" rel="nofollow">https://vla-2.github.io</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2510.14719" rel="nofollow">Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-tawa-automatic-warp-specialization-for-modern-gpus-with-asynchronous-references-" class="anchor" aria-label="Permalink: 20. Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References" href="#20-tawa-automatic-warp-specialization-for-modern-gpus-with-asynchronous-references-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14719
<strong>Authors:</strong> Hongzheng Chen, Bin Fan, Alexander Collins, Bastian Hagedorn, Evghenii Gaburov, Masahiro Masuda, Matthew Brookhart, Chris Sullivan, Jason Knight, Zhiru Zhang, Vinod Grover</p>
<p><strong>Abstract:</strong> arXiv:2510.14719v1 Announce Type: new  Abstract: Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT programming model is fundamentally misaligned with this task-parallel hardware, creating a significant programmability gap. While hardware-level warp specialization is the key to unlocking peak performance, it forces developers to manually orchestrate complex, low-level communication and software pipelines--a process that is labor-intensive, error-prone, and unsustainable. To address this challenge, we present Tawa, an automated compiler that systematically generates high-performance, warp-specialized code from a high-level, tile-based program. Central to our approach is a novel IR abstraction, asynchronous references (aref), which expresses warp-level communication without exposing low-level hardware details. Using this abstraction, Tawa automatically partitions programs into producer-consumer roles and manages the intricate dataflow pipeline, relieving developers of invasive kernel rewriting. Evaluation on NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers high hardware utilization, achieving up to 1.1$\times$ speedup over highly optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains 1.2$\times$ speedup over Triton and matches the performance of the hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming effort.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2510.14808" rel="nofollow">Agentic NL2SQL to Reduce Computational Costs</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-agentic-nl2sql-to-reduce-computational-costs-" class="anchor" aria-label="Permalink: 21. Agentic NL2SQL to Reduce Computational Costs" href="#21-agentic-nl2sql-to-reduce-computational-costs-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14808
<strong>Authors:</strong> Dominik Jehle, Lennart Purucker, Frank Hutter</p>
<p><strong>Abstract:</strong> arXiv:2510.14808v1 Announce Type: new  Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87% and thus allows for substantial cost reductions while maintaining competitive performance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2510.14388" rel="nofollow">Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-hi-agent-hierarchical-vision-language-agents-for-mobile-device-control-" class="anchor" aria-label="Permalink: 22. Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control" href="#22-hi-agent-hierarchical-vision-language-agents-for-mobile-device-control-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.14388
<strong>Authors:</strong> Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi</p>
<p><strong>Abstract:</strong> arXiv:2510.14388v1 Announce Type: new  Abstract: Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) and Mixture of Experts (MoE) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research introduces a novel multimodal large model that integrates text, language, and vision modalities. The goal is to advance the performance and generalization capabilities of the model by establishing a new approach for training and fusing these modalities effectively, rather than focusing on incremental optimizations.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>