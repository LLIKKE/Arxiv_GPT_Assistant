<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/gpt_paper_assistant_ori</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/gpt_paper_assistant_ori/"><meta property="og:title" content="LLIKKE/gpt_paper_assistant_ori"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/gpt_paper_assistant_ori/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/gpt_paper_assistant_ori"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/gpt_paper_assistant_ori"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">通过近似验证加速推测性解码</h2><a id="user-content-通过近似验证加速推测性解码" class="anchor" aria-label="Permalink: 通过近似验证加速推测性解码" href="#通过近似验证加速推测性解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.04557v1 公告类型：新研究  摘要：推测解码（Speculative Decoding, SD）是近期提出的一种加速大型语言模型（LLMs）推理的技术。SD通过使用较小的草稿LLM自回归生成一系列标记，并利用较大的目标LLM进行并行验证，以确保统计一致性。然而，定期对目标LLM进行并行验证调用阻碍了SD实现更低的延迟。我们提出了SPRINTER，它采用一个低复杂度验证器，该验证器经过训练，能够预测草稿LLM生成的标记是否会被目标LLM接受。通过执行近似的顺序验证，SPRINTER无需目标LLM的验证，仅在标记被认为不可接受时才调用目标LLM。这减少了调用较大LLM的次数，并能实现进一步的加速。我们对SPRINTER进行了理论分析，考察了生成标记的统计特性，以及验证器作用下预期延迟的减少。我们在多个数据集和模型对上评估了SPRINTER，证明近似验证在进一步降低延迟的同时，仍能保持高质量的生成。例如，在Wiki-Summaries数据集上，当使用GPT2-Small和GPT2-XL作为草稿/目标模型时，SPRINTER实现了1.7倍的延迟加速，并减少了8.3倍的浮点运算次数，同时仍能生成高质量的响应。</p>
<div class="markdown-heading"><h2 class="heading-element">QuEST：使用1比特权重和激活值稳定训练大型语言模型</h2><a id="user-content-quest使用1比特权重和激活值稳定训练大型语言模型" class="anchor" aria-label="Permalink: QuEST：使用1比特权重和激活值稳定训练大型语言模型" href="#quest使用1比特权重和激活值稳定训练大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.05003v1 公告类型：新研究  摘要：降低大型语言模型（LLMs）巨额成本的一种方法是采用量化或稀疏表示进行训练或部署。尽管训练后压缩方法非常流行，但通过直接在这些表示上训练以获得更精确的压缩模型，即量化感知训练（QAT），这一问题仍未完全解决：例如，最近的一项研究（arXiv:2411.04330v2）将使用QAT训练模型时保持与标准FP16/BF16精度竞争性的“最佳”位宽定为8位权重和激活。我们通过一种名为QuEST的新方法推进了这一前沿，该方法与FP16形成帕累托竞争，即在更小的模型尺寸下提供更高的准确性，同时使用4位或更低的权重和激活训练模型。此外，QuEST允许使用1位权重和激活进行稳定训练。QuEST通过改进QAT方法的两个关键方面实现这一点：（1）通过Hadamard归一化和MSE最优拟合，对权重和激活的（连续）分布进行准确快速的量化；（2）基于显式最小化在量化状态上计算的噪声梯度与“真实”（但未知）全精度梯度之间误差的思想，提出了一种新的信任梯度估计器。在Llama型架构上的实验表明，QuEST在整个硬件支持的精度范围内引发了稳定的扩展定律，并且可以扩展到稀疏表示。我们提供了GPU内核支持，显示QuEST生成的模型可以高效执行。我们的代码可在<a href="https://github.com/IST-DASLab/QuEST%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IST-DASLab/QuEST获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">KVTuner：面向高效且近乎无损的大语言模型推理，基于敏感度感知的分层混合精度键值缓存量化技术</h2><a id="user-content-kvtuner面向高效且近乎无损的大语言模型推理基于敏感度感知的分层混合精度键值缓存量化技术" class="anchor" aria-label="Permalink: KVTuner：面向高效且近乎无损的大语言模型推理，基于敏感度感知的分层混合精度键值缓存量化技术" href="#kvtuner面向高效且近乎无损的大语言模型推理基于敏感度感知的分层混合精度键值缓存量化技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.04420v1 公告类型：新  摘要：KV缓存量化能够在长上下文和大批量场景中提高大型语言模型（LLMs）的推理吞吐量和延迟，同时保持LLMs的有效性。然而，现有方法存在三个未解决的问题：忽视了层间对KV缓存量化的敏感性、在线细粒度决策的高开销以及对不同LLMs和约束条件的低灵活性。因此，我们深入分析了层间Transformer注意力模式与KV缓存量化误差之间的内在关联，并研究了为什么键缓存比值缓存对减少量化误差更为重要。我们进一步提出了一个简单而有效的框架KVTuner，通过多目标优化自适应地搜索适合硬件的层间KV量化精度对，用于粗粒度KV缓存，并在在线推理时直接利用离线搜索的配置。为了降低离线校准的计算成本，我们利用层内KV精度对剪枝和层间聚类来减少搜索空间。实验结果表明，对于像Llama-3.1-8B-Instruct这样的LLMs，我们能够实现几乎无损的3.25位混合精度KV缓存量化，而对于像Qwen2.5-7B-Instruct这样在数学推理任务上敏感的模型，则能实现4.0位的量化。与KV8量化相比，在不同上下文长度下，最大推理吞吐量可提高38.3%。</p>
<div class="markdown-heading"><h2 class="heading-element">CMoE：快速切割专家混合体以实现高效LLM推理</h2><a id="user-content-cmoe快速切割专家混合体以实现高效llm推理" class="anchor" aria-label="Permalink: CMoE：快速切割专家混合体以实现高效LLM推理" href="#cmoe快速切割专家混合体以实现高效llm推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.04416v1 公告类型：新研究  摘要：大型语言模型（LLMs）通过扩展模型参数实现了令人印象深刻的性能，但这伴随着显著的推理开销。前馈网络（FFNs）占据了LLM参数的主导地位，其隐藏神经元表现出高度的激活稀疏性。为了利用这一点，研究人员提出了使用专家混合（MoE）架构，其中仅激活一部分参数。然而，现有方法通常需要大量的训练数据和资源，限制了其实用性。我们提出了CMoE（Carved MoE），一种从密集模型中高效雕刻出MoE模型的新框架。CMoE通过高效的专家分组和轻量级适应实现了卓越的性能。首先，根据激活率将神经元分组为共享专家和路由专家。接着，我们构建了一个无需从头训练的路由机制，结合了可微分路由过程和负载均衡。使用少量数据，CMoE在五分钟内从70亿参数的密集模型中生成一个设计精良、可用的MoE。通过轻量级微调，它在一小时内实现了高性能恢复。我们的代码已在<a href="https://github.com/JarvisPei/CMoE%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/JarvisPei/CMoE公开。</a></p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/gpt_paper_assistant_ori" href="https://github.com/LLIKKE/gpt_paper_assistant_ori" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>