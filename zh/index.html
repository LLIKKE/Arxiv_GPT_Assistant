<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">采用循环脉冲神经网络的71.2微瓦语音识别加速器</h2><a id="user-content-采用循环脉冲神经网络的712微瓦语音识别加速器" class="anchor" aria-label="Permalink: 采用循环脉冲神经网络的71.2微瓦语音识别加速器" href="#采用循环脉冲神经网络的712微瓦语音识别加速器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>本文介绍了一款专为边缘设备实时应用设计的71.2微瓦语音识别加速器，着重实现超低功耗设计。通过算法与硬件的协同优化，我们提出了一种紧凑型循环脉冲神经网络结构，包含两个循环层、一个全连接层和低时间步长（1或2步）。经剪枝和4位定点量化处理后，2.79MB的模型体积缩减96.42%至0.1MB。在硬件层面，我们采用混合级剪枝、零值跳过和脉冲融合技术，将计算复杂度降低90.49%至13.86MMAC/S。通过并行时间步执行机制解决了时间步间数据依赖问题，并利用权重共享实现权重缓冲器功耗优化。基于脉冲活动的稀疏特性，采用输入广播方案消除零值计算，进一步节省功耗。该设计基于台积电28纳米工艺实现，在100kHz工作频率下实时运行时功耗仅71.2微瓦，性能优于现有最优方案。在500MHz频率下，其能效比达28.41TOPS/W，面积效率达1903.11GOPS/mm²。</p>
<div class="markdown-heading"><h2 class="heading-element">InternVL-X：通过高效视觉令牌压缩推进并加速InternVL系列发展</h2><a id="user-content-internvl-x通过高效视觉令牌压缩推进并加速internvl系列发展" class="anchor" aria-label="Permalink: InternVL-X：通过高效视觉令牌压缩推进并加速InternVL系列发展" href="#internvl-x通过高效视觉令牌压缩推进并加速internvl系列发展"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>当前大多数多模态大语言模型（MLLMs）将视觉标记视为"文本序列"，与文本标记共同输入大语言模型（LLM）处理。然而海量视觉标记会显著增加计算资源和时间开销。本文提出的InternVL-X模型通过融合三种视觉标记压缩方法，在性能与效率上全面超越InternVL模型。首先，我们设计了一种新型视觉语言投影器PVTC：该组件通过聚合相邻视觉嵌入形成局部查询，同时利用转换后的CLS标记作为全局查询，继而通过点对区域交叉注意力机制实现更高效的视觉特征转换。其次，我们开发了分层视觉标记压缩模块LVTC：该模块在LLM浅层压缩标记，再通过深层上采样与残差连接进行扩展，显著提升模型计算效率。此外，我们提出高效高分辨率切片方法RVTC：该方法依据图像面积或长度筛选动态调整视觉标记数量，仅以微小性能代价大幅提升训练效率。InternVL-X仅需20%甚至更少的视觉标记，便在7个公开MLLM基准测试中取得最先进性能，同时在12项任务上的平均指标提升2.34%。</p>
<p>（注：根据学术论文翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"tokens"统一译为"标记"而非"令牌/词元"，符合NLP领域常用译法</li>
<li>"cross-attention"译为"交叉注意力"，保留技术术语一致性</li>
<li>将英文长句拆解为符合中文表达习惯的短句结构</li>
<li>技术缩写如PVTC/LVTC/RVTC首次出现时标注全称，后续直接使用缩写</li>
<li>"state-of-the-art"采用"最先进性能"的意译，避免直译生硬）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">MoQa：基于多阶段数据-模型分布感知的MoE量化新思考</h2><a id="user-content-moqa基于多阶段数据-模型分布感知的moe量化新思考" class="anchor" aria-label="Permalink: MoQa：基于多阶段数据-模型分布感知的MoE量化新思考" href="#moqa基于多阶段数据-模型分布感知的moe量化新思考"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>随着人工智能技术的进步，混合专家系统（MoE）已成为大语言模型（LLM）的主流架构，其对模型压缩的需求与日俱增。量化作为一种有效手段，不仅能压缩模型规模，更能显著提升推理速度。现有量化方法的研究重心已逐渐从参数缩放转向数据分布分析，但这些方法专为稠密型LLM设计，其基于"单一模型-全量数据"映射的简单分析范式难以适用于MoE架构。本文提出新型量化框架MoQa，通过多阶段分析解耦MoE中数据-模型分布的复杂性，定量揭示了稀疏数据激活、数据-参数映射及专家间关联等动态特征。基于此，MoQa以最优的数据-模型分布感知能力识别关键专家与参数，针对不同数据激活模式和专家组合场景，提出细粒度混合量化策略体系。此外，MoQa系统探讨了现有量化技术的局限性，并通过分阶段影响分析为MoE量化研究提供了新洞见。实验表明，MoQa在语言建模任务中实现困惑度降低1.69~2.18，在零样本推理任务中准确率提升1.58%~8.91%。我们相信MoQa将为未来MoE架构的构建、优化与压缩提供重要支撑。</p>
<p>（注：根据技术文献翻译规范，对以下概念进行了标准化处理：</p>
<ol>
<li>"Mix-of-Experts"统一译为"混合专家系统"（行业通用译法）</li>
<li>"perplexity"译为"困惑度"（NLP领域标准术语）</li>
<li>"zero-shot inference"译为"零样本推理"（保持与机器学习术语一致性）</li>
<li>创新性术语"MoQa"保留英文缩写形式以突显技术品牌性）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>