<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">放射学的最后一次考试（RadLE）：针对人类专家的前沿多模式人工智能基准和放射学视觉推理错误分类</h2><a id="user-content-放射学的最后一次考试radle针对人类专家的前沿多模式人工智能基准和放射学视觉推理错误分类" class="anchor" aria-label="Permalink: 放射学的最后一次考试（RadLE）：针对人类专家的前沿多模式人工智能基准和放射学视觉推理错误分类" href="#放射学的最后一次考试radle针对人类专家的前沿多模式人工智能基准和放射学视觉推理错误分类"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25559v1宣布类型：新摘要：临床医生和患者越来越多地使用大型语言模型（LLM）和视觉语言模型（VLM）等通用多模式人工智能系统，通过广泛可用的面向消费者的聊天机器人进行医学图像解释。大多数声称专家级表现的评估都是在包含常见病理的公共数据集上进行的。对困难诊断病例的前沿模型的严格评估仍然有限。我们开发了一个由多种成像方式的50个专家级“点诊断”病例组成的试点基准，以评估前沿人工智能模型针对委员会认证的放射科医生和放射学实习生的性能。为了反映现实世界的使用，通过其原生网络界面测试了五种流行前沿人工智能模型的推理模式，即OpenAI o3、OpenAI GPT-5、Gemini 2.5 Pro、Grok-4和Claude Opus 4.1。由盲法专家对准确性进行评分，并评估三次独立运行的重现性。GPT-5在各种推理模式中进行了额外评估。推理质量错误进行了评估，并定义了视觉推理错误的分类。委员会认证的放射科医生实现了最高的诊断准确率（83%），优于受训人员（45%）和所有AI模型（GPT-5显示的最佳性能：30%）。GPT-5和o3的可靠性很高，Gemini 2.5 Pro和Grok-4的可靠性中等，Claude Opus 4.1的可靠性较差。这些发现表明，在具有挑战性的诊断病例中，先进的前沿模型远远落后于放射科医生。我们的基准强调了通用人工智能在医学成像中的当前局限性，并警告不要无监督的临床使用。我们还提供推理痕迹的定性分析，并提出人工智能模型视觉推理错误的实用分类，以更好地了解其故障模式、为评估标准提供信息并指导更稳健的模型开发。</p>
<div class="markdown-heading"><h2 class="heading-element">显著性引导的纵向医学视觉问题分类</h2><a id="user-content-显著性引导的纵向医学视觉问题分类" class="anchor" aria-label="Permalink: 显著性引导的纵向医学视觉问题分类" href="#显著性引导的纵向医学视觉问题分类"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25374v1宣布类型：新摘要：纵向医学视觉问答（Diff-VQA）需要比较不同时间点的配对研究，并回答有关具有临床意义的变化的问题。在这种设置下，差异信号和视觉焦点随时间的一致性比绝对的单图像发现信息更多。我们提出了一种用于胸部X射线DV-VQA的显着性引导编码器-解码器，它将事后显着性转化为可操作的监督。该模型首先执行轻量级的近身份仿射预对齐，以减少访问之间的滋扰运动。然后执行周期内两步循环：步骤1从答案中提取医学相关关键词，并在两个图像上生成关键词条件化的Grad-CAM，以获得以疾病为中心的显着性;步骤2将共享显着性掩蔽应用于两个时间点并生成最终答案。这关闭了语言-视觉循环，以便重要的术语也指导模型的外观，对相应的解剖结构强制空间一致的关注。在Medical-Diff-VQA上，该方法在BLEU、ROUGE-L、CIDerer和METEOR上获得了有竞争力的性能，同时提供了内在的可解释性。值得注意的是，主干和解码器是经过通用域预训练的，而没有特定放射学的预训练，突出了实用性和可移植性。这些结果支持具有轻度预对齐的显着性条件生成作为医学VQA纵向推理的原则框架。</p>
<div class="markdown-heading"><h2 class="heading-element">迈向社交媒体中的统一多模式错误信息检测：基准数据集和基线</h2><a id="user-content-迈向社交媒体中的统一多模式错误信息检测基准数据集和基线" class="anchor" aria-label="Permalink: 迈向社交媒体中的统一多模式错误信息检测：基准数据集和基线" href="#迈向社交媒体中的统一多模式错误信息检测基准数据集和基线"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25991v1宣布类型：新摘要：近年来，检测社交媒体上的虚假多模式内容引起了越来越多的关注。主要有两种形式的欺骗：人为制造的错误信息（例如，谣言和误导性帖子）以及由图像合成模型或视觉语言模型（VLM）生成的人工智能生成的内容。尽管两者都有欺骗意图，但通常是孤立研究的。NLP研究重点关注人类编写的错误信息，而简历社区则针对人工智能生成的工件。因此，现有模型通常仅专门针对一种类型的虚假内容。然而，在现实世界的场景中，多模式邮政的类型通常是未知的，从而限制了此类专业系统的有效性。为了弥合这一差距，我们构建了多模式新闻欺骗综合数据集（OmniFake），这是一个包含127 K个样本的综合基准，将来自现有资源的人类策划的错误信息与新合成的人工智能生成的示例集成在一起。基于该数据集，我们提出了统一多模式虚假内容检测（UMDet），这是一个旨在处理两种形式欺骗的框架。UMFDet利用VLM主干网（VLM主干网），该主干网由类别感知的专家混合（MoE）适配器增强来捕获特定于类别的线索，并利用归因思维链机制，该机制为定位显着的欺骗性信号提供隐式推理指导。大量实验表明，UMDet在两种错误信息类型中都实现了稳健且一致的性能，优于专业基线，并为现实世界的多模式欺骗检测提供了实用的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">通过多模式和多步骤管道自动发现模型</h2><a id="user-content-通过多模式和多步骤管道自动发现模型" class="anchor" aria-label="Permalink: 通过多模式和多步骤管道自动发现模型" href="#通过多模式和多步骤管道自动发现模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25946v1宣布类型：新摘要：自动模型发现是在大型组合搜索空间上自动搜索和识别给定数据集最合适的模型的过程。然而，现有方法在平衡细粒度细节的捕获与确保具有合理模型复杂性的训练数据制度之外的概括性方面经常面临挑战。本文中，我们提出了一种用于有效自动化模型发现的多模式和多步骤流水线。我们的方法利用两个基于视觉语言的模块（VLM）AnalyzerVLM和EvolutionVLM，以代理的方式进行有效的模型提案和评估。AnalyzerVLM自主规划和执行多步骤分析，以提出有效的候选模型。EvolutionVLM从定量和感知上评估候选模型，包括局部细节的适应性和整体趋势的普遍性。我们的结果表明，我们的管道可以有效地发现能够捕捉精细细节并确保强大的概括性的模型。此外，广泛的消融研究表明，多模式和多步骤推理在发现有利模型方面发挥着至关重要的作用。</p>
<div class="markdown-heading"><h2 class="heading-element">跳过它？视觉语言模型中跳层的理论条件</h2><a id="user-content-跳过它视觉语言模型中跳层的理论条件" class="anchor" aria-label="Permalink: 跳过它？视觉语言模型中跳层的理论条件" href="#跳过它视觉语言模型中跳层的理论条件"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25584v1宣布类型：新摘要：视觉语言模型（VLM）在广泛的任务中实现了令人难以置信的性能，但它们的大规模使得推理成本高昂。最近的工作表明，选择性地跳过VLM层可以提高效率，同时最小的性能损失甚至性能改进。然而，由于对何时跳层是有益的理解有限，这种技术仍然未得到充分利用。在本文中，我们开发了一个框架，使用信息和学习理论来表征条件下，跳层提高效率，而不牺牲性能。受这些观察的启发，我们通过LLM骨干分析了VLM隐藏表示的演变，并表明我们的框架预测的具有大冗余的层与实践中流行的跳层方法跳过的层相吻合，为多种有效的推理技术提供了统一的理论框架。我们的实验表明，跳过这些层会产生更快的推理，从而保持性能，并且还表明在这些条件之外应用跳过会导致模型退化。</p>
<div class="markdown-heading"><h2 class="heading-element">先学先看后看：从语言预培训中揭开LLM视觉先验的神秘面纱</h2><a id="user-content-先学先看后看从语言预培训中揭开llm视觉先验的神秘面纱" class="anchor" aria-label="Permalink: 先学先看后看：从语言预培训中揭开LLM视觉先验的神秘面纱" href="#先学先看后看从语言预培训中揭开llm视觉先验的神秘面纱"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26625v1宣布类型：新摘要：大型语言模型（LLM）尽管仅根据文本进行训练，但令人惊讶地开发出丰富的视觉先验。这些先验允许针对具有相对少量多模式数据的视觉任务解锁潜在视觉能力，并且在某些情况下，在从未见过图像的情况下执行视觉任务。通过系统分析，我们揭示了视觉先验--在语言预训练期间获得的关于视觉世界的隐性、涌现的知识--是由可分离的感知和推理先验组成的，具有独特的扩展趋势和起源。我们表明，LLM的潜在视觉推理能力主要是通过对以推理为中心的数据进行预训练来发展的（例如，代码、数学、学术界）并逐步扩展。这种从语言预训练中获得的推理先验可以转移并普遍适用于视觉推理。相比之下，感知先验从广泛的库中更加广泛地出现，并且感知能力对视觉编码器和视觉指令调整数据更加敏感。与此同时，描述视觉世界的文本被证明至关重要，尽管其性能影响迅速饱和。利用这些见解，我们为预训练视觉感知LLM提出了一种以数据为中心的配方，并在1 T代币规模预训练中对其进行验证。我们的研究结果基于100多个受控实验，消耗500，000个GPU小时，跨越整个MLLM构建管道-从LLM预训练到视觉对齐和监督多模式微调-跨越五个模型尺度、广泛的数据类别和混合以及多种适应设置。除了我们的主要发现之外，我们还提出并研究了几个假设，并介绍了多层次存在工作台（MLE-Bench）。总的来说，这项工作提供了一种从语言预训练中有意识地培养视觉先验的新方法，为下一代多模式LLM铺平了道路。</p>
<div class="markdown-heading"><h2 class="heading-element">CIMNAS：内存计算感知神经架构搜索的联合框架</h2><a id="user-content-cimnas内存计算感知神经架构搜索的联合框架" class="anchor" aria-label="Permalink: CIMNAS：内存计算感知神经架构搜索的联合框架" href="#cimnas内存计算感知神经架构搜索的联合框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25862v1宣布类型：新摘要：为了最大限度地提高人工智能（AI）应用程序的基于内存计算（IOM）的神经网络加速器的硬件效率和性能准确性，共同优化软件和硬件设计参数至关重要。由于参数数量庞大且相互依赖关系复杂，手动调整是不切实际的。为了有效地自动化基于Sim的神经网络加速器的设计和优化，可以应用硬件感知神经架构搜索（HW-NAS）技术。这项工作介绍了CIMNAS，这是一个用于TIM架构的联合模型量化硬件优化框架。CIMNAS同时搜索软件参数、量化策略和广泛的硬件参数，并结合设备级、电路级和架构级协同优化。CIMNAS实验是在9. 9 x10 ' 85个潜在参数组合的搜索空间上进行的，其中以MobileNet模型作为基线，并基于RAM的TIM架构。在ImageNet数据集上进行评估后，CIMNAS实现了能量延迟面积积（EDAP）从90.1x降低到104.5x，TOPS/W在4.68x和4.82x之间提高，TOPS/mm 2从11.3x提高到12.78x，相对于各种基线，同时保持了73.81%的准确性。CIMNAS的适应性和稳健性通过扩展框架以支持基于RAM的ResNet 50架构来证明，EDAP减少高达819.5倍。与其他最先进的方法不同，CIMNAS在没有任何准确性损失的情况下实现了以EDAP为重点的优化，为高性能基于Sim的神经网络设计生成多样化的软件-硬件参数组合。CIMNAS的源代码可访问<a href="https://github.com/OlgaKrestinskaya/CIMNAS%E3%80%82">https://github.com/OlgaKrestinskaya/CIMNAS。</a></p>
<div class="markdown-heading"><h2 class="heading-element">MIDAS：用于不平衡多模式学习的基于误解的数据增强策略</h2><a id="user-content-midas用于不平衡多模式学习的基于误解的数据增强策略" class="anchor" aria-label="Permalink: MIDAS：用于不平衡多模式学习的基于误解的数据增强策略" href="#midas用于不平衡多模式学习的基于误解的数据增强策略"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25831v1宣布类型：新摘要：多模式模型通常过度依赖主导模式，未能实现最佳性能。虽然之前的工作重点是修改培训目标或优化程序，但以数据为中心的解决方案仍然没有得到充分的探索。我们提出MIDAS，这是一种新型的数据增强策略，可以生成具有语义不一致的跨模式信息的未对齐样本，并使用单峰置信度分数进行标记，以迫使从矛盾的信号中学习。然而，这种基于信心的标签仍然有利于更自信的模式。为了在我们未对齐的样本中解决这个问题，我们引入了弱模式加权，它动态地增加了最不自信模式的损失权重，从而帮助模型充分利用较弱的模式。此外，当未对齐的特征与已对齐的特征表现出更大的相似性时，这些未对齐的样本将构成更大的挑战，从而使模型能够更好地区分类别。为了利用这一点，我们提出了硬样本加权，它优先考虑此类语义模糊的未对齐样本。多个多模式分类基准的实验表明，MIDAS在解决模式失衡方面的表现显着优于相关基线。</p>
<div class="markdown-heading"><h2 class="heading-element">MAESTRO：多模态动态时间序列的自适应稀疏注意和鲁棒学习</h2><a id="user-content-maestro多模态动态时间序列的自适应稀疏注意和鲁棒学习" class="anchor" aria-label="Permalink: MAESTRO：多模态动态时间序列的自适应稀疏注意和鲁棒学习" href="#maestro多模态动态时间序列的自适应稀疏注意和鲁棒学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25278v1宣布类型：新摘要：从临床医疗保健到日常生活，跨多种方式的连续传感器监测已为现实世界的智能决策显示出巨大的前景，但也面临着各种挑战。在这项工作中，我们引入了MAESTRO，这是一种新颖的框架，克服了现有多模式学习方法的关键局限性：（1）依赖单一主要模式进行对齐，（2）模式的成对建模，（3）假设完整的模式观察。这些限制阻碍了这些方法在现实世界的多模式时间序列设置中的适用性，其中主要模式先验通常不清楚，模式的数量可能很大（使得成对建模不切实际），并且传感器故障通常导致任意缺失观察。MAESTRO的核心是基于任务相关性促进动态模式内和跨模式交互，并利用符号标记化和自适应注意力预算来构建长多模式序列，并通过稀疏跨模式注意力进行处理。由此产生的跨模式令牌通过稀疏的专家混合（MoE）机制进行路由，从而实现不同模式组合下的黑匣子专业化。我们根据跨越三个应用程序的四个不同数据集的10个基线评估MAESTRO，并观察到在完整观察下，与现有最佳多模式和多变量方法相比，平均相对改进分别为4%和8%。在部分观察下（多达40%的模式缺失），MAESTRO平均实现了9%的改善。进一步的分析还证明了MAESTRO从动态时间序列中学习的稀疏、模式感知设计的稳健性和效率。</p>
<div class="markdown-heading"><h2 class="heading-element">VLHSA：具有腐蚀间隙的拼图解决的视觉语言分层语义对齐</h2><a id="user-content-vlhsa具有腐蚀间隙的拼图解决的视觉语言分层语义对齐" class="anchor" aria-label="Permalink: VLHSA：具有腐蚀间隙的拼图解决的视觉语言分层语义对齐" href="#vlhsa具有腐蚀间隙的拼图解决的视觉语言分层语义对齐"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25202v1宣布类型：新摘要：拼图解谜在计算机视觉中仍然具有挑战性，需要了解局部碎片细节和全球空间关系。虽然大多数传统方法只关注边缘匹配和视觉连贯性等视觉线索，但很少有方法探索自然语言描述以在具有挑战性的场景中进行语义指导，尤其是对于侵蚀性的间隙谜题。我们提出了一个视觉语言框架，利用文本上下文来增强谜题组装性能。我们的方法以视觉语言分层语义对齐（VLHSA）模块为中心，该模块通过从本地标记到全局上下文的多层语义匹配将视觉补丁与文本描述对齐。此外，该模块还集成了一个多模式架构，该架构将双视觉编码器与跨模式推理的语言功能相结合。实验表明，我们的方法在各种数据集中的表现显着优于最先进的模型，实现了重大改进，包括单品准确性提高了14.2个百分点。消融研究证实了VLHSA模块在推动改进纯视觉方法方面的关键作用。我们的工作通过融入多模式语义见解，为拼图解谜建立了一个新的范式。</p>
<div class="markdown-heading"><h2 class="heading-element">dVLA：具有多模态思维链的扩散视觉-语言-动作模型</h2><a id="user-content-dvla具有多模态思维链的扩散视觉-语言-动作模型" class="anchor" aria-label="Permalink: dVLA：具有多模态思维链的扩散视觉-语言-动作模型" href="#dvla具有多模态思维链的扩散视觉-语言-动作模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25681v1宣布类型：新摘要：视觉-语言-动作（VLA）模型正在成为机器人的下一代范式。我们引入了dVLA，这是一种基于扩散的VLA，利用多模式思维链将视觉感知、语言推理和机器人控制统一在单个系统中。dVLA在单一扩散目标下联合优化感知、语言理解和动作，从而实现更强的跨模式推理和对新颖指令和对象的更好概括。对于实际部署，我们通过合并两种加速策略（前置注意力屏蔽和KV缓存）来降低推理延迟，从而在测试时推理时产生高达大约倍的加速。我们在模拟和现实世界中评估dVLA：在LIBERO基准上，它实现了最先进的性能，平均成功率为96.4%，始终超越离散和连续动作策略;在真正的Franka机器人上，它在不同的任务套件中取得成功，包括需要多步规划的具有挑战性的捡箱任务，展示了强大的现实世界性能。这些结果共同强调了统一扩散框架用于实用、高性能VLA机器人的前景。</p>
<div class="markdown-heading"><h2 class="heading-element">InfMasking：通过对比多模式交互释放协同信息</h2><a id="user-content-infmasking通过对比多模式交互释放协同信息" class="anchor" aria-label="Permalink: InfMasking：通过对比多模式交互释放协同信息" href="#infmasking通过对比多模式交互释放协同信息"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25270v1公告类型：新摘要：在多模态表征学习中，模态之间的协同交互不仅提供互补信息，而且通过特定的交互模式创造独特的结果，这是任何单一模态都无法单独实现的。现有的方法可能难以有效地捕获全方位的协同信息，导致在这种相互作用至关重要的任务中的次优性能。这尤其成问题，因为协同信息构成了多模式表示的基本价值主张。为了应对这一挑战，我们引入了InfMasking，这是一种对比协同信息提取方法，旨在通过\textBF{Inf}inite \textBF{Masking}策略增强协同信息。InfMasking随机遮挡融合期间每个模式的大部分特征，仅保留部分信息以创建具有各种协同模式的表示。然后，通过互信息最大化将未掩蔽的融合表示与掩蔽的融合表示对齐，以编码全面的协同信息。这种无限掩蔽策略能够通过在训练期间将模型暴露于不同的部分模式组合来捕获更丰富的交互。由于计算具有无限掩蔽的互信息估计在计算上是禁止的，因此我们推导出InfMasking损失来逼近该计算。通过对照实验，我们证明InfMasking有效地增强了模式之间的协同信息。在对大规模现实世界数据集的评估中，InfMasking在七个基准上实现了最先进的性能。代码发布于<a href="https://github.com/brightest66/InfMasking%E3%80%82">https://github.com/brightest66/InfMasking。</a></p>
<div class="markdown-heading"><h2 class="heading-element">从感知到认知：多模式大型语言模型中视觉-语言交互推理概览</h2><a id="user-content-从感知到认知多模式大型语言模型中视觉-语言交互推理概览" class="anchor" aria-label="Permalink: 从感知到认知：多模式大型语言模型中视觉-语言交互推理概览" href="#从感知到认知多模式大型语言模型中视觉-语言交互推理概览"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25373v1宣布类型：新摘要：多模式大型语言模型（MLLM）努力实现对物理世界的深刻、类似人类的理解并与物理世界互动，但在获取信息（感知）和进行推理（认知）时通常表现出肤浅且不连贯的集成。这种脱节导致一系列推理失败，其中幻觉是最突出的。总的来说，这些问题暴露了一个根本性挑战：处理像素的能力尚未赋予构建连贯、可信的内部世界模型的能力。为了系统地剖析和应对这一挑战，这项调查引入了一个新颖且统一的分析框架：“从感知到认知。“我们将视觉-语言交互理解的复杂过程解构为两个相互依赖的层：感知，准确提取视觉信息并实现与文本指令细粒度一致的基本能力;认知是建立在这一感知基础上的主动、多步骤、目标导向推理的更高级能力，其核心是形成动态的搜索-思考-验证推理循环。本文以该框架为指导，系统分析了当前MLLM两层的关键瓶颈。它调查了旨在应对这些挑战的尖端方法的概况，从增强低级视觉表示的技术到改进高级推理范式的技术。此外，我们还审查了关键基准并划定了未来的研究方向。这项调查旨在为研究界提供一个清晰、结构化的视角，以了解当前MLLM的内在局限性，并阐明构建能够深入推理和真正理解世界的下一代模型的道路。</p>
<div class="markdown-heading"><h2 class="heading-element">AccidentBench：在车辆事故及其他情况下进行多模式理解和推理的基准</h2><a id="user-content-accidentbench在车辆事故及其他情况下进行多模式理解和推理的基准" class="anchor" aria-label="Permalink: AccidentBench：在车辆事故及其他情况下进行多模式理解和推理的基准" href="#accidentbench在车辆事故及其他情况下进行多模式理解和推理的基准"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26636v1宣布类型：新摘要：多模式模型的快速发展需要严格评估安全关键、动态现实世界环境中的理解和推理的基准。我们介绍了AccidentBench，这是一个大型基准，将车辆事故场景与Beyond领域相结合，Beyond领域是空气和水中的安全关键设置，强调空间和时间推理（例如，导航、方向、多车辆运动）。该基准包含大约2000个视频和超过19000个人类注释的问答对，跨越多个视频长度（短/中/长）和难度级别（简单/中/难）。任务系统地探索核心能力：时间、空间和意图理解和推理。通过将以事故为中心的交通场景与更广泛的空气和水中安全关键场景统一起来，AccidentBench提供了一个全面的、物理接地的测试平台，用于评估现实世界变化下的模型。最先进模型的评估（例如，Gemini-2.5 Pro和GPT-5）表明，即使是最强大的模型，在最困难的任务和最长的视频上也只能达到约18%的准确性，揭示了现实世界时间、空间和意图推理方面的巨大差距。AccidentBench旨在揭露这些关键差距并推动多模式模型的开发，这些模型更安全、更稳健，并且更好地适应现实世界的安全关键挑战。代码和数据集可访问：<a href="https://github.com/SafeRL-Lab/AccidentBench">https://github.com/SafeRL-Lab/AccidentBench</a></p>
<div class="markdown-heading"><h2 class="heading-element">即插即用情感图用于零镜头语音情感识别中的成分绘图</h2><a id="user-content-即插即用情感图用于零镜头语音情感识别中的成分绘图" class="anchor" aria-label="Permalink: 即插即用情感图用于零镜头语音情感识别中的成分绘图" href="#即插即用情感图用于零镜头语音情感识别中的成分绘图"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25458v1宣布类型：新摘要：大型音频语言模型（LALM）在语音任务中表现出很强的零射击性能，但由于弱的双语言建模和有限的跨模式推理，在语音情感识别（BER）方面遇到了困难。我们提出了情感推理的合成思维链预测（CCoT-Emo），这是一个引入结构化情感图（EGs）的框架，以指导LALM进行情感推理，而无需微调。每个EG编码七个声学特征（例如，音调、语速、抖动、闪烁）、文本情感、关键词和跨模式关联。嵌入到提示中，EGs提供可解释和组合的表示，以增强LALM推理。跨BER基准测试的实验表明，CCoT-Emo优于之前的SOTA，并提高了相对于零触发基线的准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">HiStyle：用于文本提示引导的可控语音合成的分层风格嵌入预测器</h2><a id="user-content-histyle用于文本提示引导的可控语音合成的分层风格嵌入预测器" class="anchor" aria-label="Permalink: HiStyle：用于文本提示引导的可控语音合成的分层风格嵌入预测器" href="#histyle用于文本提示引导的可控语音合成的分层风格嵌入预测器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25842v1宣布类型：新摘要：可控语音合成是指通过操纵特定的韵律和非语言属性（例如性别、音量、语速、音调和音调波动）来精确控制说话风格。随着先进的生成模型，特别是大型语言模型（LLM）和扩散模型的集成，可控文本转语音（TTC）系统已越来越多地从基于标签的控制过渡到基于自然语言描述的控制，这通常通过预测来自文本提示的全局风格嵌入来实现。然而，这种简单的预测忽视了风格嵌入的潜在分布，这可能会阻碍可控TTC系统的充分潜力。在这项研究中，我们使用t-SNE分析来可视化和分析各种主流RTS系统的全球风格嵌入分布，揭示了一个清晰的分层集群模式：嵌入首先按音色进行集群，然后根据风格属性细分为更细的集群。基于这一观察，我们提出了HiStyle，这是一种两阶段风格嵌入预测器，它根据文本提示分层预测风格嵌入，并进一步结合对比学习以帮助对齐文本和音频嵌入空间。此外，我们提出了一种风格注释策略，该策略利用统计方法和人类听觉偏好的互补优势来生成更准确、感知一致的文本提示，以进行风格控制。全面的实验表明，当应用于基本TTC模型时，HiStyle比替代风格嵌入预测方法实现了明显更好的风格可控性，同时在自然性和可懂性方面保持了高语音质量。音频样本可在<a href="https://anonymous.4open.science/w/HiStyle-2517/%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82" rel="nofollow">https://anonymous.4open.science/w/HiStyle-2517/上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">GroundSight：通过基础信息和消除幻觉增强视觉语言模型</h2><a id="user-content-groundsight通过基础信息和消除幻觉增强视觉语言模型" class="anchor" aria-label="Permalink: GroundSight：通过基础信息和消除幻觉增强视觉语言模型" href="#groundsight通过基础信息和消除幻觉增强视觉语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25669v1公告类型：新摘要：我们提出了一种方法，以提高视觉问答（VQA）检索增强生成（RAG）通过引入文本为基础的对象定位。我们的方法不是基于整个图像检索信息，而是使模型能够在与问题最相关的对象周围生成一个边界框，从而实现有针对性的图像裁剪和集中检索。这可以减少背景噪音，改善视觉和文本线索之间的一致性，并有助于减轻幻觉。我们的RAG方法增强了上下文感知VQA响应，与基线Llama-3.2-Vision-11B代理相比，准确性从22.19%提高到25.64%，绝对增加了3.45个百分点。我们还提出了一种基于问题类型的去幻觉方法，可以有效地将幻觉率从65.79%降低到13.88%，并提高真实性分数。</p>
<div class="markdown-heading"><h2 class="heading-element">实时适应量化：感知型LoRA，用于对量化LLM进行高效微调</h2><a id="user-content-实时适应量化感知型lora用于对量化llm进行高效微调" class="anchor" aria-label="Permalink: 实时适应量化：感知型LoRA，用于对量化LLM进行高效微调" href="#实时适应量化感知型lora用于对量化llm进行高效微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25214v1宣布类型：新摘要：随着越来越大的预训练模型的发布，将它们部署在边缘设备上以实现隐私保护应用程序需要有效的压缩。最近的作品将量化与高精度LoRA适配器的微调相结合，可以大幅减少模型大小，同时减轻量化带来的准确性损失。然而，边缘设备本质上具有不同的能力，而对每个量化设置执行配置微调在计算上是禁止的。在本文中，我们提出了CoA-LoRA，这是一种将LoRA适配器动态调整为任意量化配置（即，预训练模型的每层比特宽度选择），而不需要重复的微调。这是通过配置感知模型来实现的，该模型将每个配置映射到其低级别调整。该模型的有效性严重取决于训练配置集，即为覆盖不同的总比特宽度预算而选择的配置集合。然而，构建高质量的配置集并非易事。因此，我们设计了一个基于帕累托的配置搜索，迭代优化训练配置集，产生更精确的低等级调整。我们的实验表明，与需要为每种配置微调单独的LoRA适配器的最先进方法不同，CoA-LoRA不会产生额外的时间成本，同时实现与这些方法相当甚至更好的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">通过时间多模式交互引导专家混合</h2><a id="user-content-通过时间多模式交互引导专家混合" class="anchor" aria-label="Permalink: 通过时间多模式交互引导专家混合" href="#通过时间多模式交互引导专家混合"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25678v1宣布类型：新摘要：专家混合（MoE）架构已成为大规模多模式模型的关键。然而，它们的路由机制通常忽视了模式之间信息丰富的、时变的交互动态。这种限制阻碍了专家专业化，因为模型无法明确利用内在的情态关系来进行有效的推理。为了解决这个问题，我们提出了一种新颖的框架，使用量化的时间交互来指导MoE路由。多模式交互感知路由器学会根据专家交互的性质向专家分发令牌。这种动态路由鼓励专家获得可推广的交互处理技能，而不仅仅是学习特定于任务的特征。我们的框架建立在时态多模式交互动态的新公式之上，用于指导专家路由。我们首先证明这些时态多模式交互揭示了应用程序中有意义的模式，然后展示了如何利用它们来改进基于MoE的模型的设计和性能。针对具有挑战性的多模式基准的全面实验验证了我们的方法，展示了增强的性能和改进的可解释性。</p>
<div class="markdown-heading"><h2 class="heading-element">AMLA：在FlashAttention Resscaling中通过ADD实现MUL</h2><a id="user-content-amla在flashattention-resscaling中通过add实现mul" class="anchor" aria-label="Permalink: AMLA：在FlashAttention Resscaling中通过ADD实现MUL" href="#amla在flashattention-resscaling中通过add实现mul"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25224v1宣布类型：新摘要：多头潜在注意力（MLA）显着减少了大型语言模型中的KVache内存使用，同时引入了大量的计算负担和中间变量扩展。这对高效的硬件实施提出了挑战--尤其是在解码阶段。本文介绍Ascend MLA（AMLA），这是一个专门针对华为Ascend NPU进行优化的高性能内核。AMLA建立在两个核心创新之上：（1）一种新颖的基于Flash Attention的算法，该算法利用FP 32和INT 32表示之间的二进制对应关系，以替换浮点相乘以进行输出块重新缩放;（2）具有分层切片的预加载管道策略，最大限度地提高FLOPS利用率：预加载管道实现了立方体绑定的性能，而分层切片与Cube核心内的数据移动和计算重叠。实验表明，在Ascend 910 NPU（集成在CloudMatrix 384中）上，AMLA可实现高达614个TFLOPS，达到理论最大FLOPS的86.8%，优于最先进的开源Flash MLA实现，其FLOPS利用率在NVIDIA H800 SXM 5上高达66.7%。AMLA内核已集成到华为的CANN中，并将很快发布。</p>
<div class="markdown-heading"><h2 class="heading-element">有效的模型修剪</h2><a id="user-content-有效的模型修剪" class="anchor" aria-label="Permalink: 有效的模型修剪" href="#有效的模型修剪"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25606v1宣布类型：新摘要：我们引入了有效模型修剪（BEP），这是一种与上下文无关、无参数的规则，用于解决有关修剪的一个基本问题：要保留多少个条目。BEP没有规定如何对参数进行评分或修剪模型;相反，它提供了一个通用的自适应阈值，可以应用于任何修剪标准：权重幅度、注意力得分、KAN重要性得分，甚至特征级信号（例如图像像素），并用于模型的结构部分或权重。给定任何得分载体s，BEP将s映射到内置有效数N_eff，该数受到贡献者的逆辛普森指数的启发。在我们的实验中，保留N_eff得分最高的条目并将其余条目归零，从而产生了性能与跨MLP、CNN、Transformers/LLM和KAN的原始密集网络相当的稀疏模型。通过利用单形的几何形状，我们在与分数载体s相关的相应有序概率单形上推导出保留质量s_eff（保留分数的总和）的紧下界。我们通过在各种标准和模型中使用缩放阈值\b{eta}*N_eff来修剪模型，进一步验证N_eff的有效性。实验表明，默认的\b{eta} = 1为模型修剪提供了稳健的阈值，而不等于1的\b{eta}仍然可以作为可选调整，以满足特定的稀疏性要求。</p>
<div class="markdown-heading"><h2 class="heading-element">迭代剩余交叉注意机制：视听导航任务的集成方法</h2><a id="user-content-迭代剩余交叉注意机制视听导航任务的集成方法" class="anchor" aria-label="Permalink: 迭代剩余交叉注意机制：视听导航任务的集成方法" href="#迭代剩余交叉注意机制视听导航任务的集成方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25652v1宣布类型：新摘要：视听导航代表了一个重要的研究领域，其中智能代理利用以自我为中心的视觉和听觉感知来识别音频目标。传统的导航方法通常采用分阶段的模块化设计，其中涉及首先执行特征融合，然后利用门控回归单元（GRU）模块进行序列建模，最后通过强化学习做出决策。虽然这种模块化方法已证明有效性，但它也可能导致特征融合和GRU序列建模阶段各个模块之间的冗余信息处理和信息传输不一致。本文提出了IRCAM-AVN（视听导航迭代剩余交叉注意力机制），这是一个端到端框架，将多模式信息融合和序列建模集成在统一的IRCAM模块中，从而取代传统的单独融合组件和GRU。这种创新机制采用多层剩余设计，将初始多峰序列与处理后的信息序列连接起来。这种方法学转变逐步优化了特征提取过程，同时减少了模型偏差并增强了模型的稳定性和概括能力。实证结果表明，采用迭代剩余交叉注意机制的智能代理具有优越的导航性能。</p>
<div class="markdown-heading"><h2 class="heading-element">速度提高90%，100%无代码：MLLM驱动的零代码3D游戏开发</h2><a id="user-content-速度提高90100无代码mllm驱动的零代码3d游戏开发" class="anchor" aria-label="Permalink: 速度提高90%，100%无代码：MLLM驱动的零代码3D游戏开发" href="#速度提高90100无代码mllm驱动的零代码3d游戏开发"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26161v1宣布类型：新摘要：开发3D游戏需要跨多个领域的专业知识，包括编程、3D建模和引擎配置，这限制了数百万潜在创作者的访问权限。最近，研究人员开始探索自动化游戏开发。然而，现有方法面临三个主要挑战：（1）2D内容生成或孤立的代码片段的范围有限;（2）需要将生成的组件手动集成到游戏引擎中;（3）处理交互式游戏逻辑和状态管理的性能较差。虽然多模式大型语言模型（MLLM）展示了轻松游戏生成任务的潜在能力，但在将这些输出转化为基于Unity和虚幻引擎等游戏引擎的可生产就绪、可执行游戏项目方面仍然存在关键差距。   为了弥合这一差距，本文引入了UniGen，这是第一个端到端协调的多代理框架，可以根据自然语言要求自动化可运行3D游戏的零编码开发。具体来说，UniGen使用一个规划代理，将用户需求解释为结构化蓝图和工程逻辑描述;之后生成代理生成可执行的C#脚本;然后自动化代理处理特定于引擎的组件绑定和场景构建;最后，Inbox Agent通过对话交互提供实时错误纠正。我们在三个不同的游戏原型上评估了UniGen。结果表明，UniGen不仅不需要用户编写代码，从而使游戏创作民主化，而且还将开发时间缩短了91.4%。我们在<a href="https://github.com/yxwan123/UniGen%E5%8F%91%E5%B8%83UniGen%E3%80%82%E8%A7%86%E9%A2%91%E6%BC%94%E7%A4%BA%E5%8F%AF%E5%9C%A8https://www.youtube.com/watch?%E4%B8%8A%E8%8E%B7%E5%BE%97v=">https://github.com/yxwan123/UniGen发布UniGen。视频演示可在https://www.youtube.com/watch?上获得v=</a> xyJjFfnxUx 0。</p>
<div class="markdown-heading"><h2 class="heading-element">DiVeQ：使用重新参数化技巧的可微向量化</h2><a id="user-content-diveq使用重新参数化技巧的可微向量化" class="anchor" aria-label="Permalink: DiVeQ：使用重新参数化技巧的可微向量化" href="#diveq使用重新参数化技巧的可微向量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26469v1宣布类型：新摘要：载体量化在深度模型中很常见，但其硬分配会阻碍梯度并阻碍端到端训练。我们提出了DiVeQ，它将量化视为添加一个模仿量化失真的误差载体，在让梯度流动的同时保持正向传递。我们还提出了一种空间填充变体（SF-DiVeQ），它分配给由连接代码字的线构建的曲线，从而减少量化误差和完全的码本使用。这两种方法都是端到端的训练，而不需要辅助损失或温度计划。在跨各种数据集的VQ-VAE压缩和VQGAN生成中，它们比替代量化方法提高了重建和样本质量。</p>
<div class="markdown-heading"><h2 class="heading-element">多LoRA下LLM微调参数共享的再思考</h2><a id="user-content-多lora下llm微调参数共享的再思考" class="anchor" aria-label="Permalink: 多LoRA下LLM微调参数共享的再思考" href="#多lora下llm微调参数共享的再思考"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25414v1宣布类型：新摘要：大型语言模型通常使用低等级自适应（LoRA）等参数高效技术进行调整，公式为$y = W_0x + BAx$，其中$W_0$是预训练的参数，$x$是调整层的输入。虽然多适配器扩展通常使用多个LoRA，但之前的研究表明，内部$A$矩阵在训练期间高度相似，因此适合共享。我们重新审视这一现象，发现这种相似性很大程度上归因于相同的初始化而不是共享的知识，其中$B$在知识编码和转移中发挥着更关键的作用。受这些见解的启发，我们提出了\textBF{ALoRA}，这是一种非对称多LoRA设计，在多任务微调中具有多个$A$矩阵和单个共享$B$，以及\textBF{Fed-ALoRA}，它在同质和异类设置下的联合微调中在客户端之间共享$B$，通过新颖的矩阵分解策略来适应客户端之间的异类排名。常识推理、数学推理、多任务NLP数据集和联邦NLP数据集的实验表明，我们的方法在任务中实现了更加平衡的性能，并且相对于现有的多LoRA方法具有相当或更高的平均准确性。代码可访问<a href="https://github.com/OptMN-Lab/ALoRA%E3%80%82">https://github.com/OptMN-Lab/ALoRA。</a></p>
<div class="markdown-heading"><h2 class="heading-element">旋转控制取消学习：利用认知旋转空间量化和控制LLM的连续取消学习</h2><a id="user-content-旋转控制取消学习利用认知旋转空间量化和控制llm的连续取消学习" class="anchor" aria-label="Permalink: 旋转控制取消学习：利用认知旋转空间量化和控制LLM的连续取消学习" href="#旋转控制取消学习利用认知旋转空间量化和控制llm的连续取消学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25743v1宣布类型：新摘要：随着大型语言模型（LLM）变得越来越普遍，其安全漏洞已经引起人们的关注。引入机器去学习是为了通过消除不良数据的影响来减轻这些风险。然而，现有的方法不仅依赖于保留的数据集来保持模型效用，而且在持续的取消学习请求下还会遭受累积的灾难性效用损失。为了解决这一困境，我们提出了一种新的方法，称为旋转控制取消学习（RCU），该方法利用RCU的旋转突出性权重来量化和控制连续取消学习过程中的取消学习程度。斜对称损失旨在构建认知旋转空间的存在，其中旋转角度的变化可以模拟连续的取消学习过程。此外，我们设计了一种垂直旋转轴规则化，以针对连续的取消学习请求强制相互垂直的旋转方向，有效地最大限度地减少干扰并解决累积的灾难性公用事业损失。在多个数据集上的实验证实，我们的方法没有保留数据集达到SOTA性能。</p>
<div class="markdown-heading"><h2 class="heading-element">NePTune：一个基于视觉语言的可调组合推理的神经Pythonic框架</h2><a id="user-content-neptune一个基于视觉语言的可调组合推理的神经pythonic框架" class="anchor" aria-label="Permalink: NePTune：一个基于视觉语言的可调组合推理的神经Pythonic框架" href="#neptune一个基于视觉语言的可调组合推理的神经pythonic框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25757v1宣布类型：新摘要：现代视觉语言模型（VLM）在各种任务中取得了令人印象深刻的性能，但它们经常难以应对组合推理、分解和重组概念以解决新问题的能力。虽然神经符号方法提供了一个有希望的方向，但它们通常受到清晰的逻辑执行或预定义的断言的限制，从而限制了灵活性。在这项工作中，我们介绍了NePTune，神经符号框架，克服了这些限制，通过混合执行模型，集成了基础视觉模型的感知能力与符号推理的组合表现力。NePTune动态地将自然语言查询转换为可执行的Python程序，这些程序将命令式控制流与能够对VLM-generated不确定性进行推理的软逻辑运算符相结合。NePTune采用模块化设计，以免训练的方式操作，将感知与推理相结合，但其可区分的操作支持微调。我们利用对抗测试在多个视觉推理基准和各个领域评估了NePTune，并证明了强基模型的显著改进，以及其在新环境中的有效组合泛化和适应能力。</p>
<div class="markdown-heading"><h2 class="heading-element">通过组块PPO和自我行为克隆的VLA模型后训练</h2><a id="user-content-通过组块ppo和自我行为克隆的vla模型后训练" class="anchor" aria-label="Permalink: 通过组块PPO和自我行为克隆的VLA模型后训练" href="#通过组块ppo和自我行为克隆的vla模型后训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25718v1宣布类型：新摘要：强化学习（RL）是训练后视觉-语言-动作（VLA）模型的一种有前途的途径，但实际部署受到稀疏奖励和不稳定训练的阻碍。这项工作通过引入基于近端策略优化（PPO）的动作块以及使用自我收集的演示的行为克隆来缓解这些挑战。将连续动作聚合成块提高了策略的时间一致性和信息反馈的密度。此外，一个辅助的行为克隆丢失应用于一个动态更新的演示缓冲区，在训练过程中不断收集高质量的任务试验。在线调整动作分块PPO目标与自身行为克隆辅助损失之间的相对权重，以稳定后训练过程。在MetaWorld基准测试上的实验表明，与监督微调相比，性能有所提高，成功率高（0.93），成功步骤少（42.17）。这些结果证明了RL用于VLA后训练的可行性，并有助于为下游VLA应用奠定基础。</p>
<div class="markdown-heading"><h2 class="heading-element">协作压缩，用于边缘大规模MoE部署</h2><a id="user-content-协作压缩用于边缘大规模moe部署" class="anchor" aria-label="Permalink: 协作压缩，用于边缘大规模MoE部署" href="#协作压缩用于边缘大规模moe部署"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25689v1宣布类型：新摘要：专家混合体系结构是扩展大型语言模型的一种重要方法。它增加了模型容量，同时保持较低的计算成本。然而，超大型MoE模型仍有数千亿个参数，需要大量内存/存储，导致在资源受限的边缘平台上部署困难。仅靠修剪或量化很难解决这个问题，因为超激进的压缩比，准确性和输出质量显着下降。为了促进超大规模MoE在边缘平台上的部署，我们通过结合专家修剪、混合精度量化和激活优化提出了一种协作压缩框架。它可以有效地将超大型MoE DeepSeek-V3的存储占用空间从1.3TB减少到103 GB，同时保持高输出质量和比传统均匀低位量化方法更好的准确性。据我们所知，我们是第一个在平台上部署超大型DeepSeek-V3压缩模型的公司，总内存限制为128 GB。我们在各种内存限制下对多个基准进行的全面实验证明了我们的方法的有效性，具有比均匀低位量化方法更小的模型大小和更高的准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">MLA：用于机器人操纵多模式理解和预测的多感官视觉动作模型</h2><a id="user-content-mla用于机器人操纵多模式理解和预测的多感官视觉动作模型" class="anchor" aria-label="Permalink: MLA：用于机器人操纵多模式理解和预测的多感官视觉动作模型" href="#mla用于机器人操纵多模式理解和预测的多感官视觉动作模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26642v1宣布类型：新摘要：视觉语言动作模型（VLs）通过继承视觉语言模型（VLs）和学习动作生成，在机器人操纵任务中表现出了概括能力。大多数VLA模型专注于解释视觉和语言以生成动作，而机器人必须在空间物理世界中感知和互动。这一差距凸显了全面了解机器人特定的多感官信息的必要性，这对于实现复杂且接触丰富的控制至关重要。为此，我们引入了一种多感官语言动作（MLA）模型，该模型协作感知不同的感官模式并预测未来的多感官目标，以促进物理世界建模。具体来说，为了增强感知表示，我们提出了一种无编码器的多模式对齐方案，该方案创新地将大型语言模型本身重新利用为感知模块，通过位置对应对齐2D图像、3D点云和触觉标记来直接解释多模式线索。为了进一步增强MLA对物理动力学的理解，我们设计了一种未来的多感官生成训练后策略，使MLA能够推理语义、几何和交互信息，为动作生成提供更稳健的条件。对于评估，MLA模型在复杂、接触丰富的现实世界任务中比之前最先进的2D和3D VLA方法分别高出12%和24%，同时还展示了对未见配置的改进的概括性。项目网站：<a href="https://sites.google.com/view/open-mla" rel="nofollow">https://sites.google.com/view/open-mla</a></p>
<div class="markdown-heading"><h2 class="heading-element">增长获胜的子网络，而不是修剪它们：稀疏神经网络中密度发现的范式</h2><a id="user-content-增长获胜的子网络而不是修剪它们稀疏神经网络中密度发现的范式" class="anchor" aria-label="Permalink: 增长获胜的子网络，而不是修剪它们：稀疏神经网络中密度发现的范式" href="#增长获胜的子网络而不是修剪它们稀疏神经网络中密度发现的范式"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25665v1宣布类型：新摘要：彩票假设表明，密集网络包含稀疏子网络，可以隔离训练这些子网络以匹配全模型性能。现有的方法--迭代修剪、动态稀疏训练和初始化修剪--要么会产生沉重的再培训成本，要么假设目标密度是预先固定的。我们引入了路径权重幅度产品偏向随机增长（PWMPP），这是一种建设性的从稀疏到密集的训练范式，可以增长网络而不是修剪网络，同时自动发现其操作密度。PWMRP从稀疏种子开始，添加受路径核启发的分数引导的边，通过随机化缓解瓶颈，并在逻辑匹配规则检测到稳定准确性时停止。CIFAR、TinyImageNet和ImageNet上的实验表明，尽管密度更高，但仍以低得多的成本（约1.5倍的密度，而MP的密度为3- 4倍）接近于由MP衍生的彩票的性能。这些结果将基于生长的密度发现确立为补充修剪和动态稀疏性的一种有前途的范式。</p>
<div class="markdown-heading"><h2 class="heading-element">Flash Omni：扩散变形金刚的统一稀疏注意力引擎</h2><a id="user-content-flash-omni扩散变形金刚的统一稀疏注意力引擎" class="anchor" aria-label="Permalink: Flash Omni：扩散变形金刚的统一稀疏注意力引擎" href="#flash-omni扩散变形金刚的统一稀疏注意力引擎"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25401v1宣布类型：新摘要：多模式扩散变形器（DiT）在视觉合成方面表现出出色的能力，但它们的部署仍然受到大量计算需求的限制。为了缓解这一瓶颈，人们提出了许多基于稀疏的加速方法。然而，它们不同的稀疏模式通常需要定制的内核来进行高性能推理，从而限制了通用性。我们提出FlashOmni，一个统一的稀疏注意力引擎兼容任意DiT架构。FlashOmni引入了灵活的稀疏符号来标准化各种稀疏策略的表示，例如特征缓存和块稀疏跳过。这种统一的抽象使不同的稀疏计算在一个单一的注意力内核的执行。此外，Flash Omni还为注意力块设计了优化的稀疏GEM，利用稀疏符号来消除冗余计算并进一步提高效率。实验表明，Flash Omni在注意力和GEMM-$Q$方面提供了近线性的、与稀疏比加速（1：1）密切匹配，并在GEMM-$O$中实现了2.5 $\乘$-3.8 $\乘$加速（最大峰值约为理论极限的87.5%）。应用多粒度稀疏策略，使浑源模型（33 K）在不降低视觉质量的情况下实现约1.5 $\times $的端到端加速。</p>
<div class="markdown-heading"><h2 class="heading-element">联邦低秩自适应中通信高效精确的聚合方法</h2><a id="user-content-联邦低秩自适应中通信高效精确的聚合方法" class="anchor" aria-label="Permalink: 联邦低秩自适应中通信高效精确的聚合方法" href="#联邦低秩自适应中通信高效精确的聚合方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2509.26399v1 Announce Type: new  Abstract: With the rapid emergence of foundation models and the increasing need for fine-tuning across distributed environments, Federated Low-Rank Adaptation (FedLoRA) has recently gained significant attention. Despite enormous potential, current FedLoRA methods face notable challenges due to inexact updates. Existing approaches have attempted to mitigate this issue, but they often introduce a \emph{local-global generalization gap} and incur \emph{substantial communication overhead}, limiting their scalability and effectiveness. To address these limitations, we propose \textbf{F}ederated \textbf{Lo}w-\textbf{R}ank \textbf{A}ggregation with \textbf{N}early \textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA matrices on the server to estimate the aggregated matrices $\hat{A}$ and $\hat{B}$, which are then distributed to clients for local updates. This surrogated aggregated matrices minimizes the divergence between ideal $\nabla \Bar{W} = \sum^{U}_{u=1}B_u A_u$ and practical updates $\nabla \hat{W} = \hat{B}\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By doing so, FLoRA-NA achieves communication efficiency and bridges the gap between local personalization and global generalization, addressing a key limitation of prior personalized FedLoRA approaches. We conduct extensive evaluations across diverse tasks, including natural language understanding, mathematical reasoning, and code-solving ability using various foundation models. Experimental results consistently demonstrate that FLoRA-NA achieves state-of-the-art global performance while maintaining low communication overhead.</p>
<div class="markdown-heading"><h2 class="heading-element">基于具体分数匹配的大型语言模型提取</h2><a id="user-content-基于具体分数匹配的大型语言模型提取" class="anchor" aria-label="Permalink: 基于具体分数匹配的大型语言模型提取" href="#基于具体分数匹配的大型语言模型提取"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25837v1宣布类型：新摘要：大型语言模型（LLM）可提供出色的性能，但部署成本高昂，从而激励知识提炼（KD）进行高效推理。现有的KD目标通常通过softmax匹配学生和教师的概率，这会模糊宝贵的logit信息。虽然直接logit蒸馏（DLD）减轻了softmax平滑，但它未能考虑logit漂移不变性，从而限制了解决方案空间。我们提出了混凝土分数蒸馏（CMA），这是一种离散分数匹配目标，它克服了softmax引发的平滑和对最优解集的限制。我们解决了自回归LLM中离散分数匹配的训练不稳定性和二次复杂性，并通过灵活的加权来调整学生和教师之间所有词汇对的相对logit差异。我们在我们的框架内提供了模式寻求和模式覆盖实例，并使用GPT-2-1.5B、OpenLLaMA-7 B和GEMA-7 B-IT在任务不可知的描述跟踪和特定任务的提炼方面评估了CPD。实验表明，CPD始终超越最近的KD目标，实现了有利的多样性权衡，并在与政策上的技术相结合时产生了补充收益，展示了其对LLM蒸馏的可扩展性和有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">专家合并：具有无监督专家对齐和导入引导层分块的模型合并</h2><a id="user-content-专家合并具有无监督专家对齐和导入引导层分块的模型合并" class="anchor" aria-label="Permalink: 专家合并：具有无监督专家对齐和导入引导层分块的模型合并" href="#专家合并具有无监督专家对齐和导入引导层分块的模型合并"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25712v1宣布类型：新摘要：模型合并将多个领域专业化的专家结合到一个模型中，提供了一种实用的途径，可以赋予大型语言模型（LLM）和多模式大型语言模型（MLLM）广泛的功能，而无需支付联合培训或为许多模型提供服务的成本。然而，免训练方法依赖于手动调整系数，而基于训练的方法主要对齐参数而不是下游任务行为，并且通常统一处理所有层，忽略层间的均匀性。我们引入了Expert Merging，这是一种轻训练方法，仅使用未标记的校准数据来学习一小群分层系数。系数经过优化，以显式地将合并模型的隐藏状态和逻辑与相应专家的隐藏状态和逻辑相匹配，并使用系数调节器来实现稳定性和任务加权损失以实现可控权衡。为了捕捉层间变化，Expert Merging++通过重要性引导的分块来增强这一设计：从学习到的系数、任务载体幅度和参数计数中推导出的标准化层重要性指标将更多的分块系数分配给高重要性层，同时保持低重要性层的轻量级。结果是一种无标签、参数高效且可扩展的跨LLM和MLLM的多专家模型合并方法。在MLLM主干（InternVLL和Qwen 2-VLL）和LLM主干（Mistral）中，我们的方法超越了强大的无训练和基于训练的合并基线，其中Expert Merking ++带来了进一步的收益，在某些情况下，甚至超过了监督的混合训练。源代码可在<a href="https://github.com/Littleor/ExpertMerging%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Littleor/ExpertMerging上获得。</a></p>
<div class="markdown-heading"><h2 class="heading-element">作为监督的澄清：视觉语言界面的强化学习</h2><a id="user-content-作为监督的澄清视觉语言界面的强化学习" class="anchor" aria-label="Permalink: 作为监督的澄清：视觉语言界面的强化学习" href="#作为监督的澄清视觉语言界面的强化学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26594v1宣布类型：新摘要：最近的纯文本模型展示了出色的数学推理能力。将这些扩展到视觉领域需要视觉语言模型将图像翻译成文本描述。然而，当前的模型经过训练，为人类读者生成字幕，通常会省略推理系统所需的精确细节。这造成了界面不匹配：推理机通常失败不是因为推理限制，而是因为他们缺乏对关键视觉信息的访问。我们提出了自适应澄清强化学习（AC-RL），它通过交互向视觉模型教授推理者需要什么信息。我们的主要见解是，培训期间的澄清请求揭示了信息差距;通过惩罚需要澄清的成功，我们为全面的初始说明创造了压力，使推理者能够一次性解决问题。AC-RL在七个视觉数学推理基准上比预训练的基线提高了4.4个百分点，分析表明，如果允许的话，它将使澄清请求减少多达39%。通过将澄清视为一种隐性监督形式，AC-RL证明，仅通过交互就可以有效学习视觉语言界面，而无需显式注释。</p>
<div class="markdown-heading"><h2 class="heading-element">用于压缩大型语言模型的分层动态排名</h2><a id="user-content-用于压缩大型语言模型的分层动态排名" class="anchor" aria-label="Permalink: 用于压缩大型语言模型的分层动态排名" href="#用于压缩大型语言模型的分层动态排名"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25622v1公告类型：新摘要：大型语言模型（LLM）的规模迅速扩大，带来了严重的内存和计算挑战，阻碍了它们的部署。基于奇异值分解（SVD）的压缩已经成为LLM的一种有吸引力的训练后压缩技术，然而大多数现有方法在所有层上应用统一的压缩比，隐含地假设各个层中包括均匀的信息。这忽视了在LLM中观察到的实质性层内不均匀性，其中中间层往往编码更丰富的信息，而早期和晚期层则更加冗余。在这项工作中，我们重新审视了现有的基于VD的压缩方法，并提出了D-Rank，这是一个具有分层平衡动态Rank分配的框架，用于LLM压缩。我们首先引入有效排名作为原则性指标来衡量权重矩阵的信息密度，然后通过基于拉格朗日乘子的优化方案分配排名，以便在固定压缩比下自适应地为信息密度较高的组分配更多容量。此外，我们重新平衡了注意力层之间分配的排名，以考虑其不同的重要性，并将D-Rank扩展到具有分组查询注意力的最新LLM。在多种压缩比下具有不同规模的各种LLM上进行的大量实验表明，D-Rank始终优于SVD-LLM、ASVD和Basis Sharing，在C4数据集上使用LLaMA-3-8B模型在20%压缩比下实现了15以上的困惑度，并在40%压缩比下使用LLaMA-7 B模型实现了高达5%的零镜头推理准确率，同时实现了更高的吞吐量。</p>
<div class="markdown-heading"><h2 class="heading-element">AST：大型语言模型的连续且可区分的半结构化稀疏感知训练</h2><a id="user-content-ast大型语言模型的连续且可区分的半结构化稀疏感知训练" class="anchor" aria-label="Permalink: AST：大型语言模型的连续且可区分的半结构化稀疏感知训练" href="#ast大型语言模型的连续且可区分的半结构化稀疏感知训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25996v1宣布类型：新摘要：稀疏性感知训练是将大型语言模型（LLM）转换为硬件友好的稀疏模式的有效方法，从而减少推理期间的延迟和内存消耗。在本文中，我们提出了连续自适应稀疏训练器（Custer），这是一个用于半结构化（或“N：M”）稀疏模型的完全连续且可区分的稀疏感知训练框架。与之前分别优化稀疏性模式和权重的方法不同，AST可以在训练期间实现无缝联合优化，同时将模型逐步转换为所需的稀疏性格式。具体来说，AST引入了三个关键组件：1）AdamS，一个稀疏性感知优化器，利用自适应L1衰变来促进所有参数的均匀稀疏化; 2）Weight Scaling，一个旨在减轻衰变引起的幅度减少的模块，同时保留所需的稀疏模式; 3）知识蒸馏，它使用密集模型作为自学来提高训练效率。我们在多个模型系列（参数范围从125 M到13 B）的2：4稀疏度模式下评估了AST。我们的结果表明，在最少的训练资源下，与之前的最先进方法相比，在困惑度和零射击准确性方面都有显着改进。值得注意的是，在LLaMA 2 - 7 B上，与仅使用2%原始预训练令牌的密集模型相比，我们的2：4稀疏模型实现了0.09的困惑度增加，零射击准确率增加了0.36%。此外，我们还建立了一个准确且稳健的经验缩放定律，以在足够的训练资源的情况下预测稀疏模型的性能。最后，我们通过在量化和微调场景下评估稀疏模型来证明稀疏模型的实际适用性。</p>
<div class="markdown-heading"><h2 class="heading-element">OmniNav：前瞻性探索和视觉语言导航的统一框架</h2><a id="user-content-omninav前瞻性探索和视觉语言导航的统一框架" class="anchor" aria-label="Permalink: OmniNav：前瞻性探索和视觉语言导航的统一框架" href="#omninav前瞻性探索和视觉语言导航的统一框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25687v1宣布类型：新摘要：并行导航是智能机器人的核心挑战，需要理解视觉环境、自然语言指令和自主探索。现有模型往往无法提供跨不同导航范式的统一解决方案，导致成功率低且通用性有限。我们引入OmniNav，这是一个统一框架，在单一架构中解决预算目标、对象目标、点目标导航和基于前沿的探索。我们的方法具有轻量级、低延迟策略，可以准确预测连续空间航路点（坐标和方向）。该策略在精确度上超越了动作块方法，并支持在高达5 Hz的控制频率下的现实世界部署。从架构上来说，OmniNav采用了快慢系统设计：快速模块使用短期视觉上下文和子任务生成航点，而缓慢模块使用长期观察和候选边界执行审慎规划，以选择后续子目标和子任务。这种合作提高了路径效率并保持轨迹一致性，特别是在探索和存储密集型场景中。至关重要的是，我们发现主要瓶颈不仅仅是导航政策学习，还包括对一般指令和对象的深入理解。为了提高概括性，OmniNav将大规模通用训练数据集（包括图像字幕和视觉识别数据集）集成到联合多任务方案中。这显着提高了成功率和稳健性。大量实验证实了OmniNav在各种导航基准上的最先进性能，现实世界的部署进一步验证了其功效。OmniNav为具体化导航提供实用见解，为实现多功能、高度通用的机器人智能绘制了一条可扩展的道路。</p>
<div class="markdown-heading"><h2 class="heading-element">RAP：联邦学习中多任务和多模式基础模型的两阶段自适应个性化</h2><a id="user-content-rap联邦学习中多任务和多模式基础模型的两阶段自适应个性化" class="anchor" aria-label="Permalink: RAP：联邦学习中多任务和多模式基础模型的两阶段自适应个性化" href="#rap联邦学习中多任务和多模式基础模型的两阶段自适应个性化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26524v1宣布类型：新摘要：尽管联邦学习（FL）在以分散的方式训练多个模型方面表现出了令人印象深刻的能力，但事实证明，它可以生成一个最终模型并不一定非常适合每个客户的需求。虽然人们对如何创建量身定制的个性化模型（称为个性化联邦学习（PFL））进行了大量工作，但人们对通过微调具有多任务和多模式属性的基础模型来实现个性化的关注较少。此外，文献中缺乏对如何在客户之间不仅在数据方面、而且在任务和模式方面都存在差异的环境中微调和个性化此类模型的了解。为了解决文献中的这一差距，我们提出了NAP（两阶段自适应个性化），它（i）利用客户端和服务器之间不匹配的模型架构，在有利于客户端的本地任务时选择性地进行替换操作，并且（ii）参与FL后知识蒸馏，以捕获有益的一般知识而不损害个性化。我们还在其模式-任务对架构下引入了服务器模型的首次收敛分析，并证明随着模式-任务对数量的增加，其满足所有任务的能力会受到影响。通过大量实验，我们证明了我们提出的算法在各种数据集和任务中与众多基线相比的有效性。实现代码可在<a href="https://github.com/lee3296/TAP%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lee3296/TAP上公开获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">STaR-Attack：一个用于统一多模式理解和生成模型的时空和叙述推理攻击框架</h2><a id="user-content-star-attack一个用于统一多模式理解和生成模型的时空和叙述推理攻击框架" class="anchor" aria-label="Permalink: STaR-Attack：一个用于统一多模式理解和生成模型的时空和叙述推理攻击框架" href="#star-attack一个用于统一多模式理解和生成模型的时空和叙述推理攻击框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26473v1宣布类型：新摘要：统一多模式理解和生成模型（UMM）在理解和生成任务方面表现出了非凡的能力。然而，我们发现了UMM中世代理解耦合引起的漏洞。攻击者可以使用生成函数来制作信息丰富的对抗图像，然后利用理解函数在一次传递中吸收它，我们称之为跨模式生成注入（CMGI）。当前针对恶意指令的攻击方法通常仅限于单一模式，同时还依赖于具有语义漂移的即时重写，从而使UMM的独特漏洞尚未被探索。我们提出了STaR-Attack，这是第一个多回合越狱攻击框架，它利用了UMM独特的安全弱点，而没有语义漂移。具体来说，我们的方法定义了与时空上下文中的目标查询强相关的恶意事件。STaR-Attack利用三幕叙事理论，生成事件前和事件后场景，同时将恶意事件隐藏为隐藏的高潮。当执行攻击策略时，前两轮利用UMM的生成能力为这些场景生成图像。随后，利用理解能力，引入了一个基于图像的猜谜游戏。STaR-Attack将最初的恶意问题嵌入到良性候选项中，迫使模型在叙述上下文的情况下选择并回答最相关的问题。大量的实验表明，STaR-Attack始终超过了之前的方法，在Gemini-2.0-Flash上实现了高达93.06%的ASR，并超过了之前最强的基线FlipAttack。我们的工作揭示了一个关键的，但欠发达的脆弱性，并强调需要在UMM安全对齐。</p>
<div class="markdown-heading"><h2 class="heading-element">VLM伪标签能否训练优于VLM的时间序列QA模型？</h2><a id="user-content-vlm伪标签能否训练优于vlm的时间序列qa模型" class="anchor" aria-label="Permalink: VLM伪标签能否训练优于VLM的时间序列QA模型？" href="#vlm伪标签能否训练优于vlm的时间序列qa模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.25696v1宣布类型：新摘要：由于缺乏标记数据，时间序列问答（TSQA）任务面临重大挑战。或者，随着大规模模型最近的进步，视觉语言模型（VLM）已经证明了以零镜头方式分析时间序列信号的潜力。在本文中，我们提出了一种使用VLM生成的伪标签的训练方法。尽管VLM可能会产生错误的标签，但TSQA模型仍然可以根据深度神经网络对此类有噪标签固有鲁棒性的特性进行有效训练。我们的实验结果表明，TSQA模型不仅可以成功使用伪标签进行训练，而且还通过利用大量未标记数据来超越VLM本身的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">TASP：拓扑感知的序列映射</h2><a id="user-content-tasp拓扑感知的序列映射" class="anchor" aria-label="Permalink: TASP：拓扑感知的序列映射" href="#tasp拓扑感知的序列映射"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2509.26541v1公告类型：新摘要：由于自注意机制的二次复杂性，长上下文大型语言模型（LLM）面临限制。主流序列并行性（SP）方法Ring Attention试图通过将查询分布到加速器之间的多个查询块中来解决这个问题，并使每个Q张量能够通过Ring AllGather通信基元访问来自其他加速器的所有KV张量。然而，它的通信效率较低，限制了其实际适用性。这种低效率源于它采用的Ring AllGather通信基元与现代加速器的AlltoAll拓扑之间的不匹配。Ring AllGather基元由环形数据传输的迭代组成，该数据传输只能利用AlltoAll布局的非常有限的一部分。   受完全有向图的Hamilton分解的启发，我们发现现代加速器布局可以分解成多个垂直环数据路径，这些路径可以在没有干扰的情况下并发传输数据。基于此，我们进一步观察到Ring AllGather基元也可以在每次迭代时分解为相同数量的并发环形数据传输。基于这些见解，我们提出了TISP，这是一种用于长上下文LLM的、具有全局性的SP方法，通过布局分解和基元分解充分利用现代加速器的通信能力。单节点和多节点NVIDIA H100系统以及单节点AMD MI 300 X系统的实验结果表明，TISP在这些现代加速器布局上实现了比Ring Attention更高的通信效率，并比Ring Attention及其变体Zigzag-Ring Attention实现了高达3.58的加速比。该代码可在<a href="https://github.com/infinigence/HamiltonAttention%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/infinigence/HamiltonAttention上获取。</a></p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>