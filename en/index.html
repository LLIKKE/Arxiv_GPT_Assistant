<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/10/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10102025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/10/2025" href="#personalized-daily-arxiv-papers-10102025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 24</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</a>
<strong>Authors:</strong> Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</p>
</li>
<li>
<p><a href="#link1">Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models</a>
<strong>Authors:</strong> Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka, Phillip Isola</p>
</li>
<li>
<p><a href="#link2">Fewer Weights, More Problems: A Practical Attack on LLM Pruning</a>
<strong>Authors:</strong> Kazuki Egashira, Robin Staab, Thibaud Gloaguen, Mark Vero, Martin Vechev</p>
</li>
<li>
<p><a href="#link3">Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning</a>
<strong>Authors:</strong> Aman Sharma, Paras Chopra</p>
</li>
<li>
<p><a href="#link4">Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots</a>
<strong>Authors:</strong> Boyu Li, Siyuan He, Hang Xu, Haoqi Yuan, Yu Zang, Liwei Hu, Junpeng Yue, Zhenxiong Jiang, Pengbo Hu, B"orje F. Karlsson, Yehui Tang, Zongqing Lu</p>
</li>
<li>
<p><a href="#link5">Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training</a>
<strong>Authors:</strong> Ruizhe Wang, Yucheng Ding, Xiao Liu, Yaoxiang Wang, Peng Cheng, Baining Guo, Zhengjun Zha, Yeyun Gong</p>
</li>
<li>
<p><a href="#link6">AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models</a>
<strong>Authors:</strong> Xiaoshuang Ji, Zhendong Zhao, Xiaoyan Gu, Xiaojun Chen, Xin Zhao, Zeyao Liu</p>
</li>
<li>
<p><a href="#link7">A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search</a>
<strong>Authors:</strong> Sixuan Wang, Jiao Yin, Jinli Cao, Mingjian Tang, Yong-Feng Ge</p>
</li>
<li>
<p><a href="#link8">Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered</a>
<strong>Authors:</strong> Jason Jabbour, Dong-Ki Kim, Max Smith, Jay Patrikar, Radhika Ghosal, Youhui Wang, Ali Agha, Vijay Janapa Reddi, Shayegan Omidshafiei</p>
</li>
<li>
<p><a href="#link9">From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill</a>
<strong>Authors:</strong> Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, Jung Ho Ahn</p>
</li>
<li>
<p><a href="#link10">Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization</a>
<strong>Authors:</strong> Jason Bohne, Pawel Polak, David Rosenberg, Brian Bloniarz, Gary Kazantsev</p>
</li>
<li>
<p><a href="#link11">Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models</a>
<strong>Authors:</strong> Yinglun Zhu, Jiancheng Zhang, Fuzhi Tang</p>
</li>
<li>
<p><a href="#link12">Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift</a>
<strong>Authors:</strong> Tianyu Bell Pan, Damon L. Woodard</p>
</li>
<li>
<p><a href="#link13">FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning</a>
<strong>Authors:</strong> Shuangyan Deng, Haizhou Peng, Jiachen Xu, Rui Mao, Ciprian Doru Giurc\u{a}neanu, Jiamou Liu</p>
</li>
<li>
<p><a href="#link14">LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics</a>
<strong>Authors:</strong> Chongyu Fan, Changsheng Wang, Yancheng Huang, Soumyadeep Pal, Sijia Liu</p>
</li>
<li>
<p><a href="#link15">Approximate Domain Unlearning for Vision-Language Models</a>
<strong>Authors:</strong> Kodai Kawamura, Yuta Goto, Rintaro Yanagi, Hirokatsu Kataoka, Go Irie</p>
</li>
<li>
<p><a href="#link16">MeSH: Memory-as-State-Highways for Recursive Transformers</a>
<strong>Authors:</strong> Chengting Yu, Xiaobo Shu, Yadao Wang, Yizhen Zhang, Haoyi Wu, Jiaang Li, Rujiao Long, Ziheng Chen, Yuchi Xu, Wenbo Su, Bo Zheng</p>
</li>
<li>
<p><a href="#link17">USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</a>
<strong>Authors:</strong> Junwen Gu, Zhiheng wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu</p>
</li>
<li>
<p><a href="#link18">LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning</a>
<strong>Authors:</strong> Yuhan Sun, Zhiwei Huang, Wanqing Cui, Shaopan Xiong, Yazhi Guo, Meiguang Jin, Junfeng Ma</p>
</li>
<li>
<p><a href="#link19">How to Teach Large Multimodal Models New Skills</a>
<strong>Authors:</strong> Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem</p>
</li>
<li>
<p><a href="#link20">IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction</a>
<strong>Authors:</strong> Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie</p>
</li>
<li>
<p><a href="#link21">MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis</a>
<strong>Authors:</strong> Qinghua Liu, Sam Heshmati, Zheda Mai, Zubin Abraham, John Paparrizos, Liu Ren</p>
</li>
<li>
<p><a href="#link22">Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling</a>
<strong>Authors:</strong> Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery</p>
</li>
<li>
<p><a href="#link23">Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation</a>
<strong>Authors:</strong> Mingyang Sun, Jiude Wei, Qichen He, Donglin Wang, Cewu Lu, Jianhua Sun</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.08396" rel="nofollow">FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-flylora-boosting-task-decoupling-and-parameter-efficiency-via-implicit-rank-wise-mixture-of-experts-" class="anchor" aria-label="Permalink: 0. FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts" href="#0-flylora-boosting-task-decoupling-and-parameter-efficiency-via-implicit-rank-wise-mixture-of-experts-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08396
<strong>Authors:</strong> Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</p>
<p><strong>Abstract:</strong> arXiv:2510.08396v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at <a href="https://github.com/gfyddha/FlyLoRA">https://github.com/gfyddha/FlyLoRA</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.08492" rel="nofollow">Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-better-together-leveraging-unpaired-multimodal-data-for-stronger-unimodal-models-" class="anchor" aria-label="Permalink: 1. Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models" href="#1-better-together-leveraging-unpaired-multimodal-data-for-stronger-unimodal-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08492
<strong>Authors:</strong> Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka, Phillip Isola</p>
<p><strong>Abstract:</strong> arXiv:2510.08492v1 Announce Type: new  Abstract: Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: <a href="https://unpaired-multimodal.github.io/" rel="nofollow">https://unpaired-multimodal.github.io/</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.07985" rel="nofollow">Fewer Weights, More Problems: A Practical Attack on LLM Pruning</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-fewer-weights-more-problems-a-practical-attack-on-llm-pruning-" class="anchor" aria-label="Permalink: 2. Fewer Weights, More Problems: A Practical Attack on LLM Pruning" href="#2-fewer-weights-more-problems-a-practical-attack-on-llm-pruning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07985
<strong>Authors:</strong> Kazuki Egashira, Robin Staab, Thibaud Gloaguen, Mark Vero, Martin Vechev</p>
<p><strong>Abstract:</strong> arXiv:2510.07985v1 Announce Type: new  Abstract: Model pruning, i.e., removing a subset of model weights, has become a prominent approach to reducing the memory footprint of large language models (LLMs) during inference. Notably, popular inference engines, such as vLLM, enable users to conveniently prune downloaded models before they are deployed. While the utility and efficiency of pruning methods have improved significantly, the security implications of pruning remain underexplored. In this work, for the first time, we show that modern LLM pruning methods can be maliciously exploited. In particular, an adversary can construct a model that appears benign yet, once pruned, exhibits malicious behaviors. Our method is based on the idea that the adversary can compute a proxy metric that estimates how likely each parameter is to be pruned. With this information, the adversary can first inject a malicious behavior into those parameters that are unlikely to be pruned. Then, they can repair the model by using parameters that are likely to be pruned, effectively canceling out the injected behavior in the unpruned model. We demonstrate the severity of our attack through extensive evaluation on five models; after any of the pruning in vLLM are applied (Magnitude, Wanda, and SparseGPT), it consistently exhibits strong malicious behaviors in a diverse set of attack scenarios (success rates of up to $95.7%$ for jailbreak, $98.7%$ for benign instruction refusal, and $99.5%$ for targeted content injection). Our results reveal a critical deployment-time security gap and underscore the urgent need for stronger security awareness in model compression.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.08146" rel="nofollow">Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-think-just-enough-sequence-level-entropy-as-a-confidence-signal-for-llm-reasoning-" class="anchor" aria-label="Permalink: 3. Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning" href="#3-think-just-enough-sequence-level-entropy-as-a-confidence-signal-for-llm-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08146
<strong>Authors:</strong> Aman Sharma, Paras Chopra</p>
<p><strong>Abstract:</strong> arXiv:2510.08146v1 Announce Type: new  Abstract: We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they've gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.07882" rel="nofollow">Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-towards-proprioception-aware-embodied-planning-for-dual-arm-humanoid-robots-" class="anchor" aria-label="Permalink: 4. Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots" href="#4-towards-proprioception-aware-embodied-planning-for-dual-arm-humanoid-robots-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07882
<strong>Authors:</strong> Boyu Li, Siyuan He, Hang Xu, Haoqi Yuan, Yu Zang, Liwei Hu, Junpeng Yue, Zhenxiong Jiang, Pengbo Hu, B"orje F. Karlsson, Yehui Tang, Zongqing Lu</p>
<p><strong>Abstract:</strong> arXiv:2510.07882v1 Announce Type: new  Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated the ability to serve as high-level planners, enabling robots to follow complex human instructions. However, their effectiveness, especially in long-horizon tasks involving dual-arm humanoid robots, remains limited. This limitation arises from two main challenges: (i) the absence of simulation platforms that systematically support task evaluation and data collection for humanoid robots, and (ii) the insufficient embodiment awareness of current MLLMs, which hinders reasoning about dual-arm selection logic and body positions during planning. To address these issues, we present DualTHOR, a new dual-arm humanoid simulator, with continuous transition and a contingency mechanism. Building on this platform, we propose Proprio-MLLM, a model that enhances embodiment awareness by incorporating proprioceptive information with motion-based position embedding and a cross-spatial encoder. Experiments show that, while existing MLLMs struggle in this environment, Proprio-MLLM achieves an average improvement of 19.75% in planning performance. Our work provides both an essential simulation platform and an effective model to advance embodied intelligence in humanoid robotics. The code is available at <a href="https://anonymous.4open.science/r/DualTHOR-5F3B" rel="nofollow">https://anonymous.4open.science/r/DualTHOR-5F3B</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.08008" rel="nofollow">Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-recycling-pretrained-checkpoints-orthogonal-growth-of-mixture-of-experts-for-efficient-large-language-model-pre-training-" class="anchor" aria-label="Permalink: 5. Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training" href="#5-recycling-pretrained-checkpoints-orthogonal-growth-of-mixture-of-experts-for-efficient-large-language-model-pre-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08008
<strong>Authors:</strong> Ruizhe Wang, Yucheng Ding, Xiao Liu, Yaoxiang Wang, Peng Cheng, Baining Guo, Zhengjun Zha, Yeyun Gong</p>
<p><strong>Abstract:</strong> arXiv:2510.08008v1 Announce Type: new  Abstract: The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerous computational costs have been invested in existing well-trained checkpoints, but many of them remain underutilized due to engineering constraints or limited model capacity. To efficiently reuse this "sunk" cost, we propose to recycle pretrained checkpoints by expanding their parameter counts and continuing training. We propose orthogonal growth method well-suited for converged Mixture-of-Experts model: interpositional layer copying for depth growth and expert duplication with injected noise for width growth. To determine the optimal timing for such growth across checkpoints sequences, we perform comprehensive scaling experiments revealing that the final accuracy has a strong positive correlation with the amount of sunk cost, indicating that greater prior investment leads to better performance. We scale our approach to models with 70B parameters and over 1T training tokens, achieving 10.66% accuracy gain over training from scratch under the same additional compute budget. Our checkpoint recycling approach establishes a foundation for economically efficient large language model pretraining.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.08034" rel="nofollow">AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-ailora-function-aware-asymmetric-initialization-for-low-rank-adaptation-of-large-language-models-" class="anchor" aria-label="Permalink: 6. AILoRA: Function-Aware Asymmetric Initialization for Low-Rank Adaptation of Large Language Models" href="#6-ailora-function-aware-asymmetric-initialization-for-low-rank-adaptation-of-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08034
<strong>Authors:</strong> Xiaoshuang Ji, Zhendong Zhao, Xiaoyan Gu, Xiaojun Chen, Xin Zhao, Zeyao Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.08034v1 Announce Type: new  Abstract: Parameter-efficient finetuning (PEFT) aims to mitigate the substantial computational and memory overhead involved in adapting large-scale pretrained models to diverse downstream tasks. Among numerous PEFT strategies, Low-Rank Adaptation (LoRA) has emerged as one of the most widely adopted approaches due to its robust empirical performance and low implementation complexity. In practical deployment, LoRA is typically applied to the $W^Q$ and $W^V$ projection matrices of self-attention modules, enabling an effective trade-off between model performance and parameter efficiency. While LoRA has achieved considerable empirical success, it still encounters challenges such as suboptimal performance and slow convergence. To address these limitations, we introduce \textbf{AILoRA}, a novel parameter-efficient method that incorporates function-aware asymmetric low-rank priors. Our empirical analysis reveals that the projection matrices $W^Q$ and $W^V$ in the self-attention mechanism exhibit distinct parameter characteristics, stemming from their functional differences. Specifically, $W^Q$ captures task-specific semantic space knowledge essential for attention distributions computation, making its parameters highly sensitive to downstream task variations. In contrast, $W^V$ encodes token-level feature representations that tend to remain stable across tasks and layers. Leveraging these insights, AILoRA performs a function-aware initialization by injecting the principal components of $W^Q$ to retain task-adaptive capacity, and the minor components of $W^V$ to preserve generalizable feature representations. This asymmetric initialization strategy enables LoRA modules to better capture the specialized roles of attention parameters, thereby enhancing both finetuning performance and convergence efficiency.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.07325" rel="nofollow">A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-a-modality-aware-cooperative-co-evolutionary-framework-for-multimodal-graph-neural-architecture-search-" class="anchor" aria-label="Permalink: 7. A Modality-Aware Cooperative Co-Evolutionary Framework for Multimodal Graph Neural Architecture Search" href="#7-a-modality-aware-cooperative-co-evolutionary-framework-for-multimodal-graph-neural-architecture-search-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07325
<strong>Authors:</strong> Sixuan Wang, Jiao Yin, Jinli Cao, Mingjian Tang, Yong-Feng Ge</p>
<p><strong>Abstract:</strong> arXiv:2510.07325v1 Announce Type: new  Abstract: Co-exploitation attacks on software vulnerabilities pose severe risks to enterprises, a threat that can be mitigated by analyzing heterogeneous and multimodal vulnerability data. Multimodal graph neural networks (MGNNs) are well-suited to integrate complementary signals across modalities, thereby improving attack-prediction accuracy. However, designing an effective MGNN architecture is challenging because it requires coordinating modality-specific components at each layer, which is infeasible through manual tuning. Genetic algorithm (GA)-based graph neural architecture search (GNAS) provides a natural solution, yet existing methods are confined to single modalities and overlook modality heterogeneity. To address this limitation, we propose a modality-aware cooperative co-evolutionary algorithm for multimodal graph neural architecture search, termed MACC-MGNAS. First, we develop a modality-aware cooperative co-evolution (MACC) framework under a divide-and-conquer paradigm: a coordinator partitions a global chromosome population into modality-specific gene groups, local workers evolve them independently, and the coordinator reassembles chromosomes for joint evaluation. This framework effectively captures modality heterogeneity ignored by single-modality GNAS. Second, we introduce a modality-aware dual-track surrogate (MADTS) method to reduce evaluation cost and accelerate local gene evolution. Third, we design a similarity-based population diversity indicator (SPDI) strategy to adaptively balance exploration and exploitation, thereby accelerating convergence and avoiding local optima. On a standard vulnerabilities co-exploitation (VulCE) dataset, MACC-MGNAS achieves an F1-score of 81.67% within only 3 GPU-hours, outperforming the state-of-the-art competitor by 8.7% F1 while reducing computation cost by 27%.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.08464" rel="nofollow">Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-dont-run-with-scissors-pruning-breaks-vla-models-but-they-can-be-recovered-" class="anchor" aria-label="Permalink: 8. Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered" href="#8-dont-run-with-scissors-pruning-breaks-vla-models-but-they-can-be-recovered-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08464
<strong>Authors:</strong> Jason Jabbour, Dong-Ki Kim, Max Smith, Jay Patrikar, Radhika Ghosal, Youhui Wang, Ali Agha, Vijay Janapa Reddi, Shayegan Omidshafiei</p>
<p><strong>Abstract:</strong> arXiv:2510.08464v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but remain challenging to deploy on resource-limited hardware. Pruning has enabled efficient compression of large language models (LLMs), yet it is largely understudied in robotics. Surprisingly, we observe that pruning VLA models leads to drastic degradation and increased safety violations. We introduce GLUESTICK, a post-pruning recovery method that restores much of the original model's functionality while retaining sparsity benefits. Our method performs a one-time interpolation between the dense and pruned models in weight-space to compute a corrective term. This correction is used during inference by each pruned layer to recover lost capabilities with minimal overhead. GLUESTICK requires no additional training, is agnostic to the pruning algorithm, and introduces a single hyperparameter that controls the tradeoff between efficiency and accuracy. Across diverse VLA architectures and tasks in manipulation and navigation, GLUESTICK achieves competitive memory efficiency while substantially recovering success rates and reducing safety violations. Additional material can be found at: <a href="https://gluestick-vla.github.io/" rel="nofollow">https://gluestick-vla.github.io/</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.08055" rel="nofollow">From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-from-tokens-to-layers-redefining-stall-free-scheduling-for-llm-serving-with-layered-prefill-" class="anchor" aria-label="Permalink: 9. From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill" href="#9-from-tokens-to-layers-redefining-stall-free-scheduling-for-llm-serving-with-layered-prefill-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08055
<strong>Authors:</strong> Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, Jung Ho Ahn</p>
<p><strong>Abstract:</strong> arXiv:2510.08055v1 Announce Type: new  Abstract: Large Language Model (LLM) inference in production must meet stringent service-level objectives for both time-to-first-token (TTFT) and time-between-token (TBT) while maximizing throughput under fixed compute, memory, and interconnect budgets. Modern serving systems adopt stall-free scheduling techniques such as chunked prefill, which splits long prompt processing along the token dimension and interleaves prefill with ongoing decode iterations. While effective at stabilizing TBT, chunked prefill incurs substantial overhead in Mixture-of-Experts (MoE) models: redundant expert weight loads increase memory traffic by up to 39% and inflate energy consumption. We propose layered prefill, a new scheduling paradigm that treats transformer layer groups as the primary scheduling unit. By vertically partitioning the model into contiguous layer groups and interleaving prefill and decode across the groups, layered prefill sustains stall-free decoding while eliminating chunk-induced MoE weight reloads. It reduces off-chip bandwidth demand, lowering TTFT by up to 70%, End-to-End latency by 41% and per-token energy by up to 22%. Evaluations show that layered prefill consistently improves the TTFT--TBT Pareto frontier over chunked prefill, reducing expert-load traffic and energy cost while maintaining stall-free decoding. Overall, shifting the scheduling axis from tokens to layers unlocks a new operating regime for high-efficiency, energy-aware LLM serving in co-located environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.08256" rel="nofollow">Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-mix--and-moe-dpo-a-variational-inference-approach-to-direct-preference-optimization-" class="anchor" aria-label="Permalink: 10. Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization" href="#10-mix--and-moe-dpo-a-variational-inference-approach-to-direct-preference-optimization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08256
<strong>Authors:</strong> Jason Bohne, Pawel Polak, David Rosenberg, Brian Bloniarz, Gary Kazantsev</p>
<p><strong>Abstract:</strong> arXiv:2510.08256v1 Announce Type: new  Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.07632" rel="nofollow">Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-test-time-matching-unlocking-compositional-reasoning-in-multimodal-models-" class="anchor" aria-label="Permalink: 11. Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models" href="#11-test-time-matching-unlocking-compositional-reasoning-in-multimodal-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07632
<strong>Authors:</strong> Yinglun Zhu, Jiancheng Zhang, Fuzhi Tang</p>
<p><strong>Abstract:</strong> arXiv:2510.07632v1 Announce Type: new  Abstract: Frontier AI models have achieved remarkable progress, yet recent studies suggest they struggle with compositional reasoning, often performing at or below random chance on established benchmarks. We revisit this problem and show that widely used evaluation metrics systematically underestimate model capability. To address this, we introduce a group matching score that better exploits group structure and reveals substantial hidden capability in both contrastive vision-language models (VLMs) and multimodal large language models (MLLMs). Moreover, simply overfitting to the induced group matchings at test time transfers this hidden capability into higher scores under standard evaluation metrics, closing much of the reported gap. This adjustment enables SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first result surpassing estimated human performance on Winoground.   Building on this insight, we propose Test-Time Matching (TTM), an iterative, self-improving algorithm that further bootstraps model performance without any external supervision. TTM delivers additional, non-trivial improvements: for example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a new state of the art. Importantly, TTM remains broadly effective even on benchmarks without metric-induced effects or group structures, achieving relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16 dataset variants spanning diverse setups, our experiments demonstrate that TTM consistently improves model performance and advances the frontier of compositional reasoning.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.07509" rel="nofollow">Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-efficient-generalization-via-multimodal-co-training-under-data-scarcity-and-distribution-shift-" class="anchor" aria-label="Permalink: 12. Efficient Generalization via Multimodal Co-Training under Data Scarcity and Distribution Shift" href="#12-efficient-generalization-via-multimodal-co-training-under-data-scarcity-and-distribution-shift-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07509
<strong>Authors:</strong> Tianyu Bell Pan, Damon L. Woodard</p>
<p><strong>Abstract:</strong> arXiv:2510.07509v1 Announce Type: new  Abstract: This paper explores a multimodal co-training framework designed to enhance model generalization in situations where labeled data is limited and distribution shifts occur. We thoroughly examine the theoretical foundations of this framework, deriving conditions under which the use of unlabeled data and the promotion of agreement between classifiers for different modalities lead to significant improvements in generalization. We also present a convergence analysis that confirms the effectiveness of iterative co-training in reducing classification errors. In addition, we establish a novel generalization bound that, for the first time in a multimodal co-training context, decomposes and quantifies the distinct advantages gained from leveraging unlabeled multimodal data, promoting inter-view agreement, and maintaining conditional view independence. Our findings highlight the practical benefits of multimodal co-training as a structured approach to developing data-efficient and robust AI systems that can effectively generalize in dynamic, real-world environments. The theoretical foundations are examined in dialogue with, and in advance of, established co-training principles.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.07852" rel="nofollow">FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-finmr-a-knowledge-intensive-multimodal-benchmark-for-advanced-financial-reasoning-" class="anchor" aria-label="Permalink: 13. FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning" href="#13-finmr-a-knowledge-intensive-multimodal-benchmark-for-advanced-financial-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07852
<strong>Authors:</strong> Shuangyan Deng, Haizhou Peng, Jiachen Xu, Rui Mao, Ciprian Doru Giurc\u{a}neanu, Jiamou Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.07852v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) have made substantial progress in recent years. However, their rigorous evaluation within specialized domains like finance is hindered by the absence of datasets characterized by professional-level knowledge intensity, detailed annotations, and advanced reasoning complexity. To address this critical gap, we introduce FinMR, a high-quality, knowledge-intensive multimodal dataset explicitly designed to evaluate expert-level financial reasoning capabilities at a professional analyst's standard. FinMR comprises over 3,200 meticulously curated and expertly annotated question-answer pairs across 15 diverse financial topics, ensuring broad domain diversity and integrating sophisticated mathematical reasoning, advanced financial knowledge, and nuanced visual interpretation tasks across multiple image types. Through comprehensive benchmarking with leading closed-source and open-source MLLMs, we highlight significant performance disparities between these models and professional financial analysts, uncovering key areas for model advancement, such as precise image analysis, accurate application of complex financial formulas, and deeper contextual financial understanding. By providing richly varied visual content and thorough explanatory annotations, FinMR establishes itself as an essential benchmark tool for assessing and advancing multimodal financial reasoning toward professional analyst-level competence.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.07626" rel="nofollow">LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-llm-unlearning-under-the-microscope-a-full-stack-view-on-methods-and-metrics-" class="anchor" aria-label="Permalink: 14. LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics" href="#14-llm-unlearning-under-the-microscope-a-full-stack-view-on-methods-and-metrics-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07626
<strong>Authors:</strong> Chongyu Fan, Changsheng Wang, Yancheng Huang, Soumyadeep Pal, Sijia Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.07626v1 Announce Type: new  Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while preserving useful model capabilities. Despite rapid progress over the past two years, research in LLM unlearning remains fragmented, with limited clarity on what constitutes effective unlearning and how it should be rigorously evaluated. In this work, we present a principled taxonomy of twelve recent stateful unlearning methods, grouped into three methodological families: divergence-driven optimization, representation misalignment, and rejection-based targeted unlearning. Building on this taxonomy, we revisit the evaluation of unlearning effectiveness (UE), utility retention (UT), and robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that current evaluations, dominated by multiple-choice question (MCQ) accuracy, offer only a narrow perspective, often overstating success while overlooking the model's actual generation behavior. To address this gap, we introduce open question-answering (Open-QA) metrics that better capture generative performance and reveal the inherent UE-UT tradeoff across method families. Furthermore, we demonstrate that robustness requires finer-grained analysis: for example, vulnerabilities differ substantially between in-domain relearning and out-of-domain fine-tuning, even though both fall under model-level attacks. Through this study, we hope to deliver a full-stack revisit of LLM unlearning and actionable guidance for designing and evaluating future methods.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2510.08132" rel="nofollow">Approximate Domain Unlearning for Vision-Language Models</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-approximate-domain-unlearning-for-vision-language-models-" class="anchor" aria-label="Permalink: 15. Approximate Domain Unlearning for Vision-Language Models" href="#15-approximate-domain-unlearning-for-vision-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08132
<strong>Authors:</strong> Kodai Kawamura, Yuta Goto, Rintaro Yanagi, Hirokatsu Kataoka, Go Irie</p>
<p><strong>Abstract:</strong> arXiv:2510.08132v1 Announce Type: new  Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize real cars while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments show that our approach outperforms baselines built upon VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code: <a href="https://kodaikawamura.github.io/Domain_Unlearning/" rel="nofollow">https://kodaikawamura.github.io/Domain_Unlearning/</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2510.07739" rel="nofollow">MeSH: Memory-as-State-Highways for Recursive Transformers</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-mesh-memory-as-state-highways-for-recursive-transformers-" class="anchor" aria-label="Permalink: 16. MeSH: Memory-as-State-Highways for Recursive Transformers" href="#16-mesh-memory-as-state-highways-for-recursive-transformers-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07739
<strong>Authors:</strong> Chengting Yu, Xiaobo Shu, Yadao Wang, Yizhen Zhang, Haoyi Wu, Jiaang Li, Rujiao Long, Ziheng Chen, Yuchi Xu, Wenbo Su, Bo Zheng</p>
<p><strong>Abstract:</strong> arXiv:2510.07739v1 Announce Type: new  Abstract: Recursive transformers reuse parameters and iterate over hidden states multiple times, decoupling compute depth from parameter depth. However, under matched compute, recursive models with fewer parameters often lag behind non-recursive counterparts. By probing hidden states, we trace this performance gap to two primary bottlenecks: undifferentiated computation, where the core is forced to adopt a similar computational pattern at every iteration, and information overload, where long-lived and transient information must coexist in a single hidden state. To address the issues, we introduce a Memory-as-State-Highways (MeSH) scheme, which externalizes state management into an explicit memory buffer and employs lightweight routers to dynamically diversify computation across iterations. Probing visualizations confirm that MeSH successfully resolves the pathologies by inducing functional specialization across iterations. On the Pythia suite (160M-1.4B), MeSH-enhanced recursive transformers consistently improve over recursive baselines and outperforms its larger non-recursive counterpart at the 1.4B scale, improving average downstream accuracy by +1.06% with 33% fewer non-embedding parameters. Our analysis establishes MeSH as a scalable and principled architecture for building stronger recursive models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2510.07869" rel="nofollow">USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-usim-and-u0-a-vision-language-action-dataset-and-model-for-general-underwater-robots-" class="anchor" aria-label="Permalink: 17. USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots" href="#17-usim-and-u0-a-vision-language-action-dataset-and-model-for-general-underwater-robots-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07869
<strong>Authors:</strong> Junwen Gu, Zhiheng wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu</p>
<p><strong>Abstract:</strong> arXiv:2510.07869v1 Announce Type: new  Abstract: Underwater environments present unique challenges for robotic operation, including complex hydrodynamics, limited visibility, and constrained communication. Although data-driven approaches have advanced embodied intelligence in terrestrial robots and enabled task-specific autonomous underwater robots, developing underwater intelligence capable of autonomously performing multiple tasks remains highly challenging, as large-scale, high-quality underwater datasets are still scarce. To address these limitations, we introduce USIM, a simulation-based multi-task Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over 561K frames from 1,852 trajectories, totaling approximately 15.6 hours of BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from visual navigation to mobile manipulation. Building upon this dataset, we propose U0, a VLA model for general underwater robots, which integrates binocular vision and other sensor modalities through multimodal fusion, and further incorporates a convolution-attention-based perception focus enhancement module (CAP) to improve spatial understanding and mobile manipulation. Across tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking, the framework achieves a success rate of 80%, while in challenging mobile manipulation tasks, it reduces the distance to the target by 21.2% compared with baseline methods, demonstrating its effectiveness. USIM and U0 show that VLA models can be effectively applied to underwater robotic applications, providing a foundation for scalable dataset construction, improved task autonomy, and the practical realization of intelligent general underwater robots.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2510.07685" rel="nofollow">LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-livethinking-enabling-real-time-efficient-reasoning-for-ai-powered-livestreaming-via-reinforcement-learning-" class="anchor" aria-label="Permalink: 18. LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning" href="#18-livethinking-enabling-real-time-efficient-reasoning-for-ai-powered-livestreaming-via-reinforcement-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07685
<strong>Authors:</strong> Yuhan Sun, Zhiwei Huang, Wanqing Cui, Shaopan Xiong, Yazhi Guo, Meiguang Jin, Junfeng Ma</p>
<p><strong>Abstract:</strong> arXiv:2510.07685v1 Announce Type: new  Abstract: In AI-powered e-commerce livestreaming, digital avatars require real-time responses to drive engagement, a task for which high-latency Large Reasoning Models (LRMs) are ill-suited. We introduce LiveThinking, a practical two-stage optimization framework to bridge this gap. First, we address computational cost by distilling a 670B teacher LRM into a lightweight 30B Mixture-of-Experts (MoE) model (3B active) using Rejection Sampling Fine-Tuning (RFT). This reduces deployment overhead but preserves the teacher's verbose reasoning, causing latency. To solve this, our second stage employs reinforcement learning with Group Relative Policy Optimization (GRPO) to compress the model's reasoning path, guided by a multi-objective reward function balancing correctness, helpfulness, and brevity. LiveThinking achieves a 30-fold reduction in computational cost, enabling sub-second latency. In real-world application on Taobao Live, it improved response correctness by 3.3% and helpfulness by 21.8%. Tested by hundreds of thousands of viewers, our system led to a statistically significant increase in Gross Merchandise Volume (GMV), demonstrating its effectiveness in enhancing user experience and commercial performance in live, interactive settings.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2510.08564" rel="nofollow">How to Teach Large Multimodal Models New Skills</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-how-to-teach-large-multimodal-models-new-skills-" class="anchor" aria-label="Permalink: 19. How to Teach Large Multimodal Models New Skills" href="#19-how-to-teach-large-multimodal-models-new-skills-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08564
<strong>Authors:</strong> Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem</p>
<p><strong>Abstract:</strong> arXiv:2510.08564v1 Announce Type: new  Abstract: How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent "forgetting" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&amp;Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at <a href="https://github.com/jessemelpolio/LMM_CL">https://github.com/jessemelpolio/LMM_CL</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2510.07778" rel="nofollow">IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-intentionvla-generalizable-and-efficient-embodied-intention-reasoning-for-human-robot-interaction-" class="anchor" aria-label="Permalink: 20. IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction" href="#20-intentionvla-generalizable-and-efficient-embodied-intention-reasoning-for-human-robot-interaction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07778
<strong>Authors:</strong> Yandu Chen, Kefan Gu, Yuqing Wen, Yucheng Zhao, Tiancai Wang, Liqiang Nie</p>
<p><strong>Abstract:</strong> arXiv:2510.07778v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions. Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions. To overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage, IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling fast inference under indirect instructions. Experimental results show that IntentionVLA substantially outperforms $\pi_0$, achieving 18% higher success rates with direct instructions and 28% higher than ECoT under intention instructions. On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40% success rate. These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2510.07513" rel="nofollow">MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-mllm4ts-leveraging-vision-and-multimodal-language-models-for-general-time-series-analysis-" class="anchor" aria-label="Permalink: 21. MLLM4TS: Leveraging Vision and Multimodal Language Models for General Time-Series Analysis" href="#21-mllm4ts-leveraging-vision-and-multimodal-language-models-for-general-time-series-analysis-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07513
<strong>Authors:</strong> Qinghua Liu, Sam Heshmati, Zheda Mai, Zubin Abraham, John Paparrizos, Liu Ren</p>
<p><strong>Abstract:</strong> arXiv:2510.07513v1 Announce Type: new  Abstract: Effective analysis of time series data presents significant challenges due to the complex temporal dependencies and cross-channel interactions in multivariate data. Inspired by the way human analysts visually inspect time series to uncover hidden patterns, we ask: can incorporating visual representations enhance automated time-series analysis? Recent advances in multimodal large language models have demonstrated impressive generalization and visual understanding capability, yet their application to time series remains constrained by the modality gap between continuous numerical data and discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel framework that leverages multimodal large language models for general time-series analysis by integrating a dedicated vision branch. Each time-series channel is rendered as a horizontally stacked color-coded line plot in one composite image to capture spatial dependencies across channels, and a temporal-aware visual patch alignment strategy then aligns visual patches with their corresponding time segments. MLLM4TS fuses fine-grained temporal details from the numerical data with global contextual information derived from the visual representation, providing a unified foundation for multimodal time-series analysis. Extensive experiments on standard benchmarks demonstrate the effectiveness of MLLM4TS across both predictive tasks (e.g., classification) and generative tasks (e.g., anomaly detection and forecasting). These results underscore the potential of integrating visual modalities with pretrained language models to achieve robust and generalizable time-series analysis.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2510.08470" rel="nofollow">Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-looking-to-learn-token-wise-dynamic-gating-for-low-resource-vision-language-modelling-" class="anchor" aria-label="Permalink: 22. Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling" href="#22-looking-to-learn-token-wise-dynamic-gating-for-low-resource-vision-language-modelling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08470
<strong>Authors:</strong> Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery</p>
<p><strong>Abstract:</strong> arXiv:2510.08470v1 Announce Type: new  Abstract: Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2510.07975" rel="nofollow">Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-executable-analytic-concepts-as-the-missing-link-between-vlm-insight-and-precise-manipulation-" class="anchor" aria-label="Permalink: 23. Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation" href="#23-executable-analytic-concepts-as-the-missing-link-between-vlm-insight-and-precise-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.07975
<strong>Authors:</strong> Mingyang Sun, Jiude Wei, Qichen He, Donglin Wang, Cewu Lu, Jianhua Sun</p>
<p><strong>Abstract:</strong> arXiv:2510.07975v1 Announce Type: new  Abstract: Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this "semantic-to-physical" gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)-mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) and Mixture of Experts (MoE) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research introduces a novel multimodal large model that integrates text, language, and vision modalities. The goal is to advance the performance and generalization capabilities of the model by establishing a new approach for training and fusing these modalities effectively, rather than focusing on incremental optimizations.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>