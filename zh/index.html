<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">SpecOffload：释放资源受限设备上LLM推理的潜在GPU能力</h2><a id="user-content-specoffload释放资源受限设备上llm推理的潜在gpu能力" class="anchor" aria-label="Permalink: SpecOffload：释放资源受限设备上LLM推理的潜在GPU能力" href="#specoffload释放资源受限设备上llm推理的潜在gpu能力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.10259v1 公告类型：新研究<br>
摘要：在资源受限设备上实现高效的大型语言模型（LLM）推理面临着计算与内存利用率的重大挑战。由于GPU内存有限，现有系统将模型权重卸载到CPU内存中，导致CPU与GPU间产生大量I/O开销。这引发了两大效率瓶颈：（1）GPU核心利用率低下，常因等待数据加载而处于空闲状态；（2）GPU内存对性能影响微弱，减少其容量对整体吞吐量几乎无影响。本文提出SpecOffload——一种将推测式解码嵌入卸载流程的高吞吐量推理引擎。其核心思想是利用闲置GPU资源存储并执行用于推测式解码的草稿模型，从而以近乎零额外成本加速推理。为此，我们精心设计了卸载流水线中目标模型与草稿模型在推测式解码中的交错执行机制，并提出用于管理张量布局及选择最优参数的规划器。相比最佳基线方案，SpecOffload将GPU核心利用率提升4.49倍，推理吞吐量提高2.54倍。代码已开源：<a href="https://github.com/MobiSense/SpecOffload">https://github.com/MobiSense/SpecOffload</a></p>
<p>（注：根据技术文档翻译规范，对以下术语进行统一处理：</p>
<ul>
<li>"offload"译为"卸载"而非"转移"</li>
<li>"speculative decoding"译为"推测式解码"（学界通用译法）</li>
<li>"throughput"译为"吞吐量"</li>
<li>长难句采用拆分策略，如将"Due to limited GPU memory..."独立成句以符合中文表达习惯）</li>
</ul>
<div class="markdown-heading"><h2 class="heading-element">模拟基础模型</h2><a id="user-content-模拟基础模型" class="anchor" aria-label="Permalink: 模拟基础模型" href="#模拟基础模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.09663v1 公告类型：新研究<br>
摘要：模拟内存计算（AIMC）是一种有望突破传统冯·诺依曼架构限制的计算范式，可提升神经网络推理的速度和能效。然而，AIMC也带来了根本性挑战，如计算噪声以及对输入输出量化的严格限制。由于这些约束和误差，现成的LLM在基于AIMC的硬件上运行时无法达到4比特级别的性能表现。虽然此前研究者们主要在小型视觉模型上探索过精度恢复方法，但尚未出现适用于基于万亿级token预训练的LLM的通用解决方案。本研究提出了一种普适且可扩展的方法，能够使LLM鲁棒地适应噪声多、精度低的模拟硬件环境。我们的方法使得包括Phi-3-mini-4k-instruct和Llama-3.2-1B-Instruct在内的前沿模型，在存在模拟噪声和量化约束的情况下，仍能保持与4比特权重、8比特激活基线相当的性能。此外，我们发现该训练方法的副产品是：模拟基础模型可被量化以部署在低精度数字硬件上。最后实验表明，相较于采用4比特权重和8比特静态输入量化的模型，我们的模型还能受益于测试时计算资源扩展，展现出更优的性能扩展特性。这项工作弥合了大容量LLM与高效模拟硬件之间的鸿沟，为能效优化的基础模型开辟了道路。代码已开源：<a href="https://github.com/IBM/analog-foundation-models">https://github.com/IBM/analog-foundation-models</a></p>
<p>（注：$\unicode{x2013}$已替换为中文破折号"——"，技术术语如"AIMC/LLM"保留英文缩写，模型名称保持原格式，超链接保留可点击状态）</p>
<div class="markdown-heading"><h2 class="heading-element">BINGO：一种创新的剪枝机制，用于缩减神经网络规模</h2><a id="user-content-bingo一种创新的剪枝机制用于缩减神经网络规模" class="anchor" aria-label="Permalink: BINGO：一种创新的剪枝机制，用于缩减神经网络规模" href="#bingo一种创新的剪枝机制用于缩减神经网络规模"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.09864v1 公告类型：新研究<br>
摘要：过去十年间，机器学习应用呈现指数级增长。模型复杂度远超以往，规模膨胀至惊人程度，参数数量动辄以百万计。但大型模型成为行业标杆的同时，也带来了高昂的训练与运维成本——单次训练常耗资数百万美元。这种经济负担不仅令企业承压，更将非富裕个体拒于科研创新门外，最终转嫁为消费者为AI服务支付更高溢价。当前主流模型剪枝方法（如迭代幅度剪枝）虽能保持精度，却依赖计算量和环境代价极高的迭代训练流程。为此，我们提出BINGO解决方案。该技术在训练过程中逐次分析神经网络的特定子集，精准评估每个参数对模型准确性的贡献度。训练结束时，BINGO会为每个权重生成重要性评分，实现一次性精准剪枝。相比现有方法，BINGO在降低计算强度的同时完美保持模型精度，为AI发展指明新方向——技术进步不必总伴随模型膨胀。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>