<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 08/20/2025</h1><a id="user-content-personalized-daily-arxiv-papers-08202025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 08/20/2025" href="#personalized-daily-arxiv-papers-08202025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 26</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</a>
<strong>Authors:</strong> Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine</p>
</li>
<li>
<p><a href="#link1">ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents</a>
<strong>Authors:</strong> Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao, Yuxiao Dong, Jie Tang</p>
</li>
<li>
<p><a href="#link2">Approximate Bayesian Inference via Bitstring Representations</a>
<strong>Authors:</strong> Aleksanteri Sladek, Martin Trapp, Arno Solin</p>
</li>
<li>
<p><a href="#link3">X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms</a>
<strong>Authors:</strong> Yueming Yuan, Ahan Gupta, Jianping Li, Sajal Dash, Feiyi Wang, Minjia Zhang</p>
</li>
<li>
<p><a href="#link4">MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search</a>
<strong>Authors:</strong> Jeremy Carleton, Debajoy Mukherjee, Srinivas Shakkottai, Dileep Kalathil</p>
</li>
<li>
<p><a href="#link5">In-Context Decision Making for Optimizing Complex AutoML Pipelines</a>
<strong>Authors:</strong> Amir Rezaei Balef, Katharina Eggensperger</p>
</li>
<li>
<p><a href="#link6">Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management</a>
<strong>Authors:</strong> Tianheng Ling, Vipin Singh, Chao Qian, Felix Biessmann, Gregor Schiele</p>
</li>
<li>
<p><a href="#link7">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</a>
<strong>Authors:</strong> Yihao Lu, Hao Tang</p>
</li>
<li>
<p><a href="#link8">Input Time Scaling</a>
<strong>Authors:</strong> Rapheal Huang (Yuming), Weilong Guo</p>
</li>
<li>
<p><a href="#link9">Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making</a>
<strong>Authors:</strong> Liuxin Bao, Zhihao Peng, Xiaofei Zhou, Runmin Cong, Jiyong Zhang, Yixuan Yuan</p>
</li>
<li>
<p><a href="#link10">Formal Algorithms for Model Efficiency</a>
<strong>Authors:</strong> Naman Tyagi, Srishti Das, Kunal, Vatsal Gupta</p>
</li>
<li>
<p><a href="#link11">One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression</a>
<strong>Authors:</strong> Miko{\l}aj Janusz, Tomasz Wojnar, Yawei Li, Luca Benini, Kamil Adamczewski</p>
</li>
<li>
<p><a href="#link12">QuickMerge++: Fast Token Merging with Autoregressive Prior</a>
<strong>Authors:</strong> Dong Liu, Yanxuan Yu</p>
</li>
<li>
<p><a href="#link13">Dynamic Design of Machine Learning Pipelines via Metalearning</a>
<strong>Authors:</strong> Edesio Alcoba\c{c}a, Andr'e C. P. L. F. de Carvalho</p>
</li>
<li>
<p><a href="#link14">Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation</a>
<strong>Authors:</strong> Zhuoling Li, Xiaoyang Wu, Zhenhua Xu, Hengshuang Zhao</p>
</li>
<li>
<p><a href="#link15">Search-Time Data Contamination</a>
<strong>Authors:</strong> Ziwen Han, Meher Mankikar, Julian Michael, Zifan Wang</p>
</li>
<li>
<p><a href="#link16">GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks</a>
<strong>Authors:</strong> Sergey Salishev, Ian Akhremchik</p>
</li>
<li>
<p><a href="#link17">MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</a>
<strong>Authors:</strong> Yu Li, Zulong Chen, Wenjian Xu, Hong Wen, Yipeng Yu, Man Lung Yiu, Yuyu Yin</p>
</li>
<li>
<p><a href="#link18">SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</a>
<strong>Authors:</strong> Jiayu Fang, Zhiqi Shao, S T Boris Choy, Junbin Gao</p>
</li>
<li>
<p><a href="#link19">Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences</a>
<strong>Authors:</strong> Cheikh Ahmed, Mahdi Mostajabdaveh, Samin Aref, Zirui Zhou</p>
</li>
<li>
<p><a href="#link20">Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models</a>
<strong>Authors:</strong> Zhaokun Chen, Chaopeng Zhang, Xiaohan Li, Wenshuo Wang, Gentiane Venture, Junqiang Xi</p>
</li>
<li>
<p><a href="#link21">Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention</a>
<strong>Authors:</strong> Sarthak Khanna, Armin Berger, David Berghaus, Tobias Deusser, Lorenz Sparrenberg, Rafet Sifa</p>
</li>
<li>
<p><a href="#link22">FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks</a>
<strong>Authors:</strong> Nicol`o Romandini, Cristian Borcea, Rebecca Montanari, Luca Foschini</p>
</li>
<li>
<p><a href="#link23">TASER: Table Agents for Schema-guided Extraction and Recommendation</a>
<strong>Authors:</strong> Nicole Cho, Kirsty Fielding, William Watson, Sumitra Ganesh, Manuela Veloso</p>
</li>
<li>
<p><a href="#link24">Communication-Efficient Federated Learning with Adaptive Number of Participants</a>
<strong>Authors:</strong> Sergey Skorik, Vladislav Dorofeev, Gleb Molodtsov, Aram Avetisyan, Dmitry Bylinkin, Daniil Medyakov, Aleksandr Beznosikov</p>
</li>
<li>
<p><a href="#link25">ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery</a>
<strong>Authors:</strong> Mohammad Izadi, Mehran Safayani</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2508.13446" rel="nofollow">CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-cast-counterfactual-labels-improve-instruction-following-in-vision-language-action-models-" class="anchor" aria-label="Permalink: 0. CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models" href="#0-cast-counterfactual-labels-improve-instruction-following-in-vision-language-action-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13446
<strong>Authors:</strong> Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine</p>
<p><strong>Abstract:</strong> arXiv:2508.13446v1 Announce Type: new  Abstract: Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2508.14040" rel="nofollow">ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-computerrl-scaling-end-to-end-online-reinforcement-learning-for-computer-use-agents-" class="anchor" aria-label="Permalink: 1. ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents" href="#1-computerrl-scaling-end-to-end-online-reinforcement-learning-for-computer-use-agents-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14040
<strong>Authors:</strong> Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao, Yuxiao Dong, Jie Tang</p>
<p><strong>Abstract:</strong> arXiv:2508.14040v1 Announce Type: new  Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks, yet remains challenging due to environmental inefficiency and instability in extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%, demonstrating significant improvements for general agents in desktop automation. The algorithm and framework are adopted in building AutoGLM (Liu et al., 2024a)</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2508.13598" rel="nofollow">Approximate Bayesian Inference via Bitstring Representations</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-approximate-bayesian-inference-via-bitstring-representations-" class="anchor" aria-label="Permalink: 2. Approximate Bayesian Inference via Bitstring Representations" href="#2-approximate-bayesian-inference-via-bitstring-representations-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13598
<strong>Authors:</strong> Aleksanteri Sladek, Martin Trapp, Arno Solin</p>
<p><strong>Abstract:</strong> arXiv:2508.13598v1 Announce Type: new  Abstract: The machine learning community has recently put effort into quantized or low-precision arithmetics to scale large models. This paper proposes performing probabilistic inference in the quantized, discrete parameter space created by these representations, effectively enabling us to learn a continuous distribution using discrete parameters. We consider both 2D densities and quantized neural networks, where we introduce a tractable learning approach using probabilistic circuits. This method offers a scalable solution to manage complex distributions and provides clear insights into model behavior. We validate our approach with various models, demonstrating inference efficiency without sacrificing accuracy. This work advances scalable, interpretable machine learning by utilizing discrete approximations for probabilistic computations.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2508.13337" rel="nofollow">X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-x-moe-enabling-scalable-training-for-emerging-mixture-of-experts-architectures-on-hpc-platforms-" class="anchor" aria-label="Permalink: 3. X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms" href="#3-x-moe-enabling-scalable-training-for-emerging-mixture-of-experts-architectures-on-hpc-platforms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13337
<strong>Authors:</strong> Yueming Yuan, Ahan Gupta, Jianping Li, Sajal Dash, Feiyi Wang, Minjia Zhang</p>
<p><strong>Abstract:</strong> arXiv:2508.13337v1 Announce Type: new  Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as DeepSeek-MoE, deliver strong model quality through fine-grained expert segmentation and large top-k routing. However, their scalability is limited by substantial activation memory overhead and costly all-to-all communication. Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs - perform suboptimally on non-NVIDIA platforms, leaving significant computational potential untapped. In this work, we present X-MoE, a novel MoE training system designed to deliver scalable training performance for next-generation MoE architectures. X-MoE achieves this via several novel techniques, including efficient padding-free MoE training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism with sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer, powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs - 10x larger than the largest trainable model with existing methods under the same hardware budget, while maintaining high training throughput. The source code of X-MoE is available at <a href="https://github.com/Supercomputing-System-AI-Lab/X-MoE">https://github.com/Supercomputing-System-AI-Lab/X-MoE</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2508.13415" rel="nofollow">MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-mavis-multi-objective-alignment-via-value-guided-inference-time-search-" class="anchor" aria-label="Permalink: 4. MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search" href="#4-mavis-multi-objective-alignment-via-value-guided-inference-time-search-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13415
<strong>Authors:</strong> Jeremy Carleton, Debajoy Mukherjee, Srinivas Shakkottai, Dileep Kalathil</p>
<p><strong>Abstract:</strong> arXiv:2508.13415v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly deployed across diverse applications that demand balancing multiple, often conflicting, objectives -- such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific preferences in such multi-objective settings typically requires fine-tuning models for each objective or preference configuration, which is computationally expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via Value-Guided Inference-Time Search -- a lightweight inference-time alignment framework that enables dynamic control over LLM behavior without modifying the base model's weights. MAVIS trains a set of small value models, each corresponding to a distinct objective. At inference time, these value models are combined using user-specified weights to produce a tilting function that adjusts the base model's output distribution toward desired trade-offs. The value models are trained using a simple iterative algorithm that ensures monotonic improvement of the KL-regularized policy. We show empirically that MAVIS outperforms baselines that fine-tune per-objective models and combine them post hoc, and even approaches the performance of the idealized setting where models are fine-tuned for a user's exact preferences.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2508.13657" rel="nofollow">In-Context Decision Making for Optimizing Complex AutoML Pipelines</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-in-context-decision-making-for-optimizing-complex-automl-pipelines-" class="anchor" aria-label="Permalink: 5. In-Context Decision Making for Optimizing Complex AutoML Pipelines" href="#5-in-context-decision-making-for-optimizing-complex-automl-pipelines-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13657
<strong>Authors:</strong> Amir Rezaei Balef, Katharina Eggensperger</p>
<p><strong>Abstract:</strong> arXiv:2508.13657v1 Announce Type: new  Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been fundamental to traditional AutoML systems. However, with the advancements of pre-trained models, modern ML workflows go beyond hyperparameter optimization and often require fine-tuning, ensembling, and other adaptation techniques. While the core challenge of identifying the best-performing model for a downstream task remains, the increasing heterogeneity of ML pipelines demands novel AutoML approaches. This work extends the CASH framework to select and adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to efficiently estimate the posterior distribution of the maximal value via in-context learning. We show how to extend this method to consider varying costs of pulling arms and to use different PFNs to model reward distributions individually per arm. Experimental results on one novel and two existing standard benchmark tasks demonstrate the superior performance of PS-PFN compared to other bandit and AutoML strategies. We make our code and data available at <a href="https://github.com/amirbalef/CASHPlus">https://github.com/amirbalef/CASHPlus</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2508.13905" rel="nofollow">Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-automated-energy-aware-time-series-model-deployment-on-embedded-fpgas-for-resilient-combined-sewer-overflow-management-" class="anchor" aria-label="Permalink: 6. Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management" href="#6-automated-energy-aware-time-series-model-deployment-on-embedded-fpgas-for-resilient-combined-sewer-overflow-management-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13905
<strong>Authors:</strong> Tianheng Ling, Vipin Singh, Chao Qian, Felix Biessmann, Gregor Schiele</p>
<p><strong>Abstract:</strong> arXiv:2508.13905v1 Announce Type: new  Abstract: Extreme weather events, intensified by climate change, increasingly challenge aging combined sewer systems, raising the risk of untreated wastewater overflow. Accurate forecasting of sewer overflow basin filling levels can provide actionable insights for early intervention, helping mitigating uncontrolled discharge. In recent years, AI-based forecasting methods have offered scalable alternatives to traditional physics-based models, but their reliance on cloud computing limits their reliability during communication outages. To address this, we propose an end-to-end forecasting framework that enables energy-efficient inference directly on edge devices. Our solution integrates lightweight Transformer and Long Short-Term Memory (LSTM) models, compressed via integer-only quantization for efficient on-device execution. Moreover, an automated hardware-aware deployment pipeline is used to search for optimal model configurations by jointly minimizing prediction error and energy consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer data, the selected 8-bit Transformer model, trained on 24 hours of historical measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ per inference. In contrast, the optimal 8-bit LSTM model requires significantly less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE 0.0432) and much longer training time. This trade-off highlights the need to align model selection with deployment priorities, favoring LSTM for ultra-low energy consumption or Transformer for higher predictive accuracy. In general, our work enables local, energy-efficient forecasting, contributing to more resilient combined sewer systems. All code can be found in the GitHub Repository (<a href="https://github.com/tianheng-ling/EdgeOverflowForecast">https://github.com/tianheng-ling/EdgeOverflowForecast</a>).</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2508.13901" rel="nofollow">Multimodal Data Storage and Retrieval for Embodied AI: A Survey</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-multimodal-data-storage-and-retrieval-for-embodied-ai-a-survey-" class="anchor" aria-label="Permalink: 7. Multimodal Data Storage and Retrieval for Embodied AI: A Survey" href="#7-multimodal-data-storage-and-retrieval-for-embodied-ai-a-survey-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13901
<strong>Authors:</strong> Yihao Lu, Hao Tang</p>
<p><strong>Abstract:</strong> arXiv:2508.13901v1 Announce Type: new  Abstract: Embodied AI (EAI) agents continuously interact with the physical world, generating vast, heterogeneous multimodal data streams that traditional management systems are ill-equipped to handle. In this survey, we first systematically evaluate five storage architectures (Graph Databases, Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series Databases), focusing on their suitability for addressing EAI's core requirements, including physical grounding, low-latency access, and dynamic scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based Optimization), revealing a fundamental tension between achieving long-term semantic coherence and maintaining real-time responsiveness. Based on this comprehensive analysis, we identify key bottlenecks, spanning from the foundational Physical Grounding Gap to systemic challenges in cross-modal integration, dynamic adaptation, and open-world generalization. Finally, we outline a forward-looking research agenda encompassing physics-aware data models, adaptive storage-retrieval co-optimization, and standardized benchmarking, to guide future research toward principled data management solutions for EAI. Our survey is based on a comprehensive review of more than 180 related studies, providing a rigorous roadmap for designing the robust, high-performance data management frameworks essential for the next generation of autonomous embodied systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2508.13654" rel="nofollow">Input Time Scaling</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-input-time-scaling-" class="anchor" aria-label="Permalink: 8. Input Time Scaling" href="#8-input-time-scaling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13654
<strong>Authors:</strong> Rapheal Huang (Yuming), Weilong Guo</p>
<p><strong>Abstract:</strong> arXiv:2508.13654v1 Announce Type: new  Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data &amp; training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2508.13754" rel="nofollow">Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-expertise-aware-multi-llm-recruitment-and-collaboration-for-medical-decision-making-" class="anchor" aria-label="Permalink: 9. Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making" href="#9-expertise-aware-multi-llm-recruitment-and-collaboration-for-medical-decision-making-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13754
<strong>Authors:</strong> Liuxin Bao, Zhihao Peng, Xiaofei Zhou, Runmin Cong, Jiyong Zhang, Yixuan Yuan</p>
<p><strong>Abstract:</strong> arXiv:2508.13754v1 Announce Type: new  Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial domain-specific expertise to effectively synthesize heterogeneous and complicated clinical information. While recent advancements in Large Language Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited by their parametric knowledge constraints and static training corpora, failing to robustly integrate the clinical information. To address this challenge, we propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC) framework to enhance the accuracy and reliability of MDM systems. It operates in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and adversarial-driven multi-agent collaboration. Specifically, in the first stage, we use a publicly available corpus to construct an LLM expertise table for capturing expertise-specific strengths of multiple LLMs across medical department categories and query difficulty levels. This table enables the subsequent dynamic selection of the optimal LLMs to act as medical expert agents for each medical query during the inference phase. In the second stage, we employ selected agents to generate responses with self-assessed confidence scores, which are then integrated through the confidence fusion and adversarial validation to improve diagnostic reliability. We evaluate our EMRC framework on three public MDM datasets, where the results demonstrate that our EMRC outperforms state-of-the-art single- and multi-LLM methods, achieving superior diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC achieves 74.45% accuracy, representing a 2.69% improvement over the best-performing closed-source model GPT- 4-0613, which demonstrates the effectiveness of our expertise-aware agent recruitment strategy and the agent complementarity in leveraging each LLM's specialized capabilities.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2508.14000" rel="nofollow">Formal Algorithms for Model Efficiency</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-formal-algorithms-for-model-efficiency-" class="anchor" aria-label="Permalink: 10. Formal Algorithms for Model Efficiency" href="#10-formal-algorithms-for-model-efficiency-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14000
<strong>Authors:</strong> Naman Tyagi, Srishti Das, Kunal, Vatsal Gupta</p>
<p><strong>Abstract:</strong> arXiv:2508.14000v1 Announce Type: new  Abstract: We introduce the Knob-Meter-Rule (KMR) framework, a unified formalism for representing and reasoning about model efficiency techniques in deep learning. By abstracting diverse methods, including pruning, quantization, knowledge distillation, and parameter-efficient architectures, into a consistent set of controllable knobs, deterministic rules, and measurable meters, KMR provides a mathematically precise and modular perspective on efficiency optimization. The framework enables systematic composition of multiple techniques, flexible policy-driven application, and iterative budgeted optimization through the Budgeted-KMR algorithm. We demonstrate how well-known efficiency methods can be instantiated as KMR triples and present concise algorithmic templates for each. The framework highlights underlying relationships between methods, facilitates hybrid pipelines, and lays the foundation for future research in automated policy learning, dynamic adaptation, and theoretical analysis of cost-quality trade-offs. Overall, KMR offers both a conceptual and practical tool for unifying and advancing model efficiency research.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2508.13836" rel="nofollow">One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-one-shot-vs-iterative-rethinking-pruning-strategies-for-model-compression-" class="anchor" aria-label="Permalink: 11. One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression" href="#11-one-shot-vs-iterative-rethinking-pruning-strategies-for-model-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13836
<strong>Authors:</strong> Miko{\l}aj Janusz, Tomasz Wojnar, Yawei Li, Luca Benini, Kamil Adamczewski</p>
<p><strong>Abstract:</strong> arXiv:2508.13836v1 Announce Type: new  Abstract: Pruning is a core technique for compressing neural networks to improve computational efficiency. This process is typically approached in two ways: one-shot pruning, which involves a single pass of training and pruning, and iterative pruning, where pruning is performed over multiple cycles for potentially finer network refinement. Although iterative pruning has historically seen broader adoption, this preference is often assumed rather than rigorously tested. Our study presents one of the first systematic and comprehensive comparisons of these methods, providing rigorous definitions, benchmarking both across structured and unstructured settings, and applying different pruning criteria and modalities. We find that each method has specific advantages: one-shot pruning proves more effective at lower pruning ratios, while iterative pruning performs better at higher ratios. Building on these findings, we advocate for patience-based pruning and introduce a hybrid approach that can outperform traditional methods in certain scenarios, providing valuable insights for practitioners selecting a pruning strategy tailored to their goals and constraints. Source code is available at <a href="https://github.com/janumiko/pruning-benchmark">https://github.com/janumiko/pruning-benchmark</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2508.13204" rel="nofollow">QuickMerge++: Fast Token Merging with Autoregressive Prior</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-quickmerge-fast-token-merging-with-autoregressive-prior-" class="anchor" aria-label="Permalink: 12. QuickMerge++: Fast Token Merging with Autoregressive Prior" href="#12-quickmerge-fast-token-merging-with-autoregressive-prior-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13204
<strong>Authors:</strong> Dong Liu, Yanxuan Yu</p>
<p><strong>Abstract:</strong> arXiv:2508.13204v1 Announce Type: new  Abstract: As generative models scale to larger inputs across language, vision, and video domains, the cost of token-level computation has become a key bottleneck. While prior work suggests that only a subset of tokens significantly influence downstream predictions, most token selection methods are static, modality-specific, or incompatible with autoregressive generation. In this paper, we propose QuickMerge, a lightweight token merging framework designed for efficient next-token prediction.   QuickMerge dynamically selects a reduced number of tokens based on attention norm magnitude, guided by an entropy-based budget estimator. To preserve autoregressive compatibility, we introduce a lightweight transformer prior trained over the merged token sequence. By combining semantic salience estimation, flexible token budgets, and AR alignment, QuickMerge enables accurate generation with fewer tokens.   We evaluate QuickMerge across multi-modality domains, demonstrating consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge reduces token counts sustantially while matching as well as exceeding the performance of learned tokenizers and fixed-patch baselines.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2508.13436" rel="nofollow">Dynamic Design of Machine Learning Pipelines via Metalearning</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-dynamic-design-of-machine-learning-pipelines-via-metalearning-" class="anchor" aria-label="Permalink: 13. Dynamic Design of Machine Learning Pipelines via Metalearning" href="#13-dynamic-design-of-machine-learning-pipelines-via-metalearning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13436
<strong>Authors:</strong> Edesio Alcoba\c{c}a, Andr'e C. P. L. F. de Carvalho</p>
<p><strong>Abstract:</strong> arXiv:2508.13436v1 Announce Type: new  Abstract: Automated machine learning (AutoML) has democratized the design of machine learning based systems, by automating model selection, hyperparameter tuning and feature engineering. However, the high computational cost associated with traditional search and optimization strategies, such as Random Search, Particle Swarm Optimization and Bayesian Optimization, remains a significant challenge. Moreover, AutoML systems typically explore a large search space, which can lead to overfitting. This paper introduces a metalearning method for dynamically designing search spaces for AutoML system. The proposed method uses historical metaknowledge to select promising regions of the search space, accelerating the optimization process. According to experiments conducted for this study, the proposed method can reduce runtime by 89% in Random Search and search space by (1.8/13 preprocessor and 4.3/16 classifier), without compromising significant predictive performance. Moreover, the proposed method showed competitive performance when adapted to Auto-Sklearn, reducing its search space. Furthermore, this study encompasses insights into meta-feature selection, meta-model explainability, and the trade-offs inherent in search space reduction strategies.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2508.14042" rel="nofollow">Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-train-once-deploy-anywhere-realize-data-efficient-dynamic-object-manipulation-" class="anchor" aria-label="Permalink: 14. Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation" href="#14-train-once-deploy-anywhere-realize-data-efficient-dynamic-object-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14042
<strong>Authors:</strong> Zhuoling Li, Xiaoyang Wu, Zhenhua Xu, Hengshuang Zhao</p>
<p><strong>Abstract:</strong> arXiv:2508.14042v1 Announce Type: new  Abstract: Realizing generalizable dynamic object manipulation is important for enhancing manufacturing efficiency, as it eliminates specialized engineering for various scenarios. To this end, imitation learning emerges as a promising paradigm, leveraging expert demonstrations to teach a policy manipulation skills. Although the generalization of an imitation learning policy can be improved by increasing demonstrations, demonstration collection is labor-intensive. To address this problem, this paper investigates whether strong generalization in dynamic object manipulation is achievable with only a few demonstrations. Specifically, we develop an entropy-based theoretical framework to quantify the optimization of imitation learning. Based on this framework, we propose a system named Generalizable Entropy-based Manipulation (GEM). Extensive experiments in simulated and real tasks demonstrate that GEM can generalize across diverse environment backgrounds, robot embodiments, motion dynamics, and object geometries. Notably, GEM has been deployed in a real canteen for tableware collection. Without any in-scene demonstration, it achieves a success rate of over 97% across more than 10,000 operations.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2508.13180" rel="nofollow">Search-Time Data Contamination</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-search-time-data-contamination-" class="anchor" aria-label="Permalink: 15. Search-Time Data Contamination" href="#15-search-time-data-contamination-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13180
<strong>Authors:</strong> Ziwen Han, Meher Mankikar, Julian Michael, Zifan Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.13180v1 Announce Type: new  Abstract: Data contamination refers to the leakage of evaluation data into model training data, resulting in overfitting to supposedly held-out test sets and compromising test validity. We identify an analogous issue, search-time contamination (STC), in evaluating search-based LLM agents which use tools to gather information from online sources when answering user queries. STC occurs when the retrieval step surfaces a source containing the test question (or a near-duplicate) alongside its answer, enabling agents to copy rather than genuinely infer or reason, undermining benchmark integrity. We find that HuggingFace, an online platform hosting evaluation datasets, appears among retrieved sources in search based agent logs. Consequently, agents often explicitly acknowledge discovering question answer pairs from HuggingFace within their reasoning chains. On three commonly used capability benchmarks: Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for approximately 3% of questions, search-based agents directly find the datasets with ground truth labels on HuggingFace. When millions of evaluation queries target the same benchmark, even small, repeated leaks can accelerate the benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace is blocked, we observe a drop in accuracy on the contaminated subset of approximately 15%. We further show through ablation experiments that publicly accessible evaluation datasets on HuggingFace may not be the sole source of STC. To this end, we conclude by proposing best practices for benchmark design and result reporting to address this novel form of leakage and ensure trustworthy evaluation of search-based LLM agents. To facilitate the auditing of evaluation results, we also publicly release the complete logs from our experiments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2508.14004" rel="nofollow">GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-gdnsq-gradual-differentiable-noise-scale-quantization-for-low-bit-neural-networks-" class="anchor" aria-label="Permalink: 16. GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks" href="#16-gdnsq-gradual-differentiable-noise-scale-quantization-for-low-bit-neural-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14004
<strong>Authors:</strong> Sergey Salishev, Ian Akhremchik</p>
<p><strong>Abstract:</strong> arXiv:2508.14004v1 Announce Type: new  Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where rounding in each layer reduces capacity as bit-width shrinks; the floating-point (FP) checkpoint sets the maximum input rate. We track capacity dynamics as the average bit-width decreases and identify resulting quantization bottlenecks by casting fine-tuning as a smooth, constrained optimization problem. Our approach employs a fully differentiable Straight-Through Estimator (STE) with learnable bit-width, noise scale and clamp bounds, and enforces a target bit-width via an exterior-point penalty; mild metric smoothing (via distillation) stabilizes training. Despite its simplicity, the method attains competitive accuracy down to the extreme W1A1 setting while retaining the efficiency of STE.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2508.13676" rel="nofollow">MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-mhsnetan-moe-based-hierarchical-semantic-representation-network-for-accurate-duplicate-resume-detection-with-large-language-model-" class="anchor" aria-label="Permalink: 17. MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model" href="#17-mhsnetan-moe-based-hierarchical-semantic-representation-network-for-accurate-duplicate-resume-detection-with-large-language-model-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13676
<strong>Authors:</strong> Yu Li, Zulong Chen, Wenjian Xu, Hong Wen, Yipeng Yu, Man Lung Yiu, Yuyu Yin</p>
<p><strong>Abstract:</strong> arXiv:2508.13676v1 Announce Type: new  Abstract: To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2508.13435" rel="nofollow">SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-svdformer-direction-aware-spectral-graph-embedding-learning-via-svd-and-transformer-" class="anchor" aria-label="Permalink: 18. SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer" href="#18-svdformer-direction-aware-spectral-graph-embedding-learning-via-svd-and-transformer-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13435
<strong>Authors:</strong> Jiayu Fang, Zhiqi Shao, S T Boris Choy, Junbin Gao</p>
<p><strong>Abstract:</strong> arXiv:2508.13435v1 Announce Type: new  Abstract: Directed graphs are widely used to model asymmetric relationships in real-world systems. However, existing directed graph neural networks often struggle to jointly capture directional semantics and global structural patterns due to their isotropic aggregation mechanisms and localized filtering mechanisms. To address this limitation, this paper proposes SVDformer, a novel framework that synergizes SVD and Transformer architecture for direction-aware graph representation learning. SVDformer first refines singular value embeddings through multi-head self-attention, adaptively enhancing critical spectral components while suppressing high-frequency noise. This enables learnable low-pass/high-pass graph filtering without requiring spectral kernels. Furthermore, by treating singular vectors as directional projection bases and singular values as scaling factors, SVDformer uses the Transformer to model multi-scale interactions between incoming/outgoing edge patterns through attention weights, thereby explicitly preserving edge directionality during feature propagation. Extensive experiments on six directed graph benchmarks demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and direction-aware baselines on node classification tasks, establishing a new paradigm for learning representations on directed graphs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2508.13437" rel="nofollow">Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-discrete-optimization-of-min-max-violation-and-its-applications-across-computational-sciences-" class="anchor" aria-label="Permalink: 19. Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences" href="#19-discrete-optimization-of-min-max-violation-and-its-applications-across-computational-sciences-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13437
<strong>Authors:</strong> Cheikh Ahmed, Mahdi Mostajabdaveh, Samin Aref, Zirui Zhou</p>
<p><strong>Abstract:</strong> arXiv:2508.13437v1 Announce Type: new  Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization problem which seeks an assignment of discrete values to variables that minimizes the largest constraint violation. This context-free mathematical formulation is applicable to a wide range of use cases that have worst-case performance requirements. After defining the DMMV problem mathematically, we explore its properties to establish a foundational understanding. To tackle DMMV instance sizes of practical relevance, we develop a GPU-accelerated heuristic that takes advantage of the mathematical properties of DMMV for speeding up the solution process. We demonstrate the versatile applicability of our heuristic by solving three optimization problems as use cases: (1) post-training quantization of language models, (2) discrete tomography, and (3) Finite Impulse Response (FIR) filter design. In quantization without outlier separation, our heuristic achieves 14% improvement on average over existing methods. In discrete tomography, it reduces reconstruction error by 16% under uniform noise and accelerates computations by a factor of 6 on GPU. For FIR filter design, it nearly achieves 50% ripple reduction compared to using the commercial integer optimization solver, Gurobi. Our comparative results point to the benefits of studying DMMV as a context-free optimization problem and the advantages that our proposed heuristic offers on three distinct problems. Our GPU-accelerated heuristic will be made open-source to further stimulate research on DMMV and its other applications. The code is available at <a href="https://anonymous.4open.science/r/AMVM-5F3E/" rel="nofollow">https://anonymous.4open.science/r/AMVM-5F3E/</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2508.13881" rel="nofollow">Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-driving-style-recognition-like-an-expert-using-semantic-privileged-information-from-large-language-models-" class="anchor" aria-label="Permalink: 20. Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models" href="#20-driving-style-recognition-like-an-expert-using-semantic-privileged-information-from-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13881
<strong>Authors:</strong> Zhaokun Chen, Chaopeng Zhang, Xiaohan Li, Wenshuo Wang, Gentiane Venture, Junqiang Xi</p>
<p><strong>Abstract:</strong> arXiv:2508.13881v1 Announce Type: new  Abstract: Existing driving style recognition systems largely depend on low-level sensor-derived features for training, neglecting the rich semantic reasoning capability inherent to human experts. This discrepancy results in a fundamental misalignment between algorithmic classifications and expert judgments. To bridge this gap, we propose a novel framework that integrates Semantic Privileged Information (SPI) derived from large language models (LLMs) to align recognition outcomes with human-interpretable reasoning. First, we introduce DriBehavGPT, an interactive LLM-based module that generates natural-language descriptions of driving behaviors. These descriptions are then encoded into machine learning-compatible representations via text embedding and dimensionality reduction. Finally, we incorporate them as privileged information into Support Vector Machine Plus (SVM+) for training, enabling the model to approximate human-like interpretation patterns. Experiments across diverse real-world driving scenarios demonstrate that our SPI-enhanced framework outperforms conventional methods, achieving F1-score improvements of 7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively used during training, while inference relies solely on sensor data, ensuring computational efficiency without sacrificing performance. These results highlight the pivotal role of semantic behavioral representations in improving recognition accuracy while advancing interpretable, human-centric driving systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2508.13327" rel="nofollow">Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-towards-unified-multimodal-financial-forecasting-integrating-sentiment-embeddings-and-market-indicators-via-cross-modal-attention-" class="anchor" aria-label="Permalink: 21. Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention" href="#21-towards-unified-multimodal-financial-forecasting-integrating-sentiment-embeddings-and-market-indicators-via-cross-modal-attention-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13327
<strong>Authors:</strong> Sarthak Khanna, Armin Berger, David Berghaus, Tobias Deusser, Lorenz Sparrenberg, Rafet Sifa</p>
<p><strong>Abstract:</strong> arXiv:2508.13327v1 Announce Type: new  Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal framework integrating numerical market indicators with sentiment-enriched news embeddings to improve daily stock-movement prediction. By combining numerical &amp; textual embeddings via feature concatenation and cross-modal attention, our unified pipeline addresses limitations of isolated analyses. Backtesting shows STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion strategies and model configurations offers evidence-based guidance for scalable multimodal financial forecasting. Source code is available on GitHub</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2508.13853" rel="nofollow">FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-fedup-efficient-pruning-based-federated-unlearning-for-model-poisoning-attacks-" class="anchor" aria-label="Permalink: 22. FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks" href="#22-fedup-efficient-pruning-based-federated-unlearning-for-model-poisoning-attacks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13853
<strong>Authors:</strong> Nicol`o Romandini, Cristian Borcea, Rebecca Montanari, Luca Foschini</p>
<p><strong>Abstract:</strong> arXiv:2508.13853v1 Announce Type: new  Abstract: Federated Learning (FL) can be vulnerable to attacks, such as model poisoning, where adversaries send malicious local weights to compromise the global model. Federated Unlearning (FU) is emerging as a solution to address such vulnerabilities by selectively removing the influence of detected malicious contributors on the global model without complete retraining. However, unlike typical FU scenarios where clients are trusted and cooperative, applying FU with malicious and possibly colluding clients is challenging because their collaboration in unlearning their data cannot be assumed. This work presents FedUP, a lightweight FU algorithm designed to efficiently mitigate malicious clients' influence by pruning specific connections within the attacked model. Our approach achieves efficiency by relying only on clients' weights from the last training round before unlearning to identify which connections to inhibit. Isolating malicious influence is non-trivial due to overlapping updates from benign and malicious clients. FedUP addresses this by carefully selecting and zeroing the highest magnitude weights that diverge the most between the latest updates from benign and malicious clients while preserving benign information. FedUP is evaluated under a strong adversarial threat model, where up to 50%-1 of the clients could be malicious and have full knowledge of the aggregation process. We demonstrate the effectiveness, robustness, and efficiency of our solution through experiments across IID and Non-IID data, under label-flipping and backdoor attacks, and by comparing it with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces malicious influence, lowering accuracy on malicious data to match that of a model retrained from scratch while preserving performance on benign data. FedUP achieves effective unlearning while consistently being faster and saving storage compared to the SOTA.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2508.13404" rel="nofollow">TASER: Table Agents for Schema-guided Extraction and Recommendation</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-taser-table-agents-for-schema-guided-extraction-and-recommendation-" class="anchor" aria-label="Permalink: 23. TASER: Table Agents for Schema-guided Extraction and Recommendation" href="#23-taser-table-agents-for-schema-guided-extraction-and-recommendation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13404
<strong>Authors:</strong> Nicole Cho, Kirsty Fielding, William Watson, Sumitra Ganesh, Manuela Veloso</p>
<p><strong>Abstract:</strong> arXiv:2508.13404v1 Announce Type: new  Abstract: Real-world financial documents report essential information about an entity's financial holdings that can span millions of different financial instrument types. Yet, these details are often buried in messy, multi-page, fragmented tables - for example, 99.4% of the tables in our dataset have no bounding boxes with the maximum number of rows amounting to 426 per table across 44 pages. To tackle these unique challenges from real-world tables, we present a continuously learning, agentic table extraction system, TASER (Table Agents for Schema-guided Extraction and Recommendation) that extracts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Our table agents execute on table detection, classification, extraction, and recommendations by leveraging an initial schema. Then, our Recommender Agent reviews the outputs, recommends schema revisions, and decides on the final recommendations, enabling TASER to outperform existing table detection models such as Table Transformer by 10.1%. Within this continuous learning process, we highlight that larger batch sizes result in a 104.3% increase in schema recommendations that are actionable and utilized, resulting in a 9.8% increase in extracted holdings - highlighting the importance of a continuous learning process. To train TASER, we have manually labeled 22,584 pages (28,150,449 tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of the first real financial table datasets. We release our dataset TASERTab to enable the research community to access real-world financial tables and outputs. Our results highlight the promise of agentic, schema-guided extraction systems for robust understanding of real-world financial tables.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2508.13803" rel="nofollow">Communication-Efficient Federated Learning with Adaptive Number of Participants</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-communication-efficient-federated-learning-with-adaptive-number-of-participants-" class="anchor" aria-label="Permalink: 24. Communication-Efficient Federated Learning with Adaptive Number of Participants" href="#24-communication-efficient-federated-learning-with-adaptive-number-of-participants-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.13803
<strong>Authors:</strong> Sergey Skorik, Vladislav Dorofeev, Gleb Molodtsov, Aram Avetisyan, Dmitry Bylinkin, Daniil Medyakov, Aleksandr Beznosikov</p>
<p><strong>Abstract:</strong> arXiv:2508.13803v1 Announce Type: new  Abstract: Rapid scaling of deep learning models has enabled performance gains across domains, yet it introduced several challenges. Federated Learning (FL) has emerged as a promising framework to address these concerns by enabling decentralized training. Nevertheless, communication efficiency remains a key bottleneck in FL, particularly under heterogeneous and dynamic client participation. Existing methods, such as FedAvg and FedProx, or other approaches, including client selection strategies, attempt to mitigate communication costs. However, the problem of choosing the number of clients in a training round remains extremely underexplored. We introduce Intelligent Selection of Participants (ISP), an adaptive mechanism that dynamically determines the optimal number of clients per round to enhance communication efficiency without compromising model accuracy. We validate the effectiveness of ISP across diverse setups, including vision transformers, real-world ECG classification, and training with gradient compression. Our results show consistent communication savings of up to 30% without losing the final quality. Applying ISP to different real-world ECG classification setups highlighted the selection of the number of clients as a separate task of federated learning.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">25. <a href="https://arxiv.org/abs/2508.14005" rel="nofollow">ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery</a> <a id="user-content-link25"></a>
</h2><a id="user-content-25-asdformer-a-transformer-with-mixtures-of-pooling-classifier-experts-for-robust-autism-diagnosis-and-biomarker-discovery-" class="anchor" aria-label="Permalink: 25. ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery" href="#25-asdformer-a-transformer-with-mixtures-of-pooling-classifier-experts-for-robust-autism-diagnosis-and-biomarker-discovery-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14005
<strong>Authors:</strong> Mohammad Izadi, Mehran Safayani</p>
<p><strong>Abstract:</strong> arXiv:2508.14005v1 Announce Type: new  Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Token-Level Expert Selection and Activation Calibration for Efficient Mixture-of-Experts Models
Relevant: To enhance the efficiency and performance of Mixture-of-Experts (MoE) models, this research direction investigates token-level expert selection strategies, sparse activation patterns, and calibration set optimization to improve routing decisions while minimizing computational overhead.
Key Focus Areas: Dynamic Token-to-Expert Assignment: Analyzing how input tokens are routed to experts, including top-k gating, noisy top-k, and learnable routing policies.
Activation Sparsity &amp; Load Balancing: Studying expert utilization and methods (e.g., auxiliary loss, expert choice routing) to prevent underused or overloaded experts.
Calibration for Robust Routing: Optimizing validation set selection and fine-tuning strategies to adapt routing behavior for downstream tasks.
Efficiency-Accuracy Trade-offs: Evaluating how different expert selection mechanisms impact model size, inference speed, and task performance.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model.</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>