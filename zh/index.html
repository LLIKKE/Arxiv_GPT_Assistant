<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">eMoE：任务感知的内存高效专家混合（MoE）模型推理</h2><a id="user-content-emoe任务感知的内存高效专家混合moe模型推理" class="anchor" aria-label="Permalink: eMoE：任务感知的内存高效专家混合（MoE）模型推理" href="#emoe任务感知的内存高效专家混合moe模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.06823v1 公告类型：新研究  摘要：近年来，专家混合模型（Mixture-of-Experts, MoE）作为一种有效方法，以亚线性计算成本提升了深度神经网络（DNN）的能力。然而，将所有专家存储在GPU上会导致显著的内存开销，增加了基于MoE推理的货币成本。为此，我们提出了eMoE，一个针对基于MoE的大型语言模型（LLMs）的内存高效推理系统，该系统基于实验测量中的观察结果。eMoE通过预测并仅加载基于专家路由中重复模式所需的专家来减少内存使用。为了在保持准确性的同时减少加载延迟，我们发现对后续提示使用相同的专家对困惑度影响极小，因此eMoE每隔几个提示才调用一次专家预测器，而非每个提示都调用。此外，它跳过对路由准确性不太敏感的任务的预测。最后，eMoE具备任务感知调度功能，通过考虑服务水平目标（SLOs）、任务特定输出长度和专家加载延迟，最小化推理延迟。实验结果显示，与现有系统相比，eMoE在保持准确性的同时，内存消耗最多减少80%，推理延迟最多降低17%。它还支持处理长度增加40倍的提示，批量大小增加4.5倍，并实现1.5倍的吞吐量提升。</p>
<div class="markdown-heading"><h2 class="heading-element">任务向量量化：实现内存高效模型合并</h2><a id="user-content-任务向量量化实现内存高效模型合并" class="anchor" aria-label="Permalink: 任务向量量化：实现内存高效模型合并" href="#任务向量量化实现内存高效模型合并"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.06921v1 公告类型：新研究<br>
摘要：模型合并通过结合任务特定的微调检查点，实现了高效的多任务模型。然而，存储多个任务特定的检查点需要大量内存，限制了可扩展性，并使得模型合并仅适用于较大的模型和多样化的任务。在本文中，我们提出量化任务向量（即预训练和微调检查点之间的差异），而不是量化微调检查点。我们观察到任务向量表现出较窄的权重范围，使得在现有的任务向量合并框架内实现低精度量化（低至4位）成为可能。为了进一步缓解超低比特精度（例如2位）下的量化误差，我们引入了残差任务向量量化，将任务向量分解为基础向量和偏移分量。我们根据量化敏感性分配比特，确保精度的同时，在内存预算内最小化误差。图像分类和密集预测的实验表明，我们的方法在使用全精度检查点所需内存的仅8%的情况下，保持或提升了模型合并的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">ResMoE：通过残差恢复实现专家混合大语言模型的高效空间压缩</h2><a id="user-content-resmoe通过残差恢复实现专家混合大语言模型的高效空间压缩" class="anchor" aria-label="Permalink: ResMoE：通过残差恢复实现专家混合大语言模型的高效空间压缩" href="#resmoe通过残差恢复实现专家混合大语言模型的高效空间压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.06881v1 公告类型：新研究  摘要：混合专家（Mixture-of-Experts, MoE）Transformer作为多个杰出语言模型的核心架构，通过仅为每个输入令牌激活部分模型参数来利用稀疏性。这种稀疏结构虽然允许恒定的时间成本，却导致了空间效率的低下：在推理过程中，我们仍需加载所有模型参数。我们提出了ResMoE，一个创新的MoE近似框架，它利用Wasserstein重心提取一个共同专家（重心专家），并近似该重心专家与原始专家之间的残差。ResMoE以一种无需重新训练且数据无关的方式，一次性提升了大规模MoE Transformers推理的空间效率，同时保持最小的精度损失，从而为更广泛地访问大型语言模型铺平了道路。我们通过在Switch Transformer、Mixtral和DeepSeekMoE模型上的广泛实验证明了ResMoE的有效性。结果表明，ResMoE能将专家参数数量减少多达75%，同时保持相当的性能。代码可在<a href="https://github.com/iDEA-iSAIL-Lab-UIUC/ResMoE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/iDEA-iSAIL-Lab-UIUC/ResMoE获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">精简注意力机制：无需牺牲准确性，将上下文记忆减半——K缓存是您实现多头注意力（MHA）所需的一切</h2><a id="user-content-精简注意力机制无需牺牲准确性将上下文记忆减半k缓存是您实现多头注意力mha所需的一切" class="anchor" aria-label="Permalink: 精简注意力机制：无需牺牲准确性，将上下文记忆减半——K缓存是您实现多头注意力（MHA）所需的一切" href="#精简注意力机制无需牺牲准确性将上下文记忆减半k缓存是您实现多头注意力mha所需的一切"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.05840v1 公告类型：新研究  摘要：Slim注意力机制通过将上下文记忆体大小缩减2倍，针对采用多头注意力（MHA）的Transformer模型，能够在大上下文窗口情况下加速推理过程，最高可达2倍。Slim注意力是对标准注意力机制的一种精确、数学上等价的实现，因此不会影响模型的准确性。换言之，Slim注意力无损地将上下文记忆体压缩了2倍。对于编码器-解码器结构的Transformer模型，上下文记忆体大小还能进一步缩减：以Whisper模型为例，Slim注意力将上下文记忆体减少了8倍，在批量大小为64的情况下，令牌生成速度可提升5倍。在极少数情况下，当MHA投影维度大于嵌入维度时，例如T5-11B模型，内存占用可减少32倍。代码及更多Transformer技巧请访问<a href="https://github.com/OpenMachine-ai/transformer-tricks%EF%BC%8C%E5%85%B3%E4%BA%8E%E6%9C%AC%E8%AE%BA%E6%96%87%E7%9A%84%E8%A7%86%E9%A2%91%E8%AE%B2%E8%A7%A3%E8%AF%B7%E8%A7%81https://www.youtube.com/watch?v=uVtk3B6YO4Y%E3%80%82">https://github.com/OpenMachine-ai/transformer-tricks，关于本论文的视频讲解请见https://www.youtube.com/watch?v=uVtk3B6YO4Y。</a></p>
<div class="markdown-heading"><h2 class="heading-element">迈向更高量化精度：一种层次敏感的方法</h2><a id="user-content-迈向更高量化精度一种层次敏感的方法" class="anchor" aria-label="Permalink: 迈向更高量化精度：一种层次敏感的方法" href="#迈向更高量化精度一种层次敏感的方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.06518v1 公告类型：新研究 摘要：大型视觉与语言模型在自然语言理解、问题解决、逻辑推理及知识检索等任务中展现了令人瞩目的人类智能水平。然而，训练和部署这些模型需要庞大的计算资源，这对其广泛应用及深入研究构成了显著障碍。为缓解这一挑战，多种模型压缩技术应运而生，旨在降低计算需求。尽管如此，现有方法多采用统一的量化配置，未能充分考虑大型神经网络模型中不同层在量化难度上的差异。本文针对这一问题，利用层敏感特性，如激活敏感度和权重分布的峰度，来识别难以精确量化的层，并分配额外的内存预算。所提出的方法分别命名为SensiBoost和KurtBoost，在量化精度上展现了显著提升，在LLama模型上，相较于基线，仅增加2%的内存预算即可实现高达9%的困惑度降低。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>