<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">最后一个注意事项：更好地利用第一个注意事项进行高效的Transformer培训</h2><a id="user-content-最后一个注意事项更好地利用第一个注意事项进行高效的transformer培训" class="anchor" aria-label="Permalink: 最后一个注意事项：更好地利用第一个注意事项进行高效的Transformer培训" href="#最后一个注意事项更好地利用第一个注意事项进行高效的transformer培训"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14614v1宣布类型：新摘要：随着训练数十亿规模的变压器变得越来越普遍，使用多个分布式图形处理器以及并行训练方法已成为一种标准实践。然而，现有的Transformer设计面临着巨大的通信负担，尤其是在张量并行主义（TP）中，其中每个块的MHA-MLP连接都需要全精简通信。通过我们的调查，我们表明MHA-MLP连接可以被绕过以提高效率，而第一层的注意力输出可以作为被绕过连接的替代信号。受观察结果的启发，我们提出了FAL（First Attention Last），这是一种高效的Transformer架构，将第一个MHA输出重定向到后续层的MLP输入，消除了每个块的MHA-MLP连接。这消除了全精简通信，并允许在单个图形处理器上并行执行MHA和MLP。我们还引入了FAL+，它将标准化的第一注意力输出添加到后续层的MHA输出中，以增强MLP输入以提高模型质量。我们的评估表明，与基线GPT相比，FAL将多图形处理器训练时间减少了高达44%，将单图形处理器吞吐量提高了高达1.18倍，并实现了更好的困惑性。FAL+在不增加训练时间的情况下实现了更低的困惑度。</p>
<div class="markdown-heading"><h2 class="heading-element">专业知识不需要垄断：视觉-语言-动作学习的专业化专家混合体</h2><a id="user-content-专业知识不需要垄断视觉-语言-动作学习的专业化专家混合体" class="anchor" aria-label="Permalink: 专业知识不需要垄断：视觉-语言-动作学习的专业化专家混合体" href="#专业知识不需要垄断视觉-语言-动作学习的专业化专家混合体"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14300v1宣布类型：新摘要：视觉-语言-动作（VLA）模型正在经历快速发展，并在机器人操纵任务中展示出有前途的能力。然而，扩展VLA模型带来了几个关键挑战：（1）从头开始训练新的VLA模型需要大量的计算资源和大量的数据集。鉴于目前机器人数据的稀缺性，在扩展过程中充分利用经过良好预训练的VLA模型权重变得特别有价值。(2)实时控制需要仔细平衡模型容量与计算效率。为了应对这些挑战，我们提出了AdaMoE，这是一种专家混合（MoE）架构，它从密集VLA模型继承预训练的权重，并通过将前向层替换为稀疏激活的MoE层来扩展动作专家。AdaMoE采用一种脱钩技术，通过与传统路由器一起工作的独立规模适配器将专家选择与专家加权分开。这使得可以根据任务相关性来选择专家，同时以独立控制的权重做出贡献，从而允许协作地利用专家，而不是赢家通吃的动态。我们的方法表明，专业知识不需要垄断。相反，通过协作利用专家，我们可以在保持计算效率的同时实现卓越的性能。AdaMoE在关键基准上的表现始终优于基线模型，在LIBERO上实现了1.8%的性能增长，在RoboTwin上实现了9.3%的性能增长。最重要的是，现实实验中21.5%的大幅改进验证了其对于机器人操纵任务的实际有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">LLM压缩中的免费午餐：重新审视修剪后的重新训练</h2><a id="user-content-llm压缩中的免费午餐重新审视修剪后的重新训练" class="anchor" aria-label="Permalink: LLM压缩中的免费午餐：重新审视修剪后的重新训练" href="#llm压缩中的免费午餐重新审视修剪后的重新训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14444v1公告类型：新摘要：虽然神经网络修剪通常需要重新训练模型以恢复修剪引起的性能下降，但最先进的大型语言模型（LLM）修剪方法反而解决了一小组校准数据上的逐层掩码选择和重建问题，以避免完全重新训练，因为它被认为在计算上对LLM不可行。孤立地重建单个矩阵具有良好的特性，例如目标的凸性和与完全再训练相比显着降低的内存需求。然而，在实践中，重建通常以较粗的粒度来实现，例如，针对其密集激活来重建整个Transformer块，而不是单个矩阵。在这项工作中，我们研究了修剪后重建或重新训练剩余权重时的关键设计选择。我们对最先进的GPT架构进行了广泛的计算研究，并报告了几项令人惊讶的发现，挑战了有关修剪后再培训的常见直觉。特别是，我们观察到一个免费的午餐场景：在每个Transformer块内分别重建注意力和MLP组件几乎是最有效的资源，但实现了最好的困惑。最重要的是，尽管只需要一小部分内存，但这种帕累托最优设置比完全重新训练获得了更好的性能。此外，我们证明，当重建步骤得到正确执行时，简单有效的修剪标准（例如Wanda）可以优于更复杂的方法，凸显了其重要性。我们的研究结果挑战了应不惜一切代价避免再培训的说法，并为LLM修剪后绩效恢复提供了重要见解。</p>
<div class="markdown-heading"><h2 class="heading-element">BitNet蒸馏</h2><a id="user-content-bitnet蒸馏" class="anchor" aria-label="Permalink: BitNet蒸馏" href="#bitnet蒸馏"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.13998v1宣布类型：新摘要：在本文中，我们介绍了BitNet Distillation（BitDistill），这是一种轻量级管道，可以微调现成的全精度LLM（例如，Qwen）转换为1.58位精度（即，针对特定下游任务的三进制权重{-1，0，1}），以最小的计算成本实现强大的任务特定性能。具体来说，BitDistill融合了三项关键技术：BitNet中引入的SubLN模块;基于MiniLM的多头注意力蒸馏;以及持续预训练，这是缓解微调全精度和1.58位LLM之间性能差距的关键热身步骤。特定任务。实验结果表明，BitDistill在模型大小方面的性能与全精度对应模型相当，同时可以节省高达10倍的内存，并将处理器的推理速度提高2.65倍。代码可在<a href="https://github.com/microsoft/BitNet%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/microsoft/BitNet上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">重新审视决策层的模式失衡</h2><a id="user-content-重新审视决策层的模式失衡" class="anchor" aria-label="Permalink: 重新审视决策层的模式失衡" href="#重新审视决策层的模式失衡"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14411v1宣布类型：新摘要：多模式学习集成来自不同模式的信息以增强模型性能，但它经常遭受模式失衡的影响，即在联合优化期间，主导模式掩盖了较弱的模式。本文揭示了这种不平衡不仅发生在表示学习过程中，而且在决策层也显着体现。视听数据集（CREMAD和Kinetic-Sounds）的实验表明，即使经过广泛的预训练和平衡优化，模型仍然对某些模式（例如音频）表现出系统性偏差。进一步的分析表明，这种偏差源于特征空间和决策权重分布的内在差异，而不仅仅是优化动态。我们认为，在融合阶段聚合未校准的模式输出会导致决策层加权有偏差，从而阻碍较弱的模式有效贡献。为了解决这个问题，我们建议未来的多模式系统应该更多地关注在决策层整合自适应权重分配机制，根据每个模式的能力实现相对平衡。</p>
<div class="markdown-heading"><h2 class="heading-element">在知识感知子空间中净化任务载体以实现模型合并</h2><a id="user-content-在知识感知子空间中净化任务载体以实现模型合并" class="anchor" aria-label="Permalink: 在知识感知子空间中净化任务载体以实现模型合并" href="#在知识感知子空间中净化任务载体以实现模型合并"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14697v1宣布类型：新摘要：模型合并旨在将单独微调模型中的特定任务能力集成到单个模型中，无需额外培训。在最近的模型合并方法中，任务载体已成为一个基本的构建模块，因为它可以封装微调的剩余信息。然而，由于任务载体中与任务无关的冗余引起的冲突，合并后的模型经常会出现显着的性能下降。现有的通过随机删除参数空间中的元素来克服冗余的努力涉及随机性并且缺乏知识意识。为了解决这些挑战，在这项研究中，我们提出了在知识感知子空间中净化Task Vectors（PAVE）。具体来说，我们从每个任务中采样一些训练示例，并将它们输入到相应的微调模型中，以在线性层之前获取协方差矩阵。然后，我们执行面向上下文的奇异值分解，这会强调与目标知识最相关的权重分量。因此，我们可以将微调后的模型权重拆分为知识感知子空间中的任务相关分量和冗余分量，并通过修剪冗余分量来净化任务载体。为了在模型中引入公平的修剪工作，我们通过优化规范化的激活修剪错误进一步引入了频谱等级分配策略。通过我们的方法作为即插即用方案进行的任务载体净化适用于各种基于任务载体的合并方法，以提高其性能。在实验中，我们展示了PAVE在一系列不同的合并方法、任务和模型架构中的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">迈向低级别权重的可逆模型合并</h2><a id="user-content-迈向低级别权重的可逆模型合并" class="anchor" aria-label="Permalink: 迈向低级别权重的可逆模型合并" href="#迈向低级别权重的可逆模型合并"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14163v1宣布类型：新摘要：模型合并旨在将多个微调模型组合到一组权重中，该权重在所有源任务中表现良好。虽然之前的工作表明，合并可以逼近每个任务的各个微调模型的性能，但它在很大程度上忽略了模型通过低等级自适应（LoRA）或训练后奇异值分解（MVD）被压缩为低等级表示的场景。我们首先证明，将传统的合并方法应用于低排名权重会导致合并模型中的性能严重下降。受此现象的启发，我们提出了一种根本不同的方法：我们不是将所有适配器折叠成一组权重，而是构建一个紧凑的基础（例如，相当于持有两个或更多模型），可以通过线性组合从其中恢复原始任务特定模型。这将合并重新定义为生成能够重建的模型空间，而不是生成单个合并模型。至关重要的是，这使我们能够在需要时“恢复”到每个单独的模型，认识到没有合并的模型能够始终优于专门负责其任务的模型。基于这一见解，我们介绍了我们的方法--可逆模型合并（RMM），这是一种高效、无数据且灵活的方法，它提供了一种封闭式解决方案，用于选择线性组合的模型权重和特定任务系数的最佳基础。跨不同数据集和模型规模的广泛实验表明，RMM始终优于现有的合并方法，大幅保留了低等级压缩模型的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">FedHFT：与异类边缘客户端进行高效联合微调</h2><a id="user-content-fedhft与异类边缘客户端进行高效联合微调" class="anchor" aria-label="Permalink: FedHFT：与异类边缘客户端进行高效联合微调" href="#fedhft与异类边缘客户端进行高效联合微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14054v1宣布类型：新摘要：微调预训练的大型语言模型（LLM）已成为下游任务和特定领域数据集上的个性化自然语言理解（NLU）应用程序的常见做法。然而，存在两个主要挑战：（i）由于专有数据机密性或隐私要求，用于微调的数据有限和/或异类，以及（ii）参与客户端（例如边缘设备）可用的计算资源不同。本文介绍了FedHFT --一个高效且个性化的联邦微调框架，可应对这两个挑战。首先，我们引入了混合的掩蔽适配器来处理参与客户端之间的资源多样性，从而实现分布式环境中跨多个客户端预训练的语言模型的高性能协作微调，同时将专有数据保持在本地。其次，我们引入了一种双层优化方法来处理基于掩蔽个性化和客户端集群的非iid数据分发。大量实验表明，与代表性的异类联邦学习方法相比，在数据和资源异类下，各种自然语言理解任务的性能和效率得到了显着提高。</p>
<div class="markdown-heading"><h2 class="heading-element">用于下一个兴趣点预测的认知一致时空大语言模型</h2><a id="user-content-用于下一个兴趣点预测的认知一致时空大语言模型" class="anchor" aria-label="Permalink: 用于下一个兴趣点预测的认知一致时空大语言模型" href="#用于下一个兴趣点预测的认知一致时空大语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14702v1宣布类型：新摘要：下一个兴趣点（PRI）推荐任务旨在根据用户的偏好和历史签到来预测用户的下一个目的地，这在基于位置的服务中具有重要价值。最近，大型语言模型（LLM）在推荐系统中表现出了巨大的潜力，该系统以生成方式处理下一个兴趣点预测。然而，这些LLM主要在大量非结构化文本库上进行预训练，缺乏对结构化地理实体和下一个兴趣点预测任务所需的顺序移动模式的原生理解。此外，在工业规模的POI预测应用中，结合世界知识和人类认知的对齐，诸如季节、天气状况、假期和用户简档（诸如习惯、职业和偏好），可以增强用户体验，同时提高推荐性能。为了解决这些问题，我们提出了CoAST（认知对齐的时空LLM），一个框架，采用自然语言作为接口，允许纳入世界知识，时空轨迹模式，配置文件和情景信息。具体而言，CoAST主要包括两个阶段：（1）通过对脱敏用户的丰富时空轨迹数据进行持续预训练来获取推荐知识;（2）通过监督微调（SFT）和随后的强化学习（RL）阶段使用丰富的训练数据将认知判断与人类偏好对齐。在各种真实世界数据集上进行的大量离线实验和部署在AMAP App主页的“Guess Where You Go”中的在线实验证明了CoAST的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">用于现实世界物联网设备识别的大型语言模型</h2><a id="user-content-用于现实世界物联网设备识别的大型语言模型" class="anchor" aria-label="Permalink: 用于现实世界物联网设备识别的大型语言模型" href="#用于现实世界物联网设备识别的大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.13817v1宣布类型：新摘要：物联网设备的快速扩张已经超过了当前的识别方法，给安全、隐私和网络问责制带来了重大风险。这些挑战在开放世界环境中更加突出，其中流量元数据通常不完整、有噪音或故意混淆。我们引入了一个语义推理管道，它将设备识别重新构建为对异类网络元数据的语言建模任务。为了构建可靠的监督，我们使用以互信息和基于信息的稳定性分数为指导的大型语言模型集合，为最大的现实世界物联网流量库物联网Inspector数据集生成高保真供应商标签。然后，我们通过课程学习对量化的LLaMA3.18B模型进行描述调整，以支持稀疏性和长尾供应商分布下的概括。我们的模型在2，015家供应商中实现了98.25%的顶级准确性和90.73%的宏准确性，同时保持对缺失字段、协议漂移和对抗性操纵的弹性。对独立物联网测试平台的评估，加上解释质量和对抗压力测试，表明经描述调整的LLM为现实世界的大规模设备识别提供了可扩展和可解释的基础。</p>
<div class="markdown-heading"><h2 class="heading-element">SUM-ActiVLN：农业视觉和语言导航的空间理解记忆</h2><a id="user-content-sum-activln农业视觉和语言导航的空间理解记忆" class="anchor" aria-label="Permalink: SUM-ActiVLN：农业视觉和语言导航的空间理解记忆" href="#sum-activln农业视觉和语言导航的空间理解记忆"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14357v1宣布类型：新摘要：农业机器人正在成为广泛农业任务中的强大助手，但仍然严重依赖手动操作或固定轨道系统进行移动。ActiVLN方法和A2 A基准开创性地将视觉与语言导航（VLN）扩展到农业领域，使机器人能够按照自然语言指令导航到目标位置。在实际的农业场景中，导航指令经常重复出现，但ActiVLN将每条指令视为一个独立的片段，忽视了过去经验为后续经验提供空间上下文的潜力。为了弥合这一差距，我们提出了农业视觉和语言导航空间理解记忆（SUM-ActiVLN）方法，其中NUM模块利用空间理解并通过3D重建和表示保存空间记忆。当在A2 A基准上进行评估时，我们的SUM-ActiVLN有效地将成功率从0.47提高到0.54，而导航误差从2.91 m轻微牺牲到2.93 m，展示了农业领域的最先进性能。代码：<a href="https://github.com/AlexTraveling/SUM-AgriVLN%E3%80%82">https://github.com/AlexTraveling/SUM-AgriVLN。</a></p>
<div class="markdown-heading"><h2 class="heading-element">高效的动态结构化稀疏训练与学习洗牌</h2><a id="user-content-高效的动态结构化稀疏训练与学习洗牌" class="anchor" aria-label="Permalink: 高效的动态结构化稀疏训练与学习洗牌" href="#高效的动态结构化稀疏训练与学习洗牌"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14812v1宣布类型：新摘要：结构化稀疏性加速了现代图形处理器的训练和推理，但在准确性上仍然落后于非结构化动态稀疏训练（DST）。这种不足源于表达能力的丧失：而密集层可以实现通过从$n$中选择任何$w$活动权重而获得的每一种可能的面具，而固定块或N：M布局仅探索这些可能性的一个子集。我们建议通过为每层学习单个置换矩阵和结构化权重矩阵来缩小这一差距。应用于三种规范结构--块、N：M和对角线--我们表明，排列增强的DST（PA-DST）在ImageNet-1 K（ViT-B/16）和Wikitext-103（GPT-2）上以90- 95%%sparity匹配非结构化基线（RigL、SET），但训练速度高达1.21，000美元/次$，推断速度高达2.9%speed $。结果位置结构+学习排列是准确性和效率之间的最佳点。</p>
<div class="markdown-heading"><h2 class="heading-element">MX+：突破微规模扩展的极限，实现高效的大型语言模型服务</h2><a id="user-content-mx突破微规模扩展的极限实现高效的大型语言模型服务" class="anchor" aria-label="Permalink: MX+：突破微规模扩展的极限，实现高效的大型语言模型服务" href="#mx突破微规模扩展的极限实现高效的大型语言模型服务"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14557v1宣布类型：新摘要：降低精度的数据格式对于以经济高效的方式提供大型语言模型（LLM）至关重要。虽然到目前为止已经引入了许多降低精度的格式，但它们通常需要对软件框架进行侵入性修改，或者对于硬件供应商的广泛采用来说是相当非常规的。在本文中，我们专注于最近行业驱动的块浮点（BFP）格式变体，并进行全面分析，以推动其限制，以实现高效的LLM服务。我们的分析表明，由于块中的异常值，现有的超低位BFP变体很难提供合理的语言模型性能。为了解决BFP的异常值，我们提出了MX+，这是一种具有成本效益且非侵入性的扩展，旨在无缝集成到微尺度（MX）格式中。MX+建立在异常值不需要在元素数据类型中使用其指数字段的关键见解之上，这使我们能够将指数字段重新用作扩展后缀，以提高异常值元素的精确度。我们的评估表明，与4位MX格式（MXFP 4）相比，MX+实现了显着更高的模型性能，存储开销和速度减慢可以忽略不计，因此为MXFP 4或MXFP 6提供了一个令人信服的替代方案，以实现高效的LLM推理。</p>
<div class="markdown-heading"><h2 class="heading-element">非结构化数据的多模式RAG：通过混合检索利用模式感知知识图</h2><a id="user-content-非结构化数据的多模式rag通过混合检索利用模式感知知识图" class="anchor" aria-label="Permalink: 非结构化数据的多模式RAG：通过混合检索利用模式感知知识图" href="#非结构化数据的多模式rag通过混合检索利用模式感知知识图"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14592v1宣布类型：新摘要：当前的检索增强生成（RAG）系统主要对单模式文本数据进行操作，限制了其对非结构化多模式文档的有效性。此类文档通常结合文本、图像、表格、方程和图形，每种文档都提供独特的信息。在这项工作中，我们提出了一种模式感知混合检索架构（MAHA），专门为通过模式感知知识图进行推理的多模式问题回答而设计。MAHA集成了密集载体检索与结构化图穿越，其中知识图编码跨模式语义和关系。该设计能够跨各种模式进行丰富的语义和上下文感知的检索。对多个基准数据集的评估表明，MAHA的表现大大优于基线方法，达到了0.486的ROUGE-L评分，提供了完整的模式覆盖。这些结果凸显了MAHA将嵌入与显式文档结构相结合的能力，从而实现有效的多模式检索。我们的工作建立了一个可扩展和可解释的检索框架，该框架通过在非结构化多模式数据上实现模式感知推理来推进RAG系统。</p>
<div class="markdown-heading"><h2 class="heading-element">合并MoE：通过专家输出合并有效压缩MoE模型</h2><a id="user-content-合并moe通过专家输出合并有效压缩moe模型" class="anchor" aria-label="Permalink: 合并MoE：通过专家输出合并有效压缩MoE模型" href="#合并moe通过专家输出合并有效压缩moe模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14436v1宣布类型：新摘要：专家混合（MoE）技术已被证明是有效扩展模型大小的一种有希望的解决方案，该解决方案已广泛应用于最近的LLM进步中。然而，MoE模型的大量内存负担使其压缩成为一个重要的研究方向。在这项工作中，我们提供了专家合并的理论分析，专家合并是最近提出的压缩MoE模型的技术。我们不是从参数聚集的传统角度来解释专家合并，而是从合并专家输出的角度来处理它。我们的主要见解是，合并过程可以被解释为将额外的矩阵插入到前向计算中，这自然会导致优化公式。在此分析的基础上，我们引入了MergeMoE，这是一种利用数学优化来构建压缩矩阵的方法。我们在多个MoE模型上评估了MergeMoE，并表明我们的算法始终优于具有相同压缩比的基线。</p>
<div class="markdown-heading"><h2 class="heading-element">安全保障的保障：当安全敏感的子空间遇到有害的零空间时</h2><a id="user-content-安全保障的保障当安全敏感的子空间遇到有害的零空间时" class="anchor" aria-label="Permalink: 安全保障的保障：当安全敏感的子空间遇到有害的零空间时" href="#安全保障的保障当安全敏感的子空间遇到有害的零空间时"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14301v1宣布类型：新摘要：大型语言模型（LLM）在不同任务中取得了显着的成功，但它们的安全一致性在适应过程中仍然脆弱。即使在良性数据上进行微调或进行低等级自适应时，预先训练的安全行为也很容易退化，导致微调模型中的有害响应。为了应对这一挑战，我们提出了GuardSpace，这是一种护栏框架，用于在整个微调过程中保持安全对齐，由两个关键组件组成：安全敏感子空间和抗伤害无效空间。首先，我们使用协方差预条件奇异值分解将预训练的权重显式分解为安全相关组件和安全无关组件，并从安全无关组件初始化低等级适配器，同时冻结安全相关组件以保留其相关的安全机制。其次，我们构建了一个空空间投影仪，它限制适配器更新在有害提示上改变安全输出，从而保持原始的拒绝行为。在多个下游任务上对各种预训练模型进行的实验表明，GuardSpace比现有方法实现了更卓越的性能。值得注意的是，对于在GSM 8 K上微调的Llama-2- 7 B-Chat，GuardSpace的表现优于最先进的方法AsFT，将平均有害分数从14.4%降低到3.6%，同时将准确性从26.0%提高到28.0%。</p>
<div class="markdown-heading"><h2 class="heading-element">CoLoR-GAN：生成性对抗网络中具有低等级适应的连续少镜头学习</h2><a id="user-content-color-gan生成性对抗网络中具有低等级适应的连续少镜头学习" class="anchor" aria-label="Permalink: CoLoR-GAN：生成性对抗网络中具有低等级适应的连续少镜头学习" href="#color-gan生成性对抗网络中具有低等级适应的连续少镜头学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2510.13869v1 Announce Type: new  Abstract: Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \textcolor{red}{\textbf{\underline{c}}}ontinual few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with \textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \href{<a href="https://github.com/munsifali11/CoLoR-GAN%7D%7BGithub">https://github.com/munsifali11/CoLoR-GAN}{Github</a>.</p>
<div class="markdown-heading"><h2 class="heading-element">气候变化背景下遗产保护的多模式方法</h2><a id="user-content-气候变化背景下遗产保护的多模式方法" class="anchor" aria-label="Permalink: 气候变化背景下遗产保护的多模式方法" href="#气候变化背景下遗产保护的多模式方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14136v1宣布类型：新摘要：由于气候变化，文化遗产地面临加速退化，但传统监测依赖于单模式分析（仅视觉检查或环境传感器），无法捕捉环境压力和物质退化之间复杂的相互作用。我们提出了一种轻量级多模式架构，将传感器数据（温度、湿度）与视觉图像融合，以预测遗产地的退化严重程度。我们的方法通过两个关键创新来适应PerceiverIO：（1）简化的编码器（64 D潜在空间），防止对小数据集（n=37个训练样本）进行过匹配，以及（2）自适应巴洛双胞胎损失，鼓励模式互补而不是冗余。根据斯特拉斯堡大教堂的数据，我们的模型实现了76.9%的准确性，比标准多模式架构（Visual BERT、Trans- former）提高了43%，比香草PerceiverIO提高了25%。消融研究表明，仅使用传感器的成功率达到61.5%，而仅使用图像的成功率达到46.2%，证实了成功的多模式协同作用。系统性超参数研究确定了最佳中等相关目标（{\tau} =0.3），该目标平衡了排列和互补性，与其他{\tau}值（{\tau} =0.1/0.5/0.7：53.8%，{\tau} =0.9：61.5%）相比，实现了69.2%的准确性。这项工作表明，架构简单性与对比正规化相结合可以在数据稀缺的遗产监控环境中实现有效的多模式学习，为人工智能驱动的关联决策支持系统提供基础。</p>
<div class="markdown-heading"><h2 class="heading-element">MLLMs可以从LLMs吸收数学推理能力作为免费午餐吗？</h2><a id="user-content-mllms可以从llms吸收数学推理能力作为免费午餐吗" class="anchor" aria-label="Permalink: MLLMs可以从LLMs吸收数学推理能力作为免费午餐吗？" href="#mllms可以从llms吸收数学推理能力作为免费午餐吗"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14387v1宣布类型：新摘要：数学推理一直是大型语言模型（LLM）的关键能力之一，近年来该模型取得了显着进展。然而，大多数工作都集中在LLM上，通过策划高质量的注释数据和复杂的训练（或推理）范式，而多模式LLM（MLLM）的数学推理性能仍然落后。由于MLLM通常由LLM和视觉模块组成，我们想知道：MLLM能否在不进行调整的情况下直接从现成的数学LLM吸收数学推理能力？最近的模型合并方法可能会为这个问题提供见解。然而，他们忽视了MLLM和LLM之间的一致性，我们发现它们的参数空间之间存在很大的差距，导致性能较低。我们的经验证据揭示了这个问题背后的两个关键因素：识别模型中关键的推理相关层以及缓解参数空间中的差距。基于经验见解，我们提出了IP合并，首先识别MLLM和Math LLM中与推理相关的参数，然后将它们投影到MLLM的子空间中，旨在保持对齐，最后合并该子空间中的参数。IP合并是一种免调方法，因为参数是直接调整的。大量实验表明，我们的IP合并方法可以直接从Math LLM增强MLLM的数学推理能力，而不会损害其其他能力。</p>
<div class="markdown-heading"><h2 class="heading-element">VLA ' 2：通过一个显着的框架来支持视觉-语言-动作模型，以实现隐形概念操纵</h2><a id="user-content-vla--2通过一个显着的框架来支持视觉-语言-动作模型以实现隐形概念操纵" class="anchor" aria-label="Permalink: VLA ' 2：通过一个显着的框架来支持视觉-语言-动作模型，以实现隐形概念操纵" href="#vla--2通过一个显着的框架来支持视觉-语言-动作模型以实现隐形概念操纵"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14902v1宣布类型：新摘要：当前的视觉-语言-动作（VLA）模型是在大规模机器人数据上预先训练的，表现出强大的多任务能力，并且可以很好地概括出视觉和语言操作指令的变化。然而，当面对训练数据之外的对象概念（例如数据集中未看到的对象描述和纹理）时，他们的成功率显着下降。为了解决这个问题，我们提出了一种新颖的代理框架VLA ' 2，它利用OpenVLA作为执行主干，并有效利用网络检索和对象检测等外部模块，为VLA提供有关目标对象的视觉和文本知识。这种方法可以减轻处理分发外对象时的概括失败。基于LIBERO模拟环境，我们引入了新颖的对象和对象描述来构建具有三个难度级别的新评估基准来测试我们方法的有效性。我们的框架在我们设计的硬级别概括基准上成功优于当前最先进的模型。与独立的OpenVLA基线相比，VLA ' 2在硬级基准测试中的成功率提高了44.2%，在所有定制环境中平均提高了20.2%，而域内任务的性能没有任何下降。项目网站：<a href="https://vla-2.github.io%E3%80%82" rel="nofollow">https://vla-2.github.io。</a></p>
<div class="markdown-heading"><h2 class="heading-element">Tawa：具有同步引用的现代图形处理器的自动扭曲专业化</h2><a id="user-content-tawa具有同步引用的现代图形处理器的自动扭曲专业化" class="anchor" aria-label="Permalink: Tawa：具有同步引用的现代图形处理器的自动扭曲专业化" href="#tawa具有同步引用的现代图形处理器的自动扭曲专业化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14719v1宣布类型：新摘要：现代图形处理器具有专门的硬件单元，可以实现高性能、同步的数据流执行。然而，传统的SIM编程模型从根本上与这种任务并行硬件不一致，造成了显着的可编程性差距。虽然硬件级扭曲专业化是释放峰值性能的关键，但它迫使开发人员手动协调复杂的低级通信和软件管道--这是一个劳动密集型、容易出错且不可持续的过程。为了应对这一挑战，我们提出了Tawa，这是一种自动化编译器，可以从高级基于瓦片的程序系统地生成高性能、扭曲专用代码。我们方法的核心是一种新颖的IR抽象，即非同步引用（aref），它在不暴露低级硬件细节的情况下表达曲速级通信。使用这种抽象，Tawa自动将程序划分为生产者-消费者角色，并管理复杂的数据流管道，减轻开发人员的侵入性内核重写。对代表性LLM内核中的NVIDIA H100图形处理器的评估表明，Tawa提供了高硬件利用率，比高度优化的cuBLAS GEMM内核实现了高达1.1 $\times $的加速。对于注意力工作负载，Tawa比Triton获得了1.2 $\times $的加速，并以更少的编程工作量与手动优化的CUTLASS C++ Flash Attention-3内核的性能相匹配。</p>
<div class="markdown-heading"><h2 class="heading-element">使用NL2SQL降低计算成本</h2><a id="user-content-使用nl2sql降低计算成本" class="anchor" aria-label="Permalink: 使用NL2SQL降低计算成本" href="#使用nl2sql降低计算成本"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14808v1宣布类型：新摘要：大型语言模型（LLM）最近支持将自然语言查询翻译为SQL查询（NL2SQL或文本转SQL）。使用LLM在大量SQL数据库上执行NL2SQL方法需要处理有关数据库的大量元信息，这反过来会导致冗长的提示，具有许多令牌和高处理成本。为了应对这一挑战，我们引入了Datalake Agent，这是一个代理系统，旨在使LLM能够更有效地解决NL2SQL任务。Datalake Agent没有使用NL2SQL的直接求解器（在提示符中使用所有元信息时调用LLM一次），而是使用交互式循环来减少使用的元信息。在循环中，LLM用于推理框架，该框架选择性地仅请求解决表格问答任务所需的信息。我们在包含100个表格问答任务的23个数据库的集合上评估了Datalake Agent。Datalake Agent将LLM使用的代币减少了高达87%，因此可以大幅降低成本，同时保持竞争性能。</p>
<div class="markdown-heading"><h2 class="heading-element">Hi-Agent：用于移动终端控制的分层视觉语言代理</h2><a id="user-content-hi-agent用于移动终端控制的分层视觉语言代理" class="anchor" aria-label="Permalink: Hi-Agent：用于移动终端控制的分层视觉语言代理" href="#hi-agent用于移动终端控制的分层视觉语言代理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.14388v1宣布类型：新摘要：自主操作移动设备的构建代理已引起越来越多的关注。虽然视觉语言模型（VLM）显示出前景，但大多数现有方法依赖于直接的状态到动作映射，这种映射缺乏结构化推理和规划，因此很难概括出新颖的任务或未见的UI布局。我们引入Hi-Agent，这是一种用于移动控制的可训练分层视觉语言代理，具有联合优化的高级推理模型和低级动作模型。为了高效的训练，我们将多步骤决策重新定义为一系列单步骤子目标，并提出前瞻优势函数，该函数利用来自低级模型的执行反馈来指导高级优化。该设计缓解了组相对策略优化（GRPO）在长期任务中遇到的路径爆炸问题，并实现稳定、无批评的联合训练。Hi-Agent在Android-in-the-Wild（AitW）基准上实现了新的最先进（SOTA）87.9%的任务成功率，在三种范式中显着优于之前的方法：基于预算的（AppAgent：17.7%）、监督的（Filtered BC：54.5%）和基于强化学习的（DigiRL：71.9%）。它还展示了ScreenSpot-v2基准测试的竞争性零镜头概括。在更具挑战性的AndroidWorld基准测试中，Hi-Agent还通过更大的主干进行有效扩展，在高复杂性移动控制场景中表现出强大的适应性。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>