<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">RASD：检索增强型推测解码</h2><a id="user-content-rasd检索增强型推测解码" class="anchor" aria-label="Permalink: RASD：检索增强型推测解码" href="#rasd检索增强型推测解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>推测解码通过为目标模型验证生成草稿令牌，加速了大型语言模型（LLMs）的推理过程。当前获取草稿令牌的方法依赖于轻量级草稿模型或额外的模型结构来生成草稿令牌，并从数据库中检索上下文。由于草稿模型规模小且训练数据有限，基于模型的推测解码在跨域场景下往往效果不佳。此外，草稿阶段的时间成本导致验证步骤中接受长度的上限较低，限制了整体效率。本文提出了RASD（检索增强型推测解码），采用检索方法来增强基于模型的推测解码。我们引入了树剪枝和树融合来实现这一目标。具体而言，我们开发了一种基于草稿模型概率分布的剪枝方法，以构建最优检索树。其次，我们采用最长前缀匹配算法将草稿模型生成的树与检索树合并，形成一个统一的验证树。实验结果表明，RASD在DocQA、摘要、代码和领域内QA等任务上实现了最先进的推理加速。此外，RASD展现出强大的可扩展性，能够无缝集成各种推测解码方法，包括基于生成和基于检索的方法。</p>
<div class="markdown-heading"><h2 class="heading-element">英文 K_量化大型语言模型（LLMs）并未显著削弱多语言性能</h2><a id="user-content-英文-k_量化大型语言模型llms并未显著削弱多语言性能" class="anchor" aria-label="Permalink: 英文 K_量化大型语言模型（LLMs）并未显著削弱多语言性能" href="#英文-k_量化大型语言模型llms并未显著削弱多语言性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>对于消费者使用本地部署的大型语言模型（LLMs），GGUF格式和k_quantization技术是保持原始模型性能同时将其缩减至可在消费级硬件上部署大小的宝贵工具。这些技术通过减少原始模型中每个权重所占用的比特数来实现，减少的程度取决于它们在模型推理过程中被认为的重要性。这种重要性是通过应用一个“重要性矩阵”来确定的——这是一个相对较小的文本文件，旨在代表LLM的标准使用场景。在网络上绝大多数可用的量化模型中，这一文档主要用英语编写。因此，一个悬而未决的问题是，通过牺牲多语言性能来保持英语任务性能的做法是否可行，以及是否可以通过使用不同语言的重要性矩阵来保持这种性能。本文通过在三种语言（英语、挪威语和马拉雅拉姆语）编写的重要性矩阵上对Llama3.3 70B进行量化，并在MixEval数据集上以英语和挪威语进行评估，来验证这些假设。所有与k_quantization相关的实验均未显示出显著结果（在所有情况下p &gt; 0.237），表明当前的量化实践并未对多语言性能造成不成比例的损害。</p>
<div class="markdown-heading"><h2 class="heading-element">超越模型架构与规模的层级熵权重量化普适性</h2><a id="user-content-超越模型架构与规模的层级熵权重量化普适性" class="anchor" aria-label="Permalink: 超越模型架构与规模的层级熵权重量化普适性" href="#超越模型架构与规模的层级熵权重量化普适性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>我们提出了一种新颖的选择性模型量化方法，该方法利用熵加权量化（EWQ）突破了针对大型语言模型（LLMs）的架构特定和大小依赖压缩方法的局限。通过分析跨变压器块的熵分布，EWQ能够确定哪些块可以安全地进行量化，而不会导致显著的性能下降，这一过程独立于模型架构或大小。我们的方法优于均匀量化方法，在保持大规模多任务语言理解（MMLU）准确率得分与未量化模型相差不超过0.5%的同时，将内存使用量减少了高达18%。我们展示了EWQ在多种架构上的有效性——从1.6B到70B参数——无论模型规模或架构设计如何，都在质量与压缩的权衡中展现了持续的改进。EWQ的一个意外发现是，它能够降低困惑度，相比未量化模型，这表明通过选择性精度降低存在有益的规则化效应。这一改进在不同模型家族中均成立，暗示了层级熵与最优精度需求之间存在根本联系。此外，我们引入了FastEWQ，一种快速熵分布分析方法，无需加载模型权重。该技术利用了熵分布在不同架构和规模间普遍存在的特性，使得量化决策几乎即时完成，同时在全熵分析下保持80%的分类准确率。我们的结果表明，有效的量化策略可以独立于特定的架构选择或模型大小来开发，为LLM的高效部署开辟了新的可能性。</p>
<div class="markdown-heading"><h2 class="heading-element">表征维度如何主导结构修剪的大型语言模型？</h2><a id="user-content-表征维度如何主导结构修剪的大型语言模型" class="anchor" aria-label="Permalink: 表征维度如何主导结构修剪的大型语言模型？" href="#表征维度如何主导结构修剪的大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>剪枝技术假设在原始的深度神经网络中存在一个子网络，该子网络能够在减少计算量的同时，达到与原始模型相当的性能表现。然而，模型性能如何随不同子网络提取方式的变化而波动，这一点尚不明确。本文中，我们选取表示维度（或称嵌入维度、模型维度，在相关文献中亦指残差流的维度）作为探讨这一问题的切入点。我们研究了大型语言模型（LLM）转换器块中的线性变换，并采用了一种特定的结构化剪枝方法——SliceGPT，来提取不同表示维度的子网络。通过机制性地分析模型前向传播过程中的激活流，我们发现表示维度主导了线性变换、模型预测，并最终影响了模型性能。本文给出了明确的解析关系，用于在不实际评估的情况下计算剪枝后模型的性能（困惑度和准确率），并通过Llama-3-8B-Instruct和Phi-3-mini-4k-Instruct模型进行了实证验证。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>