<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 08/25/2025</h1><a id="user-content-personalized-daily-arxiv-papers-08252025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 08/25/2025" href="#personalized-daily-arxiv-papers-08252025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 18</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing</a>
<strong>Authors:</strong> Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che</p>
</li>
<li>
<p><a href="#link1">TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine</a>
<strong>Authors:</strong> Tim Langer, Matthias Widra, Volkhard Beyer</p>
</li>
<li>
<p><a href="#link2">Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation</a>
<strong>Authors:</strong> Guangyu Sun, Jingtao Li, Weiming Zhuang, Chen Chen, Chen Chen, Lingjuan Lyu</p>
</li>
<li>
<p><a href="#link3">FEST: A Unified Framework for Evaluating Synthetic Tabular Data</a>
<strong>Authors:</strong> Weijie Niu, Alberto Huertas Celdran, Karoline Siarsky, Burkhard Stiller</p>
</li>
<li>
<p><a href="#link4">Post Hoc Regression Refinement via Pairwise Rankings</a>
<strong>Authors:</strong> Kevin Tirta Wijaya, Michael Sun, Minghao Guo, Hans-Peter Seidel, Wojciech Matusik, Vahid Babaei</p>
</li>
<li>
<p><a href="#link5">On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View</a>
<strong>Authors:</strong> Tao Guo, Junxiao Wang, Fushuo Huo, Laizhong Cui, Song Guo, Jie Gui, Dacheng Tao</p>
</li>
<li>
<p><a href="#link6">Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services</a>
<strong>Authors:</strong> Renxuan Tan, Rongpeng Li, Xiaoxue Yu, Xianfu Chen, Xing Xu, Zhifeng Zhao</p>
</li>
<li>
<p><a href="#link7">TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill &amp; Decode Inference</a>
<strong>Authors:</strong> Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang</p>
</li>
<li>
<p><a href="#link8">PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis</a>
<strong>Authors:</strong> Bin Wen, Tien-Ping Tan</p>
</li>
<li>
<p><a href="#link9">Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</a>
<strong>Authors:</strong> Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng</p>
</li>
<li>
<p><a href="#link10">AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs</a>
<strong>Authors:</strong> Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, Jun Wang</p>
</li>
<li>
<p><a href="#link11">Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining</a>
<strong>Authors:</strong> Samiul Basir Bhuiyan, Md. Sazzad Hossain Adib, Mohammed Aman Bhuiyan, Muhammad Rafsan Kabir, Moshiur Farazi, Shafin Rahman, Nabeel Mohammed</p>
</li>
<li>
<p><a href="#link12">Low-dimensional embeddings of high-dimensional data</a>
<strong>Authors:</strong> Cyril de Bodt, Alex Diaz-Papkovich, Michael Bleher, Kerstin Bunte, Corinna Coupette, Sebastian Damrich, Enrique Fita Sanmartin, Fred A. Hamprecht, Em\H{o}ke-'Agnes Horv'at, Dhruv Kohli, Smita Krishnaswamy, John A. Lee, Boudewijn P. F. Lelieveldt, Leland McInnes, Ian T. Nabney, Maximilian Noichl, Pavlin G. Poli\v{c}ar, Bastian Rieck, Guy Wolf, Gal Mishne, Dmitry Kobak</p>
</li>
<li>
<p><a href="#link13">RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs</a>
<strong>Authors:</strong> Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa</p>
</li>
<li>
<p><a href="#link14">GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation</a>
<strong>Authors:</strong> Sungmin Kang, Jisoo Kim, Salman Avestimehr, Sunwoo Lee</p>
</li>
<li>
<p><a href="#link15">Modular Embedding Recomposition for Incremental Learning</a>
<strong>Authors:</strong> Aniello Panariello, Emanuele Frascaroli, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</p>
</li>
<li>
<p><a href="#link16">FraPPE: Fast and Efficient Preference-based Pure Exploration</a>
<strong>Authors:</strong> Udvas Das, Apurv Shukla, Debabrota Basu</p>
</li>
<li>
<p><a href="#link17">Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning</a>
<strong>Authors:</strong> Andreas Loizou, Dimitrios Tsoumakos</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2508.16134" rel="nofollow">CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-commonkv-compressing-kv-cache-with-cross-layer-parameter-sharing-" class="anchor" aria-label="Permalink: 0. CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing" href="#0-commonkv-compressing-kv-cache-with-cross-layer-parameter-sharing-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16134
<strong>Authors:</strong> Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che</p>
<p><strong>Abstract:</strong> arXiv:2508.16134v1 Announce Type: new  Abstract: Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98% compression ratio without significant performance loss.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2508.16553" rel="nofollow">TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-tinyml-towards-industry-40-resource-efficient-process-monitoring-of-a-milling-machine-" class="anchor" aria-label="Permalink: 1. TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine" href="#1-tinyml-towards-industry-40-resource-efficient-process-monitoring-of-a-milling-machine-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16553
<strong>Authors:</strong> Tim Langer, Matthias Widra, Volkhard Beyer</p>
<p><strong>Abstract:</strong> arXiv:2508.16553v1 Announce Type: new  Abstract: In the context of industry 4.0, long-serving industrial machines can be retrofitted with process monitoring capabilities for future use in a smart factory. One possible approach is the deployment of wireless monitoring systems, which can benefit substantially from the TinyML paradigm. This work presents a complete TinyML flow from dataset generation, to machine learning model development, up to implementation and evaluation of a full preprocessing and classification pipeline on a microcontroller. After a short review on TinyML in industrial process monitoring, the creation of the novel MillingVibes dataset is described. The feasibility of a TinyML system for structure-integrated process quality monitoring could be shown by the development of an 8-bit-quantized convolutional neural network (CNN) model with 12.59kiB parameter storage. A test accuracy of 100.0% could be reached at 15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex M4F microcontroller, serving as a reference for future TinyML process monitoring solutions.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2508.16568" rel="nofollow">Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-closer-to-reality-practical-semi-supervised-federated-learning-for-foundation-model-adaptation-" class="anchor" aria-label="Permalink: 2. Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation" href="#2-closer-to-reality-practical-semi-supervised-federated-learning-for-foundation-model-adaptation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16568
<strong>Authors:</strong> Guangyu Sun, Jingtao Li, Weiming Zhuang, Chen Chen, Chen Chen, Lingjuan Lyu</p>
<p><strong>Abstract:</strong> arXiv:2508.16568v1 Announce Type: new  Abstract: Foundation models (FMs) exhibit remarkable generalization but require adaptation to downstream tasks, particularly in privacy-sensitive applications. Due to data privacy regulations, cloud-based FMs cannot directly access private edge data, limiting their adaptation. Federated learning (FL) provides a privacy-aware alternative, but existing FL approaches overlook the constraints imposed by edge devices -- namely, limited computational resources and the scarcity of labeled data. To address these challenges, we introduce Practical Semi-Supervised Federated Learning (PSSFL), where edge devices hold only unlabeled, low-resolution data, while the server has limited labeled, high-resolution data. In this setting, we propose the Federated Mixture of Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox tackles computational and resolution mismatch challenges via a sparse Mixture-of-Experts architecture, employing a spatial router to align features across resolutions and a Soft-Mixture strategy to stabilize semi-supervised learning. We take object detection as a case study, and experiments on real-world autonomous driving datasets demonstrate that FedMox effectively adapts FMs under PSSFL, significantly improving performance with constrained memory costs on edge devices. Our work paves the way for scalable and privacy-preserving FM adaptation in federated scenarios.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2508.16254" rel="nofollow">FEST: A Unified Framework for Evaluating Synthetic Tabular Data</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-fest-a-unified-framework-for-evaluating-synthetic-tabular-data-" class="anchor" aria-label="Permalink: 3. FEST: A Unified Framework for Evaluating Synthetic Tabular Data" href="#3-fest-a-unified-framework-for-evaluating-synthetic-tabular-data-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16254
<strong>Authors:</strong> Weijie Niu, Alberto Huertas Celdran, Karoline Siarsky, Burkhard Stiller</p>
<p><strong>Abstract:</strong> arXiv:2508.16254v1 Announce Type: new  Abstract: Synthetic data generation, leveraging generative machine learning techniques, offers a promising approach to mitigating privacy concerns associated with real-world data usage. Synthetic data closely resembles real-world data while maintaining strong privacy guarantees. However, a comprehensive assessment framework is still missing in the evaluation of synthetic data generation, especially when considering the balance between privacy preservation and data utility in synthetic data. This research bridges this gap by proposing FEST, a systematic framework for evaluating synthetic tabular data. FEST integrates diverse privacy metrics (attack-based and distance-based), along with similarity and machine learning utility metrics, to provide a holistic assessment. We develop FEST as an open-source Python-based library and validate it on multiple datasets, demonstrating its effectiveness in analyzing the privacy-utility trade-off of different synthetic data generation models. The source code of FEST is available on Github.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2508.16495" rel="nofollow">Post Hoc Regression Refinement via Pairwise Rankings</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-post-hoc-regression-refinement-via-pairwise-rankings-" class="anchor" aria-label="Permalink: 4. Post Hoc Regression Refinement via Pairwise Rankings" href="#4-post-hoc-regression-refinement-via-pairwise-rankings-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16495
<strong>Authors:</strong> Kevin Tirta Wijaya, Michael Sun, Minghao Guo, Hans-Peter Seidel, Wojciech Matusik, Vahid Babaei</p>
<p><strong>Abstract:</strong> arXiv:2508.16495v1 Announce Type: new  Abstract: Accurate prediction of continuous properties is essential to many scientific and engineering tasks. Although deep-learning regressors excel with abundant labels, their accuracy deteriorates in data-scarce regimes. We introduce RankRefine, a model-agnostic, plug-and-play post hoc method that refines regression with expert knowledge coming from pairwise rankings. Given a query item and a small reference set with known properties, RankRefine combines the base regressor's output with a rank-based estimate via inverse variance weighting, requiring no retraining. In molecular property prediction task, RankRefine achieves up to 10% relative reduction in mean absolute error using only 20 pairwise comparisons obtained through a general-purpose large language model (LLM) with no finetuning. As rankings provided by human experts or general-purpose LLMs are sufficient for improving regression across diverse domains, RankRefine offers practicality and broad applicability, especially in low-data settings.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2508.16261" rel="nofollow">On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-on-the-evolution-of-federated-post-training-large-language-models-a-model-accessibility-view-" class="anchor" aria-label="Permalink: 5. On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View" href="#5-on-the-evolution-of-federated-post-training-large-language-models-a-model-accessibility-view-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16261
<strong>Authors:</strong> Tao Guo, Junxiao Wang, Fushuo Huo, Laizhong Cui, Song Guo, Jie Gui, Dacheng Tao</p>
<p><strong>Abstract:</strong> arXiv:2508.16261v1 Announce Type: new  Abstract: Federated Learning (FL) enables training models across decentralized data silos while preserving client data privacy. Recent research has explored efficient methods for post-training large language models (LLMs) within FL to address computational and communication challenges. While existing approaches often rely on access to LLMs' internal information, which is frequently restricted in real-world scenarios, an inference-only paradigm (black-box FedLLM) has emerged to address these limitations. This paper presents a comprehensive survey on federated tuning for LLMs. We propose a taxonomy categorizing existing studies along two axes: model access-based and parameter efficiency-based optimization. We classify FedLLM approaches into white-box, gray-box, and black-box techniques, highlighting representative methods within each category. We review emerging research treating LLMs as black-box inference APIs and discuss promising directions and open challenges for future research.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2508.16037" rel="nofollow">Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-pareto-actor-critic-for-communication-and-computation-co-optimization-in-non-cooperative-federated-learning-services-" class="anchor" aria-label="Permalink: 6. Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services" href="#6-pareto-actor-critic-for-communication-and-computation-co-optimization-in-non-cooperative-federated-learning-services-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16037
<strong>Authors:</strong> Renxuan Tan, Rongpeng Li, Xiaoxue Yu, Xianfu Chen, Xing Xu, Zhifeng Zhao</p>
<p><strong>Abstract:</strong> arXiv:2508.16037v1 Announce Type: new  Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is fundamentally hampered by non-cooperative dynamics, where privacy constraints and competing interests preclude the centralized optimization of multi-SP communication and computation resources. In this paper, we introduce PAC-MCoFL, a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs act as agents to jointly optimize client assignment, adaptive quantization, and resource allocation. Within the framework, we integrate Pareto Actor-Critic (PAC) principles with expectile regression, enabling agents to conjecture optimal joint policies to achieve Pareto-optimal equilibria while modeling heterogeneous risk profiles. To manage the high-dimensional action space, we devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant featuring a parameterized conjecture generator that substantially reduces computational complexity with a provably bounded error. Alongside theoretical convergence guarantees, our framework's superiority is validated through extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2% improvements in total reward and hypervolume indicator (HVI), respectively, over the latest MARL solutions. The results also demonstrate that our method can more effectively balance individual SP and system performance in scaled deployments and under diverse data heterogeneity.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2508.15881" rel="nofollow">TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill &amp; Decode Inference</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-tpla-tensor-parallel-latent-attention-for-efficient-disaggregated-prefill--decode-inference-" class="anchor" aria-label="Permalink: 7. TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill &amp; Decode Inference" href="#7-tpla-tensor-parallel-latent-attention-for-efficient-disaggregated-prefill--decode-inference-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15881
<strong>Authors:</strong> Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang</p>
<p><strong>Abstract:</strong> arXiv:2508.15881v1 Announce Type: new  Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2508.15852" rel="nofollow">PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-pgf-net-a-progressive-gated-fusion-framework-for-efficient-multimodal-sentiment-analysis-" class="anchor" aria-label="Permalink: 8. PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis" href="#8-pgf-net-a-progressive-gated-fusion-framework-for-efficient-multimodal-sentiment-analysis-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15852
<strong>Authors:</strong> Bin Wen, Tien-Ping Tan</p>
<p><strong>Abstract:</strong> arXiv:2508.15852v1 Announce Type: new  Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep learning framework designed for efficient and interpretable multimodal sentiment analysis. Our framework incorporates three primary innovations. Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a Cross-Attention mechanism empowers the textual representation to dynamically query and integrate non-linguistic features from audio and visual streams within the deep layers of a Transformer encoder. This enables a deeper, context-dependent fusion process. Secondly, the model incorporates an Adaptive Gated Arbitration mechanism, which acts as a dynamic controller to balance the original linguistic information against the newly fused multimodal context, ensuring stable and meaningful integration while preventing noise from overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning (PEFT) strategy is employed, synergistically combining global adaptation via LoRA with local refinement through Post-Fusion Adapters. This significantly reduces trainable parameters, making the model lightweight and suitable for resource-limited scenarios. These innovations are integrated into a hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic, and interpretable multimodal sentiment analysis while maintaining exceptional parameter efficiency. Experimental results on MOSI dataset demonstrate that our proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves these results with only 3.09M trainable parameters, showcasing a superior balance between performance and computational efficiency.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2508.16059" rel="nofollow">Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-integrating-time-series-into-llms-via-multi-layer-steerable-embedding-fusion-for-enhanced-forecasting-" class="anchor" aria-label="Permalink: 9. Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting" href="#9-integrating-time-series-into-llms-via-multi-layer-steerable-embedding-fusion-for-enhanced-forecasting-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16059
<strong>Authors:</strong> Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng</p>
<p><strong>Abstract:</strong> arXiv:2508.16059v1 Announce Type: new  Abstract: Time series (TS) data are ubiquitous across various application areas, rendering time series forecasting (TSF) a fundamental task. With the astounding advances in large language models (LLMs), a variety of methods have been developed to adapt LLMs for time series forecasting. Despite unlocking the potential of LLMs in comprehending TS data, existing methods are inherently constrained by their shallow integration of TS information, wherein LLMs typically access TS representations at shallow layers, primarily at the input layer. This causes the influence of TS representations to progressively fade in deeper layers and eventually leads to ineffective adaptation between textual embeddings and TS representations. In this paper, we propose the Multi-layer Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to directly access time series patterns at all depths, thereby mitigating the progressive loss of TS information in deeper layers. Specifically, MSEF leverages off-the-shelf time series foundation models to extract semantically rich embeddings, which are fused with intermediate text representations across LLM layers via layer-specific steering vectors. These steering vectors are designed to continuously optimize the alignment between time series and textual modalities and facilitate a layer-specific adaptation mechanism that ensures efficient few-shot learning capabilities. Experimental results on seven benchmarks demonstrate significant performance improvements by MSEF compared with baselines, with an average reduction of 31.8% in terms of MSE. The code is available at <a href="https://github.com/One1sAll/MSEF">https://github.com/One1sAll/MSEF</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2508.16153" rel="nofollow">AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-agentfly-fine-tuning-llm-agents-without-fine-tuning-llms-" class="anchor" aria-label="Permalink: 10. AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs" href="#10-agentfly-fine-tuning-llm-agents-without-fine-tuning-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16153
<strong>Authors:</strong> Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, Jun Wang</p>
<p><strong>Abstract:</strong> arXiv:2508.16153v1 Announce Type: new  Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely AgentFly, which attains top-1 on GAIA validation ($87.88%$ Pass@$3$) and $79.40%$ on the test set. It reaches $66.6%$ F1 and $80.4%$ PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds $4.7%$ to $9.6%$ absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at <a href="https://github.com/Agent-on-the-Fly/AgentFly">https://github.com/Agent-on-the-Fly/AgentFly</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2508.15828" rel="nofollow">Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-z-pruner-post-training-pruning-of-large-language-models-for-efficiency-without-retraining-" class="anchor" aria-label="Permalink: 11. Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining" href="#11-z-pruner-post-training-pruning-of-large-language-models-for-efficiency-without-retraining-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15828
<strong>Authors:</strong> Samiul Basir Bhuiyan, Md. Sazzad Hossain Adib, Mohammed Aman Bhuiyan, Muhammad Rafsan Kabir, Moshiur Farazi, Shafin Rahman, Nabeel Mohammed</p>
<p><strong>Abstract:</strong> arXiv:2508.15828v1 Announce Type: new  Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving remarkable performance across a wide range of natural language processing tasks. However, this progress has come at the cost of increasingly large model sizes, which pose significant challenges for deployment, scalability, and energy efficiency. To address these limitations, post-training pruning has emerged as a promising approach for reducing model size and inference latency without the need for retraining. Despite these advantages, many existing pruning methods result in substantial performance degradation or require computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a novel post-training pruning method designed to induce sparsity in pretrained LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages both weight update magnitudes and activation patterns to identify and eliminate redundant parameters more effectively. Our method is model-agnostic, efficient, and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of standard language benchmarks. Experimental results demonstrate that Z-Pruner surpasses state-of-the-art pruning methods that require intensive weight updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the highest overall average score for zero-shot accuracy. We have made the corresponding codes publicly available at <a href="https://github.com/sazzadadib/Z-Pruner">https://github.com/sazzadadib/Z-Pruner</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2508.15929" rel="nofollow">Low-dimensional embeddings of high-dimensional data</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-low-dimensional-embeddings-of-high-dimensional-data-" class="anchor" aria-label="Permalink: 12. Low-dimensional embeddings of high-dimensional data" href="#12-low-dimensional-embeddings-of-high-dimensional-data-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15929
<strong>Authors:</strong> Cyril de Bodt, Alex Diaz-Papkovich, Michael Bleher, Kerstin Bunte, Corinna Coupette, Sebastian Damrich, Enrique Fita Sanmartin, Fred A. Hamprecht, Em\H{o}ke-'Agnes Horv'at, Dhruv Kohli, Smita Krishnaswamy, John A. Lee, Boudewijn P. F. Lelieveldt, Leland McInnes, Ian T. Nabney, Maximilian Noichl, Pavlin G. Poli\v{c}ar, Bastian Rieck, Guy Wolf, Gal Mishne, Dmitry Kobak</p>
<p><strong>Abstract:</strong> arXiv:2508.15929v1 Announce Type: new  Abstract: Large collections of high-dimensional data have become nearly ubiquitous across many academic fields and application domains, ranging from biology to the humanities. Since working directly with high-dimensional data poses challenges, the demand for algorithms that create low-dimensional representations, or embeddings, for data visualization, exploration, and analysis is now greater than ever. In recent years, numerous embedding algorithms have been developed, and their usage has become widespread in research and industry. This surge of interest has resulted in a large and fragmented research field that faces technical challenges alongside fundamental debates, and it has left practitioners without clear guidance on how to effectively employ existing methods. Aiming to increase coherence and facilitate future work, in this review we provide a detailed and critical overview of recent developments, derive a list of best practices for creating and using low-dimensional embeddings, evaluate popular approaches on a variety of datasets, and discuss the remaining challenges and open problems in the field.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2508.16546" rel="nofollow">RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-rl-is-neither-a-panacea-nor-a-mirage-understanding-supervised-vs-reinforcement-learning-fine-tuning-for-llms-" class="anchor" aria-label="Permalink: 13. RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs" href="#13-rl-is-neither-a-panacea-nor-a-mirage-understanding-supervised-vs-reinforcement-learning-fine-tuning-for-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16546
<strong>Authors:</strong> Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa</p>
<p><strong>Abstract:</strong> arXiv:2508.16546v1 Announce Type: new  Abstract: Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, we revisit how these two stages reshape model representation and OOD performance. Our key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Our spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2508.16191" rel="nofollow">GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-gem-a-scale-aware-and-distribution-sensitive-sparse-fine-tuning-framework-for-effective-downstream-adaptation-" class="anchor" aria-label="Permalink: 14. GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation" href="#14-gem-a-scale-aware-and-distribution-sensitive-sparse-fine-tuning-framework-for-effective-downstream-adaptation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16191
<strong>Authors:</strong> Sungmin Kang, Jisoo Kim, Salman Avestimehr, Sunwoo Lee</p>
<p><strong>Abstract:</strong> arXiv:2508.16191v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt large pre-trained models to new tasks. Most PEFT methods update only a small subset of parameters while freezing the rest, avoiding redundant computation. As they maximize the absolute size of the updates without regard to the parameters' original scale, the resulting changes in model behavior can be minimal. In contrast, we maximize updates relative to each parameter's scale, yielding more meaningful downstream adaptation. We propose Gradient-to-Weight Ratio and Entropy-guided Masking (GEM), a parameter scale-aware, distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters whose updates are significant in proportion to their initial pre-trained values. It also adaptively determines how many parameters to tune at each layer based on the entropy of parameter values, thereby making the most effective use of the computational budget in PEFT. Our empirical study demonstrates the efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in fine-tuning accuracy over full fine-tuning while updating only 0.1% of model parameters.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2508.16463" rel="nofollow">Modular Embedding Recomposition for Incremental Learning</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-modular-embedding-recomposition-for-incremental-learning-" class="anchor" aria-label="Permalink: 15. Modular Embedding Recomposition for Incremental Learning" href="#15-modular-embedding-recomposition-for-incremental-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16463
<strong>Authors:</strong> Aniello Panariello, Emanuele Frascaroli, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</p>
<p><strong>Abstract:</strong> arXiv:2508.16463v1 Announce Type: new  Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by devising an approach that transforms preservation into enhancement of the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets. The codebase is available at <a href="https://github.com/aimagelab/mammoth">https://github.com/aimagelab/mammoth</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2508.16487" rel="nofollow">FraPPE: Fast and Efficient Preference-based Pure Exploration</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-frappe-fast-and-efficient-preference-based-pure-exploration-" class="anchor" aria-label="Permalink: 16. FraPPE: Fast and Efficient Preference-based Pure Exploration" href="#16-frappe-fast-and-efficient-preference-based-pure-exploration-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16487
<strong>Authors:</strong> Udvas Das, Apurv Shukla, Debabrota Basu</p>
<p><strong>Abstract:</strong> arXiv:2508.16487v1 Announce Type: new  Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given confidence level the set of Pareto optimal arms in a vector-valued (aka multi-objective) bandit, where the reward vectors are ordered via a (given) preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied, there does not exist a computationally efficient algorithm that can optimally track the existing lower bound for arbitrary preference cones. We successfully fill this gap by efficiently solving the minimisation and maximisation problems in the lower bound. First, we derive three structural properties of the lower bound that yield a computationally tractable reduction of the minimisation problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation problem in the lower bound. Together, these techniques solve the maxmin optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with $K$ arms and $L$ dimensional reward, which is a significant acceleration over the literature. We further prove that our proposed PrePEx algorithm, FraPPE, asymptotically achieves the optimal sample complexity. Finally, we perform numerical experiments across synthetic and real datasets demonstrating that FraPPE achieves the lowest sample complexities to identify the exact Pareto set among the existing algorithms.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2508.16255" rel="nofollow">Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-chunked-data-shapley-a-scalable-dataset-quality-assessment-for-machine-learning-" class="anchor" aria-label="Permalink: 17. Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning" href="#17-chunked-data-shapley-a-scalable-dataset-quality-assessment-for-machine-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.16255
<strong>Authors:</strong> Andreas Loizou, Dimitrios Tsoumakos</p>
<p><strong>Abstract:</strong> arXiv:2508.16255v1 Announce Type: new  Abstract: As the volume and diversity of available datasets continue to increase, assessing data quality has become crucial for reliable and efficient Machine Learning analytics. A modern, game-theoretic approach for evaluating data quality is the notion of Data Shapley which quantifies the value of individual data points within a dataset. State-of-the-art methods to scale the NP-hard Shapley computation also face severe challenges when applied to large-scale datasets, limiting their practical use. In this work, we present a Data Shapley approach to identify a dataset's high-quality data tuples, Chunked Data Shapley (C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and estimates the contribution of each chunk using optimized subset selection and single-iteration stochastic gradient descent. This approach drastically reduces computation time while preserving high quality results. We empirically benchmark our method on diverse real-world classification and regression tasks, demonstrating that C-DaSh outperforms existing Shapley approximations in both computational efficiency (achieving speedups between 80x - 2300x) and accuracy in detecting low-quality data regions. Our method enables practical measurement of dataset quality on large tabular datasets, supporting both classification and regression pipelines.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model. Furthermore, it incorporates Mixture-of-Experts (MoE) architectures to significantly decrease deployment overhead and accelerate inference speed, enabling more efficient and scalable model serving in resource-constrained environments.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>