<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">压缩技术对医疗领域大型多模态语言模型的影响</h2><a id="user-content-压缩技术对医疗领域大型多模态语言模型的影响" class="anchor" aria-label="Permalink: 压缩技术对医疗领域大型多模态语言模型的影响" href="#压缩技术对医疗领域大型多模态语言模型的影响"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.21976v1 公告类型：新研究<br>
摘要：多模态大语言模型（MLLMs）在医疗领域具有巨大应用潜力，但其高昂的计算成本亟需高效压缩技术。本文评估了结构剪枝和激活感知量化对医疗应用微调版LLAVA模型的影响，提出了一种新颖的剪枝层选择方法，分析了不同量化技术，并在剪枝-监督微调-量化流程中评估了性能权衡。我们提出的方法使70亿参数MLLMs仅需4GB显存即可运行，内存占用降低70%，同时在相同压缩率下，模型性能较传统剪枝与量化技术提升4%。</p>
<p>（说明：根据学术文献翻译规范，处理了以下要点：</p>
<ol>
<li>专业术语统一："pruning"译为"剪枝"，"quantization"译为"量化"</li>
<li>技术概念准确表达："activation-aware"译为"激活感知"，"SFT"扩展为"监督微调"</li>
<li>数据呈现优化：将"7B"译为"70亿"符合中文计数习惯</li>
<li>被动语态转换："are necessitated"译为主动态"亟需"</li>
<li>复杂句式拆分：将原文复合长句拆分为符合中文表达习惯的短句）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">MemShare：通过KV缓存复用实现大型推理模型的高效内存推理</h2><a id="user-content-memshare通过kv缓存复用实现大型推理模型的高效内存推理" class="anchor" aria-label="Permalink: MemShare：通过KV缓存复用实现大型推理模型的高效内存推理" href="#memshare通过kv缓存复用实现大型推理模型的高效内存推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.21433v1 公告类型：新研究<br>
摘要：大型推理模型（LRMs）在数学推理和形式逻辑任务中取得了显著进展。然而，其生成冗长思维链的倾向导致推理过程中出现巨大的内存开销。我们发现，LRMs频繁产生高度相似的中间推理步骤，这些步骤对应着各层间相似的KV缓存状态。受此启发，我们提出MemShare——一种新型KV缓存管理方法，能有效降低内存开销。该方案采用协同过滤算法高效识别可复用的KV缓存块，并通过零拷贝缓存重用技术显著减少内存占用，在保持准确率的同时提升吞吐量。实验结果表明，与现有KV缓存管理方法相比，MemShare在保持更高准确率的同时，最高可实现84.79%的吞吐量提升。</p>
<p>（注：根据学术论文摘要的文体特点，翻译时进行了以下处理：</p>
<ol>
<li>专业术语保留英文缩写（LRMs/KV）并首次出现时标注全称</li>
<li>"chain-of-thought"采用通用译法"思维链"</li>
<li>"zero copy"译为技术领域惯用表述"零拷贝"</li>
<li>数据呈现方式调整为中文科技论文常用格式（84.79%→84.79%）</li>
<li>被动语态转换为中文主动表述（"are observed"→"我们发现"）</li>
<li>长难句拆分重组（如将"Motivated by..."状语从句转化为独立短句））</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>