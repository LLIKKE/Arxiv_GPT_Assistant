<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 08/21/2025</h1><a id="user-content-personalized-daily-arxiv-papers-08212025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 08/21/2025" href="#personalized-daily-arxiv-papers-08212025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 17</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">EEGDM: EEG Representation Learning via Generative Diffusion Model</a>
<strong>Authors:</strong> Jia Hong Puah, Sim Kuan Goh, Ziwei Zhang, Zixuan Ye, Chow Khuen Chan, Kheng Seang Lim, Si Lei Fong, Kok Sin Woon</p>
</li>
<li>
<p><a href="#link1">Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs</a>
<strong>Authors:</strong> Orestis Konstantaropoulos, Stelios Manolis Smirnakis, Maria Papadopouli</p>
</li>
<li>
<p><a href="#link2">The Agent Behavior: Model, Governance and Challenges in the AI Digital Age</a>
<strong>Authors:</strong> Qiang Zhang, Pei Yan, Yijia Xu, Chuanpo Fu, Yong Fang, Yang Liu</p>
</li>
<li>
<p><a href="#link3">On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks</a>
<strong>Authors:</strong> Junwei Su, Chuan Wu</p>
</li>
<li>
<p><a href="#link4">Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data</a>
<strong>Authors:</strong> Ahmed Mujtaba, Gleb Radchenko, Radu Prodan, Marc Masana</p>
</li>
<li>
<p><a href="#link5">A Guide for Manual Annotation of Scientific Imagery: How to Prepare for Large Projects</a>
<strong>Authors:</strong> Azim Ahmadzadeh, Rohan Adhyapak, Armin Iraji, Kartik Chaurasiya, V Aparna, Petrus C. Martens</p>
</li>
<li>
<p><a href="#link6">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</a>
<strong>Authors:</strong> Valter Sch"utz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani</p>
</li>
<li>
<p><a href="#link7">On Defining Neural Averaging</a>
<strong>Authors:</strong> Su Hyeong Lee, Richard Ngo</p>
</li>
<li>
<p><a href="#link8">FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy</a>
<strong>Authors:</strong> Yijin Chen, Wenqiang Xu, Zhenjun Yu, Tutian Tang, Yutong Li, Siqiong Yao, Cewu Lu</p>
</li>
<li>
<p><a href="#link9">HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents</a>
<strong>Authors:</strong> Thomas Carta, Cl'ement Romac, Loris Gaven, Pierre-Yves Oudeyer, Olivier Sigaud, Sylvain Lamprier</p>
</li>
<li>
<p><a href="#link10">Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method</a>
<strong>Authors:</strong> Suleyman Olcay Polat, Poli A. Nemkova, Mark V. Albert</p>
</li>
<li>
<p><a href="#link11">MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding</a>
<strong>Authors:</strong> Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Mohsen Imani</p>
</li>
<li>
<p><a href="#link12">GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation</a>
<strong>Authors:</strong> Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao</p>
</li>
<li>
<p><a href="#link13">Multi-view Graph Condensation via Tensor Decomposition</a>
<strong>Authors:</strong> N'icolas Roque dos Santos, Dawon Ahn, Diego Minatel, Alneu de Andrade Lopes, Evangelos E. Papalexakis</p>
</li>
<li>
<p><a href="#link14">Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization</a>
<strong>Authors:</strong> Kim Hammar, Tao Li</p>
</li>
<li>
<p><a href="#link15">Cross-Modality Controlled Molecule Generation with Diffusion Language Model</a>
<strong>Authors:</strong> Yunzhe Zhang, Yifei Wang, Khanh Vinh Nguyen, Pengyu Hong</p>
</li>
<li>
<p><a href="#link16">STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers</a>
<strong>Authors:</strong> Donghwa Kang, Doohyun Kim, Sang-Ki Ko, Jinkyu Lee, Brent ByungHoon Kang, Hyeongboo Baek</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2508.14086" rel="nofollow">EEGDM: EEG Representation Learning via Generative Diffusion Model</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-eegdm-eeg-representation-learning-via-generative-diffusion-model-" class="anchor" aria-label="Permalink: 0. EEGDM: EEG Representation Learning via Generative Diffusion Model" href="#0-eegdm-eeg-representation-learning-via-generative-diffusion-model-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14086
<strong>Authors:</strong> Jia Hong Puah, Sim Kuan Goh, Ziwei Zhang, Zixuan Ye, Chow Khuen Chan, Kheng Seang Lim, Si Lei Fong, Kok Sin Woon</p>
<p><strong>Abstract:</strong> arXiv:2508.14086v1 Announce Type: new  Abstract: While electroencephalogram (EEG) has been a crucial tool for monitoring the brain and diagnosing neurological disorders (e.g., epilepsy), learning meaningful representations from raw EEG signals remains challenging due to limited annotations and high signal variability. Recently, EEG foundation models (FMs) have shown promising potential by adopting transformer architectures and self-supervised pre-training methods from large language models (e.g., masked prediction) to learn representations from diverse EEG data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large models often incurred high computational costs during both training and inference, with only marginal performance improvements as model size increases. In this work, we proposed EEG representation learning framework building upon Generative Diffusion Model (EEGDM). Specifically, we developed structured state-space model for diffusion pretraining (SSMDP) to better capture the temporal dynamics of EEG signals and trained the architecture using a Denoising Diffusion Probabilistic Model. The resulting latent EEG representations were then used for downstream classification tasks via our proposed latent fusion transformer (LFT). To evaluate our method, we used the multi-event Temple University EEG Event Corpus and compared EEGDM with current state-of-the-art approaches, including EEG FMs. Empirical results showed that our method outperformed existing methods while being approximately 19x more lightweight. These findings suggested that EEGDM offered a promising alternative to current FMs. Our code is available at: <a href="https://github.com/jhpuah/EEGDM">https://github.com/jhpuah/EEGDM</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2508.14140" rel="nofollow">Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-neuro-inspired-ensemble-to-ensemble-communication-primitives-for-sparse-and-efficient-anns-" class="anchor" aria-label="Permalink: 1. Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs" href="#1-neuro-inspired-ensemble-to-ensemble-communication-primitives-for-sparse-and-efficient-anns-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14140
<strong>Authors:</strong> Orestis Konstantaropoulos, Stelios Manolis Smirnakis, Maria Papadopouli</p>
<p><strong>Abstract:</strong> arXiv:2508.14140v1 Announce Type: new  Abstract: The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2508.14415" rel="nofollow">The Agent Behavior: Model, Governance and Challenges in the AI Digital Age</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-the-agent-behavior-model-governance-and-challenges-in-the-ai-digital-age-" class="anchor" aria-label="Permalink: 2. The Agent Behavior: Model, Governance and Challenges in the AI Digital Age" href="#2-the-agent-behavior-model-governance-and-challenges-in-the-ai-digital-age-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14415
<strong>Authors:</strong> Qiang Zhang, Pei Yan, Yijia Xu, Chuanpo Fu, Yong Fang, Yang Liu</p>
<p><strong>Abstract:</strong> arXiv:2508.14415v1 Announce Type: new  Abstract: Advancements in AI have led to agents in networked environments increasingly mirroring human behavior, thereby blurring the boundary between artificial and human actors in specific contexts. This shift brings about significant challenges in trust, responsibility, ethics, security and etc. The difficulty in supervising of agent behaviors may lead to issues such as data contamination and unclear accountability. To address these challenges, this paper proposes the "Network Behavior Lifecycle" model, which divides network behavior into 6 stages and systematically analyzes the behavioral differences between humans and agents at each stage. Based on these insights, the paper further introduces the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity (HABD)" model, which examine the fundamental distinctions between human and agent behaviors across 5 dimensions: decision mechanism, execution efficiency, intention-behavior consistency, behavioral inertia, and irrational patterns. The effectiveness of the model is verified through real-world cases such as red team penetration and blue team defense. Finally, the paper discusses future research directions in dynamic cognitive governance architecture, behavioral disparity quantification, and meta-governance protocol stacks, aiming to provide a theoretical foundation and technical roadmap for secure and trustworthy human-agent collaboration.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2508.14338" rel="nofollow">On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-on-the-interplay-between-graph-structure-and-learning-algorithms-in-graph-neural-networks-" class="anchor" aria-label="Permalink: 3. On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks" href="#3-on-the-interplay-between-graph-structure-and-learning-algorithms-in-graph-neural-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14338
<strong>Authors:</strong> Junwei Su, Chuan Wu</p>
<p><strong>Abstract:</strong> arXiv:2508.14338v1 Announce Type: new  Abstract: This paper studies the interplay between learning algorithms and graph structure for graph neural networks (GNNs). Existing theoretical studies on the learning dynamics of GNNs primarily focus on the convergence rates of learning algorithms under the interpolation regime (noise-free) and offer only a crude connection between these dynamics and the actual graph structure (e.g., maximum degree). This paper aims to bridge this gap by investigating the excessive risk (generalization performance) of learning algorithms in GNNs within the generalization regime (with noise). Specifically, we extend the conventional settings from the learning theory literature to the context of GNNs and examine how graph structure influences the performance of learning algorithms such as stochastic gradient descent (SGD) and Ridge regression. Our study makes several key contributions toward understanding the interplay between graph structure and learning in GNNs. First, we derive the excess risk profiles of SGD and Ridge regression in GNNs and connect these profiles to the graph structure through spectral graph theory. With this established framework, we further explore how different graph structures (regular vs. power-law) impact the performance of these algorithms through comparative analysis. Additionally, we extend our analysis to multi-layer linear GNNs, revealing an increasing non-isotropic effect on the excess risk profile, thereby offering new insights into the over-smoothing issue in GNNs from the perspective of learning algorithms. Our empirical results align with our theoretical predictions, \emph{collectively showcasing a coupling relation among graph structure, GNNs and learning algorithms, and providing insights on GNN algorithm design and selection in practice.}</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2508.14769" rel="nofollow">Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-federated-distillation-on-edge-devices-efficient-client-side-filtering-for-non-iid-data-" class="anchor" aria-label="Permalink: 4. Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data" href="#4-federated-distillation-on-edge-devices-efficient-client-side-filtering-for-non-iid-data-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14769
<strong>Authors:</strong> Ahmed Mujtaba, Gleb Radchenko, Radu Prodan, Marc Masana</p>
<p><strong>Abstract:</strong> arXiv:2508.14769v1 Announce Type: new  Abstract: Federated distillation has emerged as a promising collaborative machine learning approach, offering enhanced privacy protection and reduced communication compared to traditional federated learning by exchanging model outputs (soft logits) rather than full model parameters. However, existing methods employ complex selective knowledge-sharing strategies that require clients to identify in-distribution proxy data through computationally expensive statistical density ratio estimators. Additionally, server-side filtering of ambiguous knowledge introduces latency to the process. To address these challenges, we propose a robust, resource-efficient EdgeFD method that reduces the complexity of the client-side density ratio estimation and removes the need for server-side filtering. EdgeFD introduces an efficient KMeans-based density ratio estimator for effectively filtering both in-distribution and out-of-distribution proxy data on clients, significantly improving the quality of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios, including strong non-IID, weak non-IID, and IID data distributions on clients, without requiring a pre-trained teacher model on the server for knowledge distillation. Experimental results demonstrate that EdgeFD outperforms state-of-the-art methods, consistently achieving accuracy levels close to IID scenarios even under heterogeneous and challenging conditions. The significantly reduced computational overhead of the KMeans-based estimator is suitable for deployment on resource-constrained edge devices, thereby enhancing the scalability and real-world applicability of federated distillation. The code is available online for reproducibility.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2508.14801" rel="nofollow">A Guide for Manual Annotation of Scientific Imagery: How to Prepare for Large Projects</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-a-guide-for-manual-annotation-of-scientific-imagery-how-to-prepare-for-large-projects-" class="anchor" aria-label="Permalink: 5. A Guide for Manual Annotation of Scientific Imagery: How to Prepare for Large Projects" href="#5-a-guide-for-manual-annotation-of-scientific-imagery-how-to-prepare-for-large-projects-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14801
<strong>Authors:</strong> Azim Ahmadzadeh, Rohan Adhyapak, Armin Iraji, Kartik Chaurasiya, V Aparna, Petrus C. Martens</p>
<p><strong>Abstract:</strong> arXiv:2508.14801v1 Announce Type: new  Abstract: Despite the high demand for manually annotated image data, managing complex and costly annotation projects remains under-discussed. This is partly due to the fact that leading such projects requires dealing with a set of diverse and interconnected challenges which often fall outside the expertise of specific domain experts, leaving practical guidelines scarce. These challenges range widely from data collection to resource allocation and recruitment, from mitigation of biases to effective training of the annotators. This paper provides a domain-agnostic preparation guide for annotation projects, with a focus on scientific imagery. Drawing from the authors' extensive experience in managing a large manual annotation project, it addresses fundamental concepts including success measures, annotation subjects, project goals, data availability, and essential team roles. Additionally, it discusses various human biases and recommends tools and technologies to improve annotation quality and efficiency. The goal is to encourage further research and frameworks for creating a comprehensive knowledge base to reduce the costs of manual annotation projects across various fields.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2508.14734" rel="nofollow">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-afabench-a-generic-framework-for-benchmarking-active-feature-acquisition-" class="anchor" aria-label="Permalink: 6. AFABench: A Generic Framework for Benchmarking Active Feature Acquisition" href="#6-afabench-a-generic-framework-for-benchmarking-active-feature-acquisition-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14734
<strong>Authors:</strong> Valter Sch"utz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani</p>
<p><strong>Abstract:</strong> arXiv:2508.14734v1 Announce Type: new  Abstract: In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from greedy information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by the lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, greedy, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed to expose the limitations of greedy selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: <a href="https://github.com/Linusaronsson/AFA-Benchmark">https://github.com/Linusaronsson/AFA-Benchmark</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2508.14832" rel="nofollow">On Defining Neural Averaging</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-on-defining-neural-averaging-" class="anchor" aria-label="Permalink: 7. On Defining Neural Averaging" href="#7-on-defining-neural-averaging-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14832
<strong>Authors:</strong> Su Hyeong Lee, Richard Ngo</p>
<p><strong>Abstract:</strong> arXiv:2508.14832v1 Announce Type: new  Abstract: What does it even mean to average neural networks? We investigate the problem of synthesizing a single neural network from a collection of pretrained models, each trained on disjoint data shards, using only their final weights and no access to training data. In forming a definition of neural averaging, we take insight from model soup, which appears to aggregate multiple models into a singular model while enhancing generalization performance. In this work, we reinterpret model souping as a special case of a broader framework: Amortized Model Ensembling (AME) for neural averaging, a data-free meta-optimization approach that treats model differences as pseudogradients to guide neural weight updates. We show that this perspective not only recovers model soup but enables more expressive and adaptive ensembling strategies. Empirically, AME produces averaged neural solutions that outperform both individual experts and model soup baselines, especially in out-of-distribution settings. Our results suggest a principled and generalizable notion of data-free model weight aggregation and defines, in one sense, how to perform neural averaging.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2508.14441" rel="nofollow">FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-fbi-learning-dexterous-in-hand-manipulation-with-dynamic-visuotactile-shortcut-policy-" class="anchor" aria-label="Permalink: 8. FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy" href="#8-fbi-learning-dexterous-in-hand-manipulation-with-dynamic-visuotactile-shortcut-policy-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14441
<strong>Authors:</strong> Yijin Chen, Wenqiang Xu, Zhenjun Yu, Tutian Tang, Yutong Li, Siqiong Yao, Cewu Lu</p>
<p><strong>Abstract:</strong> arXiv:2508.14441v1 Announce Type: new  Abstract: Dexterous in-hand manipulation is a long-standing challenge in robotics due to complex contact dynamics and partial observability. While humans synergize vision and touch for such tasks, robotic approaches often prioritize one modality, therefore limiting adaptability. This paper introduces Flow Before Imitation (FBI), a visuotactile imitation learning framework that dynamically fuses tactile interactions with visual observations through motion dynamics. Unlike prior static fusion methods, FBI establishes a causal link between tactile signals and object motion via a dynamics-aware latent model. FBI employs a transformer-based interaction module to fuse flow-derived tactile features with visual inputs, training a one-step diffusion policy for real-time execution. Extensive experiments demonstrate that the proposed method outperforms the baseline methods in both simulation and the real world on two customized in-hand manipulation tasks and three standard dexterous manipulation tasks. Code, models, and more results are available in the website <a href="https://sites.google.com/view/dex-fbi" rel="nofollow">https://sites.google.com/view/dex-fbi</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2508.14751" rel="nofollow">HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-herakles-hierarchical-skill-compilation-for-open-ended-llm-agents-" class="anchor" aria-label="Permalink: 9. HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents" href="#9-herakles-hierarchical-skill-compilation-for-open-ended-llm-agents-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14751
<strong>Authors:</strong> Thomas Carta, Cl'ement Romac, Loris Gaven, Pierre-Yves Oudeyer, Olivier Sigaud, Sylvain Lamprier</p>
<p><strong>Abstract:</strong> arXiv:2508.14751v1 Announce Type: new  Abstract: Open-ended AI agents need to be able to learn efficiently goals of increasing complexity, abstraction and heterogeneity over their lifetime. Beyond sampling efficiently their own goals, autotelic agents specifically need to be able to keep the growing complexity of goals under control, limiting the associated growth in sample and computational complexity. To adress this challenge, recent approaches have leveraged hierarchical reinforcement learning (HRL) and language, capitalizing on its compositional and combinatorial generalization capabilities to acquire temporally extended reusable behaviours. Existing approaches use expert defined spaces of subgoals over which they instantiate a hierarchy, and often assume pre-trained associated low-level policies. Such designs are inadequate in open-ended scenarios, where goal spaces naturally diversify across a broad spectrum of difficulties. We introduce HERAKLES, a framework that enables a two-level hierarchical autotelic agent to continuously compile mastered goals into the low-level policy, executed by a small, fast neural network, dynamically expanding the set of subgoals available to the high-level policy. We train a Large Language Model (LLM) to serve as the high-level controller, exploiting its strengths in goal decomposition and generalization to operate effectively over this evolving subgoal space. We evaluate HERAKLES in the open-ended Crafter environment and show that it scales effectively with goal complexity, improves sample efficiency through skill compilation, and enables the agent to adapt robustly to novel challenges over time.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2508.14783" rel="nofollow">Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-synthetic-adaptive-guided-embeddings-sage-a-novel-knowledge-distillation-method-" class="anchor" aria-label="Permalink: 10. Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method" href="#10-synthetic-adaptive-guided-embeddings-sage-a-novel-knowledge-distillation-method-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14783
<strong>Authors:</strong> Suleyman Olcay Polat, Poli A. Nemkova, Mark V. Albert</p>
<p><strong>Abstract:</strong> arXiv:2508.14783v1 Announce Type: new  Abstract: Model distillation enables the transfer of knowledge from large-scale models to compact student models, facilitating deployment in resource-constrained environments. However, conventional distillation approaches often suffer from computational overhead and limited generalization. We propose a novel adaptive distillation framework that dynamically augments training data in regions of high student model loss. Using UMAP-based dimensionality reduction and nearest neighbor sampling, our method identifies underperforming regions in the embedding space and generates targeted synthetic examples to guide student learning. To further improve efficiency, we introduce a lightweight teacher-student interface that bypasses the teacher's input layer, enabling direct distillation on vectorized representations. Experiments across standard NLP benchmarks demonstrate that our 66M-parameter student model consistently matches or surpasses established baselines, achieving 91.2% on QNLI and 92.3% on SST-2, while training with fewer epochs. These results highlight the promise of loss-aware data augmentation and vectorized distillation for efficient and effective model compression.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2508.14746" rel="nofollow">MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-missionhd-data-driven-refinement-of-reasoning-graph-structure-through-hyperdimensional-causal-path-encoding-and-decoding-" class="anchor" aria-label="Permalink: 11. MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding" href="#11-missionhd-data-driven-refinement-of-reasoning-graph-structure-through-hyperdimensional-causal-path-encoding-and-decoding-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14746
<strong>Authors:</strong> Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Mohsen Imani</p>
<p><strong>Abstract:</strong> arXiv:2508.14746v1 Announce Type: new  Abstract: Reasoning graphs from Large Language Models (LLMs) are often misaligned with downstream visual tasks such as video anomaly detection (VAD). Existing Graph Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly optimizes graph structure using downstream task data, and propose MissionHD, a hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses an efficient encode-decode process to refine the graph, guided by the downstream task signal. Experiments on challenging VAD and VAR benchmarks show significant performance improvements when using our refined graphs, validating our approach as an effective pre-processing step.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2508.14302" rel="nofollow">GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-glass-test-time-acceleration-for-llms-via-global-local-neural-importance-aggregation-" class="anchor" aria-label="Permalink: 12. GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation" href="#12-glass-test-time-acceleration-for-llms-via-global-local-neural-importance-aggregation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14302
<strong>Authors:</strong> Amirmohsen Sattarifard, Sepehr Lavasani, Ehsan Imani, Kunlin Zhang, Hanlin Xu, Fengyu Sun, Negar Hassanpour, Chao Gao</p>
<p><strong>Abstract:</strong> arXiv:2508.14302v1 Announce Type: new  Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2508.14330" rel="nofollow">Multi-view Graph Condensation via Tensor Decomposition</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-multi-view-graph-condensation-via-tensor-decomposition-" class="anchor" aria-label="Permalink: 13. Multi-view Graph Condensation via Tensor Decomposition" href="#13-multi-view-graph-condensation-via-tensor-decomposition-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14330
<strong>Authors:</strong> N'icolas Roque dos Santos, Dawon Ahn, Diego Minatel, Alneu de Andrade Lopes, Evangelos E. Papalexakis</p>
<p><strong>Abstract:</strong> arXiv:2508.14330v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable results in various real-world applications, including drug discovery, object detection, social media analysis, recommender systems, and text classification. In contrast to their vast potential, training them on large-scale graphs presents significant computational challenges due to the resources required for their storage and processing. Graph Condensation has emerged as a promising solution to reduce these demands by learning a synthetic compact graph that preserves the essential information of the original one while maintaining the GNN's predictive performance. Despite their efficacy, current graph condensation approaches frequently rely on a computationally intensive bi-level optimization. Moreover, they fail to maintain a mapping between synthetic and original nodes, limiting the interpretability of the model's decisions. In this sense, a wide range of decomposition techniques have been applied to learn linear or multi-linear functions from graph data, offering a more transparent and less resource-intensive alternative. However, their applicability to graph condensation remains unexplored. This paper addresses this gap and proposes a novel method called Multi-view Graph Condensation via Tensor Decomposition (GCTD) to investigate to what extent such techniques can synthesize an informative smaller graph and achieve comparable downstream task performance. Extensive experiments on six real-world datasets demonstrate that GCTD effectively reduces graph size while preserving GNN performance, achieving up to a 4.0\ improvement in accuracy on three out of six datasets and competitive performance on large graphs compared to existing approaches. Our code is available at <a href="https://anonymous.4open.science/r/gctd-345A" rel="nofollow">https://anonymous.4open.science/r/gctd-345A</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2508.14385" rel="nofollow">Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-online-incident-response-planning-under-model-misspecification-through-bayesian-learning-and-belief-quantization-" class="anchor" aria-label="Permalink: 14. Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization" href="#14-online-incident-response-planning-under-model-misspecification-through-bayesian-learning-and-belief-quantization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14385
<strong>Authors:</strong> Kim Hammar, Tao Li</p>
<p><strong>Abstract:</strong> arXiv:2508.14385v1 Announce Type: new  Abstract: Effective responses to cyberattacks require fast decisions, even when information about the attack is incomplete or inaccurate. However, most decision-support frameworks for incident response rely on a detailed system model that describes the incident, which restricts their practical utility. In this paper, we address this limitation and present an online method for incident response planning under model misspecification, which we call MOBAL: Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture about the model through Bayesian learning as new information becomes available, which facilitates model adaptation as the incident unfolds. To determine effective responses online, we quantize the conjectured model into a finite Markov model, which enables efficient response planning through dynamic programming. We prove that Bayesian learning is asymptotically consistent with respect to the information feedback. Additionally, we establish bounds on misspecification and quantization errors. Experiments on the CAGE-2 benchmark show that MOBAL outperforms the state of the art in terms of adaptability and robustness to model misspecification.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2508.14748" rel="nofollow">Cross-Modality Controlled Molecule Generation with Diffusion Language Model</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-cross-modality-controlled-molecule-generation-with-diffusion-language-model-" class="anchor" aria-label="Permalink: 15. Cross-Modality Controlled Molecule Generation with Diffusion Language Model" href="#15-cross-modality-controlled-molecule-generation-with-diffusion-language-model-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14748
<strong>Authors:</strong> Yunzhe Zhang, Yifei Wang, Khanh Vinh Nguyen, Pengyu Hong</p>
<p><strong>Abstract:</strong> arXiv:2508.14748v1 Announce Type: new  Abstract: Current SMILES-based diffusion models for molecule generation typically support only unimodal constraint. They inject conditioning signals at the start of the training process and require retraining a new model from scratch whenever the constraint changes. However, real-world applications often involve multiple constraints across different modalities, and additional constraints may emerge over the course of a study. This raises a challenge: how to extend a pre-trained diffusion model not only to support cross-modality constraints but also to incorporate new ones without retraining. To tackle this problem, we propose the Cross-Modality Controlled Molecule Generation with Diffusion Language Model (CMCM-DLM), demonstrated by two distinct cross modalities: molecular structure and chemical properties. Our approach builds upon a pre-trained diffusion model, incorporating two trainable modules, the Structure Control Module (SCM) and the Property Control Module (PCM), and operates in two distinct phases during the generation process. In Phase I, we employs the SCM to inject structural constraints during the early diffusion steps, effectively anchoring the molecular backbone. Phase II builds on this by further introducing PCM to guide the later stages of inference to refine the generated molecules, ensuring their chemical properties match the specified targets. Experimental results on multiple datasets demonstrate the efficiency and adaptability of our approach, highlighting CMCM-DLM's significant advancement in molecular generation for drug discovery applications.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2508.14138" rel="nofollow">STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-stas-spatio-temporal-adaptive-computation-time-for-spiking-transformers-" class="anchor" aria-label="Permalink: 16. STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers" href="#16-stas-spatio-temporal-adaptive-computation-time-for-spiking-transformers-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14138
<strong>Authors:</strong> Donghwa Kang, Doohyun Kim, Sang-Ki Ko, Jinkyu Lee, Brent ByungHoon Kang, Hyeongboo Baek</p>
<p><strong>Abstract:</strong> arXiv:2508.14138v1 Announce Type: new  Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural networks (ANNs) but suffer from high latency and computational overhead due to their multi-timestep operational nature. While various dynamic computation methods have been developed to mitigate this by targeting spatial, temporal, or architecture-specific redundancies, they remain fragmented. While the principles of adaptive computation time (ACT) offer a robust foundation for a unified approach, its application to SNN-based vision Transformers (ViTs) is hindered by two core issues: the violation of its temporal similarity prerequisite and a static architecture fundamentally unsuited for its principles. To address these challenges, we propose STAS (Spatio-Temporal Adaptive computation time for Spiking transformers), a framework that co-designs the static architecture and dynamic computation policy. STAS introduces an integrated spike patch splitting (I-SPS) module to establish temporal stability by creating a unified input representation, thereby solving the architectural problem of temporal dissimilarity. This stability, in turn, allows our adaptive spiking self-attention (A-SSA) module to perform two-dimensional token pruning across both spatial and temporal axes. Implemented on spiking Transformer architectures and validated on CIFAR-10, CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%, and 30.1%, respectively, while simultaneously improving accuracy over SOTA models.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>