<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">RAP：面向大语言模型推理的运行时自适应剪枝技术</h2><a id="user-content-rap面向大语言模型推理的运行时自适应剪枝技术" class="anchor" aria-label="Permalink: RAP：面向大语言模型推理的运行时自适应剪枝技术" href="#rap面向大语言模型推理的运行时自适应剪枝技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"Runtime-Adaptive"译为"运行时自适应"符合计算机领域术语规范，强调系统在运行过程中动态调整的特性</li>
<li>"Pruning"保留专业术语"剪枝"，指神经网络中通过移除冗余参数优化模型的技术</li>
<li>增补"技术"二字使中文命名更完整，同时用"面向"替代"for"更符合中文技术文献表述习惯</li>
<li>"LLM"采用行业通用译法"大语言模型"，全称与缩写形式根据中文表达习惯选择保留）</li>
</ol>
<p>arXiv:2505.17138v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）在语言理解与生成方面表现卓越，但其巨大的计算和内存需求阻碍了实际部署。模型压缩为缓解这些限制提供了潜在解决方案。然而，现有方法大多依赖固定启发式规则，无法适应运行时内存波动或多样化用户请求导致的异构KV缓存需求。为突破这些局限，我们提出RAP——一个由强化学习（RL）驱动的弹性剪枝框架，能够以运行时感知的方式动态调整压缩策略。具体而言，RAP在实际执行过程中动态追踪模型参数与KV缓存比值的演变规律。鉴于前馈网络（FFNs）承载大部分参数，而参数较少的注意力层主导KV缓存构成，RL智能体仅保留在当前内存预算下能最大化效用的组件，其决策基于瞬时工作负载和设备状态。大量实验结果表明，RAP优于现有最先进基线方法，首次实现了模型权重与KV缓存的实时联合优化。</p>
<p>（注：根据学术论文摘要的文体特征，翻译时注重以下处理：</p>
<ol>
<li>技术术语标准化："KV-cache"统一译为"KV缓存"，"FFNs"译为专业术语"前馈网络"</li>
<li>长句拆分：将原文复合长句按中文表达习惯分解为多个短句</li>
<li>逻辑显化：如"conditioned on..."译为"其决策基于..."以明确因果关系</li>
<li>动态表述："on the fly"译为"实时"而非字面直译</li>
<li>学术用语："state-of-the-art"规范译为"最先进的"）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">TrimR：基于验证器的免训练思维压缩技术，助力高效测试时扩展</h2><a id="user-content-trimr基于验证器的免训练思维压缩技术助力高效测试时扩展" class="anchor" aria-label="Permalink: TrimR：基于验证器的免训练思维压缩技术，助力高效测试时扩展" href="#trimr基于验证器的免训练思维压缩技术助力高效测试时扩展"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17155v1 公告类型：新研究<br>
摘要：大型推理模型（LRMs）通过扩展的思维链（CoT）推理，在解决复杂数学、逻辑和编程任务方面展现出卓越能力。测试时扩展方法（如通过显式的令牌级探索延长CoT）虽能突破LRMs的准确率边界，但会带来显著的解码开销。一个关键的低效根源在于LRMs常生成冗余的思维链，这些链条呈现出明显的结构化过度思考与思考不足模式。受人类认知推理过程与数值优化理论启发，我们提出TrimR——一个基于验证器的、免训练的、高效的动态CoT压缩框架，通过修剪推理过程来增强测试时扩展能力，专为生产级部署设计。该方法采用轻量级预训练指令微调验证器，无需对LRM或验证器进行微调即可检测并截断LRMs的冗余中间思维。我们同时提出了面向高吞吐工业应用的核心算法与异步在线系统。在昇腾NPU和vLLM上的实证评估表明，该框架在大批量工作负载下显著提升推理效率。具体而言，在MATH500、AIME24、AIME25和GPQA四个基准测试中，盘古R-38B、QwQ-32B和DeepSeek-R1-Distill-Qwen-32B的推理运行时间最高缩短70%，且对准确率影响可忽略。</p>
<p>（注：根据学术文献翻译规范，技术术语采用如下处理：</p>
<ol>
<li>"Chain-of-Thought (CoT)" 保留英文缩写但首译全称"思维链"</li>
<li>模型名称如"Pangu-R-38B"等保持原貌</li>
<li>"NPUs"译为"NPU"（神经处理单元），"vLLM"作为专有名词保留</li>
<li>机构名"Ascend"按华为官方译法保留"昇腾"</li>
<li>技术概念"fine-tuning"译为"微调"，"throughput"译为"吞吐量"</li>
<li>长难句按中文表达习惯拆分重组，如将"which demonstrate..."从句转为独立短句）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">斑马-骆驼：迈向极致高效的混合模型</h2><a id="user-content-斑马-骆驼迈向极致高效的混合模型" class="anchor" aria-label="Permalink: 斑马-骆驼：迈向极致高效的混合模型" href="#斑马-骆驼迈向极致高效的混合模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17272v1 公告类型：新研究<br>
摘要：随着大规模语言模型（LLMs）在各领域应用需求的激增，提升其推理效率对实现可持续发展和普惠化访问至关重要。然而，针对用户特定需求重新训练LLMs成本极其高昂且不符合环保理念。本研究提出了一种实用且可扩展的替代方案：通过组合现有预训练模型构建高效混合语言模型。我们的Zebra-Llama方法通过融合状态空间模型（SSMs）与多头潜在注意力（MLA）层，配合精炼的初始化及后训练流程，构建了1B、3B和8B参数的混合模型系列，可高效迁移预训练Transformer的知识。Zebra-Llama仅需7-11B训练token（相比预训练所需的万亿级token）和8B教师模型，即能达到Transformer级精度与接近SSM的效率。更突出的是，该模型将KV缓存压缩至原始大小的3.9%（1B）、2%（3B）和2.73%（8B），同时在LM Harness基准测试中保持100%（1B/3B）和&gt;97%（8B）的平均零样本性能。相较于MambaInLLaMA、X-EcoMLA、Minitron及Llamba等模型，Zebra-Llama在显著减少训练token、缩小教师模型规模、大幅降低KV缓存内存的条件下，始终提供竞争力相当或更优的准确率。尤为值得注意的是，Zebra-Llama-8B仅用八分之一训练token、超过12倍小的KV缓存及更小的教师模型（8B vs 15B），其少样本准确率反超Minitron-8B达7%。在32k上下文长度内，其吞吐量（token/秒）更是MambaInLlama的2.6-3.8倍。我们将在论文录用后公开代码与模型检查点。</p>
<p>（注：根据学术文献翻译规范，对技术术语保持统一，如"zero-shot performance"译为"零样本性能"；对数量级表述采用中文习惯单位如"万亿级"；长难句按中文表达习惯拆分重组；专业缩写首次出现时标注全称）</p>
<div class="markdown-heading"><h2 class="heading-element">神经网络与数据集的高效压缩</h2><a id="user-content-神经网络与数据集的高效压缩" class="anchor" aria-label="Permalink: 神经网络与数据集的高效压缩" href="#神经网络与数据集的高效压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17469v1 公告类型：新研究<br>
摘要：我们比较、改进并提出了一系列方法，这些方法能在保持较高测试精度的同时大幅减少神经网络参数量。当应用这些方法最小化描述长度时，我们获得了非常高效的数据压缩算法。具体而言：</p>
<ol>
<li>针对非线性模型的$\ell_0$正则化优化问题，我们提出了无需蒙特卡洛采样的概率化重构方案，相比现有方法更具优势；</li>
<li>改进了基于$\ell_0$范数平滑逼近的方法；</li>
<li>探索了分层优化策略。</li>
</ol>
<p>我们在不同架构和数据集上进行了全面测试，包括图像数据集训练的卷积网络和维基百科数据训练的Transformer模型。为系统研究连续场景下的压缩特性，还专门构建了合成师生模型实验框架。</p>
<p>最后，我们从理论上建立了压缩算法与所罗门诺夫归纳推理理论的关联，并通过实验验证了正则化模型具有更高样本效率的预测结论。</p>
<div class="markdown-heading"><h2 class="heading-element">预训练专家剪枝与检索：在有限内存条件下轻量化专家混合模型</h2><a id="user-content-预训练专家剪枝与检索在有限内存条件下轻量化专家混合模型" class="anchor" aria-label="Permalink: 预训练专家剪枝与检索：在有限内存条件下轻量化专家混合模型" href="#预训练专家剪枝与检索在有限内存条件下轻量化专家混合模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17639v1 公告类型：新研究<br>
摘要：混合专家（MoE）架构能够将大语言模型（LLMs）的参数规模扩展至巨大数量，而无需按比例增加计算成本。然而，大型MoE模型对内存的高需求阻碍了其在各类计算环境（从云服务器到消费级设备）中的部署。本研究首先揭示了MoE层中专家激活模式存在显著的任务特异性。基于此发现，我们提出了PreMoe框架，为内存受限环境下的超大规模MoE模型部署提供了高效解决方案。</p>
<p>PreMoe包含两大核心组件：概率性专家剪枝（PEP）与任务自适应专家检索（TAER）。PEP采用一种名为"任务条件期望选择分数"（TCESS）的新度量标准，该指标通过路由器逻辑值量化专家对特定任务的重要性，从而识别出最关键的专家子集。TAER则利用这些任务导向的专家重要性特征实现高效推理：预先计算并存储不同任务的紧凑专家模式，当收到用户查询时，TAER快速匹配最相关的存储任务模式，仅加载该任务所需的关键专家来重构模型。</p>
<p>该方法显著降低了各类部署场景下的内存占用：DeepSeek-R1 671B模型在8/128配置（专家数减少50%）下保持MATH500数据集97.2%的准确率，即使采用激进的8/32剪枝（专家数减少87.5%）仍能达到72.0%；Pangu-Ultra-MoE 718B模型经8/128剪枝后，在MATH500和AIME24上分别取得97.15%和81.3%的准确率，而进一步剪枝至4/64配置（内存占用390GB）时仍保持MATH500 96.95%的准确率。相关代码已开源：<a href="https://github.com/JarvisPei/PreMoe%E3%80%82">https://github.com/JarvisPei/PreMoe。</a></p>
<p>（注：专业术语处理说明：</p>
<ol>
<li>"router logits"译为"路由器逻辑值"以保持技术文档一致性</li>
<li>"memory footprint"采用"内存占用"这一通用译法</li>
<li>模型名称DeepSeek/Pangu-Ultra保留原名体现技术品牌</li>
<li>数学表达如"8/128"保留原格式确保技术准确性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">在嵌入式FPGA上利用微型Transformer实现多功能时间序列分析的自动化</h2><a id="user-content-在嵌入式fpga上利用微型transformer实现多功能时间序列分析的自动化" class="anchor" aria-label="Permalink: 在嵌入式FPGA上利用微型Transformer实现多功能时间序列分析的自动化" href="#在嵌入式fpga上利用微型transformer实现多功能时间序列分析的自动化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17662v1 公告类型：新研究<br>
摘要：基于Transformer的模型在多元时间序列任务中展现出强劲性能，但其在资源受限设备上的部署仍面临高内存与计算需求的挑战。尽管先前针对微控制器单元（MCU）的研究探索了硬件专用优化方案，这类方法往往局限于特定任务且仅支持8位定点精度。现场可编程门阵列（FPGA）提供了更高灵活性，能实现对数据精度和架构的细粒度控制。然而现有基于FPGA的时间序列Transformer部署方案通常聚焦于需手动配置的高密度平台。本文提出一种面向嵌入式FPGA的Tiny Transformer统一全自动部署框架，支持在预测、分类和异常检测三类代表性时间序列任务中部署精简的仅编码器架构。该框架融合量化感知训练（最低至4位）、基于Optuna的硬件感知超参数搜索，以及自动VHDL生成技术实现无缝部署。我们在两种嵌入式FPGA平台上对六个公共数据集进行评估，结果表明：该框架生成的纯整数型任务专用Transformer加速器在AMD Spartan-7平台上单次推理能耗低至0.033毫焦，延迟达毫秒级，同时为Lattice iCE40平台的部署可行性提供了实践洞见。全部源代码将在GitHub仓库发布（<a href="https://github.com/Edwina1030/TinyTransformer4TS%EF%BC%89%E3%80%82">https://github.com/Edwina1030/TinyTransformer4TS）。</a></p>
<p>（注：根据学术文本翻译规范，对以下术语进行了标准化处理：</p>
<ol>
<li>"resource-constrained devices"译为"资源受限设备"</li>
<li>"fine-grained control"译为"细粒度控制"</li>
<li>"quantization-aware training"保留专业术语特征译为"量化感知训练"</li>
<li>"millisecond latency"译为"毫秒级延迟"以符合中文科技文献表述习惯</li>
<li>长难句采用拆分策略，如将原文最后一句拆分为结果陈述与补充说明两个分句）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">ECHO-LLaMA：高效缓存赋能高性能LLaMA训练</h2><a id="user-content-echo-llama高效缓存赋能高性能llama训练" class="anchor" aria-label="Permalink: ECHO-LLaMA：高效缓存赋能高性能LLaMA训练" href="#echo-llama高效缓存赋能高性能llama训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译时做了以下处理：</p>
<ol>
<li>保留专业术语"LLaMA"不译，符合技术文献惯例</li>
<li>"Efficient Caching"译为"高效缓存"，突出技术特性</li>
<li>"High-Performance"译为"高性能"，符合中文科技表达</li>
<li>使用"赋能"替代直译"for"，更符合中文技术场景的动宾结构</li>
<li>整体采用"主标题+副标题"结构，通过冒号分隔保持原格式</li>
<li>使用四字格"高效缓存"与"高性能"形成对仗，增强专业性）</li>
</ol>
<p>arXiv:2505.17331v1 公告类型：新研究<br>
摘要：本文提出ECHO-LLaMA，一种高效LLaMA架构，旨在提升LLaMA系列模型的训练速度与推理吞吐量，同时保持其学习能力。该架构通过将特定层转换为共享KV缓存机制，显著降低了键值计算复杂度，且语言性能未受损失甚至有所提升。实验结果表明，ECHO-LLaMA在训练阶段每秒可处理token数量最高提升77%，模型浮点运算利用率（MFU）最高提升16%，在同等token训练量下损失值降低达14%。对于11亿参数模型，其推理吞吐量较基线模型提升约7%。通过引入计算高效的适配机制，ECHO-LLaMA为大语言模型的预训练与微调提供了可扩展且经济高效的解决方案，在保持性能的前提下实现更快速、更资源优化的训练过程。</p>
<div class="markdown-heading"><h2 class="heading-element">NeUQI：近最优均匀量化参数初始化</h2><a id="user-content-neuqi近最优均匀量化参数初始化" class="anchor" aria-label="Permalink: NeUQI：近最优均匀量化参数初始化" href="#neuqi近最优均匀量化参数初始化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17595v1 公告类型：新研究<br>
摘要：大型语言模型（LLMs）在各领域展现出卓越性能，但在消费级GPU或个人设备（如笔记本电脑）上部署时，因高内存消耗和推理成本面临重大挑战。后训练量化（PTQ）技术为降低LLMs内存占用和解码延迟提供了可行方案。实践中，采用均匀量化表示的PTQ因其高效性和易部署性备受青睐——主流硬件和软件库普遍支持均匀量化。近期关于≥2位均匀量化的研究显著提升了量化后模型性能，但这些研究主要聚焦量化方法本身，而量化参数初始化策略尚未充分探索，仍依赖次优的Min-Max方法。</p>
<p>本研究提出NeUQI方法，致力于高效确定均匀量化的近最优初始参数。NeUQI与现有量化方法正交，可无缝集成。在LLaMA和Qwen系列模型上的多任务实验表明，NeUQI始终优于现有方法。此外，结合轻量级蒸馏策略时，NeUQI能达到比资源密集型方法PV-tuning更优的性能。</p>
<p>（注：根据学术文献翻译规范，保留技术术语如"PTQ"、"Min-Max"等专业缩写；"orthogonal"译为"正交"以保持数学隐喻；"seamlessly integrate"译为"无缝集成"符合中文技术表达习惯；被动语态如"is favored"转化为主动句式"备受青睐"以符合中文表达逻辑。）</p>
<div class="markdown-heading"><h2 class="heading-element">广义费舍尔加权奇异值分解：面向大规模语言模型压缩的可扩展克罗内克积费舍尔近似方法</h2><a id="user-content-广义费舍尔加权奇异值分解面向大规模语言模型压缩的可扩展克罗内克积费舍尔近似方法" class="anchor" aria-label="Permalink: 广义费舍尔加权奇异值分解：面向大规模语言模型压缩的可扩展克罗内克积费舍尔近似方法" href="#广义费舍尔加权奇异值分解面向大规模语言模型压缩的可扩展克罗内克积费舍尔近似方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"Generalized Fisher-Weighted SVD"译为"广义费舍尔加权奇异值分解"，完整保留专业术语特征</li>
<li>"Scalable"译为"可扩展的"，突出算法规模适应性</li>
<li>"Kronecker-Factored Fisher Approximation"译为"克罗内克积费舍尔近似"，准确翻译数学概念"Kronecker积"</li>
<li>副标题采用"面向...的..."句式，符合中文技术文献表述习惯</li>
<li>"Large Language Models"统一译为"大规模语言模型"，与国内人工智能领域术语保持一致</li>
<li>整体采用学术论文标题的简洁风格，通过冒号分隔主副标题，保持中英文标题结构一致性）</li>
</ol>
<p>arXiv:2505.17974v1 公告类型：新研究<br>
摘要：费舍尔信息是表征神经网络参数敏感性的基本概念。然而，对于大型模型而言，利用完整的观测费舍尔信息计算成本过高，因此大多数方法依赖于简单的对角近似。虽然高效，但这种方法忽略了参数相关性，往往导致下游任务性能下降。在本研究中，我们通过提出广义费舍尔加权奇异值分解（GFWSVD）来缓解这些局限性——这是一种考虑费舍尔信息矩阵对角与非对角元素的后训练大语言模型压缩技术，能更准确地反映参数重要性。为实现该方法的高效计算，我们针对观测费舍尔信息提出了一种可扩展的克罗内克分解近似算法。我们在LLM压缩任务中验证了方法的有效性：在MMLU基准测试20倍压缩率下，本方法比基于费舍尔信息对角近似的FWSVD提升5%，较SVD-LLM提升3%，较ASVD提升6%压缩率。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"Fisher information"统一译为"费舍尔信息"（统计学标准译名）</li>
<li>"Kronecker-factored approximation"译为"克罗内克分解近似"（矩阵运算领域通用译法）</li>
<li>保持"LLM"（大语言模型）、"SVD"（奇异值分解）等缩写形式与原文一致</li>
<li>对长句进行合理切分，如将原文最后一句的多个比较数据整合为符合中文表达习惯的排比结构）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">无SVD的低秩自适应梯度优化在大型语言模型中的应用</h2><a id="user-content-无svd的低秩自适应梯度优化在大型语言模型中的应用" class="anchor" aria-label="Permalink: 无SVD的低秩自适应梯度优化在大型语言模型中的应用" href="#无svd的低秩自适应梯度优化在大型语言模型中的应用"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17967v1 公告类型：新研究<br>
摘要：低秩优化已成为训练大语言模型（LLMs）的重要方向，其通过将学习过程约束到低维空间来减少自适应优化器的内存占用。现有工作通常采用基于奇异值分解（SVD）的方法对线性层梯度进行投影。然而，对大型模型中每个层单独应用SVD流程计算成本高昂，且因需存储投影矩阵而产生额外内存开销。本研究提出一种计算高效、概念简洁的双步骤方法，用于近似实现基于SVD的低维梯度投影：首先，我们利用离散余弦变换（DCT）的预定义正交矩阵构建完整正交基；其次，根据各层梯度与基向量的对齐程度自适应选择基列。本方法的每个投影矩阵仅需一次矩阵乘法运算和轻量级排序步骤即可确定最相关基向量。由于正交基的预定义特性，其仅在训练开始时计算一次。训练过程中，我们仅存储所选列的索引，无需为每个层保存完整投影矩阵。在预训练和微调任务上的数值实验表明，这种双重策略能有效逼近最优低秩投影，其性能与高成本的SVD方法相当，同时实现了更快的运行速度和更低的内存消耗。</p>
<div class="markdown-heading"><h2 class="heading-element">VeriThinker：学会验证让推理模型更高效</h2><a id="user-content-verithinker学会验证让推理模型更高效" class="anchor" aria-label="Permalink: VeriThinker：学会验证让推理模型更高效" href="#verithinker学会验证让推理模型更高效"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.17941v1 公告类型：新论文<br>
摘要：大型推理模型（LRMs）通过思维链（CoT）推理在复杂任务中表现出色，但其过度思考倾向会导致不必要的冗长推理链条，显著增加推理成本。为缓解这一问题，我们提出VeriThinker——一种创新的CoT压缩方法。与传统方法（使用合成的简洁CoT数据直接在原始推理任务上微调LRMs）不同，我们创新性地仅通过辅助验证任务来微调模型。通过训练LRMs准确验证CoT解决方案的正确性，模型会从根本上提高对后续自省步骤必要性的判断力，从而有效抑制过度思考。大量实验证明，VeriThinker在保持甚至略微提升准确率的同时，大幅缩短了推理链长度。当应用于DeepSeek-R1-Distill-Qwen-7B模型时，我们的方法将MATH500数据集的推理token从3790减少到2125，同时准确率提升0.8%（从94.0%升至94.8%）；在AIME25数据集上，token从14321降至10287，准确率提升2.1%（从38.7%升至40.8%）。此外，实验表明VeriThinker也能零样本泛化至推测性推理。代码已开源：<a href="https://github.com/czg1225/VeriThinker">https://github.com/czg1225/VeriThinker</a></p>
<p>（注：根据学术规范，技术术语如"zero-shot"保留英文形式更符合中文计算机领域表述习惯；模型名称DeepSeek-R1-Distill-Qwen-7B采用原名以保持可追溯性；数据集名称MATH500/AIME25未采用中文译名以避免歧义）</p>
<div class="markdown-heading"><h2 class="heading-element">闪铸科技：面向大型语言模型解码的超高效前缀感知注意力机制</h2><a id="user-content-闪铸科技面向大型语言模型解码的超高效前缀感知注意力机制" class="anchor" aria-label="Permalink: 闪铸科技：面向大型语言模型解码的超高效前缀感知注意力机制" href="#闪铸科技面向大型语言模型解码的超高效前缀感知注意力机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：根据技术翻译的惯例，"FlashForge"作为专有名词保留不译；"Ultra-Efficient Prefix-Aware Attention"采用"超高效前缀感知注意力机制"的专业译法，其中"前缀感知"对应"Prefix-Aware"这一注意力机制特性；"LLM Decoding"译为"大型语言模型解码"，LLM作为行业通用缩写首次出现时采用全称。整体翻译在保持专业性的同时，通过"面向..."的句式体现技术方案的应用方向。）</p>
<p>arXiv:2505.17694v1 公告类型：新研究<br>
摘要：多提示间的共享前缀为合并共享前缀操作提供了可能，而解码阶段的注意力计算随着上下文长度增加成为关键瓶颈，这一内存密集型过程需要对前缀的键值（KV）缓存进行大量内存访问。为此，本文探索了共享前缀在解码阶段注意力计算中的潜力。然而，前缀共享的树状结构为注意力计算带来了重大挑战：需高效处理共享KV缓存的访问模式，同时管理复杂依赖关系并平衡不规则工作负载。</p>
<p>为解决上述挑战，我们提出了一种专用于合并解码阶段共享前缀内存访问的注意力计算内核——FlashForge。该方案包含两大创新：一是新型共享前缀注意力内核，通过优化内存层次结构同时利用块内与块间并行性；二是全面的负载均衡机制，能高效估算计算成本、划分任务并调度执行。实验结果表明，在解码阶段注意力计算方面，FlashForge较当前最先进的FlashDecoding内核平均实现1.9倍加速和120.9倍内存访问减少；与vLLM相比，每个输出token的端到端时间缩短3.8倍。</p>
<p>（注：根据学术论文翻译规范，对部分术语进行了标准化处理："key-value (KV) cache"统一译为"键值（KV）缓存"，"kernel"在计算机领域译为"内核"，"workload balancing"译为"负载均衡"。同时采用中文科技论文常用的四字结构如"内存密集型""树状结构"等，确保专业性与可读性平衡。）</p>
<div class="markdown-heading"><h2 class="heading-element">倒计时：上下文稀疏激活在向下投影中过滤不必要权重</h2><a id="user-content-倒计时上下文稀疏激活在向下投影中过滤不必要权重" class="anchor" aria-label="Permalink: 倒计时：上下文稀疏激活在向下投影中过滤不必要权重" href="#倒计时上下文稀疏激活在向下投影中过滤不必要权重"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此处采用技术术语的常见译法，并兼顾中文表达习惯：</p>
<ol>
<li>"Countdown" 译为"倒计时"保留原意象</li>
<li>"Contextually Sparse Activation" 译为"上下文稀疏激活"，准确传达神经网络中稀疏激活的概念</li>
<li>"Filtering Out Unnecessary Weights" 意译为"过滤不必要权重"，突出计算效率优化</li>
<li>"Down Projection" 译为"向下投影"，符合深度学习领域对降维操作的通用译法
整体采用"主标题+副标题"结构，既保持技术文档的严谨性，又通过冒号分层实现语义清晰）</li>
</ol>
<p>arXiv:2505.17701v1 公告类型：新论文<br>
摘要：大型语言模型规模的持续增长导致了显著的算力低效问题。为应对这一挑战，稀疏激活方法在推理过程中选择性关闭非必要参数，从而降低前馈神经网络（FFNN）层的计算成本。现有方法主要关注非线性门控机制，而我们提出假设：FFNN层的稀疏性本质上体现为其内部降维矩阵的线性组合形式。基于这一洞见，我们提出两种创新方案：利用间接系数的M-COUNTDOWN，以及采用直接线性组合系数的D-COUNTDOWN。实验结果表明，D-COUNTDOWN在理想情况下可省略90%计算量而性能损失仅5.5%；M-COUNTDOWN则无需预测模块即可实现比现有方法最高29.4%的性能保留优势。我们专门设计的核函数实现成功将这些理论增益转化为显著的现实加速效果。</p>
<p>（注：根据学术论文摘要的文体特点，翻译时进行了以下处理：</p>
<ol>
<li>专业术语统一："FFNN layers"规范译为"前馈神经网络层"</li>
<li>被动语态转换：将英文被动式调整为中文主动表述（如"has created"译为"导致了"）</li>
<li>长句拆分：将复合长句分解为符合中文阅读习惯的短句结构</li>
<li>概念显化："kernel implementations"具体化为"核函数实现"</li>
<li>数据呈现：精确保留百分比数值和比较关系</li>
<li>术语一致性："sparse activation methods"统一译为"稀疏激活方法"）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>