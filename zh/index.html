<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">任务-电路量化：利用知识定位与可解释性实现压缩</h2><a id="user-content-任务-电路量化利用知识定位与可解释性实现压缩" class="anchor" aria-label="Permalink: 任务-电路量化：利用知识定位与可解释性实现压缩" href="#任务-电路量化利用知识定位与可解释性实现压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.07389v1 公告类型：新研究<br>
摘要：训练后量化（PTQ）通过将全精度权重映射为低位宽权重来减少模型内存占用，且无需昂贵的重新训练，但在2至3比特的低位宽设置下可能显著降低模型下游性能。我们提出了一种新型混合精度PTQ方法——任务电路量化（TaCQ），其灵感源自自动电路发现技术，直接将量化过程与特定权重电路（定义为与下游任务性能相关的权重集合）关联。这些关键权重保持16位精度，其余权重则被量化，在仅增加边际内存成本的同时维持性能。具体而言，TaCQ通过对比未量化模型与均匀量化模型的权重变化，结合梯度信息预测量化对任务性能的影响，从而精准保留任务关键权重。我们将TaCQ与现有混合精度量化方法在通用数据和任务特定数据条件下进行对比。在Llama-3和Qwen2.5模型的问答、数学推理及文本转SQL任务中，TaCQ在相同校准数据和更低权重预算下均优于基线方法，尤其在2至3比特区间实现显著提升：仅用3.1比特即可恢复Llama-3-8B-Instruct模型16位精度下96%的MMLU性能，较SPQR提升5.25%绝对值；在2比特区间平均领先最强基线SliM-LLM达14.74%。值得注意的是，即使在非任务特定条件下，TaCQ仍实现7.20%的性能提升，证明其权重重要性识别能力具有普适性。</p>
<div class="markdown-heading"><h2 class="heading-element">Pychop：在数值方法与神经网络中模拟低精度算术运算</h2><a id="user-content-pychop在数值方法与神经网络中模拟低精度算术运算" class="anchor" aria-label="Permalink: Pychop：在数值方法与神经网络中模拟低精度算术运算" href="#pychop在数值方法与神经网络中模拟低精度算术运算"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.07835v1 公告类型：新研究<br>
摘要：受计算科学领域对低精度算术日益增长的需求启发，我们在Python中实现了低精度仿真技术——Python被公认为数值分析与机器学习领域的主导编程语言。低精度训练通过实现更高效的计算、降低内存与能耗消耗，同时保持模型保真度，已然彻底改变了深度学习领域。为更好地支持低精度计算的数值实验与探索，我们开发了Pychop库，该库支持可定制的浮点数格式及完整的舍入模式集合，使用户能在众多应用中享受快速低精度仿真带来的优势。Pychop还创新性地提供了PyTorch和JAX的接口，可在GPU上实现高效的神经网络低精度训练与推理，其灵活性无与伦比。</p>
<p>本文全面阐述了Pychop的设计原理、实现方法、验证流程与实际应用，将其确立为推进高效混合精度算法的基础工具。此外，我们基于公开数据集展示了图像分类与目标检测任务的低精度仿真实证结果，揭示了低精度使用的敏感阈值，并提供了关于其影响的宝贵洞见。Pychop支持深入探究数值精度的影响效应，助力新型硬件加速器的开发，并能无缝集成到现有深度学习工作流中。软件与实验代码已开源：<a href="https://github.com/inEXASCALE/pychop%E3%80%82">https://github.com/inEXASCALE/pychop。</a></p>
<p>（注：根据学术论文翻译规范，保留专业术语如"arXiv"、"PyTorch"、"JAX"等原名；"low-precision emulation"译为"低精度仿真"以体现计算机领域术语特征；长句按中文习惯拆分为短句；被动语态转换为主动表述；URL保留原格式确保可操作性）</p>
<div class="markdown-heading"><h2 class="heading-element">Apt-Serve：面向可扩展大模型推理服务的混合缓存自适应请求调度系统</h2><a id="user-content-apt-serve面向可扩展大模型推理服务的混合缓存自适应请求调度系统" class="anchor" aria-label="Permalink: Apt-Serve：面向可扩展大模型推理服务的混合缓存自适应请求调度系统" href="#apt-serve面向可扩展大模型推理服务的混合缓存自适应请求调度系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"Apt-Serve"作为专有技术名词保留不译，体现技术品牌性</li>
<li>"Adaptive"译为"自适应"准确传达系统智能调节特性</li>
<li>"Hybrid Cache"译为"混合缓存"符合计算机领域术语规范</li>
<li>"Scalable LLM Inference Serving"采用"可扩展大模型推理服务"的译法：
<ul>
<li>"Scalable"译为"可扩展"是技术领域标准译法</li>
<li>"LLM"译为"大语言模型"并简化为"大模型"符合中文技术文档惯例</li>
<li>"Inference Serving"译为"推理服务"准确表达模型部署场景</li>
</ul>
</li>
<li>整体采用"系统"作为补充说明词，使中文表述更完整</li>
<li>通过"面向...的"句式构建技术方案与应用场景的逻辑关系，比直译更符合中文技术文档表达习惯）</li>
</ol>
<p>arXiv:2504.07494v1 公告类型：新研究<br>
摘要：大语言模型（LLM）推理服务系统是各类LLM应用的核心支撑。随着LLM服务需求持续增长，如何在满足延迟服务级别目标（SLO）的前提下扩展系统以处理高请求率（即提升有效吞吐量）成为关键挑战。现有系统往往难以提升有效吞吐量，主要归因于首令牌生成时间（TTFT）的SLO达标率显著下降。我们揭示了造成这一瓶颈的两大根源：（1）受GPU内存限制的内存密集型KV缓存阻碍了批处理规模扩展；（2）默认先到先服务调度策略导致的僵化批处理组合。本文提出Apt-Serve——一个专为提升LLM推理服务有效吞吐量设计的可扩展框架。该框架创新性地采用混合缓存方案，将KV缓存与内存高效的可复用输入隐藏状态向量缓存相结合，从而支持更大批处理规模并提升请求并发能力。基于此混合缓存，Apt-Serve搭载自适应运行时调度机制，动态优化批处理组合。我们形式化定义了自适应调度优化问题，并提出具有理论保证的高效算法。在三个真实数据集和13B至66B参数规模的LLM上的广泛实验表明，相较于最先进的推理服务系统，Apt-Serve可实现最高8.8倍的有效吞吐量提升。</p>
<p>（注：根据学术文献翻译规范，对以下术语进行了专业处理：</p>
<ol>
<li>"Service-Level Objectives (SLOs)" 译为"服务级别目标"</li>
<li>"Time To First Token (TTFT)" 译为"首令牌生成时间"</li>
<li>"KV cache" 保留技术缩写"KV缓存"</li>
<li>"hidden cache" 意译为"隐藏状态向量缓存"以保持技术准确性</li>
<li>"theoretical guarantees" 译为"理论保证"符合计算机领域表述习惯）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>