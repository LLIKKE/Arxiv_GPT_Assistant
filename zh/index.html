<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">PCDVQ：基于极坐标解耦增强大语言模型的向量量化技术</h2><a id="user-content-pcdvq基于极坐标解耦增强大语言模型的向量量化技术" class="anchor" aria-label="Permalink: PCDVQ：基于极坐标解耦增强大语言模型的向量量化技术" href="#pcdvq基于极坐标解耦增强大语言模型的向量量化技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.05432v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）因其庞大的参数量，在边缘设备部署上面临重大挑战。向量量化（VQ）作为一种基于聚类的量化方法，凭借其极低位宽（可低至2比特）和保持较高精度的特性，成为解决该问题的主流方案。由于向量在数学和物理学中是兼具方向与大小的量，现有VQ工作通常对二者进行耦合式量化。但我们发现，方向对量化的敏感度远高于大小——例如在LLaMA-2-7B模型中对权重向量方向和大小分别聚类时，零样本任务准确率分别下降46.5%和2.3%，且该差距随聚类中心减少进一步扩大。此外，当前VQ工作中衡量向量相似性的常用指标欧氏距离更侧重减小大小误差，这一特性与前述发现相悖，必然导致更大的量化误差。</p>
<p>为此，本文提出极坐标解耦向量量化框架（PCDVQ），其核心包含两个创新模块：1）极坐标解耦（PCD）：将向量转换为极坐标表示，分别对方向参数与大小参数进行独立量化；2）分布对齐码本构建（DACC）：根据原始分布特性分别优化方向码本和大小码本。实验表明，PCDVQ在2比特量化级别上以至少1.5%的零样本准确率优势超越基线方法，为高精度、高压缩比的LLMs建立了新范式。</p>
<div class="markdown-heading"><h2 class="heading-element">GPU上支持多样化掩码的快速稀疏Transformer之灵活算子融合</h2><a id="user-content-gpu上支持多样化掩码的快速稀疏transformer之灵活算子融合" class="anchor" aria-label="Permalink: GPU上支持多样化掩码的快速稀疏Transformer之灵活算子融合" href="#gpu上支持多样化掩码的快速稀疏transformer之灵活算子融合"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.06095v1 公告类型：新研究<br>
摘要：大型语言模型因其强大的理解能力在全球范围内广受欢迎。作为其核心组件，通过并行化加速Transformer逐渐成为研究热点。掩码层通过引入稀疏性来减少Transformer的计算量，但现有研究鲜少关注稀疏Transformer的性能优化。此外，基于规则的机制忽视了混合类型算子的融合机会，且难以适应多样化的序列长度。针对上述问题，我们提出STOF框架——通过GPU上的灵活掩码与算子融合实现稀疏Transformer优化。我们首先统一了多头注意力机制的存储格式与内核实现，随后将融合方案映射至编译模板，并通过两阶段搜索引擎确定最优参数配置。实验表明，相较于最先进方案，STOF在多头注意力计算中最高实现1.7倍加速，在端到端推理中最高获得1.5倍性能提升。</p>
<p>（注：根据学术文献翻译规范，专业术语如"Transformer"、"GPU"等保留英文原名；"MHA"作为常用缩写首次出现时译为全称"多头注意力机制"，后文可直接使用缩写；技术概念如"two-stage search engine"采用"两阶段搜索引擎"等符合中文计算机领域表述习惯的译法；长句按中文表达习惯拆分为短句，同时保持技术描述的准确性。）</p>
<div class="markdown-heading"><h2 class="heading-element">BAQ：面向大型语言模型的高效比特分配量化方法</h2><a id="user-content-baq面向大型语言模型的高效比特分配量化方法" class="anchor" aria-label="Permalink: BAQ：面向大型语言模型的高效比特分配量化方法" href="#baq面向大型语言模型的高效比特分配量化方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.05664v1 公告类型：新研究<br>
摘要：训练后模型量化是一种被广泛采用的、用于降低大语言模型（LLM）内存与计算开销的技术。然而，现有方法多依赖均匀或启发式比特位宽分配策略，未能充分考虑权重对量化噪声的非均匀敏感性。本文提出一种基于海森矩阵代理敏感性度量的新型比特位宽分配框架。通过关键假设，我们将逐层/组件的损失函数显式表达为比特位宽的函数，从而将比特分配问题转化为凸优化任务，其闭式解通过自适应调整权重精度来最小化分层量化损失。通过对解的解析，我们获得了若干重要发现（如等损失结构），并据此设计出\textbf{BAQ}（比特分配量化）算法。该算法在损失最小化与复杂度之间实现了良好平衡，且能以极小开销整合至标准量化流程。实验表明，在125M至30B参数规模的大语言模型上，BAQ始终优于GPTQ，相同比特位宽下困惑度最高降低56倍。基于最优比特分配问题的解析结果，我们还为观测到的性能提升提供了理论解释。本文全部代码已开源：<a href="https://github.com/CSU-ModelCompression/BAQ%E3%80%82">https://github.com/CSU-ModelCompression/BAQ。</a></p>
<p>（注：译文严格遵循技术文献的表述规范，处理要点包括：</p>
<ol>
<li>专业术语标准化："Hessian proxy"译为"海森矩阵代理"，"perplexity"保留专业译法"困惑度"</li>
<li>数学概念准确转化："closed-form solution"译为"闭式解"，"convex optimization"译为"凸优化"</li>
<li>算法名称保留英文缩写BAQ并补充中文全称，符合计算机领域惯例</li>
<li>数量级表述采用中文习惯："56$\times$ lower"译为"降低56倍"</li>
<li>被动语态转换："are derived from"译为"通过...获得"，符合中文主动表达习惯）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>