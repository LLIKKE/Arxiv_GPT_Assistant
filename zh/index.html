<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">StreamMem：面向流媒体视频理解的查询无关键值缓存内存技术</h2><a id="user-content-streammem面向流媒体视频理解的查询无关键值缓存内存技术" class="anchor" aria-label="Permalink: StreamMem：面向流媒体视频理解的查询无关键值缓存内存技术" href="#streammem面向流媒体视频理解的查询无关键值缓存内存技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>多模态大语言模型（MLLMs）在视觉语言推理方面取得显著进展，但其处理长视频的能力仍存在局限。尽管近期长上下文MLLMs有所突破，但存储和处理长视觉内容的关键值缓存（KV cache）会导致巨大的内存和计算开销。现有视觉压缩方法需在压缩前编码完整视觉上下文，或需预先获取问题信息，这在实际长视频理解和多轮对话场景中并不适用。本研究提出StreamMem——一种面向流式视频理解的查询无关KV缓存记忆机制。该方案通过流式编码新视频帧，利用视觉标记与通用查询标记间的注意力分数压缩KV缓存，同时维持固定大小的KV记忆库，从而在内存受限的长视频场景中实现高效问答。在三个长视频理解基准和两个流式视频问答基准上的评估表明，StreamMem在查询无关KV缓存压缩方面达到最先进性能，并与查询感知压缩方法具有竞争力。</p>
<div class="markdown-heading"><h2 class="heading-element">NiceWebRL：一个用于在强化学习环境中进行人类受试实验的Python库</h2><a id="user-content-nicewebrl一个用于在强化学习环境中进行人类受试实验的python库" class="anchor" aria-label="Permalink: NiceWebRL：一个用于在强化学习环境中进行人类受试实验的Python库" href="#nicewebrl一个用于在强化学习环境中进行人类受试实验的python库"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>我们推出NiceWebRL——一款研究工具，使研究人员能够利用机器学习强化学习（RL）环境进行在线人类受试者实验。该Python库可将任何基于Jax的环境转换为在线交互界面，同时支持单智能体与多智能体环境。通过NiceWebRL，人工智能研究者可将其算法与人类表现进行对比，认知科学家可测试机器学习算法作为人类认知理论的可行性，多智能体研究者则能开发人机协作算法。我们通过三个案例研究展示NiceWebRL的潜力：助力开发类人人工智能、人本兼容人工智能和人机辅助人工智能。在首个案例（类人AI）中，NiceWebRL支持开发新型认知强化学习模型，通过在网格世界和2D Minecraft领域Craftax中与人类参与者对比测试该模型。第二案例（人本兼容AI）中，该工具助力开发新型多智能体强化学习算法，能在《Overcooked》游戏中泛化至人类合作伙伴。第三案例（人机辅助AI）则展现在包含数百万层级任务的XLand-Minigrid环境中，如何通过NiceWebRL研究大语言模型辅助人类完成复杂任务。该库已开源：<a href="https://github.com/KempnerInstitute/nicewebrl%E3%80%82">https://github.com/KempnerInstitute/nicewebrl。</a></p>
<div class="markdown-heading"><h2 class="heading-element">语言引导调优：通过文本反馈增强数值优化</h2><a id="user-content-语言引导调优通过文本反馈增强数值优化" class="anchor" aria-label="Permalink: 语言引导调优：通过文本反馈增强数值优化" href="#语言引导调优通过文本反馈增强数值优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>配置优化依然是机器学习中的关键瓶颈，需要协调模型架构、训练策略、特征工程和超参数等多个维度的调优。传统方法将这些维度割裂处理且缺乏可解释性，而近期自动化方法则难以实现动态适应性，且缺乏对优化决策的语义推理能力。我们提出语言引导调优（LGT）这一创新框架，通过多智能体大语言模型实现基于自然语言推理的智能配置优化。该框架引入文本梯度——一种定性反馈信号，通过提供对训练动态和配置相互依赖关系的语义理解，与数值优化形成互补。LGT协调三个专业智能体：提出配置调整建议的顾问、评估进展的评估器，以及优化决策过程的优化器，共同构建自我完善的反馈循环。在六个不同数据集上的综合评估表明，LGT相较传统优化方法实现显著提升，在保持高可解释性的同时获得性能增益。</p>
<div class="markdown-heading"><h2 class="heading-element">让我们共建无偏见社区：通过新增链接引导图结构公平性</h2><a id="user-content-让我们共建无偏见社区通过新增链接引导图结构公平性" class="anchor" aria-label="Permalink: 让我们共建无偏见社区：通过新增链接引导图结构公平性" href="#让我们共建无偏见社区通过新增链接引导图结构公平性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>图神经网络（GNNs）已在众多应用领域取得显著成功。然而，由于图结构本身存在的偏见，图神经网络在公平性方面面临重大挑战。尽管原始用户图结构通常具有偏向性，但通过引入新连接来引导现有结构向无偏状态发展具有广阔前景。这种通过新增连接实现的公平性引导能够促进无偏见社区的形成，从而提升下游应用的公平性。针对这一问题，我们提出了名为FairGuide的创新框架。具体而言，为确保在公平性引导图上训练的下游任务实现公平，我们引入了可微分社区检测任务作为伪下游任务。理论分析进一步证明，通过优化该伪任务的公平性可有效增强结构公平性，进而促进跨多样化下游应用的公平性泛化。此外，FairGuide采用高效策略，利用从公平性引导目标中提取的元梯度来识别能显著提升结构公平性的新连接。大量实验结果表明，我们提出的方法在各类基于图的公平性任务中均展现出卓越的有效性和泛化能力。</p>
<div class="markdown-heading"><h2 class="heading-element">JEDI-linear：面向FPGA上喷注标记的快速高效图神经网络</h2><a id="user-content-jedi-linear面向fpga上喷注标记的快速高效图神经网络" class="anchor" aria-label="Permalink: JEDI-linear：面向FPGA上喷注标记的快速高效图神经网络" href="#jedi-linear面向fpga上喷注标记的快速高效图神经网络"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>图神经网络（GNN），特别是交互网络（IN），在欧洲核子研究中心高亮度大型强子对撞机（HL-LHC）的喷注标记任务中展现出卓越性能。然而其计算复杂度高且内存访问模式不规则，在硬件触发系统中部署于FPGA时面临严峻挑战——这类系统对延迟和资源有着严格限制。本研究提出JEDI-linear，一种具有线性计算复杂度的新型GNN架构，通过共享变换和全局聚合消除了显式成对交互。为提升硬件效率，我们引入细粒度量化感知训练（支持逐参数位宽优化），并采用基于分布式算法的无乘法器乘累加运算。评估结果表明：基于FPGA的JEDI-linear相比现有最优设计，延迟降低3.7至11.5倍，初始间隔缩短高达150倍，LUT使用量减少高达6.2倍，同时模型精度更高且完全无需DSP模块（而现有方案需消耗超过8,700个DSP）。这是首个实现低于60纳秒延迟的交互式GNN，目前满足HL-LHC紧凑缪子线圈（CMS）一级触发系统的要求。该研究通过实现精准、可扩展且资源高效的实时GNN推理，推动了下一代触发系统的发展。我们开源的模板将进一步提升科学应用中的可复现性与广泛适用性。</p>
<div class="markdown-heading"><h2 class="heading-element">SparK：基于查询感知的非结构化稀疏性与可恢复KV缓存通道剪枝技术</h2><a id="user-content-spark基于查询感知的非结构化稀疏性与可恢复kv缓存通道剪枝技术" class="anchor" aria-label="Permalink: SparK：基于查询感知的非结构化稀疏性与可恢复KV缓存通道剪枝技术" href="#spark基于查询感知的非结构化稀疏性与可恢复kv缓存通道剪枝技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此处采用技术术语的常见译法："Query-Aware"译为"查询感知"，"Unstructured Sparsity"译为"非结构化稀疏性"，"Recoverable"译为"可恢复"，"KV Cache"保留英文缩写并补充"键值"全称，"Channel Pruning"译为"通道剪枝"，整体采用技术文献常用的破折号连接形式，保持专业性与可读性平衡）</p>
<p>大型语言模型（LLM）的长上下文推理正日益受到KV缓存瓶颈的制约：内存使用量随序列长度线性增长，而注意力计算量呈二次方增长。现有方法通过时间轴上的KV缓存压缩策略（如令牌驱逐或合并）来降低内存和计算开销，但这些方法往往忽略了特征维度（即通道轴）上的细粒度重要性差异，从而限制了其有效平衡效率与模型精度的能力。实际上，我们观察到通道显著性在查询和位置间存在剧烈波动：某些特征通道对特定查询携带近乎零信息，而其他通道的相关性却急剧飙升。针对这一疏漏，我们提出SPARK——一种无需训练的即插即用方法，通过在通道层级实施非结构化稀疏化来修剪KV缓存，并在注意力分数计算过程中动态恢复被修剪的条目。值得注意的是，该方法与现有KV压缩及量化技术正交，可与之兼容集成以实现进一步加速。通过消除通道级冗余，SPARK能够在相同内存预算下处理更长的序列。对于等长序列，SPARK不仅保持或提升模型精度，相比基于驱逐的方法还能减少30%以上的KV缓存存储。即使在80%的激进修剪比率下，SPARK仍能维持性能，其性能衰减较基线驱逐方法低5%以内，证明了方法的鲁棒性与有效性。代码已开源：<a href="https://github.com/Xnhyacinth/SparK%E3%80%82">https://github.com/Xnhyacinth/SparK。</a></p>
<div class="markdown-heading"><h2 class="heading-element">非线性动态系统中的贝叶斯推断与学习：整合显性与隐性先验知识的框架</h2><a id="user-content-非线性动态系统中的贝叶斯推断与学习整合显性与隐性先验知识的框架" class="anchor" aria-label="Permalink: 非线性动态系统中的贝叶斯推断与学习：整合显性与隐性先验知识的框架" href="#非线性动态系统中的贝叶斯推断与学习整合显性与隐性先验知识的框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在动力学系统模型学习中，准确性与泛化能力是核心目标。为从有限数据中获取此类模型，现有研究多利用系统的先验知识与假设。然而，如何将多样化的先验知识（如部分已知的系统方程和关于未知模型部分的平滑性假设）与数据信息相融合，仍是一个具有挑战性的问题——在存在潜系统状态的输入-输出场景中尤为如此。特别需要指出的是，学习嵌套在已知系统方程内部的函数通常需要依赖专家经验，这一过程既繁琐又容易出错。本文针对潜状态推断与未知模型部分的学习展开研究，旨在实现数据信息与多源先验知识的融合。主要贡献在于开发了一种通用系统辨识工具，首次为在线/离线贝叶斯推断与学习提供统一解决方案，同时支持显式和隐式先验系统知识的整合。我们提出了一种创新接口，可将已知动力学函数与基于学习的未知系统部分近似相结合。基于所提出的模型结构，推导出用于高效参数边缘化的闭式密度函数。该方法无需用户定制坐标变换或模型求逆，使本框架成为推断与学习的通用工具。通过三个独立案例研究（包括实验数据集）验证了所提出框架的广泛适用性。</p>
<div class="markdown-heading"><h2 class="heading-element">通过具有秩感知束GRPO的Transformer发现隐藏代数结构</h2><a id="user-content-通过具有秩感知束grpo的transformer发现隐藏代数结构" class="anchor" aria-label="Permalink: 通过具有秩感知束GRPO的Transformer发现隐藏代数结构" href="#通过具有秩感知束grpo的transformer发现隐藏代数结构"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>近期研究拓展了Transformer在逻辑推理与符号计算方面的能力。本文重点针对多元多项式分解这一具有挑战性的代数任务，探究其在函数分解场景下发现非线性潜在模式的能力。该问题被证明属于NP难问题，在科学与工程领域具有广泛应用，既需要计算精度又要求深刻洞察。我们的贡献包括三方面：首先开发了能精细控制问题复杂度的合成数据生成流程；其次通过监督学习训练Transformer模型，并从规模扩展性和泛化能力等四个关键维度进行评估；第三提出了适用于困难代数问题的秩感知强化学习方法——束群相对策略优化（BGRPO）。采用BGRPO进行微调可在将束宽度缩减高达一半的同时提升准确率，使推理计算量降低约75%。此外，我们的模型在多项式简化任务中展现出竞争优势，在多类案例中的表现超越Mathematica。</p>
<div class="markdown-heading"><h2 class="heading-element">SemToken：面向高效长上下文语言建模的语义感知分词技术</h2><a id="user-content-semtoken面向高效长上下文语言建模的语义感知分词技术" class="anchor" aria-label="Permalink: SemToken：面向高效长上下文语言建模的语义感知分词技术" href="#semtoken面向高效长上下文语言建模的语义感知分词技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译采用技术文档常用命名风格，在保持原意的基础上：</p>
<ol>
<li>"Semantic-Aware"译为"语义感知"符合技术术语惯例</li>
<li>"Tokenization"译为"分词技术"明确其NLP领域特性</li>
<li>"Efficient Long-Context"处理为"高效长上下文"准确传达技术优势</li>
<li>整体采用"核心术语：功能说明"的技术标题目录结构）</li>
</ol>
<p>分词在语言建模中起着关键作用，但现有方法如字节对编码（BPE）或WordPiece仅基于频率统计进行操作，忽略了文本底层的语义结构。这导致语义冗余片段被过度分词，且未能充分利用上下文连贯性——尤其在长上下文场景中更为明显。本研究提出\textbf{SemToken}，一种语义感知的分词框架，可同时降低词汇冗余并提升计算效率。该框架首先通过轻量级编码器提取上下文语义嵌入，执行局部语义聚类以合并语义等价的词元；随后根据语义密度分配异构化的词汇粒度，在内容密集区域采用细粒度分词，而在重复性或低信息熵片段进行粗粒度压缩。SemToken可与现代语言模型及注意力加速方法无缝集成。在WikiText-103和LongBench等长上下文语言建模基准测试中，该方法实现了最高$2.4\times$的词汇量削减和$1.9\times$的加速效果，且困惑度与下游任务准确率几乎无衰减。我们的研究表明：语义结构为优化大语言模型中的分词与计算提供了全新的优化维度。</p>
<div class="markdown-heading"><h2 class="heading-element">词汇定制器：针对小型语言模型下游任务的动态词汇选择机制</h2><a id="user-content-词汇定制器针对小型语言模型下游任务的动态词汇选择机制" class="anchor" aria-label="Permalink: 词汇定制器：针对小型语言模型下游任务的动态词汇选择机制" href="#词汇定制器针对小型语言模型下游任务的动态词汇选择机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：译文采用意译方式，在准确传达原意的基础上：</p>
<ol>
<li>将"VocabTailor"译为"词汇定制器"，体现个性化配置功能</li>
<li>"Dynamic Vocabulary Selection"译为"动态词汇选择机制"，补充"机制"二字明确技术属性</li>
<li>"Downstream Tasks"采用行业通用译法"下游任务"</li>
<li>整体句式调整为中文技术文献常用的偏正结构，符合学术翻译规范）</li>
</ol>
<p>小型语言模型（SLM）在资源受限环境中具有计算优势，但内存限制仍是边缘设备部署的关键瓶颈。由于词汇量庞大，SLM内存占用的相当一部分来自词汇相关组件，尤其是嵌入层和语言建模头。现有静态词汇剪枝方法虽能降低内存使用，但其僵化的"一刀切"设计会导致预填充阶段信息丢失且缺乏灵活性。本研究揭示了词汇压缩挑战背后的两个核心原则：词汇局部性原理（即单次推理仅需少量词汇子集）以及SLM词汇相关组件间的计算特性不对称性。基于这些发现，我们提出VocabTailor——一种新型解耦动态词汇选择框架，通过嵌入层卸载机制解决内存约束，并为语言建模头设计混合式动静词汇选择策略，实现词汇组件的按需加载。在多类下游任务的综合实验表明，VocabTailor在保持任务性能零衰减或最小衰减的前提下，将词汇相关组件内存占用降低高达99%，显著优于现有静态词汇剪枝方法。</p>
<div class="markdown-heading"><h2 class="heading-element">使用SparseLoCo实现通信高效的大型语言模型预训练</h2><a id="user-content-使用sparseloco实现通信高效的大型语言模型预训练" class="anchor" aria-label="Permalink: 使用SparseLoCo实现通信高效的大型语言模型预训练" href="#使用sparseloco实现通信高效的大型语言模型预训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>通信高效的分布式训练算法近期备受关注，因其能在带宽受限场景（如跨数据中心和互联网）中为大规模语言模型（LLM）训练带来显著优势。尽管这些方法通过降低通信频率来优化，但仍通常需要传输完整的模型梯度副本——即使对于跨数据中心链路而言，这仍会造成通信瓶颈。此外，与朴素的AdamW DDP基线相比，这些方法可能轻微降低模型性能。虽然量化和误差反馈技术常被用于压缩伪梯度大小，但在LLM预训练场景中，现有方法始终未能有效结合稀疏化技术，且量化程度有限。本研究提出SparseLoCo——一种面向LLM的通信高效训练算法，通过协同运用Top-K稀疏化与量化技术，实现了高达1-3%稀疏度和2比特量化的极端压缩比，同时性能超越全精度DiLoCo。我们的核心发现是：外部动量可通过误差反馈结合激进稀疏化进行本地近似，且稀疏聚合反而能提升模型性能。通过一系列通信受限的LLM训练场景实证，我们证明SparseLoCo在性能和通信成本方面均能带来显著提升。</p>
<div class="markdown-heading"><h2 class="heading-element">代码理解任务中知识蒸馏的实证研究</h2><a id="user-content-代码理解任务中知识蒸馏的实证研究" class="anchor" aria-label="Permalink: 代码理解任务中知识蒸馏的实证研究" href="#代码理解任务中知识蒸馏的实证研究"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>预训练语言模型（PLM）已成为代码理解领域的强大工具。然而，由于计算强度大和推理延迟高，在大规模应用中部署这些PLM面临实际挑战。知识蒸馏（KD）作为一种前景广阔的模型压缩与加速技术，通过将大型教师模型的知识迁移至紧凑型学生模型，在保持教师模型大部分能力的同时实现高效推理，从而突破这些限制。尽管该技术在自然语言处理和计算机视觉领域已取得显著成功，但其在代码理解任务中的潜力仍未被充分探索。本文系统研究了知识蒸馏在代码理解任务中的有效性和应用方式。我们的研究涵盖两种主流KD方法（基于logit和基于特征的KD方法），在三个下游任务上对来自不同领域的八个学生模型和两个教师PLM进行实验。实验结果表明，与标准微调相比，KD能持续为不同规模的学生模型带来显著性能提升。值得注意的是，代码专用PLM作为教师模型展现出更优效果。在所有KD方法中，最新的基于特征的方法表现最为卓越，使学生模型仅需5%参数量即可保留教师模型98%的性能。关于学生模型架构，实验表明与教师模型架构相似并不必然带来更好性能。我们进一步讨论了KD过程和推理阶段的效率与行为特征，总结研究发现的实际意义，并指出未来值得探索的方向。</p>
<div class="markdown-heading"><h2 class="heading-element">GraSP：一个基于图的可扩展统一框架，用于SFT与DPO的合成数据生成、质量标注及管理</h2><a id="user-content-grasp一个基于图的可扩展统一框架用于sft与dpo的合成数据生成质量标注及管理" class="anchor" aria-label="Permalink: GraSP：一个基于图的可扩展统一框架，用于SFT与DPO的合成数据生成、质量标注及管理" href="#grasp一个基于图的可扩展统一框架用于sft与dpo的合成数据生成质量标注及管理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：SFT指监督微调Supervised Fine-Tuning，DPO指直接偏好优化Direct Preference Optimization。译文采用技术术语直译加注的方式，既保留专业缩写又确保中文读者理解具体技术指向。"Graph-Based"译为"基于图的"符合计算机领域术语规范，"Scalable"译为"可扩展"准确传达系统性能特征。通过"生成、标注、管理"三个动词并列清晰呈现框架功能，最后用"及"字自然连接介词结构，保持技术文档的简洁性与专业性。）</p>
<p>大型语言模型（LLMs）的发展高度依赖于监督微调（SFT）、直接偏好优化（DPO）等对齐任务所需的高质量数据集。本研究提出了一种综合性合成数据生成框架，能够针对这些训练范式实现可扩展、可配置且高保真的合成数据生成。该方案采用模块化、基于配置的流程，能以最少的人工干预模拟复杂对话流。框架通过结合启发式规则与基于LLM评估的双阶段质量标记机制，自动筛选和评分从OASST格式对话中提取的数据，确保高质量对话样本的筛选。最终生成的数据集采用支持SFT和DPO用例的灵活架构，可无缝集成到多样化训练工作流中。这些创新共同构成了规模化生成和管理合成对话数据的强大解决方案，显著降低了LLM训练流程中数据准备的开销。</p>
<div class="markdown-heading"><h2 class="heading-element">无需反向传播的测试时自适应：基于概率高斯对齐的方法</h2><a id="user-content-无需反向传播的测试时自适应基于概率高斯对齐的方法" class="anchor" aria-label="Permalink: 无需反向传播的测试时自适应：基于概率高斯对齐的方法" href="#无需反向传播的测试时自适应基于概率高斯对齐的方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>测试时自适应（TTA）技术通过利用推理过程中的未标注测试数据，增强了模型在分布偏移下的零样本鲁棒性。尽管已取得显著进展，该技术仍面临若干限制其广泛应用的关键挑战：首先，现有方法大多依赖反向传播或迭代优化，制约了可扩展性并阻碍实时部署；其次，这些方法缺乏对类条件特征分布的显式建模——这种建模对生成可靠决策边界和校准预测至关重要，但由于测试时既无源数据又无监督信号，该方向仍探索不足。本文提出ADAPT方法，一种先进的免反向传播分布感知测试时自适应方案。通过采用渐进更新的类均值与共享协方差矩阵来建模类条件似然，我们将TTA重新构建为高斯概率推理任务，从而实现闭式、无需训练的参数推断。为修正潜在似然偏差，我们引入由CLIP先验和历史知识库引导的轻量化正则化策略。ADAPT无需源数据、梯度更新或完整目标数据集访问权限，可同时支持在线与转导学习场景。大量实验表明，我们的方法在多种分布偏移条件下均达到最先进性能，兼具卓越的可扩展性与鲁棒性。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>