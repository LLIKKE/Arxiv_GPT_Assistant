<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">调优压缩（TuneComp）：面向大型基础模型的联合微调与压缩技术</h2><a id="user-content-调优压缩tunecomp面向大型基础模型的联合微调与压缩技术" class="anchor" aria-label="Permalink: 调优压缩（TuneComp）：面向大型基础模型的联合微调与压缩技术" href="#调优压缩tunecomp面向大型基础模型的联合微调与压缩技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>注：翻译说明：</p>
<ol>
<li>"TuneComp"采用音译+意译结合方式，译为"调优压缩"，既保留原词"Tuning"和"Compression"的核心含义，又形成简洁的技术术语</li>
<li>"Joint"译为"联合"准确传达多任务协同处理的含义</li>
<li>"Fine-tuning"译为"微调"符合机器学习领域术语规范</li>
<li>"Large Foundation Models"译为"大型基础模型"准确反映当前AI领域对GPT等预训练模型的称谓</li>
<li>整体采用"技术方案描述式"标题结构，符合中文技术文献命名习惯，同时通过冒号分层保持原标题的信息层次</li>
</ol>
<p>arXiv:2505.21835v1 公告类型：新研究<br>
摘要：为在训练后缩减模型规模，知识蒸馏、低秩近似和剪枝等压缩方法通常在对模型微调后实施。然而，这种先微调后压缩的串行操作会牺牲模型性能，且会不必要地生成一个过渡阶段的冗余大模型。本研究致力于通过在下游任务指导下直接构建小型模型来缩小这一差距。我们提出通过逐步将模型蒸馏至剪枝后的低秩结构，实现微调与压缩的联合优化。实验表明，联合微调压缩方法显著优于其他串行压缩方案。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"fine-tuning"统一译为"微调"而非"调优"以保持领域术语一致性</li>
<li>"low-rank approximation"译为"低秩近似"而非"低阶近似"符合线性代数术语</li>
<li>将"sequential...sacrifices performance"的主动语态转换为"串行操作会牺牲"更符合中文表达习惯</li>
<li>"jointly"译为"联合"而非"共同"以体现算法协同性</li>
<li>保留"arXiv"原始标识符不翻译以符合学术惯例）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">通过训练后量化压缩正弦激活的低秩适配器</h2><a id="user-content-通过训练后量化压缩正弦激活的低秩适配器" class="anchor" aria-label="Permalink: 通过训练后量化压缩正弦激活的低秩适配器" href="#通过训练后量化压缩正弦激活的低秩适配器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.21895v1 公告类型：新研究<br>
摘要：低秩自适应（LoRA）已成为参数高效微调的标准方法，通过将参数更新建模为两个低秩矩阵的乘积，可大幅减少可训练参数量。尽管有效，低秩约束本质上限制了表示能力，通常导致性能逊色于全秩微调。Ji等人（2025）的最新研究通过向低秩适配器施加固定频率的正弦变换，在不引入额外参数的情况下提高了其稳定秩，从而解决了这一局限。这引发了一个关键问题：在训练后量化的背景下，同样的正弦激活技术能否成功应用，以在模型压缩后仍保留优势？本文通过将正弦变换框架扩展至量化LoRA适配器来研究该问题。我们建立的理论分析表明，量化适配器的稳定秩与其全精度对应物紧密相关，这促使即使在量化条件下仍可使用此类秩增强函数。实验结果表明，正弦非线性带来的表达能力提升在量化后依然存在，能产生高性能的极致压缩适配器。我们在语言、视觉和文生图等微调任务中验证了该方法，在保持竞争力的准确度同时实现了显著的内存节省。</p>
<p>（注：根据学术惯例，保留arXiv编号格式；"Post-Training Quantization"译为"训练后量化"是领域标准译法；"stable rank"译为"稳定秩"符合线性代数术语；通过拆分长句为中文惯用的短句结构（如将"motivating..."处理为因果分句），并采用"局限""逊色""引发"等符合中文科技论文表达的词汇，在保持专业性的同时确保行文流畅。）</p>
<div class="markdown-heading"><h2 class="heading-element">无需注意力的扩展推理</h2><a id="user-content-无需注意力的扩展推理" class="anchor" aria-label="Permalink: 无需注意力的扩展推理" href="#无需注意力的扩展推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.22425v1 公告类型：新研究<br>
摘要：尽管大语言模型（LLMs）在复杂推理任务上取得了显著进展，但仍受限于两大核心挑战：因依赖Transformer架构导致的效率瓶颈，以及缺乏针对高难度领域的结构化微调方法。我们提出\ourmodel——一种无需注意力机制的语言模型，通过架构革新与数据优化双管齐下解决上述问题。该模型基于Mamba-2的状态空间对偶（SSD）层构建，摒弃了自注意力机制与键值缓存需求，实现了固定内存占用与恒定时间推理。为培养其复杂推理能力，我们基于\textsc{PromptCoT}合成范式设计了两阶段课程微调策略：通过抽象概念筛选与原理引导生成，构建具有教学结构的训练问题。基准测试表明，\ourmodel-7B在同等规模下优于强Transformer与混合模型，更以2.6%（AIME 24）、0.6%（AIME 25）和3.0%（Livecodebench）的优势超越体积大得多的Gemma3-27B。这些成果揭示了状态空间模型作为高效、可扩展架构替代注意力机制的潜力，尤其适用于高容量推理场景。</p>
<p>（注：译文保留技术术语的准确性，如"state space dual"译为专业术语"状态空间对偶"；"PromptCoT"作为专有名词保留不译；模型名称\ourmodel因原文未明确采用意译处理；通过"双管齐下""构建""培养"等动词增强技术文本的可读性；长句拆分符合中文表达习惯，如将英文复合句转化为多个流水句；百分比数据采用中文数字格式规范呈现）</p>
<div class="markdown-heading"><h2 class="heading-element">ACE：探索激活余弦相似度与方差以实现精准且校准高效的大语言模型剪枝</h2><a id="user-content-ace探索激活余弦相似度与方差以实现精准且校准高效的大语言模型剪枝" class="anchor" aria-label="Permalink: ACE：探索激活余弦相似度与方差以实现精准且校准高效的大语言模型剪枝" href="#ace探索激活余弦相似度与方差以实现精准且校准高效的大语言模型剪枝"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.21987v1 公告类型：新研究<br>
摘要：随着大语言模型（LLMs）的快速扩张，对内存和计算资源的需求显著增长。近期LLM剪枝技术的进展旨在降低这些模型的体积和计算成本。然而，现有方法往往存在剪枝效果欠佳或剪枝过程耗时过长的问题。本研究提出了一种高效且精准的剪枝方法，通过改进校准效率，同步实现了优异的剪枝性能和快速的剪枝速度。我们的方法包含两项关键创新：（1）基于激活余弦相似度的损失引导剪枝指标——通过衡量稠密模型与剪枝模型输出激活之间的角度偏差；（2）基于激活方差的引导剪枝指标——通过保持剪枝后输出激活的语义区分度，使得采用更短输入序列进行有效剪枝成为可能。这两个组件可灵活组合，从准确性和效率双重维度提升LLM剪枝效果。实验结果表明，在LLaMA、LLaMA-2和OPT等主流大语言模型上，我们的方法最高可降低18%的困惑度，并减少63%的剪枝时间。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"pruning"统一译为"剪枝"而非"修剪"，符合计算机领域术语习惯</li>
<li>"perplexity"译为"困惑度"而非"混淆度"，采用自然语言处理标准译法</li>
<li>技术表述采用"激活余弦相似度/方差"等完整术语，确保专业性</li>
<li>长难句按中文表达习惯拆分重组，如将嵌套从句转换为分号连接的并列结构</li>
<li>保留"LLaMA-2"等模型专有名词不翻译，维持技术文档准确性）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>