<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/03/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10032025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/03/2025" href="#personalized-daily-arxiv-papers-10032025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 27</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Do AI Models Perform Human-like Abstract Reasoning Across Modalities?</a>
<strong>Authors:</strong> Claas Beger, Ryan Yi, Shuhao Fu, Arseny Moskvichev, Sarah W. Tsai, Sivasankaran Rajamanickam, Melanie Mitchell</p>
</li>
<li>
<p><a href="#link1">FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models</a>
<strong>Authors:</strong> Zijun Lin, Jiafei Duan, Haoquan Fang, Dieter Fox, Ranjay Krishna, Cheston Tan, Bihan Wen</p>
</li>
<li>
<p><a href="#link2">xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity</a>
<strong>Authors:</strong> Maximilian Beck, Kajetan Schweighofer, Sebastian B"ock, Sebastian Lehner, Sepp Hochreiter</p>
</li>
<li>
<p><a href="#link3">RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models</a>
<strong>Authors:</strong> Zukang Xu, Xing Hu, Qiang Wu, Dawei Yang</p>
</li>
<li>
<p><a href="#link4">Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning</a>
<strong>Authors:</strong> Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu</p>
</li>
<li>
<p><a href="#link5">UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models</a>
<strong>Authors:</strong> Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie</p>
</li>
<li>
<p><a href="#link6">KaVa: Latent Reasoning via Compressed KV-Cache Distillation</a>
<strong>Authors:</strong> Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi</p>
</li>
<li>
<p><a href="#link7">ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models</a>
<strong>Authors:</strong> Akshat Ramachandran, Marina Neseem, Charbel Sakr, Rangharajan Venkatesan, Brucek Khailany, Tushar Krishna</p>
</li>
<li>
<p><a href="#link8">Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation</a>
<strong>Authors:</strong> Haotian Xiang, Jinwen Xu, Qin Lu</p>
</li>
<li>
<p><a href="#link9">Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction</a>
<strong>Authors:</strong> Adam Filipek</p>
</li>
<li>
<p><a href="#link10">Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls</a>
<strong>Authors:</strong> Feiyang Kang, Newsha Ardalani, Michael Kuchnik, Youssef Emad, Mostafa Elhoushi, Shubhabrata Sengupta, Shang-Wen Li, Ramya Raghavendra, Ruoxi Jia, Carole-Jean Wu</p>
</li>
<li>
<p><a href="#link11">LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions</a>
<strong>Authors:</strong> Yunhan Lin, Wenqi Wu, Zhijie Zhang, Huasong Min</p>
</li>
<li>
<p><a href="#link12">Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models</a>
<strong>Authors:</strong> Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng Zhao</p>
</li>
<li>
<p><a href="#link13">A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining</a>
<strong>Authors:</strong> Sipeng Zhang, Longfei Yun, Zilong Wang, Jingbo Shang, Letian Peng</p>
</li>
<li>
<p><a href="#link14">Support Basis: Fast Attention Beyond Bounded Entries</a>
<strong>Authors:</strong> Maryam Aliakbarpour, Vladimir Braverman, Junze Yin, Haochen Zhang</p>
</li>
<li>
<p><a href="#link15">Accelerating Attention with Basis Decomposition</a>
<strong>Authors:</strong> Jialin Zhao</p>
</li>
<li>
<p><a href="#link16">The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM</a>
<strong>Authors:</strong> Kwanhee Lee, Hyeondo Jang, Dongyeop Lee, Dan Alistarh, Namhoon Lee</p>
</li>
<li>
<p><a href="#link17">Contrastive Representation Regularization for Vision-Language-Action Models</a>
<strong>Authors:</strong> Taeyoung Kim, Jimin Lee, Myungkyu Koo, Dongyoung Kim, Kyungmin Lee, Changyeon Kim, Younggyo Seo, Jinwoo Shin</p>
</li>
<li>
<p><a href="#link18">Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression</a>
<strong>Authors:</strong> Joykirat Singh, Justin Chih-Yao Chen, Archiki Prasad, Elias Stengel-Eskin, Akshay Nambi, Mohit Bansal</p>
</li>
<li>
<p><a href="#link19">Multimodal Foundation Models for Early Disease Detection</a>
<strong>Authors:</strong> Md Talha Mohsin, Ismail Abdulrashid</p>
</li>
<li>
<p><a href="#link20">VaPR -- Vision-language Preference alignment for Reasoning</a>
<strong>Authors:</strong> Rohan Wadhawan, Fabrice Y Harel-Canada, Zi-Yi Dou, Suhaila Shakiah, Robinson Piramuthu, Nanyun Peng</p>
</li>
<li>
<p><a href="#link21">Randomized Gradient Subspaces for Efficient Large Language Model Training</a>
<strong>Authors:</strong> Sahar Rajabi, Nayeema Nonta, Samanvay Vajpayee, Sirisha Rambhatla</p>
</li>
<li>
<p><a href="#link22">Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving</a>
<strong>Authors:</strong> Haibo Hu, Lianming Huang, Xinyu Wang, Yufei Cui, Nan Guan, Chun Jason Xue</p>
</li>
<li>
<p><a href="#link23">Optimal Stopping vs Best-of-$N$ for Inference Time Optimization</a>
<strong>Authors:</strong> Yusuf Kalayci, Vinod Raman, Shaddin Dughmi</p>
</li>
<li>
<p><a href="#link24">Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency</a>
<strong>Authors:</strong> Yaron Meirovitch, Fuming Yang, Jeff Lichtman, Nir Shavit</p>
</li>
<li>
<p><a href="#link25">VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning</a>
<strong>Authors:</strong> Rui Liu, Dian Yu, Tong Zheng, Runpeng Dai, Zongxia Li, Wenhao Yu, Zhenwen Liang, Linfeng Song, Haitao Mi, Pratap Tokekar, Dong Yu</p>
</li>
<li>
<p><a href="#link26">INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models</a>
<strong>Authors:</strong> Ulas Berk Karli, Ziyao Shangguan, Tesca FItzgerald</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.02125" rel="nofollow">Do AI Models Perform Human-like Abstract Reasoning Across Modalities?</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-do-ai-models-perform-human-like-abstract-reasoning-across-modalities-" class="anchor" aria-label="Permalink: 0. Do AI Models Perform Human-like Abstract Reasoning Across Modalities?" href="#0-do-ai-models-perform-human-like-abstract-reasoning-across-modalities-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.02125
<strong>Authors:</strong> Claas Beger, Ryan Yi, Shuhao Fu, Arseny Moskvichev, Sarah W. Tsai, Sivasankaran Rajamanickam, Melanie Mitchell</p>
<p><strong>Abstract:</strong> arXiv:2510.02125v1 Announce Type: new  Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI benchmark, but does that mean state-of-the-art models recognize and reason with the abstractions that the task creators intended? We investigate models' abstraction abilities on ConceptARC. We evaluate models under settings that vary the input modality (textual vs. visual), whether the model is permitted to use external Python tools, and, for reasoning models, the amount of reasoning effort. In addition to measuring output accuracy, we perform fine-grained evaluation of the natural-language rules that models generate to explain their solutions. This dual evaluation lets us assess whether models solve tasks using the abstractions ConceptARC was designed to elicit, rather than relying on surface-level patterns. Our results show that, while some models using text-based representations match human output accuracy, the best models' rules are often based on surface-level ``shortcuts'' and capture intended abstractions far less often than humans. Thus their capabilities for general abstract reasoning may be overestimated by evaluations based on accuracy alone. In the visual modality, AI models' output accuracy drops sharply, yet our rule-level analysis reveals that models might be underestimated, as they still exhibit a substantial share of rules that capture intended abstractions, but are often unable to correctly apply these rules. In short, our results show that models still lag humans in abstract reasoning, and that using accuracy alone to evaluate abstract reasoning on ARC-like tasks may overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities. We believe that our evaluation framework offers a more faithful picture of multimodal models' abstract reasoning abilities and a more principled way to track progress toward human-like, abstraction-centered intelligence.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.01642" rel="nofollow">FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-failsafe-reasoning-and-recovery-from-failures-in-vision-language-action-models-" class="anchor" aria-label="Permalink: 1. FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models" href="#1-failsafe-reasoning-and-recovery-from-failures-in-vision-language-action-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01642
<strong>Authors:</strong> Zijun Lin, Jiafei Duan, Haoquan Fang, Dieter Fox, Ranjay Krishna, Cheston Tan, Bihan Wen</p>
<p><strong>Abstract:</strong> arXiv:2510.01642v1 Announce Type: new  Abstract: Recent advances in robotic manipulation have integrated low-level robotic control into Vision-Language Models (VLMs), extending them into Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve strong performance in downstream robotic applications, supported by large-scale crowd-sourced robot training data, they still inevitably encounter failures during execution. Enabling robots to reason about and recover from unpredictable and abrupt failures remains a critical challenge. Existing robotic manipulation datasets, collected in either simulation or the real world, primarily provide only ground-truth trajectories, leaving robots unable to recover once failures occur. Moreover, the few datasets that address failure detection typically offer only textual explanations, which are difficult to utilize directly in VLA models. To address this gap, we introduce FailSafe, a novel failure generation and recovery system that automatically produces diverse failure cases paired with executable recovery actions. FailSafe can be seamlessly applied to any manipulation task in any simulator, enabling scalable creation of failure-action data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that FailSafe-VLM successfully helps robotic arm detect and recover from potential failures, improving the performance of three state-of-the-art VLA models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different spatial configurations, camera viewpoints, and robotic embodiments. We plan to release the FailSafe code to the community.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.02228" rel="nofollow">xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-xlstm-scaling-laws-competitive-performance-with-linear-time-complexity-" class="anchor" aria-label="Permalink: 2. xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity" href="#2-xlstm-scaling-laws-competitive-performance-with-linear-time-complexity-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.02228
<strong>Authors:</strong> Maximilian Beck, Kajetan Schweighofer, Sebastian B"ock, Sebastian Lehner, Sepp Hochreiter</p>
<p><strong>Abstract:</strong> arXiv:2510.02228v1 Announce Type: new  Abstract: Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.01240" rel="nofollow">RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-rsavq-riemannian-sensitivity-aware-vector-quantization-for-large-language-models-" class="anchor" aria-label="Permalink: 3. RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models" href="#3-rsavq-riemannian-sensitivity-aware-vector-quantization-for-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01240
<strong>Authors:</strong> Zukang Xu, Xing Hu, Qiang Wu, Dawei Yang</p>
<p><strong>Abstract:</strong> arXiv:2510.01240v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resource-constrained devices. Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints. Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.02091" rel="nofollow">Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-demystifying-the-roles-of-llm-layers-in-retrieval-knowledge-and-reasoning-" class="anchor" aria-label="Permalink: 4. Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning" href="#4-demystifying-the-roles-of-llm-layers-in-retrieval-knowledge-and-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.02091
<strong>Authors:</strong> Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.02091v1 Announce Type: new  Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.02194" rel="nofollow">UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-upsafecircc-upcycling-for-controllable-safety-in-large-language-models-" class="anchor" aria-label="Permalink: 5. UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models" href="#5-upsafecircc-upcycling-for-controllable-safety-in-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.02194
<strong>Authors:</strong> Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie</p>
<p><strong>Abstract:</strong> arXiv:2510.02194v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide range of tasks, but remain vulnerable to safety risks such as harmful content generation and jailbreak attacks. Existing safety techniques -- including external guardrails, inference-time guidance, and post-training alignment -- each face limitations in balancing safety, utility, and controllability. In this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM safety through safety-aware upcycling. Our approach first identifies safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE) structure, where the router acts as a soft guardrail that selectively activates original MLPs and added safety experts. We further introduce a two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities. To enable flexible control at inference time, we introduce a safety temperature mechanism, allowing dynamic adjustment of the trade-off between safety and utility. Experiments across multiple benchmarks, base model, and model scales demonstrate that UpSafe$^\circ$C achieves robust safety improvements against harmful and jailbreak inputs, while maintaining competitive performance on general tasks. Moreover, analysis shows that safety temperature provides fine-grained inference-time control that achieves the Pareto-optimal frontier between utility and safety. Our results highlight a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.02312" rel="nofollow">KaVa: Latent Reasoning via Compressed KV-Cache Distillation</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-kava-latent-reasoning-via-compressed-kv-cache-distillation-" class="anchor" aria-label="Permalink: 6. KaVa: Latent Reasoning via Compressed KV-Cache Distillation" href="#6-kava-latent-reasoning-via-compressed-kv-cache-distillation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.02312
<strong>Authors:</strong> Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi</p>
<p><strong>Abstract:</strong> arXiv:2510.02312v1 Announce Type: new  Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.01290" rel="nofollow">ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-thinkv-thought-adaptive-kv-cache-compression-for-efficient-reasoning-models-" class="anchor" aria-label="Permalink: 7. ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models" href="#7-thinkv-thought-adaptive-kv-cache-compression-for-efficient-reasoning-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01290
<strong>Authors:</strong> Akshat Ramachandran, Marina Neseem, Charbel Sakr, Rangharajan Venkatesan, Brucek Khailany, Tushar Krishna</p>
<p><strong>Abstract:</strong> arXiv:2510.01290v1 Announce Type: new  Abstract: The long-output context generation of large reasoning models enables extended chain of thought (CoT) but also drives rapid growth of the key-value (KV) cache, quickly overwhelming GPU memory. To address this challenge, we propose ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on the observation that attention sparsity reveals distinct thought types with varying importance within the CoT. It applies a hybrid quantization-eviction strategy, assigning token precision by thought importance and progressively evicting tokens from less critical thoughts as reasoning trajectories evolve. Furthermore, to implement ThinKV, we design a kernel that extends PagedAttention to enable efficient reuse of evicted tokens' memory slots, eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show that ThinKV achieves near-lossless accuracy with less than 5% of the original KV cache, while improving performance with up to 5.8x higher inference throughput over state-of-the-art baselines.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.01471" rel="nofollow">Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-fine-tuning-llms-with-variational-bayesian-last-layer-for-high-dimensional-bayesian-optimzation-" class="anchor" aria-label="Permalink: 8. Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation" href="#8-fine-tuning-llms-with-variational-bayesian-last-layer-for-high-dimensional-bayesian-optimzation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01471
<strong>Authors:</strong> Haotian Xiang, Jinwen Xu, Qin Lu</p>
<p><strong>Abstract:</strong> arXiv:2510.01471v1 Announce Type: new  Abstract: A plethora of applications entail solving black-box optimization problems with high evaluation costs, including drug discovery, material design, as well as hyperparameter tuning. Toward finding the global optimum of such black-box optimization problems with sample efficiency, Bayesian optimization (BO) is a theoretically elegant framework that relies on a probabilistic surrogate model so as to iteratively select the query point with well-balanced exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto choice for surrogate modeling, has achieved compelling performances for vanilla BO with low-dimensional continuous variables. However, GPs fall short in coping with high-dimensional counterparts with {\it irregular} variables (e.g., categorical, ordinal, etc.). To alleviate this, neural network-based surrogates have been explored. Inspired by the powerful capabilities of LLMs, we adopt the LLM as the surrogate to model the mapping from the high-dimensional input variables to the objective function. To adapt to the current problem, we leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters together with the posterior of a linear regression head via the variational Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only computationally light compared to existing alternatives, but also admits recursive updates. To automate the critical selection of the LoRA rank as well as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has been devised, which further accommodates continual update of the per-model weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive experimental results demonstrate the compelling performance of the proposed (ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the real-world molecular optimization tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.01817" rel="nofollow">Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-sparse-query-attention-sqa-a-computationally-efficient-attention-mechanism-with-query-heads-reduction-" class="anchor" aria-label="Permalink: 9. Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction" href="#9-sparse-query-attention-sqa-a-computationally-efficient-attention-mechanism-with-query-heads-reduction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01817
<strong>Authors:</strong> Adam Filipek</p>
<p><strong>Abstract:</strong> arXiv:2510.01817v1 Announce Type: new  Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.01631" rel="nofollow">Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-demystifying-synthetic-data-in-llm-pre-training-a-systematic-study-of-scaling-laws-benefits-and-pitfalls-" class="anchor" aria-label="Permalink: 10. Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls" href="#10-demystifying-synthetic-data-in-llm-pre-training-a-systematic-study-of-scaling-laws-benefits-and-pitfalls-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01631
<strong>Authors:</strong> Feiyang Kang, Newsha Ardalani, Michael Kuchnik, Youssef Emad, Mostafa Elhoushi, Shubhabrata Sengupta, Shang-Wen Li, Ramya Raghavendra, Ruoxi Jia, Carole-Jean Wu</p>
<p><strong>Abstract:</strong> arXiv:2510.01631v1 Announce Type: new  Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling, yet high quality data is of limited supply. Synthetic data techniques offer a potential path toward sidestepping these limitations. We conduct a large-scale empirical investigation (&gt;1000 LLMs with &gt;100k GPU hours) using a unified protocol and scaling laws, comparing natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures of natural and synthetic data. Specifically, we found pre-training on rephrased synthetic data \textit{alone} is not faster than pre-training on natural web texts; while pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts can speed up 5-10x (to reach the same validation loss) at larger data budgets. Pre-training on textbook-style synthetic data \textit{alone} results in notably higher loss on many downstream domains especially at small data budgets. "Good" ratios of synthetic data in training data mixtures depend on the model size and data budget, empirically converging to ~30% for rephrased synthetic data. Larger generator models do not necessarily yield better pre-training data than ~8B-param models. These results contribute mixed evidence on "model collapse" during large-scale single-round (n=1) model training on synthetic data--training on rephrased synthetic data shows no degradation in performance in foreseeable scales whereas training on mixtures of textbook-style pure-generated synthetic data shows patterns predicted by "model collapse". Our work demystifies synthetic data in pre-training, validates its conditional benefits, and offers practical guidance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.02104" rel="nofollow">LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-langgrasp-leveraging-fine-tuned-llms-for-language-interactive-robot-grasping-with-ambiguous-instructions-" class="anchor" aria-label="Permalink: 11. LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions" href="#11-langgrasp-leveraging-fine-tuned-llms-for-language-interactive-robot-grasping-with-ambiguous-instructions-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.02104
<strong>Authors:</strong> Yunhan Lin, Wenqi Wu, Zhijie Zhang, Huasong Min</p>
<p><strong>Abstract:</strong> arXiv:2510.02104v1 Announce Type: new  Abstract: The existing language-driven grasping methods struggle to fully handle ambiguous instructions containing implicit intents. To tackle this challenge, we propose LangGrasp, a novel language-interactive robotic grasping framework. The framework integrates fine-tuned large language models (LLMs) to leverage their robust commonsense understanding and environmental perception capabilities, thereby deducing implicit intents from linguistic instructions and clarifying task requirements along with target manipulation objects. Furthermore, our designed point cloud localization module, guided by 2D part segmentation, enables partial point cloud localization in scenes, thereby extending grasping operations from coarse-grained object-level to fine-grained part-level manipulation. Experimental results show that the LangGrasp framework accurately resolves implicit intents in ambiguous instructions, identifying critical operations and target information that are unstated yet essential for task completion. Additionally, it dynamically selects optimal grasping poses by integrating environmental information. This enables high-precision grasping from object-level to part-level manipulation, significantly enhancing the adaptability and task execution efficiency of robots in unstructured environments. More information and code are available here: <a href="https://github.com/wu467/LangGrasp">https://github.com/wu467/LangGrasp</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.01304" rel="nofollow">Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-agentic-jigsaw-interaction-learning-for-enhancing-visual-perception-and-reasoning-in-vision-language-models-" class="anchor" aria-label="Permalink: 12. Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models" href="#12-agentic-jigsaw-interaction-learning-for-enhancing-visual-perception-and-reasoning-in-vision-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01304
<strong>Authors:</strong> Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng Zhao</p>
<p><strong>Abstract:</strong> arXiv:2510.01304v1 Announce Type: new  Abstract: Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at <a href="https://github.com/yuzeng0-0/AGILE">https://github.com/yuzeng0-0/AGILE</a> .</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.01427" rel="nofollow">A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-a-tale-of-llms-and-induced-small-proxies-scalable-agents-for-knowledge-mining-" class="anchor" aria-label="Permalink: 13. A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining" href="#13-a-tale-of-llms-and-induced-small-proxies-scalable-agents-for-knowledge-mining-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01427
<strong>Authors:</strong> Sipeng Zhang, Longfei Yun, Zilong Wang, Jingbo Shang, Letian Peng</p>
<p><strong>Abstract:</strong> arXiv:2510.01427v1 Announce Type: new  Abstract: At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively expensive to deploy at scale, while traditional pipelines of classifiers and extractors remain efficient yet brittle and unable to generalize to new tasks. We introduce Falconer, a collaborative framework that combines the agentic reasoning of LLMs with lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act as planners, decomposing user instructions into executable pipelines, and as annotators, generating supervision to train small proxies. The framework unifies classification and extraction into two atomic operations, get label and get span, enabling a single instruction-following model to replace multiple task-specific components. To evaluate the consistency between proxy models incubated by Falconer and annotations provided by humans and large models, we construct new benchmarks covering both planning and end-to-end execution. Experiments show that Falconer closely matches state-of-the-art LLMs in instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x, offering an efficient and scalable foundation for Deep Research.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.01643" rel="nofollow">Support Basis: Fast Attention Beyond Bounded Entries</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-support-basis-fast-attention-beyond-bounded-entries-" class="anchor" aria-label="Permalink: 14. Support Basis: Fast Attention Beyond Bounded Entries" href="#14-support-basis-fast-attention-beyond-bounded-entries-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01643
<strong>Authors:</strong> Maryam Aliakbarpour, Vladimir Braverman, Junze Yin, Haochen Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.01643v1 Announce Type: new  Abstract: The quadratic complexity of softmax attention remains a central bottleneck in scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a sub-quadratic attention approximation algorithm, but it works only under the restrictive bounded-entry assumption. Since this assumption rarely holds in practice, its applicability to modern LLMs is limited.   In this paper, we introduce support-basis decomposition, a new framework for efficient attention approximation beyond bounded entries. We empirically demonstrate that the entries of the query and key matrices exhibit sub-Gaussian behavior. Our approach uses this property to split large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components. We establish rigorous theoretical guarantees, proving a sub-quadratic runtime, and extend the method to a multi-threshold setting that eliminates all distributional assumptions. Furthermore, we provide the first theoretical justification for the empirical success of polynomial attention [Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be closely approximated by a combination of multiple polynomial attentions with sketching.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2510.01718" rel="nofollow">Accelerating Attention with Basis Decomposition</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-accelerating-attention-with-basis-decomposition-" class="anchor" aria-label="Permalink: 15. Accelerating Attention with Basis Decomposition" href="#15-accelerating-attention-with-basis-decomposition-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01718
<strong>Authors:</strong> Jialin Zhao</p>
<p><strong>Abstract:</strong> arXiv:2510.01718v1 Announce Type: new  Abstract: Attention is a core operation in large language models (LLMs) and vision-language models (VLMs). We present BD Attention (BDA), the first lossless algorithmic reformulation of attention. BDA is enabled by a simple matrix identity from Basis Decomposition (BD), which restructures multi-head projections into a compact form while preserving exact outputs. Unlike I/O-aware system optimizations such as FlashAttention, BDA provides a mathematically guaranteed acceleration that is architecture-agnostic. On DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with no retraining required and, on modern GPUs, achieves 32% faster key/value projections and 25% smaller weights, while increasing end-to-end perplexity (PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model performance. These results position BDA as the first theoretically exact method for lossless attention acceleration that is complementary to existing engineering-level optimizations. Our code is available at <a href="https://github.com/abcbdf/basis-decomposition-official">https://github.com/abcbdf/basis-decomposition-official</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2510.01650" rel="nofollow">The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-the-unseen-frontier-pushing-the-limits-of-llm-sparsity-with-surrogate-free-admm-" class="anchor" aria-label="Permalink: 16. The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM" href="#16-the-unseen-frontier-pushing-the-limits-of-llm-sparsity-with-surrogate-free-admm-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01650
<strong>Authors:</strong> Kwanhee Lee, Hyeondo Jang, Dongyeop Lee, Dan Alistarh, Namhoon Lee</p>
<p><strong>Abstract:</strong> arXiv:2510.01650v1 Announce Type: new  Abstract: Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Furthermore, we present $\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees. These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2510.01711" rel="nofollow">Contrastive Representation Regularization for Vision-Language-Action Models</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-contrastive-representation-regularization-for-vision-language-action-models-" class="anchor" aria-label="Permalink: 17. Contrastive Representation Regularization for Vision-Language-Action Models" href="#17-contrastive-representation-regularization-for-vision-language-action-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01711
<strong>Authors:</strong> Taeyoung Kim, Jimin Lee, Myungkyu Koo, Dongyoung Kim, Kyungmin Lee, Changyeon Kim, Younggyo Seo, Jinwoo Shin</p>
<p><strong>Abstract:</strong> arXiv:2510.01711v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have shown its capabilities in robot manipulation by leveraging rich representations from pre-trained Vision-Language Models (VLMs). However, their representations arguably remain suboptimal, lacking sensitivity to robotic signals such as control actions and proprioceptive states. To address the issue, we introduce Robot State-aware Contrastive Loss (RS-CL), a simple and effective representation regularization for VLA models, designed to bridge the gap between VLM representations and robotic signals. In particular, RS-CL aligns the representations more closely with the robot's proprioceptive states, by using relative distances between the states as soft supervision. Complementing the original action prediction objective, RS-CL effectively enhances control-relevant representation learning, while being lightweight and fully compatible with standard VLA training pipeline. Our empirical results demonstrate that RS-CL substantially improves the manipulation performance of state-of-the-art VLA models; it pushes the prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen, through more accurate positioning during grasping and placing, and boosts success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2510.01581" rel="nofollow">Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-think-right-learning-to-mitigate-under-over-thinking-via-adaptive-attentive-compression-" class="anchor" aria-label="Permalink: 18. Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression" href="#18-think-right-learning-to-mitigate-under-over-thinking-via-adaptive-attentive-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01581
<strong>Authors:</strong> Joykirat Singh, Justin Chih-Yao Chen, Archiki Prasad, Elias Stengel-Eskin, Akshay Nambi, Mohit Bansal</p>
<p><strong>Abstract:</strong> arXiv:2510.01581v1 Announce Type: new  Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2510.01899" rel="nofollow">Multimodal Foundation Models for Early Disease Detection</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-multimodal-foundation-models-for-early-disease-detection-" class="anchor" aria-label="Permalink: 19. Multimodal Foundation Models for Early Disease Detection" href="#19-multimodal-foundation-models-for-early-disease-detection-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01899
<strong>Authors:</strong> Md Talha Mohsin, Ismail Abdulrashid</p>
<p><strong>Abstract:</strong> arXiv:2510.01899v1 Announce Type: new  Abstract: Healthcare generates diverse streams of data, including electronic health records (EHR), medical imaging, genetics, and ongoing monitoring from wearable devices. Traditional diagnostic models frequently analyze these sources in isolation, which constrains their capacity to identify cross-modal correlations essential for early disease diagnosis. Our research presents a multimodal foundation model that consolidates diverse patient data through an attention-based transformer framework. At first, dedicated encoders put each modality into a shared latent space. Then, they combine them using multi-head attention and residual normalization. The architecture is made for pretraining on many tasks, which makes it easy to adapt to new diseases and datasets with little extra work. We provide an experimental strategy that uses benchmark datasets in oncology, cardiology, and neurology, with the goal of testing early detection tasks. The framework includes data governance and model management tools in addition to technological performance to improve transparency, reliability, and clinical interpretability. The suggested method works toward a single foundation model for precision diagnostics, which could improve the accuracy of predictions and help doctors make decisions.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2510.01700" rel="nofollow">VaPR -- Vision-language Preference alignment for Reasoning</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-vapr----vision-language-preference-alignment-for-reasoning-" class="anchor" aria-label="Permalink: 20. VaPR -- Vision-language Preference alignment for Reasoning" href="#20-vapr----vision-language-preference-alignment-for-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01700
<strong>Authors:</strong> Rohan Wadhawan, Fabrice Y Harel-Canada, Zi-Yi Dou, Suhaila Shakiah, Robinson Piramuthu, Nanyun Peng</p>
<p><strong>Abstract:</strong> arXiv:2510.01700v1 Announce Type: new  Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with AI-generated feedback have shown promise in aligning Large Vision-Language Models (LVLMs) with human preferences. However, existing techniques overlook the prevalence of noise in synthetic preference annotations in the form of stylistic and length biases. To this end, we introduce a hard-negative response generation framework based on LLM-guided response editing, that produces rejected responses with targeted errors, maintaining stylistic and length similarity to the accepted ones. Using this framework, we develop the VaPR dataset, comprising 30K high-quality samples, to finetune three LVLM families: LLaVA-V1.5, Qwen2VL &amp; Qwen2.5VL (2B-13B sizes). Our VaPR models deliver significant performance improvements across ten benchmarks, achieving average gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable improvements on reasoning tasks. A scaling analysis shows that performance consistently improves with data size, with LLaVA models benefiting even at smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we show that the framework generalizes to open-source LLMs as editors, with models trained on VaPR-OS achieving ~99% of the performance of models trained on \name, which is synthesized using GPT-4o. Our data, models, and code can be found on the project page <a href="https://vap-r.github.io" rel="nofollow">https://vap-r.github.io</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2510.01878" rel="nofollow">Randomized Gradient Subspaces for Efficient Large Language Model Training</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-randomized-gradient-subspaces-for-efficient-large-language-model-training-" class="anchor" aria-label="Permalink: 21. Randomized Gradient Subspaces for Efficient Large Language Model Training" href="#21-randomized-gradient-subspaces-for-efficient-large-language-model-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01878
<strong>Authors:</strong> Sahar Rajabi, Nayeema Nonta, Samanvay Vajpayee, Sirisha Rambhatla</p>
<p><strong>Abstract:</strong> arXiv:2510.01878v1 Announce Type: new  Abstract: Training large language models (LLMs) is often bottlenecked by extreme memory demands, with optimizer states dominating the footprint. Recent works mitigates this cost by projecting gradients into low-dimensional subspaces using sophisticated update strategies. In this paper, we analyze the dynamics of gradient space and its underlying subspaces. We find that while a small subspace captures most gradient energy, a significant portion still resides in the residual bulk; moreover, the influence of the core subspace diminishes over time and in deeper layers. We also observe that the gradient space exhibits near-flat curvature, calling for algorithms that explicitly account for this geometry. Motivated by these insights, we introduce a suite of randomized algorithms, GrassWalk and GrassJump, which exploit subspace and achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2510.01795" rel="nofollow">Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-nav-ee-navigation-guided-early-exiting-for-efficient-vision-language-models-in-autonomous-driving-" class="anchor" aria-label="Permalink: 22. Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving" href="#22-nav-ee-navigation-guided-early-exiting-for-efficient-vision-language-models-in-autonomous-driving-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01795
<strong>Authors:</strong> Haibo Hu, Lianming Huang, Xinyu Wang, Yufei Cui, Nan Guan, Chun Jason Xue</p>
<p><strong>Abstract:</strong> arXiv:2510.01795v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving for unified perception and reasoning, but high inference latency hinders real-time deployment. Early-exit reduces latency by terminating inference at intermediate layers, yet its task-dependent nature limits generalization across diverse scenarios. We observe that this limitation aligns with autonomous driving: navigation systems can anticipate upcoming contexts (e.g., intersections, traffic lights), indicating which tasks will be required. We propose Nav-EE, a navigation-guided early-exit framework that precomputes task-specific exit layers offline and dynamically applies them online based on navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE achieves accuracy comparable to full inference while reducing latency by up to 63.9%. Real-vehicle integration with Autoware Universe further demonstrates reduced inference latency (600ms to 300ms), supporting faster decision-making in complex scenarios. These results suggest that coupling navigation foresight with early-exit offers a viable path toward efficient deployment of large models in autonomous systems. Code and data are available at our anonymous repository: <a href="https://anonymous.4open.science/r/Nav-EE-BBC4" rel="nofollow">https://anonymous.4open.science/r/Nav-EE-BBC4</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2510.01394" rel="nofollow">Optimal Stopping vs Best-of-$N$ for Inference Time Optimization</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-optimal-stopping-vs-best-of-n-for-inference-time-optimization-" class="anchor" aria-label="Permalink: 23. Optimal Stopping vs Best-of-$N$ for Inference Time Optimization" href="#23-optimal-stopping-vs-best-of-n-for-inference-time-optimization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01394
<strong>Authors:</strong> Yusuf Kalayci, Vinod Raman, Shaddin Dughmi</p>
<p><strong>Abstract:</strong> arXiv:2510.01394v1 Announce Type: new  Abstract: Large language model (LLM) generation often requires balancing output quality against inference cost, especially when using multiple generations. We introduce a new framework for inference-time optimization based on the classical Pandora's Box problem. Viewing each generation as opening a costly "box" with random reward, we develop algorithms that decide when to stop generating without knowing the underlying reward distribution. Our first contribution is a UCB-style Pandora's Box algorithm, which achieves performance that is provably close to Weitzman's algorithm, the optimal strategy when the distribution is known. We further adapt this method to practical LLM settings by addressing reward scaling across prompts via a Bradley-Terry inspired transformation. This leads to an adaptive inference-time optimization method that normalizes rewards and learns stopping thresholds on the fly. Experiments on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs, show that our adaptive strategy can obtain the same performance as non-adaptive Best-of-N sampling while requiring 15-35 percent fewer generations on average. Our results establish a principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2510.01263" rel="nofollow">Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-budgeted-broadcast-an-activity-dependent-pruning-rule-for-neural-network-efficiency-" class="anchor" aria-label="Permalink: 24. Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency" href="#24-budgeted-broadcast-an-activity-dependent-pruning-rule-for-neural-network-efficiency-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01263
<strong>Authors:</strong> Yaron Meirovitch, Fuming Yang, Jeff Lichtman, Nir Shavit</p>
<p><strong>Abstract:</strong> arXiv:2510.01263v1 Announce Type: new  Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g., magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each unit a local traffic budget (the product of its long-term on-rate $a_i$ and fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding entropy under a global traffic budget yields a selectivity-audience balance, $\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local actuators that prune either fan-in (to lower activity) or fan-out (to reduce broadcast). In practice, BB increases coding entropy and decorrelation and improves accuracy at matched sparsity across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction, sometimes exceeding dense baselines. On electron microscopy images, it attains state-of-the-art F1 and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests a path toward learning more diverse and efficient representations.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">25. <a href="https://arxiv.org/abs/2510.01444" rel="nofollow">VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning</a> <a id="user-content-link25"></a>
</h2><a id="user-content-25-vogue-guiding-exploration-with-visual-uncertainty-improves-multimodal-reasoning-" class="anchor" aria-label="Permalink: 25. VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning" href="#25-vogue-guiding-exploration-with-visual-uncertainty-improves-multimodal-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01444
<strong>Authors:</strong> Rui Liu, Dian Yu, Tong Zheng, Runpeng Dai, Zongxia Li, Wenhao Yu, Zhenwen Liang, Linfeng Song, Haitao Mi, Pratap Tokekar, Dong Yu</p>
<p><strong>Abstract:</strong> arXiv:2510.01444v1 Announce Type: new  Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large language models (LLMs) but struggles with exploration, an issue that still persists for multimodal LLMs (MLLMs). Current methods treat the visual input as a fixed, deterministic condition, overlooking a critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided Exploration)}$, a novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as a stochastic context, VOGUE quantifies the policy's sensitivity to visual perturbations using the symmetric KL divergence between a "raw" and "noisy" branch, creating a direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with a token-entropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">26. <a href="https://arxiv.org/abs/2510.01389" rel="nofollow">INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models</a> <a id="user-content-link26"></a>
</h2><a id="user-content-26-insight-inference-time-sequence-introspection-for-generating-help-triggers-in-vision-language-action-models-" class="anchor" aria-label="Permalink: 26. INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models" href="#26-insight-inference-time-sequence-introspection-for-generating-help-triggers-in-vision-language-action-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01389
<strong>Authors:</strong> Ulas Berk Karli, Ziyao Shangguan, Tesca FItzgerald</p>
<p><strong>Abstract:</strong> arXiv:2510.01389v1 Announce Type: new  Abstract: Recent Vision-Language-Action (VLA) models show strong generalization capabilities, yet they lack introspective mechanisms for anticipating failures and requesting help from a human supervisor. We present \textbf{INSIGHT}, a learning framework for leveraging token-level uncertainty signals to predict when a VLA should request help. Using $\pi_0$-FAST as the underlying model, we extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based estimates of \emph{aleatoric and epistemic uncertainty}, and train compact transformer classifiers to map these sequences to help triggers. We explore supervision regimes for strong or weak supervision, and extensively compare them across in-distribution and out-of-distribution tasks. Our results show a trade-off: strong labels enable models to capture fine-grained uncertainty dynamics for reliable help detection, while weak labels, though noisier, still support competitive introspection when training and evaluation are aligned, offering a scalable path when dense annotation is impractical. Crucially, we find that modeling the temporal evolution of token-level uncertainty signals with transformers provides far greater predictive power than static sequence-level scores. This study provides the first systematic evaluation of uncertainty-based introspection in VLAs, opening future avenues for active learning and for real-time error mitigation through selective human intervention.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) and Mixture of Experts (MoE) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research introduces a novel multimodal large model that integrates text, language, and vision modalities. The goal is to advance the performance and generalization capabilities of the model by establishing a new approach for training and fusing these modalities effectively, rather than focusing on incremental optimizations.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>