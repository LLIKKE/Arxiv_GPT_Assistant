<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">RetroInfer：一种面向可扩展长上下文LLM推理的向量存储方案</h2><a id="user-content-retroinfer一种面向可扩展长上下文llm推理的向量存储方案" class="anchor" aria-label="Permalink: RetroInfer：一种面向可扩展长上下文LLM推理的向量存储方案" href="#retroinfer一种面向可扩展长上下文llm推理的向量存储方案"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明如下）</p>
<ol>
<li>"RetroInfer"作为专有技术名称保留不译，符合技术领域术语处理惯例</li>
<li>"Vector-Storage Approach"译为"向量存储方案"，其中：
<ul>
<li>"Vector"采用计算机领域标准译法"向量"（非"矢量"）</li>
<li>"Approach"根据技术语境译为"方案"（比"方法"更体现系统性）</li>
</ul>
</li>
<li>"Scalable Long-Context LLM Inference"译为"可扩展长上下文LLM推理"：
<ul>
<li>保持"LLM"（大语言模型）缩写形式，符合AI领域术语规范</li>
<li>"Long-Context"译为"长上下文"，准确传达技术概念</li>
<li>"Scalable"译为"可扩展"，突出系统扩展能力</li>
</ul>
</li>
<li>整体采用"定语前置"的中文技术文献命名习惯，结构紧凑专业</li>
</ol>
<p>arXiv:2505.02922v1 公告类型：新成果<br>
摘要：随着大语言模型（LLM）上下文长度的持续增长，GPU内存与带宽限制导致的推理效率问题日益凸显。我们提出RetroInfer系统，通过将键值（KV）缓存重构为向量存储系统，利用注意力机制固有的稀疏性来加速长上下文LLM推理。其核心是波浪索引（wave index）——一种"注意力感知向量索引"，通过三重注意力近似、精度有界的注意力估计及分段聚类等技术，实现关键令牌的高效精准检索。与之配合的波浪缓冲区（wave buffer）协调KV缓存布局，在GPU与CPU间重叠计算与数据传输以维持高吞吐。不同于以往稀疏化方法在令牌选择和硬件协同上的局限，RetroInfer在保持模型精度的同时提供强劲性能。长上下文基准测试显示：在GPU内存限制内较全注意力提速达4.5倍；当KV缓存扩展至CPU内存时，较稀疏注意力基线提升10.5倍，且始终维持全注意力级别的准确率。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语保留英文缩写（如LLM/KV/GPU/CPU）并添加必要中文注解</li>
<li>"wave index"译为"波浪索引"并保留英文原名首现标注，技术名词如"tripartite attention approximation"采用意译</li>
<li>长难句拆分重组，如将"techniques such as..."处理为中文惯用的顿号列举结构</li>
<li>被动语态转换（如"are extended to"译为主动式"扩展至"）</li>
<li>数字单位标准化（"4.5X"→"4.5倍"）</li>
<li>保持学术文本严谨性，避免口语化表达）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">SPAP：基于交替优化与惩罚方法的结构化剪枝</h2><a id="user-content-spap基于交替优化与惩罚方法的结构化剪枝" class="anchor" aria-label="Permalink: SPAP：基于交替优化与惩罚方法的结构化剪枝" href="#spap基于交替优化与惩罚方法的结构化剪枝"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.03373v1 公告类型：新论文<br>
摘要：大型语言模型（LLM）的部署常受限于其巨大的计算与内存开销。虽然结构化剪枝通过移除整个网络组件提供了可行方案，但现有方法存在性能下降、依赖启发式指标或微调成本高昂等问题。为应对这些挑战，我们提出SPAP（基于交替优化与惩罚方法的结构化剪枝）——一种基于优化理论的新型高效LLM结构化剪枝框架。SPAP通过混合整数优化模型构建剪枝问题，采用能有效最小化剪枝误差的惩罚方法进行剪枝决策，并针对可拆分问题结构设计了交替最小化算法，以实现高效的权重更新与性能恢复。在OPT、LLaMA-3/3.1/3.2及Qwen2.5模型上的大量实验表明，SPAP优于当前最先进方法，可实现线性推理加速（30%稀疏度时达1.29倍）与按比例降低内存占用。本研究为保持模型性能的同时进行LLM剪枝提供了实用的优化驱动解决方案。</p>
<p>（注：根据学术论文摘要的文体特征，翻译时注意了以下要点：</p>
<ol>
<li>专业术语统一："structured pruning"译为"结构化剪枝"、"mixed-integer optimization"译为"混合整数优化"等</li>
<li>技术概念准确传达：如"penalty method"译为"惩罚方法"而非"惩罚机制"以保持数学原意</li>
<li>长句拆分：将原文复合句按中文习惯分解为多个短句</li>
<li>数据呈现规范：保留数学符号格式（如1.29×）</li>
<li>被动语态转换："is formulated"译为主动态的"构建"</li>
<li>机构名称保留：OPT/LLaMA等模型名称不翻译）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">广播：大型语言模型压缩的率失真优化</h2><a id="user-content-广播大型语言模型压缩的率失真优化" class="anchor" aria-label="Permalink: 广播：大型语言模型压缩的率失真优化" href="#广播大型语言模型压缩的率失真优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.03031v1 公告类型：新研究<br>
摘要：近年来，大型语言模型（LLM）的压缩已成为推动LLM在资源有限设备上部署、降低计算成本并减轻大规模AI基础设施对环境影响的关键问题。本文从率失真理论的视角奠定了LLM量化的理论基础，并提出了一种基于简单率失真优化的量化技术。我们的技术可扩展至包含数千亿权重参数的模型，并允许用户在训练后根据需求将模型压缩至指定大小或精度，提供灵活的压缩选择。</p>
<p>（注：根据学术文献翻译规范，专业术语如"rate-distortion theory"采用"率失真理论"这一标准译法；"post-training"译为"训练后"以准确体现模型压缩阶段；"flexibility"译为"灵活性"时通过增补"提供...选择"使中文表达更流畅；长句如"scales to models..."通过拆分和"可扩展至...并允许..."的句式转换实现中文的流水句效果；被动语态"is specified by"转化为主动式"根据需求"以符合中文表达习惯。）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>