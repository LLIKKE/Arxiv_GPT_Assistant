<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">FlyLoRA：通过隐式级别专家混合提高任务脱钩和参数效率</h2><a id="user-content-flylora通过隐式级别专家混合提高任务脱钩和参数效率" class="anchor" aria-label="Permalink: FlyLoRA：通过隐式级别专家混合提高任务脱钩和参数效率" href="#flylora通过隐式级别专家混合提高任务脱钩和参数效率"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08396v1宣布类型：新摘要：低等级自适应（LoRA）是一种广泛使用的基础模型参数高效微调方法，但它受到参数干扰，导致性能次优。尽管基于专家混合（MoE）的LoRA变体在单任务指令调优中显示出缓解任务内相关性的希望，但它们引入了额外的路由器参数，并且在出现任务间干扰的多任务模型合并中仍然无效。受苍蝇嗅觉电路的启发，我们提出了FlyLoRA，这是一种基于MoE的隐式LoRA变体，它引入了：（1）上投影矩阵中的按等级的专家激活，以及（2）统一专家路由和下投影的隐式路由器，其中冻结的稀疏随机投影矩阵取代了传统的密集可训练版本。该设计通过消除对显式路由器的需要，解决了任务内去相关和计算效率之间的权衡，同时由于随机矩阵的垂直性而从本质上减轻了任务间干扰。跨越四个领域（常识理解、科学问题回答、数学推理和代码生成）的广泛实验证明了与现有方法相比一致的性能改进。除了经验收益之外，FlyLoRA还强调了生物结构如何激发人工智能技术的创新。代码可在<a href="https://github.com/gfyddha/FlyLoRA%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gfyddha/FlyLoRA上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">更好地在一起：利用未配对的多峰数据建立更强大的单峰模型</h2><a id="user-content-更好地在一起利用未配对的多峰数据建立更强大的单峰模型" class="anchor" aria-label="Permalink: 更好地在一起：利用未配对的多峰数据建立更强大的单峰模型" href="#更好地在一起利用未配对的多峰数据建立更强大的单峰模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08492v1宣布类型：新摘要：传统的多模式学习者为视觉问答等任务找到统一的表示，但严重依赖配对数据集。然而，一个被忽视但潜在强大的问题是：可以利用辅助的未配对多模式数据来直接增强目标模式中的表示学习吗？我们介绍了统一建模语言：Unpayed Multimodal Learner，一种模式不可知的训练范式，其中单个模型交替处理来自不同模式的输入，同时共享它们之间的参数。该设计利用了不同的模式是共享基本现实的投影的假设，使模型能够从跨模式结构中受益，而无需显式配对。从理论上讲，在线性数据生成假设下，我们表明，不成对的辅助数据可以产生比单模式训练更详细的有关数据生成过程的表示。从经验上看，我们表明，使用来自辅助模式（例如文本、音频或图像）的未配对数据可以持续提高图像和音频等各种单模式目标的下游性能。我们的项目页面：<a href="https://unpaired-multimodal.github.io/" rel="nofollow">https://unpaired-multimodal.github.io/</a></p>
<div class="markdown-heading"><h2 class="heading-element">更少的权重，更多的问题：对LLM修剪的实用攻击</h2><a id="user-content-更少的权重更多的问题对llm修剪的实用攻击" class="anchor" aria-label="Permalink: 更少的权重，更多的问题：对LLM修剪的实用攻击" href="#更少的权重更多的问题对llm修剪的实用攻击"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07985v1宣布类型：新摘要：模型修剪，即，删除模型权重的子集已成为减少推理期间大型语言模型（LLM）内存占用的一种主要方法。值得注意的是，vLLM等流行推理引擎使用户能够在部署下载的模型之前方便地修剪它们。虽然修剪方法的实用性和效率有了显着提高，但修剪的安全影响仍然没有得到充分的研究。在这项工作中，我们首次表明现代LLM修剪方法可以被恶意利用。特别是，对手可以构建一个看起来良性但一旦修剪，就会表现出恶意行为的模型。我们的方法基于这样的想法：对手可以计算代理指标，该指标估计每个参数被修剪的可能性。有了这些信息，对手可以首先将恶意行为注入到那些不太可能被修剪的参数中。然后，他们可以通过使用可能被修剪的参数来修复模型，从而有效地抵消未修剪模型中注入的行为。我们通过对五个模型的广泛评估来证明攻击的严重性;应用vLLM中的任何修剪（Magnitude、Wanda和SparseGPT）后，它在各种攻击场景中始终表现出强烈的恶意行为（越狱成功率高达95.7美元，良性指令拒绝成功率高达98.7美元，定向内容注入成功率高达99.5美元）。我们的结果揭示了一个关键的部署时安全差距，并强调了模型压缩中迫切需要更强的安全意识。</p>
<div class="markdown-heading"><h2 class="heading-element">想够了：序列级熵作为LLM推理的置信信号</h2><a id="user-content-想够了序列级熵作为llm推理的置信信号" class="anchor" aria-label="Permalink: 想够了：序列级熵作为LLM推理的置信信号" href="#想够了序列级熵作为llm推理的置信信号"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08146v1宣布类型：新摘要：我们引入了一个简单但新颖的基于熵的框架，以在推理任务期间提高大型语言模型中的标记效率。我们的方法使用来自令牌级logprobs的香农信息作为置信信号来实现早期停止，在保持任务准确性的同时节省25-50%的计算量。至关重要的是，我们证明了基于信息量的置信度校准代表了现代推理模型中存在的高级训练后优化的一个新兴属性，但在标准的描述调整和预训练模型中明显缺乏这种属性（Llama 3.3 70 B）。我们表明，停止推理的熵阈值因模型而异，但只需使用现有推理数据集中的几个示例即可轻松一次性计算。我们的结果表明，高级推理模型通常知道他们很早就得到了正确的答案，并且可以利用这种新出现的信心意识来保存令牌并减少延迟。该框架展示了推理优化模型家族之间的一致性能，计算成本降低了25-50%，同时保持了准确性，揭示了置信机制代表了现代后训练推理系统与其前身的显着特征。</p>
<div class="markdown-heading"><h2 class="heading-element">双臂仿人机器人的自我感知优先规划</h2><a id="user-content-双臂仿人机器人的自我感知优先规划" class="anchor" aria-label="Permalink: 双臂仿人机器人的自我感知优先规划" href="#双臂仿人机器人的自我感知优先规划"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07882v1宣布类型：新摘要：近年来，多模式大型语言模型（MLLM）已经证明了充当高级规划者的能力，使机器人能够遵循复杂的人类指令。然而，它们的有效性，尤其是在涉及双臂人形机器人的长期任务中，仍然有限。这一限制源于两个主要挑战：（i）缺乏系统性地支持人形机器人任务评估和数据收集的模拟平台，以及（ii）当前MLLM的化身意识不足，这阻碍了对双臂选择逻辑和身体位置的推理。规划期间。为了解决这些问题，我们推出了DualTHOR，这是一种新型双臂人形模拟器，具有连续过渡和应急机制。在这个平台上，我们提出了Proprio-MLLM，这是一个通过将主体感受信息与基于运动的位置嵌入和跨空间编码器结合来增强化身感知的模型。实验表明，虽然现有的MLLM在这种环境中遇到困难，但Proprio-MLLM在规划性能方面实现了19.75%的平均改进。我们的工作提供了一个重要的模拟平台和一个有效的模型来推进人形机器人中的体现智能。该代码可在<a href="https://anonymous.4open.science/r/DualTHOR-5F3B%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82" rel="nofollow">https://anonymous.4open.science/r/DualTHOR-5F3B上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">回收预训练检查点：专家混合的垂直增长，以实现高效的大型语言模型预训练</h2><a id="user-content-回收预训练检查点专家混合的垂直增长以实现高效的大型语言模型预训练" class="anchor" aria-label="Permalink: 回收预训练检查点：专家混合的垂直增长，以实现高效的大型语言模型预训练" href="#回收预训练检查点专家混合的垂直增长以实现高效的大型语言模型预训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08008v1宣布类型：新摘要：预训练大型语言模型的计算成本迅速增加，需要更有效的方法。现有的训练有素的检查站投入了大量计算成本，但由于工程限制或模型容量有限，其中许多检查站仍然没有得到充分利用。为了有效地重复使用这种“沉没”成本，我们建议通过扩大参数计数和继续训练来回收预训练的检查点。我们提出了非常适合融合的专家混合模型的垂直增长方法：插入层复制以实现深度增长，专家复制以实现宽度增长。为了确定检查点序列之间这种增长的最佳时机，我们进行了全面的扩展实验，揭示了最终的准确性与沉没成本的量具有很强的正相关性，这表明更大的前期投资会带来更好的性能。我们将方法扩展到具有70 B参数和超过1 T训练令牌的模型，在相同的额外计算预算下，比从头开始训练实现了10.66%的准确性提高。我们的检查点回收方法为经济高效的大型语言模型预训练奠定了基础。</p>
<div class="markdown-heading"><h2 class="heading-element">AILoRA：用于大型语言模型低等级适应的功能感知不对称工作空间</h2><a id="user-content-ailora用于大型语言模型低等级适应的功能感知不对称工作空间" class="anchor" aria-label="Permalink: AILoRA：用于大型语言模型低等级适应的功能感知不对称工作空间" href="#ailora用于大型语言模型低等级适应的功能感知不对称工作空间"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08034v1宣布类型：新摘要：参数高效微调（PEFT）旨在减轻将大规模预训练模型适应各种下游任务所涉及的大量计算和内存负担。在众多PEFT策略中，低等级自适应（LoRA）因其稳健的经验性能和较低的实现复杂性而成为最广泛采用的方法之一。在实际部署中，LoRA通常应用于自我注意模块的$W &amp; Q$和$W &amp; V$投影矩阵，从而实现模型性能和参数效率之间的有效权衡。虽然LoRA取得了相当大的经验成功，但它仍然面临着次优性能和收敛缓慢等挑战。为了解决这些限制，我们引入了\textBF{AILoRA}，这是一种新型的参数高效方法，它结合了功能感知的非对称低等级先验。我们的实证分析表明，自我注意机制中的投影矩阵$W ' Q '和$W ' V '表现出明显的参数特征，这源于它们的功能差异。具体来说，$W ' Q$捕获了对于注意力分布计算至关重要的特定任务语义空间知识，使其参数对下游任务变化高度敏感。相比之下，$W ' V$编码符号级特征表示，这些特征往往在任务和层之间保持稳定。利用这些见解，AILoRA通过注入$W &amp; Q$的主成分以保留任务自适应能力，并注入$W &amp; V$的次成分以保留可概括的特征表示来执行功能感知初始化。这种非对称初始化策略使LoRA模块能够更好地捕捉注意力参数的特定角色，从而提高微调性能和收敛效率。</p>
<div class="markdown-heading"><h2 class="heading-element">用于多峰图神经架构搜索的模式感知合作协同进化框架</h2><a id="user-content-用于多峰图神经架构搜索的模式感知合作协同进化框架" class="anchor" aria-label="Permalink: 用于多峰图神经架构搜索的模式感知合作协同进化框架" href="#用于多峰图神经架构搜索的模式感知合作协同进化框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07325v1宣布类型：新摘要：对软件漏洞的共同利用攻击给企业带来了严重的风险，可以通过分析异类和多模式漏洞数据来缓解这一威胁。多模式图神经网络（MGNN）非常适合整合跨模式的互补信号，从而提高攻击预测准确性。然而，设计有效的MGNN架构具有挑战性，因为它需要协调每层的特定模式组件，而通过手动调优这是不可行的。基于遗传算法（GA）的图神经架构搜索（GNAS）提供了一种自然的解决方案，但现有方法仅限于单一模式并忽视了模式的多样性。为了解决这一局限性，我们提出了一种用于多模式图神经架构搜索的模式感知合作协同进化算法，称为MAC-MGNAS。首先，我们在分而治之下开发了一个模式感知的合作协同进化（MACC）框架：协调员将全球染色体群体划分为模式特定的基因组，当地工人独立进化它们，协调员重新组装染色体进行联合评估。该框架有效地捕捉了单模式GNAS忽视的模式多样性。其次，我们引入了一种模式感知的双轨代理（MASYS）方法来降低评估成本并加速局部基因进化。第三，我们设计了基于相似性的人口多样性指标（SLDI）策略，以自适应地平衡探索和利用，从而加速收敛并避免局部最优。在标准漏洞协同利用（VulCE）数据集上，MAACC-MGNAS仅在3个GPU-h内就获得了81.67%的F1评分，比最先进的竞争对手高出8.7%，同时将计算成本降低了27%。</p>
<div class="markdown-heading"><h2 class="heading-element">不要拿着剪刀跑步：修剪会破坏VLA模型，但可以修复</h2><a id="user-content-不要拿着剪刀跑步修剪会破坏vla模型但可以修复" class="anchor" aria-label="Permalink: 不要拿着剪刀跑步：修剪会破坏VLA模型，但可以修复" href="#不要拿着剪刀跑步修剪会破坏vla模型但可以修复"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08464v1宣布类型：新摘要：视觉-语言-动作（VLA）模型具有先进的机器人功能，但在资源有限的硬件上部署仍然具有挑战性。修剪可以有效压缩大型语言模型（LLM），但机器人技术中对其研究很大程度上不足。令人惊讶的是，我们观察到修剪VLA模型会导致严重退化和增加安全违规行为。我们引入GLUESTICK，这是一种修剪后恢复方法，可以恢复原始模型的大部分功能，同时保留稀疏性优势。我们的方法在权重空间中的密集模型和修剪模型之间执行一次性插值，以计算纠正项。每个修剪层在推理期间使用此纠正，以最小的负担恢复丢失的功能。GLUESTICK不需要额外的训练，对修剪算法不可知，并引入了一个控制效率和准确性之间权衡的超参数。在各种VLA架构和操作和导航任务中，GLUESTICK实现了有竞争力的内存效率，同时大幅恢复成功率并减少安全违规行为。其他材料可在<a href="https://gluestick-vla.github.io/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82" rel="nofollow">https://gluestick-vla.github.io/上找到。</a></p>
<div class="markdown-heading"><h2 class="heading-element">从代币到层：重新定义采用分层预填充服务的LLM无失速调度</h2><a id="user-content-从代币到层重新定义采用分层预填充服务的llm无失速调度" class="anchor" aria-label="Permalink: 从代币到层：重新定义采用分层预填充服务的LLM无失速调度" href="#从代币到层重新定义采用分层预填充服务的llm无失速调度"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08055v1宣布类型：新摘要：生产中的大型语言模型（LLM）推理必须满足首个令牌时间（TTFT）和令牌间隔时间（TBT）的严格服务级别目标，同时在固定计算、内存和互连预算下最大化吞吐量。现代服务系统采用无停顿调度技术，如分块预填充，它沿着令牌维度分割长提示处理，并将预填充与正在进行的解码迭代交织。虽然在稳定TBT方面有效，但分块预填充在专家混合（MoE）模型中会产生大量开销：冗余专家权重负载会使内存流量增加高达39%，并增加能耗。我们提出分层预填充，这是一种新的调度范式，将Transformer层组视为主要调度单元。通过将模型垂直划分为连续的层组并在组之间交织预填充和解码，分层预填充可以维持无失速解码，同时消除块引起的MoE权重重新加载。它降低了片外带宽需求，将TTFT降低了70%，端到端延迟降低了41%，每个令牌的能量降低了22%。评估结果表明，分层预填充一贯提高TTFT-TBT帕累托前沿分块预填充，减少专家负载流量和能源成本，同时保持无失速解码。总的来说，将调度轴从令牌转移到层解锁了一种新的操作机制，用于在协同定位的环境中提供高效率，能量感知的LLM服务。</p>
<div class="markdown-heading"><h2 class="heading-element">混合和MoE-DPO：直接偏好优化的变分推理方法</h2><a id="user-content-混合和moe-dpo直接偏好优化的变分推理方法" class="anchor" aria-label="Permalink: 混合和MoE-DPO：直接偏好优化的变分推理方法" href="#混合和moe-dpo直接偏好优化的变分推理方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08256v1宣布类型：新摘要：直接偏好优化（DPO）最近成为人类反馈强化学习（RL HF）的一种简单有效的替代方案，用于将大型语言模型（LLM）与用户偏好对齐。然而，现有的DPO配方依赖于单一的整体模型，这限制了它们在多任务环境中的表现力及其对异类或多样化偏好分布的适应性。在这项工作中，我们提出了Mix-and MoE-DPO，这是一个使用随机变分推理方法、通过软混合模型和混合专家（MoE）架构扩展DPO的框架。我们的方法在专家任务上引入了潜变量模型，并优化了变分证据下限（ELBO），从而能够从偏好数据中稳定有效地学习专业专家政策。混合和MoE-DPO提供了比标准DPO的三个关键优势：（i）通过混合通过泛函数逼近进行概括;（ii）通过针对不同偏好模式定制的专家组件进行奖励和政策专业化;（iii）通过依赖于输入的软门控进行上下文对齐，从而实现特定于用户的混合政策。我们的框架支持具有专家特定政策负责人的共享基础架构和完全独立的专家模型，允许在参数效率和专业化之间灵活权衡。我们在各种模型大小和多偏好数据集上验证了我们的方法，证明Mix-和MoE-DPO为基于偏好的LLM对齐提供了一种强大且可扩展的方法。</p>
<div class="markdown-heading"><h2 class="heading-element">测试时间匹配：解锁多模式模型中的成分推理</h2><a id="user-content-测试时间匹配解锁多模式模型中的成分推理" class="anchor" aria-label="Permalink: 测试时间匹配：解锁多模式模型中的成分推理" href="#测试时间匹配解锁多模式模型中的成分推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07632v1宣布类型：新摘要：前沿人工智能模型已经取得了显着的进展，但最近的研究表明，它们在合成推理方面遇到了困难，通常在既定基准上以随机机会或以下的表现。我们重新审视这个问题，并表明广泛使用的评估指标系统性地低估了模型的能力。为了解决这个问题，我们引入了一种组匹配分数，它可以更好地利用组结构，并揭示对比视觉语言模型（VLMS）和多模式大型语言模型（MLLM）中的大量隐藏能力。此外，简单地过度适应测试时诱导的组匹配，就可以将这种隐藏的能力转化为标准评估指标下的更高分数，从而缩小了大部分报告的差距。此次调整使SigLIP-B16能够超越所有之前的结果，并使GPT-4.1产生第一个超越Winoground上估计的人类表现的结果。   基于这一见解，我们提出了测试时间匹配（TTM），这是一种迭代的自我改进算法，可以在没有任何外部监督的情况下进一步引导模型性能。TTM提供了额外的、非平凡的改进：例如，TTM使SigLIP-B16在MMVP-VLM上超越GPT-4.1，建立了新的最新技术水平。重要的是，即使在没有指标诱导效应或组结构的基准上，TTM仍然广泛有效，在WhatsUp等具有挑战性的数据集上实现了高达85.7%的相对收益。在跨越不同设置的16个数据集变体中，我们的实验表明TTM持续提高模型性能并推进组合推理的前沿。</p>
<div class="markdown-heading"><h2 class="heading-element">数据稀缺和分布转移下通过多模式联合训练进行高效概括</h2><a id="user-content-数据稀缺和分布转移下通过多模式联合训练进行高效概括" class="anchor" aria-label="Permalink: 数据稀缺和分布转移下通过多模式联合训练进行高效概括" href="#数据稀缺和分布转移下通过多模式联合训练进行高效概括"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07509v1宣布类型：新摘要：本文探讨了一种多模式联合训练框架，旨在在标记数据有限且发生分布变化的情况下增强模型的概括性。我们彻底研究了这个框架的理论基础，得出了使用未标记数据和促进不同模式的分类器之间的一致性导致概括性显着改进的条件。我们还提出了收敛分析，证实了迭代协同训练在减少分类错误方面的有效性。此外，我们还建立了一个新颖的概括界限，该界限首次在多模式联合训练环境中分解和量化了利用未标记的多模式数据、促进视图间一致和维护条件视图独立性所获得的独特优势。我们的研究结果强调了多模式联合训练作为一种开发数据高效且稳健的人工智能系统的结构化方法的实际好处，该系统可以在动态的现实世界环境中有效地概括。理论基础是在与既定联合培训原则对话并在既定联合培训原则之前进行的。</p>
<div class="markdown-heading"><h2 class="heading-element">FinMR：高级金融推理的知识密集型多模式基准</h2><a id="user-content-finmr高级金融推理的知识密集型多模式基准" class="anchor" aria-label="Permalink: FinMR：高级金融推理的知识密集型多模式基准" href="#finmr高级金融推理的知识密集型多模式基准"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07852v1宣布类型：新摘要：多模式大型语言模型（MLLM）近年来取得了实质性进展。然而，由于缺乏以专业水平知识强度、详细注释和高级推理复杂性为特征的数据集，它们在金融等专业领域的严格评估受到阻碍。为了解决这一关键差距，我们引入了FinMR，这是一种高质量、知识密集型的多模式数据集，明确旨在以专业分析师的标准评估专家级金融推理能力。FinMR由3，200多个精心策划和专业注释的问答对组成，涵盖15个不同的金融主题，确保广泛的领域多样性，并集成复杂的数学推理、先进的金融知识和多种图像类型的细致入微的视觉解释任务。通过与领先的开源和开源MLLM进行全面的基准测试，我们强调了这些模型与专业财务分析师之间的显着绩效差异，揭示了模型进步的关键领域，例如精确的图像分析、复杂财务公式的准确应用以及更深入的背景财务理解。通过提供丰富多样的视觉内容和彻底的解释性注释，FinMR将自己确立为评估和推进多模式财务推理以提高专业分析师级别能力的重要基准工具。</p>
<div class="markdown-heading"><h2 class="heading-element">LLM显微镜下的学习：全栈视图的方法和技巧</h2><a id="user-content-llm显微镜下的学习全栈视图的方法和技巧" class="anchor" aria-label="Permalink: LLM显微镜下的学习：全栈视图的方法和技巧" href="#llm显微镜下的学习全栈视图的方法和技巧"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07626v1宣布类型：新摘要：大型语言模型（LLM）的机器去学习旨在删除不需要的数据、知识和行为（例如，为了安全、隐私或版权），同时保留有用的模型功能。尽管过去两年取得了迅速的进展，但LLM忘记学习的研究仍然支离破碎，对于什么构成有效的忘记学习以及如何严格评估它的清晰度有限。在这项工作中，我们对最近的十二种有状态的去学习方法提出了原则性的分类，分为三个方法族：分歧驱动的优化、表示失准和基于拒绝的有针对性的去学习。在此分类法的基础上，我们重新审视了对取消学习有效性（UE）、效用保留（UT）和稳健性（Rob）的评估，重点关注WMDP基准。我们的分析表明，当前的评估以多项选择题（MCQ）准确性为主，仅提供了狭隘的视角，经常夸大成功，而忽视了模型的实际生成行为。为了解决这一差距，我们引入了开放式问答（Open-QA）指标，可以更好地捕捉生成性能并揭示方法系列之间固有的UE-UT权衡。此外，我们证明稳健性需要更细粒度的分析：例如，域内重新学习和域外微调之间的漏洞存在很大差异，尽管两者都受到模型级攻击。通过这项研究，我们希望对LLM的遗忘进行全面回顾，并为设计和评估未来的方法提供可操作的指导。</p>
<div class="markdown-heading"><h2 class="heading-element">视觉语言模型的近似领域取消学习</h2><a id="user-content-视觉语言模型的近似领域取消学习" class="anchor" aria-label="Permalink: 视觉语言模型的近似领域取消学习" href="#视觉语言模型的近似领域取消学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08132v1宣布类型：新摘要：预训练的视觉语言模型（VLM）具有强大的概括能力，使它们能够在无需额外训练的情况下识别不同领域的广泛对象。然而，它们通常保留超出特定下游任务要求的无关信息，从而引发了对计算效率和潜在信息泄露的担忧。这激发了人们对近似取消学习的兴趣，其目的是选择性地删除不必要的知识，同时保留整体模型性能。现有的近似去学习方法主要集中在类去学习，其中VLM被重新训练，以无法识别指定对象类，同时保持其他对象类的准确性。然而，在实际应用中，仅仅忘记对象类通常是不够的。例如，自动驾驶系统应该准确地识别真实汽车，同时避免将路边广告中描绘的插图汽车误识别为真实汽车，这可能是危险的。在本文中，我们引入了近似域取消学习（ADU），这是一种新颖的问题设置，需要降低指定域（例如，插图）同时保留其他域的准确性（例如，真实的）。ADU提出了新的技术挑战：由于预训练的VLM具有强大的域概括能力，域分布在特征空间中高度纠缠，使得基于惩罚目标域的天真方法无效。为了解决这一限制，我们提出了一种新颖的方法，可以显式地解开域分布并自适应地捕获特定于实例的域信息。大量实验表明，我们的方法优于基于VLM调优技术构建的基线，为VLM中的实用细粒度去学习铺平了道路。代码：<a href="https://kodaikawamura.github.io/Domain_Unlearning/%E3%80%82" rel="nofollow">https://kodaikawamura.github.io/Domain_Unlearning/。</a></p>
<div class="markdown-heading"><h2 class="heading-element">MeSH：回归变形金刚的记忆状态高速公路</h2><a id="user-content-mesh回归变形金刚的记忆状态高速公路" class="anchor" aria-label="Permalink: MeSH：回归变形金刚的记忆状态高速公路" href="#mesh回归变形金刚的记忆状态高速公路"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07739v1宣布类型：新摘要：回归变换器多次重复使用参数并对隐藏状态进行迭代，将计算深度与参数深度脱钩。然而，在匹配计算下，参数较少的回归模型往往落后于非回归模型。通过探测隐藏状态，我们将这种性能差距追溯到两个主要瓶颈：无差异计算（核心被迫在每次迭代中采用类似的计算模式）和信息过载（长期生存和瞬时信息必须共存于单个隐藏状态）。为了解决这些问题，我们引入了一种存储状态高速公路（MeSH）方案，该方案将状态管理外部化到显式存储缓冲区中，并采用轻量级路由器来动态地使迭代中的计算多样化。探索可视化证实MeSH通过在迭代中诱导功能专门化成功解决了病理问题。在Pythia套件（160 M-1.4B）上，MeSH增强的回归转换器持续优于回归基线，并在1.4B规模上优于更大的非回归转换器，将平均下游准确性提高+1.06%，非嵌入参数减少33%。我们的分析建立MeSH作为一个可扩展的和原则性的架构，用于构建更强大的递归模型。</p>
<div class="markdown-heading"><h2 class="heading-element">NUM和U 0：通用水下机器人的视觉-语言-动作数据集和模型</h2><a id="user-content-num和u-0通用水下机器人的视觉-语言-动作数据集和模型" class="anchor" aria-label="Permalink: NUM和U 0：通用水下机器人的视觉-语言-动作数据集和模型" href="#num和u-0通用水下机器人的视觉-语言-动作数据集和模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07869v1宣布类型：新摘要：水下环境给机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的可见性和受限的通信。尽管数据驱动方法在陆地机器人中提高了具体智能，并实现了特定任务的自主水下机器人，但开发能够自主执行多项任务的水下智能仍然具有高度挑战性，因为大规模、高质量的水下数据集仍然稀缺。为了解决这些限制，我们引入了NUM，这是一种用于水下机器人的基于模拟的多任务视觉-语言-动作（VLA）数据集。NUM包含来自1，852个轨迹的超过561 K个帧，总共在9种不同场景中的20项任务中进行约15.6小时的BlueROV 2交互，范围从视觉导航到移动操纵。在此数据集的基础上，我们提出了U 0，这是一种适用于通用水下机器人的VLA模型，它通过多模式融合集成了双眼视觉和其他传感器模式，并进一步集成了基于卷积注意力的感知焦点增强模块（CAP）来改善空间理解和移动操纵。在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操纵任务中，与基线方法相比，它将与目标的距离缩短了21.2%，证明了其有效性。NUM和U 0表明VLA模型可以有效应用于水下机器人应用，为可扩展的数据集构建、提高任务自主性以及智能通用水下机器人的实际实现提供了基础。</p>
<div class="markdown-heading"><h2 class="heading-element">LiveThinking：通过强化学习为人工智能驱动的直播实现实时高效推理</h2><a id="user-content-livethinking通过强化学习为人工智能驱动的直播实现实时高效推理" class="anchor" aria-label="Permalink: LiveThinking：通过强化学习为人工智能驱动的直播实现实时高效推理" href="#livethinking通过强化学习为人工智能驱动的直播实现实时高效推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07685v1宣布类型：新摘要：在人工智能驱动的电子商务直播中，数字化身需要实时响应来推动参与度，而高延迟大型推理模型（LRM）不适合完成这项任务。我们引入LiveThinking，这是一个实用的两阶段优化框架，以弥补这一差距。首先，我们通过使用拒绝采样微调（RFT）将670 B教师LRM提炼成轻量级的30 B专家混合（MoE）模型（3B活动）来解决计算成本。这减少了部署负担，但保留了教师的冗长推理，从而导致延迟。为了解决这个问题，我们的第二阶段采用强化学习和组相对政策优化（GRPO）来压缩模型的推理路径，并在平衡正确性、帮助性和简洁性的多目标奖励函数的指导下。LiveThinking将计算成本降低30倍，实现亚秒级延迟。在Taobao Live的现实应用中，响应正确性提高了3.3%，帮助性提高了21.8%。经过数十万观众的测试，我们的系统导致商品总销量（GMV）在统计上显着增长，证明了其在增强实时交互环境中的用户体验和商业表现方面的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">如何教授大型多峰模型新技能</h2><a id="user-content-如何教授大型多峰模型新技能" class="anchor" aria-label="Permalink: 如何教授大型多峰模型新技能" href="#如何教授大型多峰模型新技能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08564v1宣布类型：新摘要：我们如何在不抹去先前能力的情况下教授大型多峰模型（LSYS）新技能？我们研究了对五种目标技能的连续微调，同时监控三个模型系列中的八个既定基准的一般能力。我们观察到，在狭隘的微调后，对已完成的任务的明显“遗忘”可以在后期部分恢复。我们将这种行为追踪到输出代币分布的可测量变化，这通过与遗忘协变的简单计数偏差探测来表现出来。在这张图片的指导下，我们确定了两种简单、稳健的调整食谱，它们可以在限制漂移的同时进行强学习：（i）仅更新自我注意投影层，以及（ii）仅更新MLP Gate &amp; Up，同时冻结向下投影。在各个模型和任务中，这些选择可以带来强劲的目标收益，同时在很大程度上保留了稳定的性能。代码可访问<a href="https://github.com/jessemelpolio/LMM_CL">https://github.com/jessemelpolio/LMM_CL</a></p>
<div class="markdown-heading"><h2 class="heading-element">IntentionVLA：用于人机交互的通用且高效的预定意图推理</h2><a id="user-content-intentionvla用于人机交互的通用且高效的预定意图推理" class="anchor" aria-label="Permalink: IntentionVLA：用于人机交互的通用且高效的预定意图推理" href="#intentionvla用于人机交互的通用且高效的预定意图推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07778v1宣布类型：新摘要：视觉-语言-动作（VLA）模型利用预先训练的视觉-语言模型（VLM）将感知与机器人控制结合起来，为实现通用具体智能提供了一条有希望的道路。然而，当前的SOTA VGA主要是在与具体场景相关性有限的多模式任务上进行预训练，然后进行微调以将显式指令映射到动作。因此，由于缺乏推理密集型的预训练和推理引导的操纵，这些模型无法执行复杂的现实世界交互所需的隐式人类意图推理。为了克服这些限制，我们提出了\textBF{IntentionVLA}，这是一个具有课程培训范式和高效推理机制的VLA框架。我们提出的方法首先利用精心设计的推理数据，这些数据结合了意图推理、空间基础和紧凑的体现推理，赋予模型推理和感知能力。在接下来的微调阶段，IntentionVLA使用紧凑的推理输出作为动作生成的上下文指导，从而实现间接指令下的快速推理。实验结果表明，IntentionVLA的表现大大优于$pi_0 $，使用直接指令时的成功率比ECoT高出18%，使用意图指令时的成功率比ECoT高出28%。在分配外意图任务中，IntentionVLA的成功率是所有基线的两倍以上，并进一步实现零镜头人机交互，成功率为40%。这些结果凸显了IntentionVLA是下一代人机交互（HRI）系统的一个有前途的范式。</p>
<div class="markdown-heading"><h2 class="heading-element">MLLM 4TS：利用视觉和多模式语言模型进行一般时间序列分析</h2><a id="user-content-mllm-4ts利用视觉和多模式语言模型进行一般时间序列分析" class="anchor" aria-label="Permalink: MLLM 4TS：利用视觉和多模式语言模型进行一般时间序列分析" href="#mllm-4ts利用视觉和多模式语言模型进行一般时间序列分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07513v1宣布类型：新摘要：由于多元数据中复杂的时间依赖性和跨渠道相互作用，时间序列数据的有效分析带来了重大挑战。受到人类分析师视觉检查时间序列以发现隐藏模式的方式的启发，我们问道：结合视觉表示能否增强自动化时间序列分析？多模式大型语言模型的最新进展已经展现出令人印象深刻的概括和视觉理解能力，但它们在时间序列中的应用仍然受到连续数字数据和离散自然语言之间的形式差距的限制。为了弥合这一差距，我们引入了MLLM 4TS，这是一个新颖的框架，通过集成专门的视觉分支，利用多模式大型语言模型进行一般时间序列分析。每个时间序列通道在一张合成图像中渲染为水平堆叠的颜色编码线图，以捕捉通道之间的空间依赖性，然后采用时间感知的视觉补丁对齐策略将视觉补丁与其相应的时间段对齐。MLLM 4TS将来自数字数据的细粒度时间细节与来自视觉表示的全局上下文信息融合，为多模式时间序列分析提供了统一的基础。对标准基准的广泛实验证明了MLLM 4TS在两项预测任务中的有效性（例如，分类）和生成任务（例如，异常检测和预测）。这些结果强调了将视觉模式与预先训练的语言模型集成以实现稳健且可推广的时间序列分析的潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">期待学习：用于低资源视觉语言建模的令牌式动态门控</h2><a id="user-content-期待学习用于低资源视觉语言建模的令牌式动态门控" class="anchor" aria-label="Permalink: 期待学习：用于低资源视觉语言建模的令牌式动态门控" href="#期待学习用于低资源视觉语言建模的令牌式动态门控"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.08470v1宣布类型：新摘要：在认知上合理的数据量上训练视觉语言模型需要重新思考模型如何集成多模式信息。在2025年BabyLM挑战赛Vision赛道的约束下，我们提出了一种基于解码器的轻量级架构，具有（1）用于语言和视觉线索的自适应融合的代币式动态门控，（2）特征调制和渠道关注度以最大限度地提高有限视觉信息的利用率和（3）视觉基础的辅助对比目标。对五个基准（BLiMP、BLiMP Supplement、EWoK、Winoground和VQA）的评估显示，性能与多模式基线相比具有竞争力或更优越。更值得注意的是，我们的动态门在没有明确监督的情况下发现可解释的模式，有利于内容词的视觉线索和功能词的语言线索。虽然我们发现了挑战约束中的局限性，例如全局图像嵌入造成的信息瓶颈和数据集分裂造成的训练不稳定性，但我们的研究结果将动态门控确立为高效多模式学习的强大工具，即使在严格的约束下也能提供可解释性和性能。</p>
<div class="markdown-heading"><h2 class="heading-element">可执行的分析概念是VLM洞察和精确操纵之间缺失的一环</h2><a id="user-content-可执行的分析概念是vlm洞察和精确操纵之间缺失的一环" class="anchor" aria-label="Permalink: 可执行的分析概念是VLM洞察和精确操纵之间缺失的一环" href="#可执行的分析概念是vlm洞察和精确操纵之间缺失的一环"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07975v1宣布类型：新摘要：使机器人能够在非结构化环境中执行精确和广义的操纵仍然是体现人工智能的一个根本挑战。虽然视觉语言模型（VLM）在语义推理和任务规划方面表现出了非凡的能力，但它们的高级理解与现实世界操纵所需的精确物理执行之间仍然存在显着差距。为了弥合这种“从语义到物理”的差距，我们引入了GRACE，这是一种新颖的框架，它通过可执行分析概念（IAC）来支持基于VLM的推理--数学定义的蓝图，对对象的可供性、几何约束和操作的语义进行编码。我们的方法集成了一个结构化的策略脚手架管道，将自然语言指令和视觉信息转化为实例化的AEC，从中我们推导出抓取姿势、力方向并规划机器人执行的物理可行的运动轨迹。因此，GRACE在高级指令理解和低级机器人控制之间提供了统一且可解释的接口，通过语义物理基础有效地实现精确且可推广的操作。大量实验表明，GRACE在模拟和现实世界环境中对各种关节对象实现了强大的零射击概括，而无需特定任务的训练。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>