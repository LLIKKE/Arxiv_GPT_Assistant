<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">可扩展的预训练框架：面向高效适配的链接预测</h2><a id="user-content-可扩展的预训练框架面向高效适配的链接预测" class="anchor" aria-label="Permalink: 可扩展的预训练框架：面向高效适配的链接预测" href="#可扩展的预训练框架面向高效适配的链接预测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.04645v1 公告类型：新研究<br>
摘要：链接预测（LP）是图机器学习中的核心任务。尽管图神经网络（GNNs）近期显著提升了LP性能，现有方法仍面临关键挑战：稀疏连接导致的监督信号不足、对初始化的敏感性，以及分布偏移下的泛化能力弱。我们探索通过预训练解决这些问题。与节点分类不同，LP本质上是成对任务，需要融合节点级和边级信息。本研究首次系统分析了这两类模块的可迁移性，并提出一种延迟融合策略以有效整合其输出，从而提升性能。针对预训练数据多样性并避免负迁移，我们引入了混合专家（MoE）框架，通过独立专家捕获不同模式，使预训练模型能无缝应用于多样化下游数据集。为实现快速适配，开发了参数高效的调优策略，使预训练模型能以极低计算开销适应未知数据集。在跨两个领域的16个数据集上的实验表明，我们的方法在低资源链接预测中达到最先进性能，同时相比端到端训练方法获得具有竞争力的结果，且计算开销降低超过10,000倍。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语统一处理："Link Prediction"译为"链接预测"，"Graph Neural Networks"保留缩写"GNNs"并首次出现时标注全称</li>
<li>技术概念准确转化："Mixture-of-Experts"采用学界通用译法"混合专家"，"negative transfer"译为"负迁移"</li>
<li>长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如将"which requires..."独立成短句</li>
<li>数量级表达本地化："over 10,000x lower"译为"降低超过10,000倍"符合中文科技文献表述</li>
<li>被动语态转化："it is demonstrated"转为主动式"实验表明"</li>
<li>保持学术文本严谨性：精确翻译"distribution shifts"为"分布偏移"而非字面"分布变化"）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">FlexQ：通过算法-系统协同设计实现高效LLM服务后训练INT6量化</h2><a id="user-content-flexq通过算法-系统协同设计实现高效llm服务后训练int6量化" class="anchor" aria-label="Permalink: FlexQ：通过算法-系统协同设计实现高效LLM服务后训练INT6量化" href="#flexq通过算法-系统协同设计实现高效llm服务后训练int6量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.04405v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）展现出卓越性能，但需要极高的内存和计算成本，限制了实际部署。现有INT4/INT8量化虽能降低开销，却常导致精度下降或效率欠佳。INT6量化在模型精度与推理效率间提供了更优平衡，但现代GPU缺乏硬件支持，只能通过高精度算术单元模拟，制约了加速效果。</p>
<p>本文提出FlexQ——一种结合算法创新与系统级优化的新型训练后INT6量化框架。FlexQ采用全层统一6比特权重量化，并通过逐层敏感度分析自适应保留部分层的8比特激活值。为最大化硬件效率，我们开发了专用高性能GPU内核，通过二进制张量核心（BTC）等效方案支持W6A6和W6A8表示的矩阵运算，有效规避了原生INT6张量核心的缺失。在LLaMA模型上的评估表明，FlexQ保持接近FP16的精度，困惑度增幅不超过0.05。所提内核在LLaMA-2-70B线性层上较ABQ-LLM平均实现1.39倍加速。端到端测试中，FlexQ相比SmoothQuant带来1.33倍推理加速和1.21倍内存节省。代码已开源：<a href="https://github.com/FlyFoxPlayer/FlexQ%E3%80%82">https://github.com/FlyFoxPlayer/FlexQ。</a></p>
<p>（注：根据技术文献翻译规范，专业术语如"Binary Tensor Core"保留英文缩写"BTC"并首次出现时标注中文全称；数学符号"$\times$"转换为中文惯用的"倍"；长句按中文表达习惯拆分为短句；被动语态转换为主动表述；项目名称"FlexQ"等专有名词不作翻译。）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>