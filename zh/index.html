<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">EEGDM：基于生成扩散模型的脑电图表征学习</h2><a id="user-content-eegdm基于生成扩散模型的脑电图表征学习" class="anchor" aria-label="Permalink: EEGDM：基于生成扩散模型的脑电图表征学习" href="#eegdm基于生成扩散模型的脑电图表征学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14086v1 公告类型：新研究<br>
摘要：尽管脑电图（EEG）已成为监测大脑和诊断神经系统疾病（如癫痫）的重要工具，但由于标注数据有限和信号高度可变性，从原始EEG信号中学习有意义的表征仍具挑战。近期，EEG基础模型（FMs）通过采用Transformer架构和来自大语言模型的自监督预训练方法（如掩码预测），展现出从多样化EEG数据中学习表征的潜力，并在特定EEG任务上微调。然而，这些大型模型在训练和推理过程中往往计算成本高昂，且随着模型规模增大，性能提升有限。本研究提出基于生成扩散模型（EEGDM）的EEG表征学习框架。具体而言，我们开发了用于扩散预训练的结构化状态空间模型（SSMDP），以更好地捕捉EEG信号的时序动态特征，并采用去噪扩散概率模型训练该架构。所得潜在EEG表征随后通过我们提出的潜在融合Transformer（LFT）用于下游分类任务。为评估方法有效性，我们使用多事件天普大学EEG事件语料库，将EEGDM与当前最先进方法（包括EEG FMs）进行比较。实验结果表明，我们的方法在性能优于现有方法的同时，模型轻量化程度提升约19倍。这些发现表明EEGDM为当前FMs提供了有前景的替代方案。代码已开源：<a href="https://github.com/jhpuah/EEGDM%E3%80%82">https://github.com/jhpuah/EEGDM。</a></p>
<div class="markdown-heading"><h2 class="heading-element">神经启发式集合间通信原语：面向稀疏高效人工神经网络的设计</h2><a id="user-content-神经启发式集合间通信原语面向稀疏高效人工神经网络的设计" class="anchor" aria-label="Permalink: 神经启发式集合间通信原语：面向稀疏高效人工神经网络的设计" href="#神经启发式集合间通信原语面向稀疏高效人工神经网络的设计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14140v1 公告类型：新成果<br>
摘要：生物神经回路的结构——模块化、层次化且稀疏互联——反映了布线成本、功能专化与鲁棒性之间的高效权衡。这些原则为人工神经网络（ANN）设计提供了宝贵见解，尤其随着网络深度与规模的增长。稀疏性尤其被广泛探索用于减少内存与计算量、提升速度并增强泛化能力。受系统神经科学发现的启发，我们探索了小鼠视觉皮层中的功能连接模式（特别是集群间通信机制）如何指导ANN设计。我们提出G2GNet这一新颖架构，在前馈层间施加稀疏的模块化连接。尽管参数量显著少于全连接模型，G2GNet在标准视觉基准测试中实现了更优精度。据我们所知，这是首个将生物观测到的功能连接模式作为结构偏置融入ANN设计的架构。我们通过动态稀疏训练（DST）机制对这一静态偏置进行补充，该机制在训练过程中剪枝并重生连接边。同时借鉴生物可塑性原理，提出基于激活相关性的赫布式重布线规则。G2GNet在Fashion-MNIST、CIFAR-10和CIFAR-100等基准测试中实现高达75%的稀疏度，精度提升最高达4.3%，以远少于密集基线的计算量实现更优性能。</p>
<div class="markdown-heading"><h2 class="heading-element">智能体行为：人工智能数字时代下的模型、治理与挑战</h2><a id="user-content-智能体行为人工智能数字时代下的模型治理与挑战" class="anchor" aria-label="Permalink: 智能体行为：人工智能数字时代下的模型、治理与挑战" href="#智能体行为人工智能数字时代下的模型治理与挑战"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14415v1 公告类型：新成果<br>
摘要：人工智能的进步使得网络环境中的智能体日益模拟人类行为，在特定情境下模糊了人工行为体与人类行为体之间的界限。这一转变给信任、责任、伦理、安全等领域带来重大挑战，智能体行为监管的困难可能导致数据污染和责任界定不清等问题。为应对这些挑战，本文提出"网络行为生命周期"模型，将网络行为划分为六个阶段，系统分析各阶段人类与智能体的行为差异。基于此，进一步提出"智能体代理（A4A）"范式与"人机行为差异（HABD）"模型，从决策机制、执行效率、意图-行为一致性、行为惯性和非理性模式五个维度剖析人类与智能体的本质差异。通过红队渗透和蓝队防御等实际案例验证了模型的有效性。最后探讨了动态认知治理架构、行为差异量化及元治理协议栈等未来研究方向，旨在为安全可信的人机协作提供理论基础与技术路线。</p>
<div class="markdown-heading"><h2 class="heading-element">图神经网络中图结构与学习算法的相互作用探析</h2><a id="user-content-图神经网络中图结构与学习算法的相互作用探析" class="anchor" aria-label="Permalink: 图神经网络中图结构与学习算法的相互作用探析" href="#图神经网络中图结构与学习算法的相互作用探析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14338v1 公告类型：新成果<br>
摘要：本文研究图神经网络（GNNs）中学习算法与图结构的交互机制。现有关于GNN学习动态的理论研究主要集中于插值场景（无噪声）下学习算法的收敛速率，且仅能粗略关联这些动态与实际图结构（如最大度数）的关系。本文旨在通过探究泛化场景（含噪声）下GNNs学习算法的超额风险（泛化性能）来弥合这一鸿沟。具体而言，我们将学习理论文献中的经典设定扩展至GNN语境，系统考察图结构如何影响随机梯度下降（SGD）和岭回归等学习算法的性能。本研究为理解图结构与GNN学习的交互关系作出多项关键贡献：首先，我们推导出GNNs中SGD与岭回归的超额风险轮廓，并通过谱图理论将这些轮廓与图结构相关联。基于此框架，我们通过对比分析进一步探究不同图结构（规则图与幂律图）如何影响算法性能。此外，我们将分析延伸至多层线性GNNs，发现超额风险轮廓中日益增强的非各向同性效应，从而从学习算法视角为GNN中的过度平滑问题提供新见解。实证结果与理论预测高度吻合，<strong>共同揭示了图结构、GNN模型与学习算法之间的耦合关系，为实践中GNN算法设计与选择提供了重要参考</strong>。</p>
<div class="markdown-heading"><h2 class="heading-element">边缘设备上的联邦蒸馏：针对非独立同分布数据的高效客户端过滤机制</h2><a id="user-content-边缘设备上的联邦蒸馏针对非独立同分布数据的高效客户端过滤机制" class="anchor" aria-label="Permalink: 边缘设备上的联邦蒸馏：针对非独立同分布数据的高效客户端过滤机制" href="#边缘设备上的联邦蒸馏针对非独立同分布数据的高效客户端过滤机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14769v1 公告类型：新研究<br>
摘要：联邦蒸馏作为一种新兴的协同机器学习方法，通过交换模型输出（软逻辑值）而非完整模型参数，在提供更强隐私保护的同时显著降低了通信开销，较传统联邦学习更具优势。然而现有方法采用复杂的选择性知识共享策略，需要客户端通过计算成本高昂的统计密度比估计器来识别分布内代理数据，且服务器端对模糊知识的过滤会引入延迟。针对这些挑战，我们提出了一种鲁棒且资源高效的EdgeFD方法，该方法既降低了客户端密度比估计的复杂度，又无需服务器端过滤。EdgeFD引入基于KMeans的高效密度比估计器，可有效过滤客户端的分布内外代理数据，显著提升知识共享质量。我们在多种实际场景中评估EdgeFD（包括强非独立同分布、弱非独立同分布及独立同分布数据），且无需在服务器端预训练教师模型进行知识蒸馏。实验结果表明，EdgeFD在异构和挑战性条件下始终能达到接近独立同分布场景的精度水平，性能优于现有最优方法。基于KMeans的估计器大幅降低计算开销，适合部署在资源受限的边缘设备上，从而增强了联邦蒸馏的可扩展性和实际应用性。代码已开源以确保可复现性。</p>
<div class="markdown-heading"><h2 class="heading-element">科学图像手动标注指南：如何为大型项目做准备</h2><a id="user-content-科学图像手动标注指南如何为大型项目做准备" class="anchor" aria-label="Permalink: 科学图像手动标注指南：如何为大型项目做准备" href="#科学图像手动标注指南如何为大型项目做准备"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14801v1 公告类型：新研究<br>
摘要：尽管对人工标注图像数据的需求高涨，但关于如何管理复杂且成本高昂的标注项目的讨论仍然不足。这部分是因为主导此类项目需要应对一系列多样且相互关联的挑战，这些挑战往往超出特定领域专家的专业范围，导致实践指南稀缺。这些挑战广泛涉及从数据收集到资源分配与人员招募，从偏差 mitigation 到标注员的有效培训等多个方面。本文提供了一个领域无关的标注项目准备指南，重点关注科学图像领域。基于作者在管理大型人工标注项目中积累的丰富经验，指南涵盖了基本概念，包括成功度量标准、标注对象、项目目标、数据可用性以及核心团队角色。此外，本文还探讨了多种人类认知偏差，并推荐了提升标注质量与效率的工具和技术。最终目标是推动进一步研究和框架建设，以构建综合知识库，降低各领域人工标注项目的成本。</p>
<div class="markdown-heading"><h2 class="heading-element">AFABench：一个用于基准测试主动特征获取的通用框架</h2><a id="user-content-afabench一个用于基准测试主动特征获取的通用框架" class="anchor" aria-label="Permalink: AFABench：一个用于基准测试主动特征获取的通用框架" href="#afabench一个用于基准测试主动特征获取的通用框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14734v1 公告类型：新论文<br>
摘要：在许多现实场景中，由于成本、延迟或隐私等因素的限制，获取数据实例的全部特征可能代价高昂或难以实现。主动特征获取（AFA）通过动态选择每个数据实例中最具信息量的特征子集来解决这一挑战，在预测性能与获取成本之间寻求平衡。尽管目前已提出从贪婪的信息论策略到非近视的强化学习方法等多种AFA方案，但缺乏标准化基准一直阻碍着对这些方法的公平系统性评估。本文首次推出AFA基准测试框架AFABench，该基准包含多样化的合成与真实数据集，支持广泛的获取策略，并采用模块化设计便于新方法与新任务的集成。我们实现并评估了包括静态策略、贪婪策略和基于强化学习的方法在内的所有主要类别代表性算法。为测试AFA策略的前瞻能力，我们创新性地设计了合成数据集AFAContext，用于揭示贪婪选择的局限性。实验结果明确了不同AFA策略的核心权衡关系，为未来研究提供了 actionable 的见解。基准代码已开源：<a href="https://github.com/Linusaronsson/AFA-Benchmark%E3%80%82">https://github.com/Linusaronsson/AFA-Benchmark。</a></p>
<div class="markdown-heading"><h2 class="heading-element">论神经平均的定义</h2><a id="user-content-论神经平均的定义" class="anchor" aria-label="Permalink: 论神经平均的定义" href="#论神经平均的定义"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14832v1 公告类型：新成果<br>
摘要：对神经网络进行平均究竟意味着什么？我们研究了一个问题：如何仅利用预训练模型的最终权重（无需访问训练数据），从多个在互斥数据分片上训练的模型中合成单一神经网络。在构建神经平均的定义时，我们借鉴了"模型汤"（model soup）的洞见——这种方法看似将多个模型聚合为单一模型，却能提升泛化性能。本工作中，我们将模型汤重新阐释为一个更广泛框架的特例：用于神经平均的摊销模型集成（AME），这是一种无数据的元优化方法，将模型差异视为伪梯度来指导神经权重更新。研究表明，这一视角不仅能复现模型汤的效果，还能实现更具表达力和自适应性的集成策略。实证显示，AME产生的平均神经解决方案在性能上超越了个体专家模型和模型汤基线，尤其在分布外场景中表现突出。我们的研究成果提出了一种原则性且可推广的无数据模型权重聚合理念，从某种意义上定义了如何实现神经平均。</p>
<div class="markdown-heading"><h2 class="heading-element">联邦调查局：利用动态视觉触觉快捷策略学习灵巧手内操作</h2><a id="user-content-联邦调查局利用动态视觉触觉快捷策略学习灵巧手内操作" class="anchor" aria-label="Permalink: 联邦调查局：利用动态视觉触觉快捷策略学习灵巧手内操作" href="#联邦调查局利用动态视觉触觉快捷策略学习灵巧手内操作"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14441v1 公告类型：新成果<br>
摘要：灵巧手内操作因复杂的接触动力学和部分可观测性，一直是机器人领域的长期挑战。虽然人类在此类任务中能协同运用视觉与触觉，但机器人方法往往侧重单一模态，从而限制了适应性。本文提出"模仿前流态融合"（FBI）框架，这是一种通过运动动力学动态融合触觉交互与视觉观测的视触觉模仿学习系统。与先前静态融合方法不同，FBI通过动力学感知潜在模型建立了触觉信号与物体运动间的因果关联。该框架采用基于Transformer的交互模块，将流场衍生的触觉特征与视觉输入融合，并训练一步扩散策略实现实时执行。大量实验表明，在两个定制化手内操作任务和三个标准灵巧操作任务中，所提方法在仿真环境与真实世界均优于基线方法。代码、模型及更多结果请访问：<a href="https://sites.google.com/view/dex-fbi%E3%80%82" rel="nofollow">https://sites.google.com/view/dex-fbi。</a></p>
<div class="markdown-heading"><h2 class="heading-element">赫拉克勒斯：面向开放式大语言代理的分层技能编译框架</h2><a id="user-content-赫拉克勒斯面向开放式大语言代理的分层技能编译框架" class="anchor" aria-label="Permalink: 赫拉克勒斯：面向开放式大语言代理的分层技能编译框架" href="#赫拉克勒斯面向开放式大语言代理的分层技能编译框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14751v1 公告类型：新研究<br>
摘要：开放式人工智能代理需要能够在其生命周期内高效学习日益复杂、抽象和异质化的目标。除高效采样自身目标外，自驱式代理特别需要控制目标复杂度的增长，限制由此带来的样本与计算复杂度提升。为应对这一挑战，近期研究采用分层强化学习（HRL）与语言技术，利用语言的组合性与组合泛化能力来获取可复用的时序扩展行为。现有方法使用专家定义的子目标空间构建层次结构，且通常假设存在预训练的低层策略。此类设计在开放式场景中存在局限，因为目标空间会自然分化出不同难度谱系。我们提出HERAKLES框架，使双层分层自驱代理能够将持续掌握的目标编译至由小型快速神经网络执行的低层策略中，动态扩展高层策略可用的子目标集合。通过训练大语言模型（LLM）作为高层控制器，利用其在目标分解与泛化方面的优势，使其能在持续演进的子目标空间中有效运作。我们在开放式Crafter环境中评估HERAKLES，证明其能随目标复杂度有效扩展，通过技能编译提升样本效率，并使代理能够随时间推移稳健适应新挑战。</p>
<div class="markdown-heading"><h2 class="heading-element">合成自适应引导嵌入（SAGE）：一种新颖的知识蒸馏方法</h2><a id="user-content-合成自适应引导嵌入sage一种新颖的知识蒸馏方法" class="anchor" aria-label="Permalink: 合成自适应引导嵌入（SAGE）：一种新颖的知识蒸馏方法" href="#合成自适应引导嵌入sage一种新颖的知识蒸馏方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14783v1 公告类型：新研究<br>
摘要：模型蒸馏能够将知识从大规模模型迁移至紧凑型学生模型，助力在资源受限环境中的部署。然而传统蒸馏方法常受计算开销大和泛化能力有限的制约。我们提出了一种新颖的自适应蒸馏框架，可动态在学生模型高损失区域增强训练数据。该方法通过UMAP降维与最近邻采样技术，识别嵌入空间中的欠拟合区域，并生成针对性合成样本来引导学生模型学习。为提升效率，我们进一步引入轻量级师生交互接口，绕过教师模型的输入层，直接在向量化表征上进行蒸馏。在标准自然语言处理基准测试中，我们的6600万参数学生模型仅用更少训练轮次便持续达到或超越现有基线水平，在QNLI和SST-2数据集上分别取得91.2%和92.3%的准确率。这些成果彰显了基于损失感知的数据增强与向量化蒸馏技术在高效模型压缩领域的应用潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">使命高清：通过超维因果路径编码与解码实现数据驱动的推理图结构优化</h2><a id="user-content-使命高清通过超维因果路径编码与解码实现数据驱动的推理图结构优化" class="anchor" aria-label="Permalink: 使命高清：通过超维因果路径编码与解码实现数据驱动的推理图结构优化" href="#使命高清通过超维因果路径编码与解码实现数据驱动的推理图结构优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14746v1 公告类型：新成果<br>
摘要：基于大语言模型（LLM）生成的推理图常与视频异常检测（VAD）等下游视觉任务存在偏差。现有图结构优化（GSR）方法难以适用于这类新型、无数据集依赖的图结构。我们提出数据驱动的图结构优化范式（D-GSR），通过下游任务数据直接优化图结构，并构建超维度计算框架MissionHD予以实现。该框架采用高效的编码-解码流程，在下游任务信号引导下完成图结构优化。在VAD和VAR等挑战性基准测试中，使用优化后的图结构均取得显著性能提升，验证了本方法作为有效预处理步骤的实用性。</p>
<div class="markdown-heading"><h2 class="heading-element">玻璃：基于全局-局部神经重要性聚合的LLM测试时加速方法</h2><a id="user-content-玻璃基于全局-局部神经重要性聚合的llm测试时加速方法" class="anchor" aria-label="Permalink: 玻璃：基于全局-局部神经重要性聚合的LLM测试时加速方法" href="#玻璃基于全局-局部神经重要性聚合的llm测试时加速方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：译文采用意译策略，将"GLASS"译为"玻璃"以保留首字母缩写意象，同时通过冒号后的解释性翻译清晰传达技术内涵。"Global-Local Neural Importance Aggregation"精准译为"全局-局部神经重要性聚合"，"Test-Time Acceleration for LLMs"处理为"LLM测试时加速方法"，保持技术术语的准确性与中文表达习惯。）</p>
<p>arXiv:2508.14302v1 公告类型：新成果<br>
摘要：在边缘硬件上部署大语言模型（LLM）需要采用激进的、提示感知的动态剪枝技术，以在不降低生成质量的前提下减少计算量。静态或基于预测器的方案要么固定单一稀疏模式，要么带来额外运行时开销，而近期依赖单次提示统计的零样本方法在短提示和/或长文本生成场景中表现不佳。我们提出A/I-GLASS：基于激活值与影响力的全局-局部神经重要性聚合前馈网络稀疏化方法——这是两种无需训练的动态选择方案，通过聚合提示局部统计量与模型固有全局神经元统计量的排序，动态筛选前馈网络单元。在多类大语言模型和基准测试中的实证结果表明，GLASS显著优于现有无需训练的方法，尤其在具有挑战性的长文本生成场景中表现突出，且无需依赖辅助预测器或增加任何推理开销。</p>
<div class="markdown-heading"><h2 class="heading-element">通过张量分解实现多视图图压缩</h2><a id="user-content-通过张量分解实现多视图图压缩" class="anchor" aria-label="Permalink: 通过张量分解实现多视图图压缩" href="#通过张量分解实现多视图图压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14330v1 公告类型：新成果<br>
摘要：图神经网络（GNNs）在药物发现、目标检测、社交媒体分析、推荐系统及文本分类等现实应用中展现出卓越成效。然而与其巨大潜力形成对比的是，在大规模图上进行训练时，因其存储与处理所需资源巨大而面临显著计算挑战。图压缩技术通过合成紧凑的保留原图核心信息的简化图，同时保持GNN的预测性能，已成为解决这一需求的有效方案。尽管现有方法效果显著，但当前图压缩方案常依赖计算密集的双层优化，且未能维持合成节点与原节点间的映射关系，限制了模型决策的可解释性。在此背景下，多种分解技术已被应用于从图数据中学习线性或多线性函数，提供了更透明且资源消耗更低的替代方案，但这些技术在图压缩中的应用尚未探索。本文填补这一空白，提出名为"基于张量分解的多视角图压缩（GCTD）"的新方法，系统探究此类技术合成信息丰富的小规模图的能力及下游任务性能表现。在六个真实数据集上的大量实验表明，GCTD能有效缩减图规模的同时保持GNN性能，在六分之三的数据集上实现最高4.0%的精度提升，并在大规模图上取得与现有方法相当的性能。代码已开源：<a href="https://anonymous.4open.science/r/gctd-345A%E3%80%82" rel="nofollow">https://anonymous.4open.science/r/gctd-345A。</a></p>
<div class="markdown-heading"><h2 class="heading-element">模型误设下的在线事件响应规划：基于贝叶斯学习与信念量化的方法</h2><a id="user-content-模型误设下的在线事件响应规划基于贝叶斯学习与信念量化的方法" class="anchor" aria-label="Permalink: 模型误设下的在线事件响应规划：基于贝叶斯学习与信念量化的方法" href="#模型误设下的在线事件响应规划基于贝叶斯学习与信念量化的方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：译文在保持学术严谨性的同时进行了专业术语的本土化处理："Model Misspecification"译为"模型误设"符合计量经济学规范；"Bayesian Learning"译为"贝叶斯学习"是机器学习领域标准译法；"Belief Quantization"译为"信念量化"既保留认知科学概念特征又体现量化分析特性；通过添加"基于...方法"的句式结构，使中文标题更符合学术表达习惯。）</p>
<p>arXiv:2508.14385v1 公告类型：新论文<br>
摘要：有效应对网络攻击需要在信息不完整或不准确的情况下快速做出决策。然而，大多数事件响应的决策支持框架依赖于描述事件的详细系统模型，这限制了其实际应用价值。本文针对这一局限性，提出了一种在模型误设情况下进行在线事件响应规划的方法，称为MOBAL：误设在线贝叶斯学习。MOBAL通过贝叶斯学习，在新信息可用时迭代完善关于模型的推测，从而在事件发展过程中实现模型自适应。为实现在线有效响应，我们将推测模型量化为有限马尔可夫模型，通过动态规划实现高效响应规划。我们证明贝叶斯学习相对于信息反馈具有渐近一致性，并建立了误设误差与量化误差的边界。在CAGE-2基准测试上的实验表明，MOBAL在模型误设的适应性和鲁棒性方面优于现有最先进方法。</p>
<div class="markdown-heading"><h2 class="heading-element">跨模态扩散语言模型控制的分子生成</h2><a id="user-content-跨模态扩散语言模型控制的分子生成" class="anchor" aria-label="Permalink: 跨模态扩散语言模型控制的分子生成" href="#跨模态扩散语言模型控制的分子生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14748v1 公告类型：新研究<br>
摘要：当前基于SMILES的分子生成扩散模型通常仅支持单模态约束。这些模型在训练过程开始时注入条件信号，且每当约束条件改变时都需要从头重新训练新模型。然而，实际应用往往涉及跨不同模态的多重约束，且在研究过程中可能出现新的约束条件。这带来了一个挑战：如何扩展预训练扩散模型不仅支持跨模态约束，还能在不重新训练的情况下整合新约束。为解决这一问题，我们提出了基于扩散语言模型的跨模态可控分子生成方法（CMCM-DLM），并通过分子结构与化学性质这两种不同模态进行验证。我们的方法基于预训练扩散模型，引入两个可训练模块——结构控制模块（SCM）和性质控制模块（PCM），并在生成过程中分两个阶段操作：第一阶段在扩散早期采用SCM注入结构约束，有效锚定分子骨架；第二阶段在此基础上引入PCM，在推理后期引导分子精细化生成，确保其化学性质符合指定目标。在多组数据集上的实验结果证明了该方法的效率与适应性，凸显了CMCM-DLM在药物研发应用分子生成领域的重大进展。</p>
<div class="markdown-heading"><h2 class="heading-element">STAS：脉冲变换器的时空自适应计算时间</h2><a id="user-content-stas脉冲变换器的时空自适应计算时间" class="anchor" aria-label="Permalink: STAS：脉冲变换器的时空自适应计算时间" href="#stas脉冲变换器的时空自适应计算时间"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>解释：该翻译保持了原术语的专业性，同时遵循了中文技术文献的命名习惯。其中：</p>
<ul>
<li>"Spatio-Temporal" 译为"时空"符合中文科技文献常用表述</li>
<li>"Adaptive Computation Time" 采用"自适应计算时间"这一通用译法</li>
<li>"Spiking Transformers" 译为"脉冲变换器"，准确传达脉冲神经网络与Transformer架构结合的概念</li>
<li>冒号后的说明性翻译采用全中文表述，符合中文技术文档规范</li>
</ul>
<p>arXiv:2508.14138v1 公告类型：新研究<br>
摘要：脉冲神经网络（SNNs）相较于人工神经网络（ANNs）具有更高的能效，但其多时间步的运行特性导致高延迟和计算开销。虽然已有多种动态计算方法通过针对空间、时间或架构特定的冗余性来缓解这一问题，但这些方法仍处于碎片化状态。尽管自适应计算时间（ACT）原则为统一方法提供了坚实基础，但其在基于SNN的视觉Transformer（ViT）中的应用面临两个核心障碍：时序相似性前提条件的违反，以及根本不适合该原则的静态架构。为解决这些挑战，我们提出STAS（脉冲Transformer的时空自适应计算时间框架），该框架协同设计静态架构与动态计算策略。STAS通过集成脉冲补丁分割（I-SPS）模块建立时序稳定性，创建统一的输入表征，从而解决时序差异的架构问题。这种稳定性使得我们的自适应脉冲自注意力（A-SSA）模块能够跨空间和时间维度执行二维令牌剪枝。在脉冲Transformer架构上实现并在CIFAR-10、CIFAR-100和ImageNet数据集上验证，STAS在分别降低45.9%、43.8%和30.1%能耗的同时，其准确率还超越了当前最先进（SOTA）模型。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>