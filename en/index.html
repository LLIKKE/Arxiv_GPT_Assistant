<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/02/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10022025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/02/2025" href="#personalized-daily-arxiv-papers-10022025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 29</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Apriel-1.5-15b-Thinker</a>
<strong>Authors:</strong> Shruthan Radhakrishna, Aman Tiwari, Aanjaneya Shukla, Masoud Hashemi, Rishabh Maheshwary, Shiva Krishna Reddy Malay, Jash Mehta, Pulkit Pattnaik, Saloni Mittal, Khalil Slimi, Kelechi Ogueji, Akintunde Oladipo, Soham Parikh, Oluwanifemi Bamgbose, Toby Liang, Ahmed Masry, Khyati Mahajan, Sai Rajeswar Mudumba, Vikas Yadav, Sathwik Tejaswi Madhusudhan, Torsten Scholak, Sagar Davasam, Srinivas Sunkara, Nicholas Chapados</p>
</li>
<li>
<p><a href="#link1">VIRTUE: Visual-Interactive Text-Image Universal Embedder</a>
<strong>Authors:</strong> Wei-Yao Wang, Kazuya Tateishi, Qiyu Wu, Shusuke Takahashi, Yuki Mitsufuji</p>
</li>
<li>
<p><a href="#link2">Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution</a>
<strong>Authors:</strong> Alessio Devoto, Maximilian Jeblick, Simon J'egou</p>
</li>
<li>
<p><a href="#link3">PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning</a>
<strong>Authors:</strong> Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, Zhi Zhang</p>
</li>
<li>
<p><a href="#link4">ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models</a>
<strong>Authors:</strong> Dongqi Zheng</p>
</li>
<li>
<p><a href="#link5">Augmenting LLMs for General Time Series Understanding and Prediction</a>
<strong>Authors:</strong> Felix Parker, Nimeesha Chan, Chi Zhang, Kimia Ghobadi</p>
</li>
<li>
<p><a href="#link6">Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs</a>
<strong>Authors:</strong> Kairun Zhang, Haoyu Li, Yanjun Zhao, Yifan Sun, Huan Zhang</p>
</li>
<li>
<p><a href="#link7">Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?</a>
<strong>Authors:</strong> Nandan Kumar Jha, Brandon Reagen</p>
</li>
<li>
<p><a href="#link8">Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction</a>
<strong>Authors:</strong> Sagnik Basu, Shubham Prakash, Ashish Maruti Barge, Siddharth D Jaiswal, Abhisek Dash, Saptarshi Ghosh, Animesh Mukherjee</p>
</li>
<li>
<p><a href="#link9">FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs</a>
<strong>Authors:</strong> Ran Liu, Yuan Fang, Xiaoli Li</p>
</li>
<li>
<p><a href="#link10">Sample-Efficient Differentially Private Fine-Tuning via Gradient Matrix Denoising</a>
<strong>Authors:</strong> Ali Dadsetan, Frank Rudzicz</p>
</li>
<li>
<p><a href="#link11">VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators</a>
<strong>Authors:</strong> Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, Weihua Su</p>
</li>
<li>
<p><a href="#link12">Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models</a>
<strong>Authors:</strong> Shutong Wu, Jiawei Zhang</p>
</li>
<li>
<p><a href="#link13">DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification</a>
<strong>Authors:</strong> Mohammad Rostami, Atik Faysal, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Yu-Dong Yao</p>
</li>
<li>
<p><a href="#link14">Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey</a>
<strong>Authors:</strong> Sicong Liu, Weiye Wu, Xiangrui Xu, Teng Li, Bowen Pang, Bin Guo, Zhiwen Yu</p>
</li>
<li>
<p><a href="#link15">ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning</a>
<strong>Authors:</strong> Yunhao Wang, Ziting Li, Shuai Chen, Tao Liu, Chao Song, Junjie Jiang, Jian Zhu, Peng Gao, Bin Qin</p>
</li>
<li>
<p><a href="#link16">Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling</a>
<strong>Authors:</strong> Ye Qiao, Haocheng Xu, Xiaofan Zhang, Sitao Huang</p>
</li>
<li>
<p><a href="#link17">The Pitfalls of KV Cache Compression</a>
<strong>Authors:</strong> Alex Chen, Renato Geh, Aditya Grover, Guy Van den Broeck, Daniel Israel</p>
</li>
<li>
<p><a href="#link18">HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy</a>
<strong>Authors:</strong> Myungkyu Koo, Daewon Choi, Taeyoung Kim, Kyungmin Lee, Changyeon Kim, Youngyo Seo, Jinwoo Shin</p>
</li>
<li>
<p><a href="#link19">ACON: Optimizing Context Compression for Long-horizon LLM Agents</a>
<strong>Authors:</strong> Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin A. Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, Saravan Rajmohan</p>
</li>
<li>
<p><a href="#link20">Hybrid Training for Vision-Language-Action Models</a>
<strong>Authors:</strong> Pietro Mazzaglia, Cansu Sancaktar, Markus Peschl, Daniel Dijkman</p>
</li>
<li>
<p><a href="#link21">Semantic-Driven AI Agent Communications: Challenges and Solutions</a>
<strong>Authors:</strong> Kaiwen Yu, Mengying Sun, Zhijin Qin, Xiaodong Xu, Ping Yang, Yue Xiao, Gang Wu</p>
</li>
<li>
<p><a href="#link22">Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning</a>
<strong>Authors:</strong> Yicheng Lang, Yihua Zhang, Chongyu Fan, Changsheng Wang, Jinghan Jia, Sijia Liu</p>
</li>
<li>
<p><a href="#link23">LoRAFusion: Efficient LoRA Fine-Tuning for LLMs</a>
<strong>Authors:</strong> Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko</p>
</li>
<li>
<p><a href="#link24">Automated Structured Radiology Report Generation with Rich Clinical Context</a>
<strong>Authors:</strong> Seongjae Kang, Dong Bok Lee, Juho Jung, Dongseop Kim, Won Hwa Kim, Sunghoon Joo</p>
</li>
<li>
<p><a href="#link25">Meaningless Tokens, Meaningful Gains: How Activation Shifts Enhance LLM Reasoning</a>
<strong>Authors:</strong> Zeru Shi, Yingjia Wan, Zhenting Wang, Qifan Wang, Fan Yang, Elisa Kreiss, Ruixiang Tang</p>
</li>
<li>
<p><a href="#link26">Randomized Matrix Sketching for Neural Network Training and Gradient Monitoring</a>
<strong>Authors:</strong> Harbir Antil, Deepanshu Verma</p>
</li>
<li>
<p><a href="#link27">Large Language Models Inference Engines based on Spiking Neural Networks</a>
<strong>Authors:</strong> Adarsha Balaji, Sandeep Madireddy</p>
</li>
<li>
<p><a href="#link28">Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs</a>
<strong>Authors:</strong> Leyla Mirvakhabova, Babak Ehteshami Bejnordi, Gaurav Kumar, Hanxue Liang, Wanru Zhao, Paul Whatmough</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.01141" rel="nofollow">Apriel-1.5-15b-Thinker</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-apriel-15-15b-thinker-" class="anchor" aria-label="Permalink: 0. Apriel-1.5-15b-Thinker" href="#0-apriel-15-15b-thinker-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01141
<strong>Authors:</strong> Shruthan Radhakrishna, Aman Tiwari, Aanjaneya Shukla, Masoud Hashemi, Rishabh Maheshwary, Shiva Krishna Reddy Malay, Jash Mehta, Pulkit Pattnaik, Saloni Mittal, Khalil Slimi, Kelechi Ogueji, Akintunde Oladipo, Soham Parikh, Oluwanifemi Bamgbose, Toby Liang, Ahmed Masry, Khyati Mahajan, Sai Rajeswar Mudumba, Vikas Yadav, Sathwik Tejaswi Madhusudhan, Torsten Scholak, Sagar Davasam, Srinivas Sunkara, Nicholas Chapados</p>
<p><strong>Abstract:</strong> arXiv:2510.01141v1 Announce Type: new  Abstract: We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.00523" rel="nofollow">VIRTUE: Visual-Interactive Text-Image Universal Embedder</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-virtue-visual-interactive-text-image-universal-embedder-" class="anchor" aria-label="Permalink: 1. VIRTUE: Visual-Interactive Text-Image Universal Embedder" href="#1-virtue-visual-interactive-text-image-universal-embedder-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00523
<strong>Authors:</strong> Wei-Yao Wang, Kazuya Tateishi, Qiyu Wu, Shusuke Takahashi, Yuki Mitsufuji</p>
<p><strong>Abstract:</strong> arXiv:2510.00523v1 Announce Type: new  Abstract: Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.00636" rel="nofollow">Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-expected-attention-kv-cache-compression-by-estimating-attention-from-future-queries-distribution-" class="anchor" aria-label="Permalink: 2. Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution" href="#2-expected-attention-kv-cache-compression-by-estimating-attention-from-future-queries-distribution-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00636
<strong>Authors:</strong> Alessio Devoto, Maximilian Jeblick, Simon J'egou</p>
<p><strong>Abstract:</strong> arXiv:2510.00636v1 Announce Type: new  Abstract: Memory consumption of the Key-Value (KV) cache represents a major bottleneck for efficient large language model inference. While attention-score-based KV cache pruning shows promise, it faces critical practical limitations: attention scores from future tokens are unavailable during compression, and modern implementations like Flash Attention do not materialize the full attention matrix, making past scores inaccessible. To overcome these challenges, we introduce $\textbf{Expected Attention, a training-free compression method}$ that estimates KV pairs importance by predicting how future queries will attend to them. Our approach leverages the distributional properties of LLM activations to compute expected attention scores in closed form for each KV pair. These scores enable principled ranking and pruning of KV pairs with minimal impact on the residual stream, achieving effective compression without performance degradation. Importantly, our method operates seamlessly across both prefilling and decoding phases, consistently outperforming state-of-the-art baselines in both scenarios. Finally, $\textbf{we release KVPress, a comprehensive library to enable researchers to implement and benchmark KV cache compression methods, already including more than 20 techniques}$.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.00192" rel="nofollow">PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-prunedlora-robust-gradient-based-structured-pruning-for-low-rank-adaptation-in-fine-tuning-" class="anchor" aria-label="Permalink: 3. PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning" href="#3-prunedlora-robust-gradient-based-structured-pruning-for-low-rank-adaptation-in-fine-tuning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00192
<strong>Authors:</strong> Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, Zhi Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.00192v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.00071" rel="nofollow">ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-ars-adaptive-reasoning-suppression-for-efficient-large-reasoning-language-models-" class="anchor" aria-label="Permalink: 4. ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models" href="#4-ars-adaptive-reasoning-suppression-for-efficient-large-reasoning-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00071
<strong>Authors:</strong> Dongqi Zheng</p>
<p><strong>Abstract:</strong> arXiv:2510.00071v1 Announce Type: new  Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with progressive suppression thresholds, achieving superior efficiency compared to static suppression methods. Our extensive evaluation across mathematical reasoning benchmarks using multiple model architectures demonstrates that ARS achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction, while maintaining or improving accuracy.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.01111" rel="nofollow">Augmenting LLMs for General Time Series Understanding and Prediction</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-augmenting-llms-for-general-time-series-understanding-and-prediction-" class="anchor" aria-label="Permalink: 5. Augmenting LLMs for General Time Series Understanding and Prediction" href="#5-augmenting-llms-for-general-time-series-understanding-and-prediction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01111
<strong>Authors:</strong> Felix Parker, Nimeesha Chan, Chi Zhang, Kimia Ghobadi</p>
<p><strong>Abstract:</strong> arXiv:2510.01111v1 Announce Type: new  Abstract: Time series data is fundamental to decision-making in many crucial domains including healthcare, finance, and environmental science. However, analyzing this data often requires incorporating unstructured contextual information, answering domain-specific questions, and generating natural language explanations -- capabilities that traditional time series models lack due to their inability to process text. While Large Language Models (LLMs) excel at contextual reasoning and knowledge integration, they struggle with numerical time series due to inefficient text-based representations and limited exposure to temporal data during pretraining. We address this gap by augmenting an LLM with specialized time series perception through a patch-based encoder-decoder architecture. We train this Time Series-augmented LLM (TsLLM) on a large corpus of over 2 million interleaved time series and text examples spanning diverse analysis tasks: forecasting with contextual information, time series question-answering, pattern explanation, classification with natural language outputs, and report generation. This training enables TsLLM to leverage both its language understanding and newly acquired temporal reasoning capabilities. While not designed to surpass specialized models on traditional benchmarks, TsLLM demonstrates strong performance on tasks requiring the integration of time series analysis with natural language -- capabilities that existing approaches cannot provide. Our work establishes a new paradigm for time series analysis that bridges numerical computation and natural language understanding, democratizing access to sophisticated temporal reasoning through natural language interaction.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.00419" rel="nofollow">Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-learning-a-zeroth-order-optimizer-for-fine-tuning-llms-" class="anchor" aria-label="Permalink: 6. Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs" href="#6-learning-a-zeroth-order-optimizer-for-fine-tuning-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00419
<strong>Authors:</strong> Kairun Zhang, Haoyu Li, Yanjun Zhao, Yifan Sun, Huan Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.00419v1 Announce Type: new  Abstract: Zeroth-order optimizers have recently emerged as a practical approach for fine-tuning large language models (LLMs), significantly reducing GPU memory consumption compared to traditional first-order methods. Yet, existing zeroth-order methods rely on hand-crafted, static sampling strategies that are not adaptable to model-specific structures. To address this, we propose ZO Fine-tuner, a learning-based zeroth-order optimizer for LLMs that automatically learns efficient perturbation strategies through a compact and memory-efficient design. Crucially, our approach is motivated by the observation that only a small number of foundation models and their derivatives are widely adopted in practice. Therefore, learning the optimizer once for a given LLM and reusing it across diverse downstream tasks is both feasible and highly desirable. Accordingly, ZO Fine-tuner is designed to scale learning to learn (L2L) to the foundation-model era by supporting one-time training per LLM with minimal overhead. Experiments on 4 LLMs and 7 datasets show that ZO Fine-tuner outperforms prior zeroth-order baselines in 82.1% of task-model combinations, thereby demonstrating strong performance and scalability for efficient LLM fine-tuning. Our code is available at <a href="https://github.com/ASTRAL-Group/ZO_Fine_tuner.git">https://github.com/ASTRAL-Group/ZO_Fine_tuner.git</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.00537" rel="nofollow">Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-spectral-scaling-laws-in-language-models-how-effectively-do-feed-forward-networks-use-their-latent-space-" class="anchor" aria-label="Permalink: 7. Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?" href="#7-spectral-scaling-laws-in-language-models-how-effectively-do-feed-forward-networks-use-their-latent-space-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00537
<strong>Authors:</strong> Nandan Kumar Jha, Brandon Reagen</p>
<p><strong>Abstract:</strong> arXiv:2510.00537v1 Announce Type: new  Abstract: As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.00088" rel="nofollow">Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-judging-by-appearances-auditing-and-intervening-vision-language-models-for-bail-prediction-" class="anchor" aria-label="Permalink: 8. Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction" href="#8-judging-by-appearances-auditing-and-intervening-vision-language-models-for-bail-prediction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00088
<strong>Authors:</strong> Sagnik Basu, Shubham Prakash, Ashish Maruti Barge, Siddharth D Jaiswal, Abhisek Dash, Saptarshi Ghosh, Animesh Mukherjee</p>
<p><strong>Abstract:</strong> arXiv:2510.00088v1 Announce Type: new  Abstract: Large language models (LLMs) have been extensively used for legal judgment prediction tasks based on case reports and crime history. However, with a surge in the availability of large vision language models (VLMs), legal judgment prediction systems can now be made to leverage the images of the criminals in addition to the textual case reports/crime history. Applications built in this way could lead to inadvertent consequences and be used with malicious intent. In this work, we run an audit to investigate the efficiency of standalone VLMs in the bail decision prediction task. We observe that the performance is poor across multiple intersectional groups and models \textit{wrongly deny bail to deserving individuals with very high confidence}. We design different intervention algorithms by first including legal precedents through a RAG pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate that these interventions substantially improve the performance of bail prediction. Our work paves the way for the design of smarter interventions on VLMs in the future, before they can be deployed for real-world legal judgment prediction.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.00894" rel="nofollow">FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-fusionadapter-for-few-shot-relation-learning-in-multimodal-knowledge-graphs-" class="anchor" aria-label="Permalink: 9. FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs" href="#9-fusionadapter-for-few-shot-relation-learning-in-multimodal-knowledge-graphs-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00894
<strong>Authors:</strong> Ran Liu, Yuan Fang, Xiaoli Li</p>
<p><strong>Abstract:</strong> arXiv:2510.00894v1 Announce Type: new  Abstract: Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including text and images, to enhance entity and relation representations. Notably, different modalities for the same entity often present complementary and diverse information. However, existing MMKG methods primarily align modalities into a shared space, which tends to overlook the distinct contributions of specific modalities, limiting their performance particularly in low-resource settings. To address this challenge, we propose FusionAdapter for the learning of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an adapter module that enables efficient adaptation of each modality to unseen relations and (2) a fusion strategy that integrates multimodal entity representations while preserving diverse modality-specific characteristics. By effectively adapting and fusing information from diverse modalities, FusionAdapter improves generalization to novel relations with minimal supervision. Extensive experiments on two benchmark MMKG datasets demonstrate that FusionAdapter achieves superior performance over state-of-the-art methods.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.01137" rel="nofollow">Sample-Efficient Differentially Private Fine-Tuning via Gradient Matrix Denoising</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-sample-efficient-differentially-private-fine-tuning-via-gradient-matrix-denoising-" class="anchor" aria-label="Permalink: 10. Sample-Efficient Differentially Private Fine-Tuning via Gradient Matrix Denoising" href="#10-sample-efficient-differentially-private-fine-tuning-via-gradient-matrix-denoising-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01137
<strong>Authors:</strong> Ali Dadsetan, Frank Rudzicz</p>
<p><strong>Abstract:</strong> arXiv:2510.01137v1 Announce Type: new  Abstract: We address the challenge of sample efficiency in differentially private fine-tuning of large language models (LLMs) using DP-SGD. While DP-SGD provides strong privacy guarantees, the added noise significantly increases the entropy of gradient matrices, disrupting their low-rank structure and slowing optimization. We propose a post-processing algorithm that leverages random matrix theory to denoise gradients, restore low-rank structure, and improve alignment with the original signal. Applied to DP-SGD fine-tuning of RoBERTa on GLUE tasks, our method improves sample efficiency compared to state-of-the-art approaches, substantially reducing training time when optimal performance is not required. This work demonstrates that matrix recovery techniques can enhance the utility of private language model training without compromising privacy guarantees.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.00406" rel="nofollow">VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-vla-rft-vision-language-action-reinforcement-fine-tuning-with-verified-rewards-in-world-simulators-" class="anchor" aria-label="Permalink: 11. VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators" href="#11-vla-rft-vision-language-action-reinforcement-fine-tuning-with-verified-rewards-in-world-simulators-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00406
<strong>Authors:</strong> Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, Weihua Su</p>
<p><strong>Abstract:</strong> arXiv:2510.00406v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to <a href="https://vla-rft.github.io/" rel="nofollow">https://vla-rft.github.io/</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.00294" rel="nofollow">Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-free-draft-and-verification-toward-lossless-parallel-decoding-for-diffusion-large-language-models-" class="anchor" aria-label="Permalink: 12. Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models" href="#12-free-draft-and-verification-toward-lossless-parallel-decoding-for-diffusion-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00294
<strong>Authors:</strong> Shutong Wu, Jiawei Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.00294v1 Announce Type: new  Abstract: Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous "reversal curse" or learning under data-constrained scenarios. However, this bidirectional nature also brings an obstacle that DLLMs are not inherently compatible with KV Cache, and consequently, the inference efficiency is not competitive compared with autoregressive models. Taking advantage of their inherent capability of multi-token prediction, existing parallel decoding algorithms can speed up the DLLM inference, but at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (Freedave), a novel fast sampling algorithm tailored for DLLMs that achieves lossless parallel decoding. Specifically, we propose a pipeline of parallel-decoded candidate generation and verification, which is guaranteed to reproduce the same sequence generated by static sampling, without introducing extra model forward calls. By applying Freedave, the throughput of DLLMs can be boosted up to $2.8\times$ without performance degradation on math reasoning tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.00316" rel="nofollow">DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-disc-amc-token--and-parameter-efficient-discretized-statistics-in-context-automatic-modulation-classification-" class="anchor" aria-label="Permalink: 13. DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification" href="#13-disc-amc-token--and-parameter-efficient-discretized-statistics-in-context-automatic-modulation-classification-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00316
<strong>Authors:</strong> Mohammad Rostami, Atik Faysal, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Yu-Dong Yao</p>
<p><strong>Abstract:</strong> arXiv:2510.00316v1 Announce Type: new  Abstract: Large Language Models (LLMs) can perform Automatic Modulation Classification (AMC) in an open-set manner without LLM fine-tuning when equipped with carefully designed in-context prompts~\cite{rostami2025plug}. Building on this prior work, we target the practical bottlenecks of long prompt contexts and large model sizes that impede in-the-loop deployment. We present Discretized Statistics in-Context Automatic Modulation Classification (DiSC-AMC), a token- and parameter-efficient variant that: (i) discretizes higher-order statistics and cumulants into compact symbolic tokens, (ii) prunes the exemplar list via a lightweight k-top neural prefilter and filters misleading/low-impact features using rationales extracted from prior LLM responses, and (iii) enforces label-only predictions through a calibrated prompt template. Together, these changes reduce both input/output tokens and the model parameter footprint by more than half while maintaining competitive accuracy. On synthetic AMC with ten modulation types under noise, a 7B \textit{DeepSeek-R1-Distill-Qwen} baseline achieves 5.2% accuracy, whereas our system, using an approximately 5B-parameter \textit{Gemini-2.5-Flash}~\cite{comanici2025gemini} model, attains 45.5% accuracy. These results demonstrate that careful discretization and context selection can cut inference cost by over 2x while preserving the advantages of prompt-based AMC and enabling practical in-the-loop use.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.00078" rel="nofollow">Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-adaptive-and-resource-efficient-agentic-ai-systems-for-mobile-and-embedded-devices-a-survey-" class="anchor" aria-label="Permalink: 14. Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey" href="#14-adaptive-and-resource-efficient-agentic-ai-systems-for-mobile-and-embedded-devices-a-survey-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00078
<strong>Authors:</strong> Sicong Liu, Weiye Wu, Xiangrui Xu, Teng Li, Bowen Pang, Bin Guo, Zhiwen Yu</p>
<p><strong>Abstract:</strong> arXiv:2510.00078v1 Announce Type: new  Abstract: Foundation models have reshaped AI by unifying fragmented architectures into scalable backbones with multimodal reasoning and contextual adaptation. In parallel, the long-standing notion of AI agents, defined by the sensing-decision-action loop, is entering a new paradigm: with FMs as their cognitive core, agents transcend rule-based behaviors to achieve autonomy, generalization, and self-reflection. This dual shift is reinforced by real-world demands such as autonomous driving, robotics, virtual assistants, and GUI agents, as well as ecosystem advances in embedded hardware, edge computing, mobile deployment platforms, and communication protocols that together enable large-scale deployment. Yet this convergence collides with reality: while applications demand long-term adaptability and real-time interaction, mobile and edge deployments remain constrained by memory, energy, bandwidth, and latency. This creates a fundamental tension between the growing complexity of FMs and the limited resources of deployment environments. This survey provides the first systematic characterization of adaptive, resource-efficient agentic AI systems. We summarize enabling techniques into elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications, and identify open challenges in balancing accuracy-latency-communication trade-offs and sustaining robustness under distribution shifts. We further highlight future opportunities in algorithm-system co-design, cognitive adaptation, and collaborative edge deployment. By mapping FM structures, cognition, and hardware resources, this work establishes a unified perspective toward scalable, adaptive, and resource-efficient agentic AI. We believe this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of agentic intelligence and intelligent agents.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2510.00690" rel="nofollow">ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-acpo-adaptive-curriculum-policy-optimization-for-aligning-vision-language-models-in-complex-reasoning-" class="anchor" aria-label="Permalink: 15. ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning" href="#15-acpo-adaptive-curriculum-policy-optimization-for-aligning-vision-language-models-in-complex-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00690
<strong>Authors:</strong> Yunhao Wang, Ziting Li, Shuai Chen, Tao Liu, Chao Song, Junjie Jiang, Jian Zhu, Peng Gao, Bin Qin</p>
<p><strong>Abstract:</strong> arXiv:2510.00690v1 Announce Type: new  Abstract: Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed clipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2510.00028" rel="nofollow">Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-rethinking-rope-scaling-in-quantized-llm-theory-outlier-and-channel-band-analysis-with-weight-rescaling-" class="anchor" aria-label="Permalink: 16. Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling" href="#16-rethinking-rope-scaling-in-quantized-llm-theory-outlier-and-channel-band-analysis-with-weight-rescaling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00028
<strong>Authors:</strong> Ye Qiao, Haocheng Xu, Xiaofan Zhang, Sitao Huang</p>
<p><strong>Abstract:</strong> arXiv:2510.00028v1 Announce Type: new  Abstract: Extending the context window support of large language models (LLMs) is crucial for tasks with long-distance dependencies. RoPE-based interpolation and extrapolation methods, such as linear scaling and frequency-aware schemes, enable longer input length support without retraining, while post-training quantization (PTQ) makes deployment practical. However, we show that combining RoPE position interpolation (PI) with PTQ degrades accuracy due to coupled effects including long-context aliasing, dynamic-range dilation, anisotropy from axis-aligned quantizers vs. rotated RoPE pairs, and outlier shifting that produces position-dependent logit noise. We provide, to the best of our knowledge, the first systematic analysis of the PI+PTQ approach and introduce two practical diagnostics: interpolation pressure (per-band sensitivity to phase scaling) and tail-inflation ratios (outlier shift from short to long contexts). Following the analysis results, we propose Q-ROAR (Quantization, RoPE-interpolation, and Outlier Aware Rescaling), a weight-only, interpolation-aware stabilization of PI for quantized LLMs. Q-ROAR groups RoPE dimensions into a small number of frequency bands and performs a lightweight search over per-band scales for Key and Query weights (with an optional symmetric variant to preserve logit scale). The search is guided by our diagnostics and uses a tiny long-context development dataset, requiring no fine-tuning to the model, no architecture or kernel changes, and no additional deployment overhead. Empirically, Q-ROAR reduces the model's perplexity on long-context workloads by more than 14%, while preserving short-context performance, inference throughput, and compatibility with existing LLM system stacks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2510.00231" rel="nofollow">The Pitfalls of KV Cache Compression</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-the-pitfalls-of-kv-cache-compression-" class="anchor" aria-label="Permalink: 17. The Pitfalls of KV Cache Compression" href="#17-the-pitfalls-of-kv-cache-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00231
<strong>Authors:</strong> Alex Chen, Renato Geh, Aditya Grover, Guy Van den Broeck, Daniel Israel</p>
<p><strong>Abstract:</strong> arXiv:2510.00231v1 Announce Type: new  Abstract: KV cache compression promises increased throughput and efficiency with negligible loss in performance. While the gains in throughput are indisputable and recent literature has indeed shown minimal degradation on particular benchmarks, in general the consequences of compression in realistic scenarios such as multi-instruction prompting have been insufficiently studied. In this paper, we identify several pitfalls practitioners should be aware of when deploying KV cache compressed LLMs. Importantly, we show that certain instructions degrade much more rapidly with compression, effectively causing them to be completely ignored by the LLM. As a practical example of that, we highlight system prompt leakage as a case study, empirically showing the impact of compression on leakage and general instruction following. We show several factors that play a role in prompt leakage: compression method, instruction order, and KV eviction bias. We then propose simple changes to KV cache eviction policies that can reduce the impact of these factors and improve the overall performance in multi-instruction tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2510.00695" rel="nofollow">HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-hamlet-switch-your-vision-language-action-model-into-a-history-aware-policy-" class="anchor" aria-label="Permalink: 18. HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy" href="#18-hamlet-switch-your-vision-language-action-model-into-a-history-aware-policy-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00695
<strong>Authors:</strong> Myungkyu Koo, Daewon Choi, Taeyoung Kim, Kyungmin Lee, Changyeon Kim, Youngyo Seo, Jinwoo Shin</p>
<p><strong>Abstract:</strong> arXiv:2510.00695v1 Announce Type: new  Abstract: Inherently, robotic manipulation tasks are history-dependent: leveraging past context could be beneficial. However, most existing Vision-Language-Action models (VLAs) have been designed without considering this aspect, i.e., they rely solely on the current observation, ignoring preceding context. In this paper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the historical context during action prediction. Specifically, we introduce moment tokens that compactly encode perceptual information at each timestep. Their representations are initialized with time-contrastive learning, allowing them to better capture temporally distinctive aspects. Next, we employ a lightweight memory module that integrates the moment tokens across past timesteps into memory features, which are then leveraged for action prediction. Through empirical evaluation, we show that HAMLET successfully transforms a state-of-the-art VLA into a history-aware policy, especially demonstrating significant improvements on long-horizon tasks that require historical context. In particular, on top of GR00T N1.5, HAMLET achieves an average success rate of 76.4% on history-dependent real-world tasks, surpassing the baseline performance by 47.2%. Furthermore, HAMLET pushes prior art performance from 64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on LIBERO, highlighting its effectiveness even under generic robot-manipulation benchmarks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2510.00615" rel="nofollow">ACON: Optimizing Context Compression for Long-horizon LLM Agents</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-acon-optimizing-context-compression-for-long-horizon-llm-agents-" class="anchor" aria-label="Permalink: 19. ACON: Optimizing Context Compression for Long-horizon LLM Agents" href="#19-acon-optimizing-context-compression-for-long-horizon-llm-agents-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00615
<strong>Authors:</strong> Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin A. Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, Saravan Rajmohan</p>
<p><strong>Abstract:</strong> arXiv:2510.00615v1 Announce Type: new  Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2510.00600" rel="nofollow">Hybrid Training for Vision-Language-Action Models</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-hybrid-training-for-vision-language-action-models-" class="anchor" aria-label="Permalink: 20. Hybrid Training for Vision-Language-Action Models" href="#20-hybrid-training-for-vision-language-action-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00600
<strong>Authors:</strong> Pietro Mazzaglia, Cansu Sancaktar, Markus Peschl, Daniel Dijkman</p>
<p><strong>Abstract:</strong> arXiv:2510.00600v1 Announce Type: new  Abstract: Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). As these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. However, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2510.00381" rel="nofollow">Semantic-Driven AI Agent Communications: Challenges and Solutions</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-semantic-driven-ai-agent-communications-challenges-and-solutions-" class="anchor" aria-label="Permalink: 21. Semantic-Driven AI Agent Communications: Challenges and Solutions" href="#21-semantic-driven-ai-agent-communications-challenges-and-solutions-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00381
<strong>Authors:</strong> Kaiwen Yu, Mengying Sun, Zhijin Qin, Xiaodong Xu, Ping Yang, Yue Xiao, Gang Wu</p>
<p><strong>Abstract:</strong> arXiv:2510.00381v1 Announce Type: new  Abstract: With the rapid growth of intelligent services, communication targets are shifting from humans to artificial intelligent (AI) agents, which require new paradigms to enable real-time perception, decision-making, and collaboration. Semantic communication, which conveys task-relevant meaning rather than raw data, offers a promising solution. However, its practical deployment remains constrained by dynamic environments and limited resources. To address these issues, this article proposes a semantic-driven AI agent communication framework and develops three enabling techniques. First, semantic adaptation transmission applies fine-tuning with real or generative samples to efficiently adapt models to varying environments. Second, semantic lightweight transmission incorporates pruning, quantization, and perception-aware sampling to reduce model complexity and alleviate computational burden on edge agents. Third, semantic self-evolution control employs distributed hierarchical decision-making to optimize multi-dimensional resources, enabling robust multi-agent collaboration in dynamic environments. Simulation results show that the proposed solutions achieve faster convergence and stronger robustness, while the proposed distributed hierarchical optimization method significantly outperforms conventional decision-making schemes, highlighting its potential for AI agent communication networks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2510.00761" rel="nofollow">Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-downgrade-to-upgrade-optimizer-simplification-enhances-robustness-in-llm-unlearning-" class="anchor" aria-label="Permalink: 22. Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning" href="#22-downgrade-to-upgrade-optimizer-simplification-enhances-robustness-in-llm-unlearning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00761
<strong>Authors:</strong> Yicheng Lang, Yihua Zhang, Chongyu Fan, Changsheng Wang, Jinghan Jia, Sijia Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.00761v1 Announce Type: new  Abstract: Large language model (LLM) unlearning aims to surgically remove the influence of undesired data or knowledge from an existing model while preserving its utility on unrelated tasks. This paradigm has shown promise in addressing privacy and safety concerns. However, recent findings reveal that unlearning effects are often fragile: post-unlearning manipulations such as weight quantization or fine-tuning can quickly neutralize the intended forgetting. Prior efforts to improve robustness primarily reformulate unlearning objectives by explicitly assuming the role of vulnerability sources. In this work, we take a different perspective by investigating the role of the optimizer, independent of unlearning objectives and formulations, in shaping unlearning robustness. We show that the 'grade' of the optimizer, defined by the level of information it exploits, ranging from zeroth-order (gradient-free) to first-order (gradient-based) to second-order (Hessian-based), is tightly linked to the resilience of unlearning. Surprisingly, we find that downgrading the optimizer, such as using zeroth-order methods or compressed-gradient variants (e.g., gradient sign-based optimizers), often leads to stronger robustness. While these optimizers produce noisier and less precise updates, they encourage convergence to harder-to-disturb basins in the loss landscape, thereby resisting post-training perturbations. By connecting zeroth-order methods with randomized smoothing, we further highlight their natural advantage for robust unlearning. Motivated by these insights, we propose a hybrid optimizer that combines first-order and zeroth-order updates, preserving unlearning efficacy while enhancing robustness. Extensive experiments on the MUSE and WMDP benchmarks, across multiple LLM unlearning algorithms, validate that our approach achieves more resilient forgetting without sacrificing unlearning quality.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2510.00206" rel="nofollow">LoRAFusion: Efficient LoRA Fine-Tuning for LLMs</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-lorafusion-efficient-lora-fine-tuning-for-llms-" class="anchor" aria-label="Permalink: 23. LoRAFusion: Efficient LoRA Fine-Tuning for LLMs" href="#23-lorafusion-efficient-lora-fine-tuning-for-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00206
<strong>Authors:</strong> Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko</p>
<p><strong>Abstract:</strong> arXiv:2510.00206v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks. Despite these benefits, we identify two key inefficiencies in existing LoRA fine-tuning systems. First, they incur substantial runtime overhead due to redundant memory accesses on large activation tensors. Second, they miss the opportunity to concurrently fine-tune multiple independent LoRA adapters that share the same base model on the same set of GPUs. This leads to missed performance gains such as reduced pipeline bubbles, better communication overlap, and improved GPU load balance.   To address these issues, we introduce LoRAFusion, an efficient LoRA fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting method that fuses memory-bound operations. This design eliminates unnecessary memory accesses and preserves the performance of compute-bound GEMMs without incurring the cost of recomputation or synchronization. At the scheduling level, LoRAFusion introduces an adaptive batching algorithm for multi-job fine-tuning. It first splits LoRA adapters into groups to intentionally stagger batch execution across jobs, and then solves a bin-packing problem within each group to generate balanced, dependency-aware microbatches. LoRAFusion achieves up to $1.96\times$ ($1.47\times$ on average) end-to-end speedup compared to Megatron-LM, and up to $1.46\times$ ($1.29\times$ on average) improvement over mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel achieves up to $1.39\times$ ($1.27\times$ on average) kernel performance improvement and can directly serve as a plug-and-play replacement in existing LoRA systems. We open-source LoRAFusion at <a href="https://github.com/CentML/lorafusion">https://github.com/CentML/lorafusion</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2510.00428" rel="nofollow">Automated Structured Radiology Report Generation with Rich Clinical Context</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-automated-structured-radiology-report-generation-with-rich-clinical-context-" class="anchor" aria-label="Permalink: 24. Automated Structured Radiology Report Generation with Rich Clinical Context" href="#24-automated-structured-radiology-report-generation-with-rich-clinical-context-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00428
<strong>Authors:</strong> Seongjae Kang, Dong Bok Lee, Juho Jung, Dongseop Kim, Won Hwa Kim, Sunghoon Joo</p>
<p><strong>Abstract:</strong> arXiv:2510.00428v1 Announce Type: new  Abstract: Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at <a href="https://github.com/vuno/contextualized-srrg">https://github.com/vuno/contextualized-srrg</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">25. <a href="https://arxiv.org/abs/2510.01032" rel="nofollow">Meaningless Tokens, Meaningful Gains: How Activation Shifts Enhance LLM Reasoning</a> <a id="user-content-link25"></a>
</h2><a id="user-content-25-meaningless-tokens-meaningful-gains-how-activation-shifts-enhance-llm-reasoning-" class="anchor" aria-label="Permalink: 25. Meaningless Tokens, Meaningful Gains: How Activation Shifts Enhance LLM Reasoning" href="#25-meaningless-tokens-meaningful-gains-how-activation-shifts-enhance-llm-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01032
<strong>Authors:</strong> Zeru Shi, Yingjia Wan, Zhenting Wang, Qifan Wang, Fan Yang, Elisa Kreiss, Ruixiang Tang</p>
<p><strong>Abstract:</strong> arXiv:2510.01032v1 Announce Type: new  Abstract: Motivated by the puzzling observation that inserting long sequences of meaningless tokens before the query prompt can consistently enhance LLM reasoning performance, this work analyzes the underlying mechanism driving this phenomenon and based on these insights proposes a more principled method that allows for similar performance gains. First, we find that the improvements arise from a redistribution of activations in the LLM's MLP layers, where near zero activations become less frequent while large magnitude activations increase. This redistribution enhances the model's representational capacity by suppressing weak signals and promoting stronger, more informative ones. Building on this insight, we propose the Activation Redistribution Module (ARM), a lightweight inference-time technique that modifies activations directly without altering the input sequence. ARM adaptively identifies near-zero activations after the non-linear function and shifts them outward, implicitly reproducing the beneficial effects of meaningless tokens in a controlled manner. Extensive experiments across diverse benchmarks and model architectures clearly show that ARM consistently improves LLM performance on reasoning tasks while requiring only a few lines of simple code to implement. Our findings deliver both a clear mechanistic explanation for the unexpected benefits of meaningless tokens and a simple yet effective technique that harnesses activation redistribution to further improve LLM performance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">26. <a href="https://arxiv.org/abs/2510.00442" rel="nofollow">Randomized Matrix Sketching for Neural Network Training and Gradient Monitoring</a> <a id="user-content-link26"></a>
</h2><a id="user-content-26-randomized-matrix-sketching-for-neural-network-training-and-gradient-monitoring-" class="anchor" aria-label="Permalink: 26. Randomized Matrix Sketching for Neural Network Training and Gradient Monitoring" href="#26-randomized-matrix-sketching-for-neural-network-training-and-gradient-monitoring-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00442
<strong>Authors:</strong> Harbir Antil, Deepanshu Verma</p>
<p><strong>Abstract:</strong> arXiv:2510.00442v1 Announce Type: new  Abstract: Neural network training relies on gradient computation through backpropagation, yet memory requirements for storing layer activations present significant scalability challenges. We present the first adaptation of control-theoretic matrix sketching to neural network layer activations, enabling memory-efficient gradient reconstruction in backpropagation. This work builds on recent matrix sketching frameworks for dynamic optimization problems, where similar state trajectory storage challenges motivate sketching techniques. Our approach sketches layer activations using three complementary sketch matrices maintained through exponential moving averages (EMA) with adaptive rank adjustment, automatically balancing memory efficiency against approximation quality. Empirical evaluation on MNIST, CIFAR-10, and physics-informed neural networks demonstrates a controllable accuracy-memory tradeoff. We demonstrate a gradient monitoring application on MNIST showing how sketched activations enable real-time gradient norm tracking with minimal memory overhead. These results establish that sketched activation storage provides a viable path toward memory-efficient neural network training and analysis.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">27. <a href="https://arxiv.org/abs/2510.00133" rel="nofollow">Large Language Models Inference Engines based on Spiking Neural Networks</a> <a id="user-content-link27"></a>
</h2><a id="user-content-27-large-language-models-inference-engines-based-on-spiking-neural-networks-" class="anchor" aria-label="Permalink: 27. Large Language Models Inference Engines based on Spiking Neural Networks" href="#27-large-language-models-inference-engines-based-on-spiking-neural-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.00133
<strong>Authors:</strong> Adarsha Balaji, Sandeep Madireddy</p>
<p><strong>Abstract:</strong> arXiv:2510.00133v1 Announce Type: new  Abstract: Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computationally challenging as the time and space complexity has a quadratic relation to the input sequence length. Several efforts exploring efficient computational paradigms and model architectures to address these limitations have been made. In this work, we explore spiking neural networks (SNNs) to design transformer models. A challenge in training large-scale SNNs, using existing surrogate learning methods is inefficient and time-consuming. On the other hand, techniques to convert existing transformer-based models to their SNN equivalent are not scalable, as achieving optimal performance comes at the cost of a large number of spike time-steps, i.e. increased latency. To address this, we propose NeurTransformer, a methodology for designing transformer-based SNN for inference using a supervised fine-tuning approach with existing conversion methods. The proposed methodology works by: (1) replacing the self-attention mechanism with a spike-based self-attention (SSA), (2) converting the feed-forward block of the trained transformer model to its equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate learning algorithms. We benchmark the proposed methodology and demonstrate its accuracy and scalability using three variants of the GPT-2 model of increasing model size. We observe that the converted GPT-2 small models demonstrate a 5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we demonstrate the energy efficiency of the SSA block compared to the ASA block and show between 64.71% and 85.28% reductions in estimated energy consumption when implementing the self-attention mechanism on a digital hardware.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">28. <a href="https://arxiv.org/abs/2510.01185" rel="nofollow">Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs</a> <a id="user-content-link28"></a>
</h2><a id="user-content-28-dirichlet-prior-shaping-guiding-expert-specialization-in-upcycled-moes-" class="anchor" aria-label="Permalink: 28. Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs" href="#28-dirichlet-prior-shaping-guiding-expert-specialization-in-upcycled-moes-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.01185
<strong>Authors:</strong> Leyla Mirvakhabova, Babak Ehteshami Bejnordi, Gaurav Kumar, Hanxue Liang, Wanru Zhao, Paul Whatmough</p>
<p><strong>Abstract:</strong> arXiv:2510.01185v1 Announce Type: new  Abstract: Upcycling pre-trained dense models into sparse Mixture-of-Experts (MoEs) efficiently increases model capacity but often suffers from poor expert specialization due to naive weight replication. Our analysis reveals that upcycled MoEs, even with conventional regularization, exhibit low-confidence, weakly differentiated routing, hindering performance. We introduce Dirichlet-Prior Shaping Loss (DPSL), a novel router regularization technique that directly shapes routing probability distributions by matching expert assignments to a target Dirichlet prior. DPSL offers fine-grained control over expert balance and specialization, and enables encoding of inductive biases such as encouraging experts to focus on specific modalities or tasks, without requiring manual intervention; notably, DPSL is a general tool applicable to any module that outputs categorical probability distributions, extending its utility beyond MoE training. Experiments on upcycled MoE vision-language models (with Qwen2, Phi3, Llama3.2 LLM backbones) show DPSL consistently outperforms upcycling strategies and regularization techniques across standard vision-language benchmarks, addressing the critical issue of poor specialization and fostering more adaptive, higher-performing models.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research aims to train a model that effectively integrates multiple modalities, such as text, language, and vision, to enhance the model's performance and generalization ability. The approach helps optimize model training and improves the synergy between different modalities during the fusion process.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>