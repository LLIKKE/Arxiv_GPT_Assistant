<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">等价剪枝器：通过动作剪枝提升基于大语言模型的搜索效率与质量</h2><a id="user-content-等价剪枝器通过动作剪枝提升基于大语言模型的搜索效率与质量" class="anchor" aria-label="Permalink: 等价剪枝器：通过动作剪枝提升基于大语言模型的搜索效率与质量" href="#等价剪枝器通过动作剪枝提升基于大语言模型的搜索效率与质量"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.16312v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）通过搜索算法擅长复杂推理，但现有策略常因对语义等价步骤的冗余探索而导致大量token消耗。当前语义相似度方法难以在数学推理等专业领域中准确识别此类等价性。为此，我们提出EquivPruner——一种简单高效的方法，能在LLM推理搜索中识别并剪枝语义等价操作。同时我们发布MathEquiv数据集（首个数学命题等价性标注数据集），用于训练轻量级等价检测器。多模型多任务的实验表明，EquivPruner显著降低token消耗，既提升搜索效率又常增强推理准确率。例如在GSM8K任务中应用Qwen2.5-Math-7B-Instruct模型时，EquivPruner减少48.1%的token使用量并同步提升准确率。代码已开源：<a href="https://github.com/Lolo1222/EquivPruner">https://github.com/Lolo1222/EquivPruner</a></p>
<p>（注：根据学术文献翻译规范，保留技术术语"token"不译；"剪枝"采用计算机领域通用译法；数据集名称MathEquiv保留原名体现专有性；百分比数字格式遵循中文排版规范；GitHub链接保留原始格式确保可操作性）</p>
<div class="markdown-heading"><h2 class="heading-element">瓶颈式Transformer：面向广义推理的周期性KV缓存抽象</h2><a id="user-content-瓶颈式transformer面向广义推理的周期性kv缓存抽象" class="anchor" aria-label="Permalink: 瓶颈式Transformer：面向广义推理的周期性KV缓存抽象" href="#瓶颈式transformer面向广义推理的周期性kv缓存抽象"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.16950v1 公告类型：新研究<br>
摘要：尽管大型语言模型展现出令人印象深刻的能力，但其在训练数据分布之外的泛化能力仍存在局限——它们往往表现出复杂的模式插值而非真正的抽象推理（外推）。本研究通过信息瓶颈理论（IB理论）的视角探讨这一局限，该理论认为模型的泛化能力源于潜在表征中输入压缩与预测信息保留之间的最优平衡。我们运用IB理论证明：仅含解码器的Transformer模型在形成任务最优序列表征方面存在固有约束。基于这一结论，我们进一步论证：对内部序列级表征（KV缓存）进行周期性全局变换，是提升Transformer在推理任务中泛化能力的必要计算步骤。</p>
<p>基于这些理论洞见，我们提出对Transformer架构的改进方案——通过新增模块定期全局重写KV缓存，将其容量从记忆输入前缀转向编码对预测未来token最有用的特征。我们的模型在数学推理基准测试中取得显著提升，其表现不仅优于参数量达3.5倍的原始Transformer模型，也超越了采用启发式缓存压缩机制的修剪方法。该方法可视为现有KV缓存压缩技术的原理性泛化：传统方法仅聚焦于压缩输入表征，却常以损失预测信息为代价，因此其能力本质上受限于无约束模型。本研究建立了运用信息论操控Transformer记忆的原理性框架，解决了单靠规模扩展无法克服的根本性推理局限。</p>
<p>（注：根据学术文本特点，翻译中采取了以下处理：</p>
<ol>
<li>专业术语如"Information Bottleneck"保留理论简称"IB理论"并首次出现时标注全称</li>
<li>"KV cache"统一译为技术圈通用表述"KV缓存"</li>
<li>长难句进行合理切分，如将原文最后复合句拆分为两个递进句</li>
<li>保持被动语态与学术客观性，如"it is proved"译为"我们证明"符合中文主动表述习惯</li>
<li>概念性表述如"pattern interpolation"意译为"模式插值"而非字面直译）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">NQKV：基于正态分布特性的键值缓存量化方案</h2><a id="user-content-nqkv基于正态分布特性的键值缓存量化方案" class="anchor" aria-label="Permalink: NQKV：基于正态分布特性的键值缓存量化方案" href="#nqkv基于正态分布特性的键值缓存量化方案"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此处采用技术文档常见的简洁译法，保留缩写"NQKV"作为方案名称。"KV Cache"译为"键值缓存"符合计算机领域术语规范，"Quantization Scheme"译为"量化方案"准确体现算法优化特性，"Normal Distribution Characteristics"译为"正态分布特性"既保持数学严谨性又符合中文表达习惯。整体翻译在保持专业性的同时确保技术概念的清晰传达。）</p>
<p>arXiv:2505.16210v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）已在广泛任务中展现出卓越能力。然而，LLMs通常需要更大的批处理规模以提升吞吐量，或更长的上下文长度以满足任务需求，这显著增加了推理过程中键值（KV）缓存的显存资源消耗，成为LLM部署的主要瓶颈。为解决该问题，量化是一种常见且直接的解决方案。当前激活函数的量化方法仅支持8比特，更低比特的量化会导致精度大幅下降。为了通过更低比特量化KV缓存进一步节省空间，我们分析了KV缓存的元素分布规律，并设计了NQKV算法。由于KV缓存每个分块内的元素服从正态分布，NQKV采用分块分位数量化技术，实现了信息论最优的量化误差。在不显著影响模型输出质量的前提下，NQKV使OPT模型能以2倍批处理规模或4倍上下文长度进行推理，与未使用KV缓存时相比，吞吐量提升了9.3倍。</p>
<p>（注：根据学术论文摘要的文体特征，翻译时进行了以下处理：</p>
<ol>
<li>技术术语如"throughput"统一译为"吞吐量"，"context length"译为"上下文长度"</li>
<li>被动语态转换为中文主动表达（如"are limited to"译为"仅支持"）</li>
<li>长难句拆分重组（如原文最后一句拆分为三个中文短句）</li>
<li>专业表述保留英文缩写（如OPT模型）同时确保首次出现时全称明确</li>
<li>量化单位"bit"根据中文计算机领域惯例译为"比特"）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">（选择性）四舍五入量化是否就是您所需的一切？</h2><a id="user-content-选择性四舍五入量化是否就是您所需的一切" class="anchor" aria-label="Permalink: （选择性）四舍五入量化是否就是您所需的一切？" href="#选择性四舍五入量化是否就是您所需的一切"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.15909v1 公告类型：新研究<br>
摘要：量化技术已成为服务日益增长的大型语言模型（LLM）的必要工具。RTN（就近取整法）可能是最简单的量化方法，早在LLM成为机器学习（ML）研究焦点之前就已存在。然而，近期更先进的量化方法几乎在各方面性能上都宣称优于RTN，导致其长期被忽视。本研究旨在打破这一固有认知，证明RTN不仅应用成本更低，其令牌生成吞吐量可超越先进方法，且准确度也能与之媲美。我们特别讨论了基于最新Marlin内核实现的RTN方案，并展示如何通过选择性提升特定模型层和模块的数据精度格式来逐步改善RTN的准确度。实验结果表明，RTN为LLM量化提供了一个切实可行的优选方案。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了统一处理：</p>
<ol>
<li>"Round-to-Nearest" 采用计算机领域通用译法"就近取整法"</li>
<li>"kernels" 保留技术语境译为"内核"而非字面"核心"</li>
<li>"throughput" 译为"吞吐量"符合计算机性能术语</li>
<li>被动语态转换为中文主动表述（如"has been largely dismissed"→"长期被忽视"）</li>
<li>长句拆分重组以符合中文表达习惯）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>