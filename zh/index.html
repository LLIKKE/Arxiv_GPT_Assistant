<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">Apriel-1.5-15b-Thinker</h2><a id="user-content-apriel-15-15b-thinker" class="anchor" aria-label="Permalink: Apriel-1.5-15b-Thinker" href="#apriel-15-15b-thinker"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01141v1宣布类型：新摘要：我们介绍了Apriel-1.5- 15 B-Thinker，这是一个150亿参数的开权多模式推理模型，通过训练设计而不是纯粹的规模来实现前沿级别的性能。从Pixtral-12 B开始，我们采用渐进的三阶段方法：（1）深度升级以扩大推理能力，而无需从头开始进行预训练，（2）分阶段连续的预训练，首先发展基础文本和视觉理解，然后通过针对空间结构、成分理解和细粒度感知的有针对性的合成数据生成来增强视觉推理，和（3）对精心策划的描述-响应对进行高质量的纯文本监督微调，具有跨越数学、编码、科学和工具使用的明确推理痕迹。值得注意的是，我们的模型在没有强化学习或偏好优化的情况下实现了有竞争力的结果，隔离了我们以数据为中心的持续预训练方法的贡献。在人工分析智能指数中，Apriel-1.5- 15 B-Thinker的得分为52，与DeepSeek-R1-0528相匹配，尽管需要的计算资源明显较少。在十个图像基准测试中，其性能平均在Gemini-2.5-Flash和Claude Sonnet-3.7的五个百分点以内，这对于在单图形处理器部署限制下运行的模型来说是一个关键成就。我们的结果表明，深思熟虑的中期训练2设计可以在不大规模的情况下缩小巨大的能力差距，使基础设施有限的组织能够使用前沿级多模式推理。我们根据麻省理工学院许可发布模型检查点、所有训练食谱和评估协议，以推进开源研究。</p>
<div class="markdown-heading"><h2 class="heading-element">VIRTUE：视觉交互式文本图像通用嵌入器</h2><a id="user-content-virtue视觉交互式文本图像通用嵌入器" class="anchor" aria-label="Permalink: VIRTUE：视觉交互式文本图像通用嵌入器" href="#virtue视觉交互式文本图像通用嵌入器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00523v1宣布类型：新摘要：多模式表示学习模型已经证明了在复杂任务中的成功操作，视觉语言模型（VLM）的集成进一步使嵌入模型具有描述跟踪能力。然而，现有的嵌入模型缺乏指定用户感兴趣区域的视觉交互能力（例如，点、边界框、面具），已在生成模型中对其进行了探索，以扩大其与人类互动的适用性。为嵌入模型配备视觉交互不仅可以解锁具有本地化用户意图基础的新应用程序（这一点尚未被探索），而且还可以使模型能够学习图像中的实体级信息，以补充其针对传统嵌入任务的全局表示。在本文中，我们提出了一种新型的可视化交互式文本图像通用嵌入器（VIRTUE），它将分割模型和视觉语言模型的能力扩展到了表示学习领域。在VIRTUE中，分割模型可以处理确定图像中特定区域的视觉提示，从而使嵌入器能够更准确地处理复杂和模糊的场景。为了评估VIRTUE的视觉交互能力，我们引入了一个由1 M个样本组成的大规模分段和场景字幕检索（SCaR）基准，旨在通过联合考虑实体与特定对象和图像场景来检索文本字幕。VIRTUE始终实现最先进的性能，在36项通用MMEB（3.1%-8.5%）和5项视觉交互SCaR（15.2%-20.3%）任务中取得了显着改进。</p>
<div class="markdown-heading"><h2 class="heading-element">期望注意力：基于未来注意力分布的KV Cache压缩</h2><a id="user-content-期望注意力基于未来注意力分布的kv-cache压缩" class="anchor" aria-label="Permalink: 期望注意力：基于未来注意力分布的KV Cache压缩" href="#期望注意力基于未来注意力分布的kv-cache压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00636v1宣布类型：新摘要：Key-Value（KV）缓存的内存消耗是高效大型语言模型推理的主要瓶颈。虽然基于注意力分数的KV缓存修剪显示出希望，但它面临着关键的实际局限性：来自未来代币的注意力分数在压缩期间不可用，而且Flash Attention等现代实现无法实现完整的注意力矩阵，使得过去的分数无法访问。为了克服这些挑战，我们引入了$\textBF{Expeded Attention，一种免训练的压缩方法}$，该方法通过预测未来查询将如何关注KV对来估计它们的重要性。我们的方法利用LLM激活的分布属性来计算每个KV对的封闭形式的预期注意力分数。这些分数可以对KV对进行原则性的排名和修剪，同时对剩余流的影响最小，实现有效的压缩，而不会降低性能。重要的是，我们的方法在预填充和解码阶段无缝运行，在这两种情况下始终优于最先进的基线。最后，$\textBF{我们发布了KVPress，这是一个综合库，使研究人员能够实施和基准测试KV缓存压缩方法，已经包括20多种技术}$。</p>
<div class="markdown-heading"><h2 class="heading-element">PrunedLoRA：用于微调中低等级自适应的鲁棒基于对象的结构化修剪</h2><a id="user-content-prunedlora用于微调中低等级自适应的鲁棒基于对象的结构化修剪" class="anchor" aria-label="Permalink: PrunedLoRA：用于微调中低等级自适应的鲁棒基于对象的结构化修剪" href="#prunedlora用于微调中低等级自适应的鲁棒基于对象的结构化修剪"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00192v1宣布类型：新摘要：低等级适应（LoRA）已成为大型语言模型参数高效微调的广泛使用的范式，但其表示能力往往落后于完全微调。在LoRA的背景下，一个关键的悬而未决问题是如何从过度参数化的空间中获得表达性的低等级适配器。我们提出了\textit{PrunedLoRA}，这是一个新框架，它利用结构化修剪从过度参数化的初始化中获得高度代表性的低等级适配器。与之前强加固定低等级预算的方法不同，PrunedLoRA在微调期间动态修剪不太重要的组件，并防止它们重新激活，从而实现灵活且自适应的等级分配。对于结构化修剪，通过最大限度地减少总体损失的修剪误差，我们在具有接地解释的基于梯度的修剪策略中提供细粒度修剪和恢复更新。我们对结构化修剪的鲁棒性进行了首次理论分析，并证明在权重扰动的影响下，就总体损失而言，基于梯度的修剪比基于激活的修剪更鲁棒。从经验上看，PrunedLoRA在数学推理、代码生成和自然语言理解方面的监督微调任务中始终优于LoRA及其变体，并且它还在不同稀疏度水平上表现出了比现有结构化修剪方法的优势。</p>
<div class="markdown-heading"><h2 class="heading-element">ARS：高效大型推理语言模型的自适应推理抑制</h2><a id="user-content-ars高效大型推理语言模型的自适应推理抑制" class="anchor" aria-label="Permalink: ARS：高效大型推理语言模型的自适应推理抑制" href="#ars高效大型推理语言模型的自适应推理抑制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00071v1宣布类型：新摘要：大型推理语言模型（LRLM或LRM）在复杂推理任务中表现出出色的能力，但由于过度思考现象而存在严重的计算效率低下。现有的高效推理方法面临着平衡推理质量与推理成本降低的挑战。我们提出了\textBF{自适应推理抑制（ARS）}，这是一种新型的免训练方法，它动态地抑制冗余推理步骤，同时通过自适应确定性监控保持准确性。ARS引入了具有渐进抑制阈值的多检查点确定性估计机制，与静态抑制方法相比，实现了更高的效率。我们对使用多种模型架构的数学推理基准进行了广泛评估，表明ARS在令牌、延迟和能量减少方面实现了高达53%、46.1%和57.9%的目标，同时保持或提高了准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">增强LLM以实现一般时间序列理解和预测</h2><a id="user-content-增强llm以实现一般时间序列理解和预测" class="anchor" aria-label="Permalink: 增强LLM以实现一般时间序列理解和预测" href="#增强llm以实现一般时间序列理解和预测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.0111v1宣布类型：新摘要：时间序列数据是医疗保健、金融和环境科学等许多关键领域决策的基础。然而，分析这些数据通常需要合并非结构化上下文信息、回答特定领域的问题并生成自然语言解释--传统时间序列模型由于无法处理文本而缺乏这些能力。虽然大型语言模型（LLM）擅长上下文推理和知识集成，但由于基于文本的表示效率低下以及预训练期间对时态数据的接触有限，它们在数字时间序列方面遇到了困难。我们通过基于补丁的编码器-解码器架构用专门的时间序列感知来增强LLM来解决这一差距。我们在一个由超过200万个交叉时间序列和文本示例组成的大型数据库上训练这个时间序列增强的LLM（TsLLM），这些示例跨越了各种分析任务：利用上下文信息进行预测、时间序列问答、模式解释、利用自然语言输出进行分类和报告生成。这项培训使TsLLM能够利用其语言理解和新获得的时态推理能力。虽然TsLLM并不是为了超越传统基准上的专业模型而设计的，但在需要将时间序列分析与自然语言集成的任务上表现出了出色的性能--而这些能力是现有方法无法提供的。我们的工作建立了时间序列分析的新范式，该范式将数字计算和自然语言理解联系起来，通过自然语言交互实现复杂时间推理的民主化。</p>
<div class="markdown-heading"><h2 class="heading-element">学习零阶优化器进行微调LLM</h2><a id="user-content-学习零阶优化器进行微调llm" class="anchor" aria-label="Permalink: 学习零阶优化器进行微调LLM" href="#学习零阶优化器进行微调llm"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00419v1宣布类型：新摘要：零阶优化器最近成为微调大型语言模型（LLM）的一种实用方法，与传统的一阶方法相比，显着减少了图形处理器内存消耗。然而，现有的零阶方法依赖于手工制作的静态采样策略，这些策略不适合特定于模型的结构。为了解决这个问题，我们提出了Zero Fine-tuner，这是一种基于学习的LLM零阶优化器，通过紧凑且内存高效的设计自动学习高效的扰动策略。至关重要的是，我们的方法的动机是这样一个观察：只有少数基础模型及其衍生物在实践中被广泛采用。因此，针对给定的LLM学习一次优化器并在不同的下游任务中重用它既是可行的，也是非常可取的。因此，Zero Fine-tuner旨在通过支持每个LLM的一次性训练以最小的费用来将从学习到学习（L2 L）扩展到基础模型时代。对4个LLM和7个数据集的实验表明，Zero Fine-tuner在82.1%的任务模型组合中优于之前的零阶基线，从而展示了高效LLM微调的强大性能和可扩展性。我们的代码可在<a href="https://github.com/ASTRAL-Group/ZO_Fine_tuner.git%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ASTRAL-Group/ZO_Fine_tuner.git上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">语言模型中的谱缩放定律：前向网络如何有效地利用其潜在空间？</h2><a id="user-content-语言模型中的谱缩放定律前向网络如何有效地利用其潜在空间" class="anchor" aria-label="Permalink: 语言模型中的谱缩放定律：前向网络如何有效地利用其潜在空间？" href="#语言模型中的谱缩放定律前向网络如何有效地利用其潜在空间"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00537v1宣布类型：新摘要：随着大型语言模型（LLM）的规模，问题不仅在于它们的规模有多大，还在于它们的容量有多大得到了有效利用。现有的缩放定律将模型大小与损失联系起来，但忽视了组件如何利用其潜在空间。我们研究前向网络（FFN）并将宽度选择重新设定为频谱利用问题。使用轻量级诊断套件--硬排名（参与率）、软排名（香农排名）、光谱浓度和复合光谱利用指数（SUI）--我们量化了LLaMA、GPT-2和nGPT家族中有多少潜在方向被有意义地激活。我们的关键发现是不对称的谱缩放定律：软排序遵循几乎完美的功率定律，具有FFN宽度，而硬排序仅次线性增长并且具有高方差。这种不对称性表明，扩大FFN主要增加了低能量尾部方向，而主导模子空间则提前饱和。此外，在宽度更大时，方差进一步塌陷到一个狭窄的子空间中，从而导致大部分潜在空间没有得到充分利用。这些结果将FFN宽度选择重新塑造为尾部容量和主导模式容量之间的原则权衡，为推理高效的LLM设计提供了具体指导。</p>
<div class="markdown-heading"><h2 class="heading-element">从外表来看？保释预测的审计和干预视觉语言模型</h2><a id="user-content-从外表来看保释预测的审计和干预视觉语言模型" class="anchor" aria-label="Permalink: 从外表来看？保释预测的审计和干预视觉语言模型" href="#从外表来看保释预测的审计和干预视觉语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00088v1宣布类型：新摘要：大型语言模型（LLM）已广泛用于基于案件报告和犯罪历史的法律判断预测任务。然而，随着大型视觉语言模型（VLM）可用性的激增，法律判断预测系统现在可以利用犯罪分子的图像以及文本案件报告/犯罪历史。以这种方式构建的应用程序可能会导致无意中的后果并被恶意使用。在这项工作中，我们运行审计来调查独立VLM在保释决策预测任务中的效率。我们观察到，多个交叉群体和模型的表现很差\textit{错误地拒绝向具有非常高信心的值得保释的个人提供保释}。我们设计不同的干预算法，首先通过RAG管道包括法律先例，然后使用创新方案微调VLM。我们证明，这些干预措施极大地提高了保释预测的性能。我们的工作为未来在VLM上设计更智能的干预措施铺平了道路，然后才能将它们部署到现实世界的法律判断预测中。</p>
<div class="markdown-heading"><h2 class="heading-element">用于多模式知识图中少镜头关系学习的FusionAdaptor</h2><a id="user-content-用于多模式知识图中少镜头关系学习的fusionadaptor" class="anchor" aria-label="Permalink: 用于多模式知识图中少镜头关系学习的FusionAdaptor" href="#用于多模式知识图中少镜头关系学习的fusionadaptor"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00894v1宣布类型：新摘要：多模式知识图（MMKG）融合了各种形式（包括文本和图像），以增强实体和关系表示。值得注意的是，同一实体的不同模式通常呈现互补且多样化的信息。然而，现有的MMKG方法主要将模式与共享空间对齐，这往往会忽视特定模式的独特贡献，从而限制了它们的性能，尤其是在低资源环境中。为了应对这一挑战，我们提出了FusionAdaptor来学习MMKG中的少镜头关系（FSRL）。FusionAdaptor引入了（1）一个适配器模块，可以将每个模式有效地适应不可见的关系，以及（2）一个融合策略，可以集成多模式实体表示，同时保留不同的模式特定特征。通过有效地适应和融合来自不同模式的信息，FusionAdaptor以最少的监督改进了对新型关系的概括。对两个基准MMKG数据集的广泛实验表明，FusionAdaptor实现了优于最先进方法的卓越性能。</p>
<div class="markdown-heading"><h2 class="heading-element">通过梯度矩阵去噪实现样本高效的差异私密微调</h2><a id="user-content-通过梯度矩阵去噪实现样本高效的差异私密微调" class="anchor" aria-label="Permalink: 通过梯度矩阵去噪实现样本高效的差异私密微调" href="#通过梯度矩阵去噪实现样本高效的差异私密微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01137v1宣布类型：新摘要：我们解决了使用DP-BCD对大型语言模型（LLM）进行差异私密微调时样本效率的挑战。虽然DP-BCD提供了强大的隐私保证，但添加的噪音显着增加了梯度矩阵的熵，扰乱了其低阶结构并减慢了优化。我们提出了一种后处理算法，该算法利用随机矩阵理论来对梯度进行降噪、恢复低等级结构并改善与原始信号的对齐。应用于GLUE任务上RoBEERTa的DP-BCD微调，与最先进的方法相比，我们的方法提高了样本效率，在不需要最佳性能时大幅缩短了训练时间。这项工作表明，矩阵恢复技术可以在不损害隐私保证的情况下增强私人语言模型训练的实用性。</p>
<div class="markdown-heading"><h2 class="heading-element">VLA-RFT：视觉-语言-动作强化微调，在世界模拟器中验证奖励</h2><a id="user-content-vla-rft视觉-语言-动作强化微调在世界模拟器中验证奖励" class="anchor" aria-label="Permalink: VLA-RFT：视觉-语言-动作强化微调，在世界模拟器中验证奖励" href="#vla-rft视觉-语言-动作强化微调在世界模拟器中验证奖励"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00406v1公告类型：新摘要：视觉-语言-动作（VLA）模型支持具体决策，但严重依赖模仿学习，导致复合错误和分布变化下的鲁棒性差。强化学习（RL）可以缓解这些问题，但通常需要昂贵的现实世界交互或遭受模拟到真实的差距。我们引入VLA-RFT，这是一种强化微调框架，利用数据驱动的世界模型作为可控模拟器。该模拟器从真实的交互数据中训练，预测未来的视觉观察结果取决于行动，从而允许政策推出，并从实现目标的参考中获得密集的、专家级奖励。该设计提供高效且与动作一致的学习信号，大幅降低了样本要求。VLA-RFT的微调步骤少于400个，超越了强监督基线，并比基于模拟器的RL实现了更高的效率。此外，它在扰动条件下表现出很强的鲁棒性，维持稳定的任务执行。我们的结果将基于世界模型的RFT建立为一种实用的训练后范式，以增强VLA模型的概括性和稳健性。欲了解更多详细信息，请访问<a href="https://vla-rft.github.io/%E3%80%82" rel="nofollow">https://vla-rft.github.io/。</a></p>
<div class="markdown-heading"><h2 class="heading-element">免费起草和验证：迈向扩散大型语言模型的无损并行解码</h2><a id="user-content-免费起草和验证迈向扩散大型语言模型的无损并行解码" class="anchor" aria-label="Permalink: 免费起草和验证：迈向扩散大型语言模型的无损并行解码" href="#免费起草和验证迈向扩散大型语言模型的无损并行解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00294v1宣布类型：新摘要：扩散大型语言模型（DLLM）已成为超越自回归下一个令牌预测的语言建模新范式。由于其双向注意力机制，DLLM更有能力捕捉上下文的联系，从而在著名的“逆转诅咒”或数据受限场景下学习等挑战中显示出独特的优势。然而，这种双向性也带来了一个障碍，即DLLM本质上与KV缓存不兼容，因此推理效率与自回归模型相比没有竞争力。现有的并行解码算法利用其固有的多令牌预测能力，可以加速DLLM推理，但代价是不可忽视的性能下降。为了克服这一挑战，我们引入了Free Draft-and-Verification（Freedave），这是一种专为DLLM量身定制的新型快速采样算法，可实现无损并行解码。具体来说，我们提出了一种并行解码候选生成和验证的管道，保证能够重现静态采样生成的相同序列，而不会引入额外的模型前向调用。通过应用Freedave，DLLM的吞吐量可以提高到2.8美元x $，而数学推理任务的性能不会下降。</p>
<div class="markdown-heading"><h2 class="heading-element">DiSC-AMC：令牌和参数高效的离散统计上下文自动调制分类</h2><a id="user-content-disc-amc令牌和参数高效的离散统计上下文自动调制分类" class="anchor" aria-label="Permalink: DiSC-AMC：令牌和参数高效的离散统计上下文自动调制分类" href="#disc-amc令牌和参数高效的离散统计上下文自动调制分类"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00316v1宣布类型：新摘要：当配备精心设计的上下文提示时，大型语言模型（LLM）可以以开放式方式执行自动调制分类（AMC），无需LLM微调~\cite{rostami 2025 plug}。在之前的工作的基础上，我们针对的是阻碍环内部署的长提示上下文和大模型尺寸的实际瓶颈。我们提出了上下文中的离散统计自动调制分类（DiSC-AMC），一个代币和参数高效的变体，它：（i）将更高级统计数据和累积量离散化为紧凑的符号标记，（ii）通过轻量级k顶神经预过滤器修剪样本列表，并使用从先前LLM响应中提取的原理过滤误导性/低影响特征，以及（iii）通过校准的提示模板强制执行仅标签预测。总而言之，这些变化将输入/输出令牌和模型参数足迹减少了一半以上，同时保持了竞争准确性。在噪音下具有十种调制类型的合成AMC上，7 B\texttit {DeepSeek-R1-Distill-Qwen}基线可实现5.2%的准确性，而我们的系统使用大约5 B参数\texttit {Gemini-2.5-Flash}~\cite{comanici 2025 gemini}模型可实现45.5%的准确性。这些结果表明，仔细的离散化和上下文选择可以将推理成本降低2倍以上，同时保留基于预算的AMC的优势并实现实际的环内使用。</p>
<div class="markdown-heading"><h2 class="heading-element">用于移动和嵌入式设备的自适应且资源高效的公共人工智能系统：调查</h2><a id="user-content-用于移动和嵌入式设备的自适应且资源高效的公共人工智能系统调查" class="anchor" aria-label="Permalink: 用于移动和嵌入式设备的自适应且资源高效的公共人工智能系统：调查" href="#用于移动和嵌入式设备的自适应且资源高效的公共人工智能系统调查"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00078v1宣布类型：新摘要：基础模型通过将碎片化的架构统一为具有多模式推理和上下文适应的可扩展主干，重塑了人工智能。与此同时，由感知-决策-动作循环定义的人工智能代理的长期概念正在进入一个新的范式：以FM为认知核心，代理超越基于规则的行为，实现自主性、概括性和自我反思。自动驾驶、机器人技术、虚拟助理和图形用户界面代理等现实世界的需求，以及嵌入式硬件、边缘计算、移动部署平台和通信协议方面的生态系统进步，共同实现了大规模部署，强化了这种双重转变。然而，这种融合与现实相冲突：虽然应用程序需要长期适应性和实时交互，但移动和边缘部署仍然受到内存、能源、带宽和延迟的限制。这在FM日益增长的复杂性与部署环境的有限资源之间造成了根本性的紧张关系。这项调查首次对自适应、资源高效的人工智能系统进行了系统性描述。我们将使能技术总结为弹性推理、测试时间自适应、动态多模式集成和代理人工智能应用，并确定了平衡准确性、延迟、通信权衡和在分布变化下维持稳健性方面的公开挑战。我们进一步强调算法-系统协同设计、认知适应和协作边缘部署方面的未来机会。通过映射FM结构、认知和硬件资源，这项工作建立了可扩展、自适应和资源高效的代理人工智能的统一视角。我们相信这项调查可以帮助读者了解使能技术之间的联系，同时促进有关代理智能和智能代理融合的进一步讨论。</p>
<div class="markdown-heading"><h2 class="heading-element">ACPO：适应性课程政策优化，以协调复杂推理中的视觉语言模型</h2><a id="user-content-acpo适应性课程政策优化以协调复杂推理中的视觉语言模型" class="anchor" aria-label="Permalink: ACPO：适应性课程政策优化，以协调复杂推理中的视觉语言模型" href="#acpo适应性课程政策优化以协调复杂推理中的视觉语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00690v1宣布类型：新摘要：通过强化学习调整大规模视觉语言模型（VLM）以进行复杂推理通常会受到现有策略优化算法的局限性的阻碍，例如静态训练时间表和近端策略优化（PPO）中的严格、统一的剪裁机制。在这项工作中，我们引入了自适应课程政策优化（ACPO），这是一个新颖的框架，通过双组件自适应学习策略来应对这些挑战。首先，ACPO采用动态课程，通过逐步增加样本重用，精心安排从稳定的、接近政策的探索阶段到高效的、非政策的利用阶段的原则性过渡。其次，我们提出了一种阈值感知自适应剪裁（AAAC）机制，该机制用由每个令牌的规范化优势调制的动态样本边界取代固定剪裁超参数。这允许进行更细、更稳健的策略更新，为高潜力样本提供更大的梯度，同时防止破坏性样本。我们对一套具有挑战性的多模式推理基准进行了广泛的实验，包括MathVista、LogicVista和MMMU-Pro。结果表明，ACPO始终优于DAPO和PAPO等强基线，实现了最先进的性能、加速的收敛和卓越的训练稳定性。</p>
<div class="markdown-heading"><h2 class="heading-element">量化LLM中重新思考RoPE量表：理论、异常值和带权重重新缩放的队列带分析</h2><a id="user-content-量化llm中重新思考rope量表理论异常值和带权重重新缩放的队列带分析" class="anchor" aria-label="Permalink: 量化LLM中重新思考RoPE量表：理论、异常值和带权重重新缩放的队列带分析" href="#量化llm中重新思考rope量表理论异常值和带权重重新缩放的队列带分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00028v1宣布类型：新摘要：扩展大型语言模型（LLM）的上下文窗口支持对于具有远程依赖关系的任务至关重要。基于RoPE的内插和外插方法（例如线性缩放和频率感知方案）无需重新训练即可支持更长的输入长度，而训练后量化（PTQ）使部署变得可行。然而，我们表明，将RoPE位置插值（PI）与PTQ相结合会降低准确性，这是由于耦合效应，包括长上下文混叠、动态范围膨胀、轴对齐量化器与旋转RoPE对的各向异性以及产生位置相关的logit噪音的异常值漂移。据我们所知，我们首次对PI+PTQ方法进行了系统分析，并引入了两种实用的诊断方法：插值压力（每频段对相缩放的敏感性）和尾部充气比（异常值从短期到长期的变化）。根据分析结果，我们提出Q-ROAR（量化、RoPE插值和离群值感知重新缩放），这是量化LLM的一种仅加权、内插感知PI稳定化。Q-ROAR将RoPE维度分组到少数频段中，并在每个频段尺度上对Key和Query权重执行轻量级搜索（具有可选的对称变体以保留logit尺度）。搜索由我们的诊断指导，并使用微小的长上下文开发数据集，不需要对模型进行微调，不需要架构或内核更改，也不需要额外的部署费用。从经验上看，Q-ROAR将模型对长上下文工作负载的复杂性降低了14%以上，同时保留了短上下文性能、推理吞吐量以及与现有LLM系统栈的兼容性。</p>
<div class="markdown-heading"><h2 class="heading-element">KV缓存压缩的陷阱</h2><a id="user-content-kv缓存压缩的陷阱" class="anchor" aria-label="Permalink: KV缓存压缩的陷阱" href="#kv缓存压缩的陷阱"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00231v1宣布类型：新摘要：KV缓存压缩承诺提高吞吐量和效率，而性能损失可忽略不计。虽然吞吐量的提高是无可争议的，而且最近的文献确实表明特定基准的退化最小，但一般来说，在现实场景（例如多指令提示）中压缩的后果还没有得到充分的研究。在本文中，我们确定了从业者在部署KV缓存压缩LLM时应该注意的几个陷阱。重要的是，我们表明某些指令随着压缩而退化得更快，从而有效地导致它们被LLM完全忽略。作为一个实际例子，我们重点介绍了系统提示泄漏作为案例研究，以经验方式展示了压缩对泄漏和一般指令遵循的影响。我们展示了在即时泄漏中发挥作用的几个因素：压缩方法、指令顺序和KV驱逐偏差。然后，我们提出对KV缓存驱逐策略的简单更改，可以减少这些因素的影响并提高多指令任务的整体性能。</p>
<div class="markdown-heading"><h2 class="heading-element">HAMLET：将您的视觉-语言-行动模型转换为历史感知政策</h2><a id="user-content-hamlet将您的视觉-语言-行动模型转换为历史感知政策" class="anchor" aria-label="Permalink: HAMLET：将您的视觉-语言-行动模型转换为历史感知政策" href="#hamlet将您的视觉-语言-行动模型转换为历史感知政策"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00695v1公告类型：新摘要：从本质上讲，机器人操作任务是依赖于历史的：利用过去的背景可能是有益的。然而，大多数现有的视觉-语言-动作模型（VLA）在设计时没有考虑这方面，即，他们仅依赖当前的观察，而忽略先前的上下文。在本文中，我们提出了HAMLET，这是一个可扩展的框架，用于调整VLA，以适应动作预测期间的历史背景。具体来说，我们引入了在每个时间步紧凑地编码感知信息的时刻令牌。它们的表示是通过时间对比学习初始化的，使它们能够更好地捕捉时间上不同的方面。接下来，我们采用了一个轻量级的内存模块，将过去时间步的时刻令牌集成到内存特征中，然后利用这些特征进行动作预测。通过实证评估，我们表明，HAMLET成功地将一个国家的最先进的VLA到一个历史意识的政策，特别是展示了显着的改善，需要历史背景下的长期任务。特别是，在GR 00 T N1.5的基础上，HAMLET在历史相关的现实任务中实现了76.4%的平均成功率，超过基线性能47.2%。此外，HAMLET将RoboCasa Kitchen（100演示设置）的现有技术性能从64.1%提高到66.4%，并将LIBERO的现有技术性能从95.6%提高到97.7%，即使在通用机器人操作基准下也能突出其有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">ACON：优化长期LLM代理的上下文压缩</h2><a id="user-content-acon优化长期llm代理的上下文压缩" class="anchor" aria-label="Permalink: ACON：优化长期LLM代理的上下文压缩" href="#acon优化长期llm代理的上下文压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00615v1宣布类型：新摘要：大型语言模型（LLM）越来越多地作为代理部署在动态的现实世界环境中，成功需要推理和有效的工具使用。代理任务的一个核心挑战是不断增长的上下文长度，因为代理必须积累长期的动作和观察历史。这种扩展增加了长期任务的成本并降低了效率，但之前关于上下文压缩的工作主要集中在一步任务或狭窄应用程序上。我们引入了代理上下文优化（ACON），这是一个统一的框架，可以将环境观察和交互历史最佳地压缩为简洁且信息丰富的浓缩。ACON利用自然语言空间中的压缩指南优化：给定完整上下文成功但压缩上下文失败的配对轨迹，有能力的LLM分析失败的原因，并相应更新压缩指南。此外，我们建议将优化的LLM压缩机提炼成更小的模型，以减少额外模块的管理费用。AppWorld、DeliverBench和多目标QA上的实验表明，ACON将内存使用量减少了26-54%（峰值令牌），同时在很大程度上保留了任务性能，在提炼到较小的压缩器中时保留了超过95%的准确性，并将较小的LM作为长期代理进行增强，性能提高高达46%。</p>
<div class="markdown-heading"><h2 class="heading-element">视觉-语言-动作模型的混合训练</h2><a id="user-content-视觉-语言-动作模型的混合训练" class="anchor" aria-label="Permalink: 视觉-语言-动作模型的混合训练" href="#视觉-语言-动作模型的混合训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00600v1宣布类型：新摘要：使用大型语言模型来产生中间思想，又名在提供答案之前，思想链（CoT）一直是解决复杂语言任务的成功秘诀。在机器人技术中，类似的具体CoT策略（在行动之前生成想法）也已被证明在使用视觉-语言-行动模型（VLA）时可以提高性能。由于这些技术增加了模型生成的输出的长度以包含想法，推断时间会受到负面影响。在现实世界的执行中，例如在机器人操纵设置中，延迟代理的动作会强烈影响方法的可用性，因为任务需要长序列的动作。然而，长思想链的产生是实现性能改进的强有力先决条件吗？在这项工作中，我们探索了混合训练（HyT）的想法，这是一个框架，使VLA能够从思想中学习并从相关的性能收益中受益，同时可以在推理期间省略CoT的生成。此外，通过学习有条件地预测一组不同的输出，HyT支持推理时的灵活性，使模型能够直接预测动作、生成想法或遵循指令。我们在一系列模拟基准测试和现实世界实验中评估了所提出的方法。</p>
<div class="markdown-heading"><h2 class="heading-element">语义驱动的人工智能代理通信：挑战和解决方案</h2><a id="user-content-语义驱动的人工智能代理通信挑战和解决方案" class="anchor" aria-label="Permalink: 语义驱动的人工智能代理通信：挑战和解决方案" href="#语义驱动的人工智能代理通信挑战和解决方案"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00381v1宣布类型：新摘要：随着智能服务的快速发展，通信目标正在从人类转向人工智能（AI）代理，这需要新的范式来实现实时感知、决策和协作。语义通信传达与任务相关的含义而不是原始数据，提供了一个有前途的解决方案。然而，其实际部署仍然受到动态环境和有限资源的限制。为了解决这些问题，本文提出了一个语义驱动的人工智能代理通信框架，并开发了三种使能技术。首先，语义适应传输应用对真实或生成样本进行微调，以有效地使模型适应不同的环境。其次，语义轻量级传输结合了修剪、量化和感知采样，以降低模型复杂性并减轻边缘代理的计算负担。第三，语义自进化控制采用分布式分层决策来优化多维资源，实现动态环境中稳健的多主体协作。仿真结果表明，提出的解决方案实现了更快的收敛和更强的鲁棒性，而提出的分布式分层优化方法的性能显着优于传统决策方案，凸显了其在人工智能代理通信网络中的潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">降级升级：优化器简化增强了LLM Unlearning的鲁棒性</h2><a id="user-content-降级升级优化器简化增强了llm-unlearning的鲁棒性" class="anchor" aria-label="Permalink: 降级升级：优化器简化增强了LLM Unlearning的鲁棒性" href="#降级升级优化器简化增强了llm-unlearning的鲁棒性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00761v1宣布类型：新摘要：大型语言模型（LLM）取消学习旨在通过外科手术消除现有模型中不需要的数据或知识的影响，同时保留其对不相关任务的实用性。这种范式在解决隐私和安全问题方面表现出了希望。然而，最近的研究结果表明，忘记学习效果通常很脆弱：忘记学习后的操作（例如权重量化或微调）可以快速中和预期的忘记。之前提高稳健性的努力主要是通过明确承担脆弱性源的角色来重新制定消除学习目标。在这项工作中，我们采取了不同的角度，研究优化器在塑造去学习鲁棒性方面的作用，独立于去学习目标和公式。我们表明，优化器的“等级”（由其利用的信息水平定义），范围从零阶（无梯度）到一阶（基于梯度）再到二阶（基于黑森州），与遗忘的弹性紧密相关。令人惊讶的是，我们发现降级优化器，例如使用零阶方法或压缩梯度变体（例如，基于梯度符号的优化器），通常会带来更强的鲁棒性。虽然这些优化器会产生噪音更大、不太精确的更新，但它们会鼓励收敛到损失景观中更难干扰的盆地，从而抵抗训练后的扰动。通过将零阶方法与随机平滑连接起来，我们进一步强调了它们在鲁棒去学习方面的天然优势。受这些见解的启发，我们提出了一种混合优化器，它结合了一阶和零阶更新，在增强鲁棒性的同时保留了去学习效率。针对多种LLM取消学习算法的MUSE和WMDP基准进行了广泛的实验，验证了我们的方法在不牺牲取消学习质量的情况下实现了更有弹性的遗忘。</p>
<div class="markdown-heading"><h2 class="heading-element">LoRAFusion：针对LLM的高效LoRA微调</h2><a id="user-content-lorafusion针对llm的高效lora微调" class="anchor" aria-label="Permalink: LoRAFusion：针对LLM的高效LoRA微调" href="#lorafusion针对llm的高效lora微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00206v1宣布类型：新摘要：低秩自适应（LoRA）已成为大型语言模型（LLM）的领先参数高效微调（PEFT）方法，因为它显著降低了GPU内存使用量，同时在下游任务上保持了具有竞争力的微调模型质量。尽管有这些好处，我们确定了现有LoRA微调系统中的两个关键低效率。首先，由于对大激活张量进行冗余内存访问，它们会产生大量的运行时负载。其次，他们错过了同时微调多个独立LoRA适配器的机会，这些适配器在同一组图形处理器上共享同一基本模型。这会导致无法获得性能提升，例如减少管道气泡、更好的通信重叠和改善的图形处理器负载平衡。   为了解决这些问题，我们引入LoRAFusion，这是一种针对LLM的高效LoRA微调系统。在内核级别，我们提出了一种融合内存限制操作的图分裂方法。这种设计消除了不必要的内存访问，并保留了计算绑定GEM的性能，而不会产生重新计算或同步的成本。在调度层面，LoRAFusion引入了自适应NPS算法，用于多作业微调。它首先将LoRA适配器分成几组，故意错开作业之间的批执行，然后解决每个组内的bin打包问题，以生成平衡的、依赖性感知的微批。与Megatron-LM相比，LoRAFusion实现了高达1.96美元（平均1.47美元）的端到端加速，并比mLoRA（最先进的多LoRA微调系统）提高了高达1.46美元（平均1.29美元）。我们的融合内核实现了高达1.39美元（平均1.27美元）的内核性能改进，并且可以直接作为现有LoRA系统的即插即用替代品。我们在<a href="https://github.com/CentML/lorafusion%E4%B8%8A%E5%BC%80%E6%BA%90LoRAFusion%E3%80%82">https://github.com/CentML/lorafusion上开源LoRAFusion。</a></p>
<div class="markdown-heading"><h2 class="heading-element">具有丰富临床背景的自动结构化放射学报告生成</h2><a id="user-content-具有丰富临床背景的自动结构化放射学报告生成" class="anchor" aria-label="Permalink: 具有丰富临床背景的自动结构化放射学报告生成" href="#具有丰富临床背景的自动结构化放射学报告生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00428v1宣布类型：新摘要：根据胸部X射线图像自动化结构化放射学报告生成（SRRG），通过以结构化格式生成报告，确保清晰性、一致性和遵守临床报告标准，为减少放射科医生的工作量提供了巨大的潜力。虽然放射科医生在诊断推理中有效利用可用的临床背景，但现有的SRRG系统忽视了这些基本要素。当参考不存在的临床背景时，这种基本差距会导致包括暂时幻觉在内的关键问题。为了解决这些局限性，我们提出了情境化的SRRG（C-SRRG），它全面整合了SRRG的丰富临床背景。我们通过整合全面的临床背景来策划C-SRRG数据集，包括1）多视图X射线图像，2）临床适应症，3）成像技术，4）基于患者历史的先前研究以及相应的比较。通过使用最先进的多模式大型语言模型进行广泛的基准测试，我们证明将临床背景与拟议的C-SRRG结合可以显着提高报告生成质量。我们在<a href="https://github.com/vuno/contextualized-srrg%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A3%80%E6%9F%A5%E7%82%B9%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E6%9C%AA%E6%9D%A5%E5%AF%B9%E4%B8%B4%E5%BA%8A%E4%B8%80%E8%87%B4%E7%9A%84%E8%87%AA%E5%8A%A8RRG%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/vuno/contextualized-srrg上公开发布数据集、代码和检查点，以促进未来对临床一致的自动RRG的研究。</a></p>
<div class="markdown-heading"><h2 class="heading-element">毫无意义的代币，有意义的收益：激活转变如何增强LLM推理</h2><a id="user-content-毫无意义的代币有意义的收益激活转变如何增强llm推理" class="anchor" aria-label="Permalink: 毫无意义的代币，有意义的收益：激活转变如何增强LLM推理" href="#毫无意义的代币有意义的收益激活转变如何增强llm推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01032v1宣布类型：新摘要：在查询提示之前插入长序列无意义的标记可以持续增强LLM推理性能这一令人困惑的观察结果的启发，这项工作分析了驱动这种现象的潜在机制，并基于这些见解提出了一种更有原则的方法，允许类似的性能收益。首先，我们发现这些改进源于LLM MLP层中激活的重新分配，其中接近零的激活变得不那么频繁，而大幅度的激活增加。这种重新分配通过抑制弱信号并促进更强、信息量更大的信号来增强模型的代表能力。基于这一见解，我们提出了激活重新分发模块（ARM），这是一种轻量级的推断时间技术，可以直接修改激活，而不改变输入序列。ARM自适应地识别非线性函数之后的接近零的激活，并将它们向外转移，以受控的方式隐性地复制无意义代币的有益效果。跨各种基准测试和模型架构的广泛实验清楚地表明，ARM持续提高了推理任务的LLM性能，同时只需要几行简单代码即可实现。我们的研究结果既为无意义代币的意想不到的好处提供了明确的机械解释，又提供了一种简单而有效的技术，可以利用激活重新分配来进一步提高LLM性能。</p>
<div class="markdown-heading"><h2 class="heading-element">用于神经网络训练和梯度监测的随机矩阵绘制</h2><a id="user-content-用于神经网络训练和梯度监测的随机矩阵绘制" class="anchor" aria-label="Permalink: 用于神经网络训练和梯度监测的随机矩阵绘制" href="#用于神经网络训练和梯度监测的随机矩阵绘制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00442v1宣布类型：新摘要：神经网络训练依赖于通过反向传播的梯度计算，但存储层激活的内存要求带来了巨大的可扩展性挑战。我们首次将控制论矩阵草图应用于神经网络层激活，从而实现反向传播中存储效率高的梯度重建。这项工作基于最近针对动态优化问题的矩阵草图框架，其中类似的状态轨迹存储挑战激发了草图技术。我们的方法使用通过指数移动平均值（EMA）和自适应排名调整来维护的三个补充草图矩阵来绘制层激活，自动平衡存储效率与逼近质量。对MNIST、CIFAR-10和物理信息神经网络的经验评估表明了可控的准确性-记忆权衡。我们在MNIST上演示了一个梯度监控应用程序，展示了草图激活如何以最小的内存负载实现实时梯度规范跟踪。这些结果证明，草图激活存储提供了实现内存高效的神经网络训练和分析的可行途径。</p>
<div class="markdown-heading"><h2 class="heading-element">基于Spiking神经网络的大型语言模型推理引擎</h2><a id="user-content-基于spiking神经网络的大型语言模型推理引擎" class="anchor" aria-label="Permalink: 基于Spiking神经网络的大型语言模型推理引擎" href="#基于spiking神经网络的大型语言模型推理引擎"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.00133v1宣布类型：新摘要：基于Transformer架构的基础模型是目前通用语言建模以及材料科学和气候等科学领域的最新技术。然而，训练和部署这些模型在计算上具有挑战性，因为时间和空间复杂性与输入序列长度呈二次关系。人们已经做出了一些努力来探索有效的计算范式和模型架构来解决这些限制。在这项工作中，我们探索尖峰神经网络（SNN）来设计Transformer模型。使用现有的代理学习方法训练大规模SNN的挑战是效率低下且耗时。另一方面，将现有的基于变换器的模型转换为它们的SNN等价物的技术是不可扩展的，因为实现最佳性能是以大量尖峰时间步长为代价的，即增加的延迟。为了解决这个问题，我们提出了NeurTransformer，这是一种使用现有转换方法的监督微调方法来设计基于变换器的SNN进行推理的方法。所提出的方法的工作原理是：（1）用基于尖峰的自注意（SSA）代替自注意机制，（2）将训练好的Transformer模型的前馈块转换为其等效的SNN，以及（3）使用基于SNN的代理学习算法微调SSA块。我们对提出的方法进行了基准测试，并使用模型大小不断增加的GPT-2模型的三个变体来证明其准确性和可扩展性。我们观察到，转换后的GPT-2小模型显示出cos相似性下降5-12%，困惑度下降9.7%。最后，我们展示了与ASA块相比，DSA块的能源效率，并显示在数字硬件上实施自我注意机制时，估计能源消耗减少了64.71%至85.28%。</p>
<div class="markdown-heading"><h2 class="heading-element">Dirichlet-Prior塑造：指导升级教育部专业化专家</h2><a id="user-content-dirichlet-prior塑造指导升级教育部专业化专家" class="anchor" aria-label="Permalink: Dirichlet-Prior塑造：指导升级教育部专业化专家" href="#dirichlet-prior塑造指导升级教育部专业化专家"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.01185v1宣布类型：新摘要：将预训练的密集模型升级为稀疏的专家混合（MoE）有效地增加了模型容量，但由于朴素权重复制，通常会遇到专家专业化较差的问题。我们的分析表明，即使采用传统的正规化，升级的MoE也表现出低置信度、弱差异化的路由，从而阻碍了性能。我们引入了Dirichlet先验整形损失（DPSL），这是一种新型的路由器正规化技术，通过将专家指派与目标Dirichlet先验匹配来直接整形路由概率分布。DPSL提供对专家平衡和专业化的细粒度控制，并能够编码归纳偏见，例如鼓励专家专注于特定的模式或任务，而不需要手动干预;值得注意的是，DPSL是一种通用工具，适用于任何输出类别概率分布的模块，将其实用性扩展到MoE培训之外。对升级版MoE视觉语言模型（具有Qwen 2、Phi 3、Llama3.2 LLM主干）的实验表明，DPSL始终优于标准视觉语言基准中的升级版策略和正规化技术，解决了专业化较差的关键问题，并培养了适应性更强、性能更高的模型。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>