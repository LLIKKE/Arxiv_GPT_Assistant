<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">采用循环脉冲神经网络的71.2微瓦语音识别加速器</h2><a id="user-content-采用循环脉冲神经网络的712微瓦语音识别加速器" class="anchor" aria-label="Permalink: 采用循环脉冲神经网络的71.2微瓦语音识别加速器" href="#采用循环脉冲神经网络的712微瓦语音识别加速器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（说明：根据技术文档标题的翻译规范，此处处理要点包括：</p>
<ol>
<li>保留专业术语"循环脉冲神经网络"的准确译法</li>
<li>将计量单位"μW"转换为中文习惯的"微瓦"</li>
<li>调整英文语序为中文技术标题常用的"特性+主体"结构</li>
<li>使用"加速器"而非直译"accelerator"更符合芯片领域术语</li>
<li>数字采用阿拉伯数字保持与原文一致</li>
<li>去掉连接词"with"并通过语序重组自然体现器件关系）</li>
</ol>
<p>本文介绍了一款功耗仅71.2微瓦的语音识别加速器，专为边缘设备实时应用设计，着重实现超低功耗。通过算法与硬件的协同优化，我们提出了一种紧凑型循环脉冲神经网络架构，包含两个循环层、一个全连接层，且仅需1-2个时间步长。经剪枝和4位定点量化处理后，模型体积从2.79 MB压缩96.42%至0.1 MB。在硬件层面，我们采用混合级剪枝、零值跳过和脉冲融合技术，将计算复杂度降低90.49%至13.86 MMAC/S。通过并行时间步执行机制，不仅解决了跨时间步数据依赖问题，还利用权重共享实现权重缓冲器的功耗优化。基于脉冲活动的稀疏特性，采用输入广播机制消除零值运算，进一步降低功耗。基于台积电28纳米工艺的实测表明，该设计在100 kHz实时运行时功耗仅71.2微瓦，性能超越现有最优方案。在500 MHz工作频率下，其能效达28.41 TOPS/W，面积效率达1903.11 GOPS/mm²。</p>
<div class="markdown-heading"><h2 class="heading-element">《通过令牌冗余缩减实现更高效的参数调优》</h2><a id="user-content-通过令牌冗余缩减实现更高效的参数调优" class="anchor" aria-label="Permalink: 《通过令牌冗余缩减实现更高效的参数调优》" href="#通过令牌冗余缩减实现更高效的参数调优"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>参数高效调优（PET）旨在通过仅学习少量参数，将预训练的基础模型迁移至下游任务。与传统微调方法（需更新整个模型）相比，无论预训练模型容量如何指数级增长，PET都能显著降低每个任务的存储和传输成本。然而，现有PET方法大多继承了大型主干模型的推理延迟，且常因附加模块（如适配器）引入额外计算开销，限制了其在计算密集型应用中的实用性。本文提出"快速参数高效调优"（FPET），这种创新方法在保持高存储效率的同时，显著提升推理速度与训练效率。具体而言，我们为PET精心设计了一个即插即用的令牌冗余削减模块：该模块通过适配器从自注意力层提炼令牌特征，学习令牌间精确相似度；并采用完全可微的令牌合并策略（利用直通估计器实现最优令牌削减）截断冗余令牌。实验证明，FPET在保持与最先进PET方法相当性能的同时，相比预训练主干模型实现了更快的推理速度和更高的内存效率。</p>
<div class="markdown-heading"><h2 class="heading-element">MoQa：基于多阶段数据-模型分布感知的MoE量化新思考</h2><a id="user-content-moqa基于多阶段数据-模型分布感知的moe量化新思考" class="anchor" aria-label="Permalink: MoQa：基于多阶段数据-模型分布感知的MoE量化新思考" href="#moqa基于多阶段数据-模型分布感知的moe量化新思考"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2503.21135v1 公告类型：新研究<br>
摘要：随着人工智能的发展，混合专家模型（MoE）已成为大语言模型（LLM）的主要形式，其对模型压缩的需求与日俱增。量化是一种有效方法，不仅能压缩模型，还能显著提升性能。现有量化方法逐渐将关注点从参数缩放转向数据分布分析，但这些分析专为稠密型LLM设计，且依赖简单的"单一模型-全数据"映射关系，无法适配MoE特性。本文提出新型量化框架MoQa，通过多阶段分析解耦MoE中数据-模型分布的复杂性，定量揭示了稀疏数据激活、数据-参数映射及专家间关联的动态特征。基于此，MoQa以最优的数据-模型分布感知能力识别特定专家及参数的重要性，提出适应不同数据激活模式与专家组合场景的细粒度混合量化策略。此外，MoQa探讨了现有量化的局限性，并逐阶段分析各环节影响，为MoE量化提供了新见解。实验表明，MoQa在语言建模任务中实现困惑度降低1.69~2.18，在零样本推理任务中准确率提升1.58%~8.91%。我们相信MoQa将在未来MoE的构建、优化与压缩中发挥作用。</p>
<p>（注：根据学术论文摘要的文体特征，译文采用以下处理：</p>
<ol>
<li>专业术语如"perplexity"译为"困惑度"、"zero-shot"译为"零样本"符合NLP领域惯例</li>
<li>长难句拆分重构，如将"proposes a series of fine-grained mix-quantization strategies..."处理为因果逻辑清晰的中文句式</li>
<li>被动语态转化，如"are designed for"译为"专为...设计"</li>
<li>技术概念保留英文缩写（MoE/LLM）同时首次出现标注全称</li>
<li>数据范围表达"1.69~2.18"保留原格式符合中文科技文献规范）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">InternVL-X：通过高效视觉令牌压缩推进并加速InternVL系列发展</h2><a id="user-content-internvl-x通过高效视觉令牌压缩推进并加速internvl系列发展" class="anchor" aria-label="Permalink: InternVL-X：通过高效视觉令牌压缩推进并加速InternVL系列发展" href="#internvl-x通过高效视觉令牌压缩推进并加速internvl系列发展"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>当前大多数多模态大语言模型（MLLMs）将视觉标记视为"文本序列"，与文本标记共同输入大语言模型（LLM）处理。然而海量视觉标记会显著增加计算资源和时间消耗。本文提出InternVL-X模型，通过融合三种视觉标记压缩方法，在性能与效率上全面超越InternVL。首先，我们设计新型视觉语言投影器PVTC：该组件通过聚合相邻视觉嵌入形成局部查询，同时利用转换后的CLS标记作为全局查询，继而通过点对区域交叉注意力机制实现更高效的视觉特征转换。其次，我们提出分层视觉标记压缩模块LVTC：该模块在LLM浅层压缩标记，再通过深层上采样与残差连接进行扩展，显著提升模型计算效率。此外，我们开发高效高分辨率切片方法RVTC：根据图像面积或长度过滤动态调整视觉标记数量，仅以轻微性能损失大幅提升训练效率。InternVL-X仅需20%或更少的视觉标记，便在7个公开MLLM基准测试中取得最先进性能，12项任务平均指标提升2.34%。</p>
<div class="markdown-heading"><h2 class="heading-element">MoLe-VLA：基于动态层跳跃与混合层策略的高效机器人操作视觉语言动作模型</h2><a id="user-content-mole-vla基于动态层跳跃与混合层策略的高效机器人操作视觉语言动作模型" class="anchor" aria-label="Permalink: MoLe-VLA：基于动态层跳跃与混合层策略的高效机器人操作视觉语言动作模型" href="#mole-vla基于动态层跳跃与混合层策略的高效机器人操作视觉语言动作模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>多模态大语言模型（MLLMs）在理解复杂语言与视觉数据方面表现卓越，使通用机器人系统能够解析指令并执行具身任务。然而，其实际部署却受制于巨大的计算与存储需求。近期关于大语言模型层级同质化特征的研究催生了稀疏化技术（如早退机制与令牌剪枝）以应对这些挑战，但这些方法往往忽视了最下游机器人任务相关语义信息的关键编码层——最终层的作用。基于神经科学领域"浅层脑假说"（SBH）的最新突破与模型稀疏化中的专家混合思想，我们将大语言模型的每个层级视作独立专家，提出动态层级激活的混合层级视觉-语言-动作模型架构（MoLe-VLA，简称MoLe）。为此，我们设计了时空感知路由器（STAR），可根据机器人当前状态选择性激活部分层级，模拟人脑中负责认知与因果推理的专用神经信号通路。此外，为弥补MoLe架构中损失的大语言模型认知能力，我们开发了认知自知识蒸馏框架（CogKD），通过利用认知特征增强对任务需求的理解，并提升任务相关动作序列的生成质量。在RLBench仿真环境与真实场景中的大量实验表明，MoLe-VLA在效率与性能上均具有显著优势：相比标准大语言模型，该架构在十项任务中的平均成功率提升8%，同时最高可降低5.6倍计算成本。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>