<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">CommonKV：通过跨层参数共享压缩键值缓存</h2><a id="user-content-commonkv通过跨层参数共享压缩键值缓存" class="anchor" aria-label="Permalink: CommonKV：通过跨层参数共享压缩键值缓存" href="#commonkv通过跨层参数共享压缩键值缓存"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16134v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）面临显著的内存挑战，其键值缓存（KV cache）随序列长度增加而急剧膨胀。作为关键应对技术，现有跨层KV缓存共享方法要么需要修改模型架构并重新预训练，要么在高压缩率下导致性能显著下降。为缓解这些问题，我们提出CommonKV——一种无需训练、通过相邻参数共享实现跨层KV缓存压缩的方法。受跨层隐藏状态高相似性启发，我们采用奇异值分解（SVD）实现相邻参数间的权重共享，从而生成更易融合的潜在KV缓存。此外，我们还引入自适应预算分配策略，该策略基于余弦相似度动态分配压缩预算，确保相似度低的缓存不会被过度压缩。在多个骨干模型及LongBench、Ruler等基准测试上的实验表明，该方法在不同压缩率下始终优于现有低秩和跨层压缩方法。值得注意的是，CommonKV的优势与其他量化和淘汰方法具有正交性，通过整合这些技术，我们最终可实现98%的压缩率且无显著性能损失。</p>
<div class="markdown-heading"><h2 class="heading-element">迈向工业4.0的微型机器学习：铣床资源高效型过程监控</h2><a id="user-content-迈向工业40的微型机器学习铣床资源高效型过程监控" class="anchor" aria-label="Permalink: 迈向工业4.0的微型机器学习：铣床资源高效型过程监控" href="#迈向工业40的微型机器学习铣床资源高效型过程监控"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：TinyML译为"微型机器学习"符合技术领域术语；"Resource-Efficient Process Monitoring"采用"资源高效型过程监控"的译法，既保持专业准确性又符合中文技术文献表达习惯；通过添加连接词"的"使定语结构更符合中文语序；整体译文兼顾技术准确性与行业语境适配性。）</p>
<p>arXiv:2508.16553v1 公告类型：新成果<br>
摘要：在工业4.0背景下，长期服役的工业机械可通过加装过程监测功能实现智能工厂的改造应用。无线监测系统是可行方案之一，其能显著受益于TinyML（微型机器学习）范式。本研究展示了完整的TinyML流程——从数据集生成、机器学习模型开发，到在微控制器上完整预处理与分类流水线的实现与评估。在简要回顾工业过程监测中的TinyML应用后，详细介绍了新型铣削振动数据集（MillingVibes）的创建过程。通过开发参数量存储仅12.59kiB的8位量化卷积神经网络（CNN）模型，验证了结构集成式过程质量监测的TinyML系统可行性。在ARM Cortex M4F微控制器上实现了100.0%的测试准确率，单次量化CNN推理耗时15.4毫秒、能耗1.462毫焦，为未来TinyML过程监测方案提供了性能基准。</p>
<div class="markdown-heading"><h2 class="heading-element">更贴近现实：实用半监督联邦学习在基础模型适配中的应用</h2><a id="user-content-更贴近现实实用半监督联邦学习在基础模型适配中的应用" class="anchor" aria-label="Permalink: 更贴近现实：实用半监督联邦学习在基础模型适配中的应用" href="#更贴近现实实用半监督联邦学习在基础模型适配中的应用"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16568v1 公告类型：新成果<br>
摘要：基础模型（FMs）虽展现出卓越的泛化能力，但需针对下游任务进行适配，尤其在隐私敏感的应用中。受数据隐私法规限制，基于云端的基础模型无法直接访问私有边缘数据，制约了其适配能力。联邦学习（FL）提供了隐私感知的替代方案，但现有方法忽略了边缘设备的两大约束——有限的计算资源和标注数据稀缺。为解决这些挑战，我们提出实用半监督联邦学习（PSSFL）框架：边缘设备仅持有未标注的低分辨率数据，而服务器拥有有限的高分辨率标注数据。在此设定下，我们创新性地提出联邦专家混合模型（FedMox），通过稀疏专家混合架构增强FL中基础模型的适配能力。FedMox采用空间路由器对齐跨分辨率特征，结合软混合策略稳定半监督学习，有效解决计算与分辨率失配问题。以目标检测为案例研究，在真实自动驾驶数据集上的实验表明，FedMox能在PSSFL框架下高效适配基础模型，在边缘设备有限内存成本约束下显著提升性能。本研究为联邦场景中可扩展且隐私保护的基础模型适配开辟了新路径。</p>
<div class="markdown-heading"><h2 class="heading-element">FEST：一个评估合成表格数据的统一框架</h2><a id="user-content-fest一个评估合成表格数据的统一框架" class="anchor" aria-label="Permalink: FEST：一个评估合成表格数据的统一框架" href="#fest一个评估合成表格数据的统一框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16254v1 公告类型：新研究<br>
摘要：利用生成式机器学习技术合成数据生成，为解决现实世界数据使用中的隐私问题提供了一种前景广阔的途径。合成数据在保持强大隐私保障的同时，能高度模拟真实数据特征。然而，当前仍缺乏针对合成数据生成的综合评估框架，特别是在权衡合成数据隐私保护与数据效用方面。本研究通过提出FEST框架填补了这一空白——一个用于评估表格型合成数据的系统化框架。FEST整合了多种隐私度量标准（包括基于攻击和基于距离的评估）、相似性度量以及机器学习效用指标，提供全面评估。我们将FEST开发为开源Python库，并在多个数据集上验证其有效性，证明该框架能有效分析不同合成数据生成模型的隐私-效用权衡关系。FEST源代码已发布于Github平台。</p>
<div class="markdown-heading"><h2 class="heading-element">通过成对排序进行事后回归优化</h2><a id="user-content-通过成对排序进行事后回归优化" class="anchor" aria-label="Permalink: 通过成对排序进行事后回归优化" href="#通过成对排序进行事后回归优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16495v1 公告类型：新论文<br>
摘要：连续属性的精确预测对众多科学与工程任务至关重要。尽管深度学习回归器在标签充足时表现卓越，但其在数据稀缺场景下的准确性会显著下降。我们提出RankRefine——一种与模型无关、即插即用的后处理方法，通过引入来自成对排序的专家知识来优化回归结果。给定一个查询项和包含已知属性值的小型参考集，RankRefine通过逆方差加权将基础回归器的输出与基于排序的估计值相结合，且无需重新训练。在分子属性预测任务中，仅使用通用大语言模型（LLM）未经微调生成的20组成对比较，RankRefine即可实现平均绝对误差相对降低10%的改进。由于人类专家或通用大语言模型提供的排序信息足以提升跨领域回归性能，RankRefine兼具实用性与广泛适用性，尤其在低数据场景中表现突出。</p>
<div class="markdown-heading"><h2 class="heading-element">论联邦式后训练大语言模型的演进：从模型可访问性视角分析</h2><a id="user-content-论联邦式后训练大语言模型的演进从模型可访问性视角分析" class="anchor" aria-label="Permalink: 论联邦式后训练大语言模型的演进：从模型可访问性视角分析" href="#论联邦式后训练大语言模型的演进从模型可访问性视角分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16261v1 公告类型：新研究<br>
摘要：联邦学习（FL）能够在去中心化的数据孤岛间训练模型，同时保护客户端数据隐私。近期研究探索了在FL框架下对大型语言模型（LLM）进行高效后训练的方法，以应对计算和通信挑战。虽然现有方法通常依赖于获取LLM的内部信息（这在现实场景中往往受限），但一种仅需推理的范式（黑盒FedLLM）应运而生以突破这些限制。本文对LLM的联邦调优进行全面综述，提出基于模型访问权限和参数效率的双维度分类法，将FedLLM方法划分为白盒、灰盒与黑盒技术，并重点阐述各类别中的代表性方法。我们审视了将LLM视为黑盒推理API的新兴研究方向，并探讨未来研究的潜在路径与开放挑战。</p>
<div class="markdown-heading"><h2 class="heading-element">非合作联邦学习服务中通信与计算协同优化的帕累托行动者-评论者方法</h2><a id="user-content-非合作联邦学习服务中通信与计算协同优化的帕累托行动者-评论者方法" class="anchor" aria-label="Permalink: 非合作联邦学习服务中通信与计算协同优化的帕累托行动者-评论者方法" href="#非合作联邦学习服务中通信与计算协同优化的帕累托行动者-评论者方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译采用学术论文标题的常见处理方式：</p>
<ol>
<li>保留专业术语"Pareto Actor-Critic"的直译"帕累托行动者-评论者"</li>
<li>"Communication and Computation Co-Optimization"译为"通信与计算协同优化"</li>
<li>"Non-Cooperative Federated Learning Services"译为"非合作联邦学习服务"</li>
<li>整体采用"领域+方法"的中文学术标题结构，符合中文论文标题表达习惯）</li>
</ol>
<p>arXiv:2508.16037v1 公告类型：新成果<br>
摘要：多服务提供商（SP）生态系统中的联邦学习（FL）因非合作动态而面临根本性障碍——隐私约束与竞争利益阻碍了跨SP通信与计算资源的集中优化。本文提出PAC-MCoFL，一个基于博弈论的多智能体强化学习（MARL）框架：SP作为智能体协同优化客户端分配、自适应量化和资源分配。该框架将帕累托行动者-评论家（PAC）原理与期望回归相结合，使智能体能推测最优联合策略，在建模异质风险偏好的同时实现帕累托最优均衡。针对高维动作空间，我们设计了三元笛卡尔分解（TCAD）机制以实现细粒度控制。进一步提出可扩展变体PAC-MCoFL-p，其参数化推测生成器在保证误差有界的前提下显著降低计算复杂度。除理论收敛性证明外，大量仿真验证了框架的优越性——PAC-MCoFL在总奖励和超体积指标（HVI）上分别较最新MARL方案提升约5.8%和4.2%。结果还表明，本方法能在大规模部署和多样化数据异构场景下更有效平衡个体SP与系统性能。</p>
<div class="markdown-heading"><h2 class="heading-element">TPLA：面向高效解耦预填充与解码推理的张量并行潜在注意力机制</h2><a id="user-content-tpla面向高效解耦预填充与解码推理的张量并行潜在注意力机制" class="anchor" aria-label="Permalink: TPLA：面向高效解耦预填充与解码推理的张量并行潜在注意力机制" href="#tpla面向高效解耦预填充与解码推理的张量并行潜在注意力机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此处采用技术术语的常见翻译方式：</p>
<ul>
<li>"Tensor Parallel" 译为"张量并行"，是分布式训练中的标准译法</li>
<li>"Latent Attention" 译为"潜在注意力机制"，保持与注意力机制术语体系的一致性</li>
<li>"Disaggregated" 译为"解耦"，符合系统架构中组件分离的概念</li>
<li>"Prefill &amp; Decode Inference" 译为"预填充与解码推理"，延续LLM推理阶段的专业表述）</li>
</ul>
<p>arXiv:2508.15881v1 公告类型：新成果<br>
摘要：DeepSeek-V2提出的多头潜在注意力（MLA）机制将键值状态压缩为低秩潜在向量，仅缓存该向量以降低内存占用。然而在张量并行（TP）架构中，注意力头需跨多设备计算，每个设备仍需加载完整缓存，削弱了MLA相比分组查询注意力（GQA）的优势。我们提出张量并行潜在注意力（TPLA）：通过跨设备划分潜在表示与每个头的输入维度，实现分片独立计算注意力，最后通过全规约操作整合结果。TPLA在保持压缩KV缓存优势的同时释放了TP效率。与分组潜在注意力（GLA）不同，TPLA中每个头仍能利用完整潜在表示，保持更强表征能力。TPLA可直接兼容MLA预训练模型：支持MLA风格预填充，无需重新训练即可实现高效张量并行解码。在TP切片前应用简单正交变换（如哈达玛变换或PCA）可进一步减少分片间干扰，将精度损失降至最低。通过对DeepSeek-V3和Kimi-K2实施设备级KV缓存压缩，我们在32K上下文长度下分别实现1.79倍和1.93倍加速，同时在常识推理和LongBench基准测试中保持性能。TPLA可通过FlashAttention-3实现，为端到端加速提供实用方案。</p>
<div class="markdown-heading"><h2 class="heading-element">PGF-Net：一种用于高效多模态情感分析的渐进式门控融合框架</h2><a id="user-content-pgf-net一种用于高效多模态情感分析的渐进式门控融合框架" class="anchor" aria-label="Permalink: PGF-Net：一种用于高效多模态情感分析的渐进式门控融合框架" href="#pgf-net一种用于高效多模态情感分析的渐进式门控融合框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：采用学术文献常用翻译规范，保留核心缩写"PGF-Net"保持专业一致性，"Progressive Gated-Fusion"译为"渐进式门控融合"准确传达技术特性，"Efficient Multimodal Sentiment Analysis"采用"高效多模态情感分析"符合中文信息处理领域术语习惯）</p>
<p>arXiv:2508.15852v1 公告类型：新成果<br>
摘要：本文提出PGF-Net（渐进式门控融合网络），这是一种专为高效可解释多模态情感分析而设计的新型深度学习框架。我们的框架包含三大创新点：首先，提出渐进式层内融合范式，通过交叉注意力机制使文本表征能够在Transformer编码器深层动态查询并整合来自音频与视觉流的非语言特征，实现更深层次的语境化融合。其次，引入自适应门控仲裁机制作为动态控制器，平衡原始语言信息与新融合的多模态语境，确保稳定有效的整合的同时防止噪声淹没有效信号。最后采用混合参数高效微调策略，通过LoRA实现全局适配与后融合适配器进行局部优化的协同组合，显著减少可训练参数量，使模型轻量化并适用于资源受限场景。这些创新被整合到分层编码器架构中，使PGF-Net在保持卓越参数效率的同时，能够执行深度、动态且可解释的多模态情感分析。在MOSI数据集上的实验结果表明，PGF-Net以0.691的平均绝对误差和86.9%的F1分数达到最先进性能，且仅需309万可训练参数，展现出性能与计算效率的卓越平衡。</p>
<div class="markdown-heading"><h2 class="heading-element">通过多层可导向嵌入融合将时间序列整合进大型语言模型以增强预测能力</h2><a id="user-content-通过多层可导向嵌入融合将时间序列整合进大型语言模型以增强预测能力" class="anchor" aria-label="Permalink: 通过多层可导向嵌入融合将时间序列整合进大型语言模型以增强预测能力" href="#通过多层可导向嵌入融合将时间序列整合进大型语言模型以增强预测能力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16059v1 公告类型：新论文<br>
摘要：时间序列（TS）数据在各应用领域普遍存在，使时间序列预测（TSF）成为一项基础任务。随着大语言模型（LLM）的惊人进展，研究者开发了多种方法使LLM适应时间序列预测。尽管现有方法释放了LLM理解时间序列数据的潜力，但其本质上受限于对时间序列信息的浅层整合——LLM通常仅在输入层等浅层接触时间序列表征。这导致时间序列表征的影响在深层网络中逐渐衰减，最终造成文本嵌入与时间序列表征间的适配失效。本文提出多层可控嵌入融合框架（MSEF），该创新框架使LLM能直接获取所有网络深度的时间序列模式，从而缓解深层网络中时间序列信息的渐进流失。具体而言，MSEF利用现成的时间序列基础模型提取语义丰富的嵌入表征，通过分层设计的导向向量将其与LLM各中间层的文本表征融合。这些导向向量可持续优化时间序列与文本模态的对齐，并实现分层特异性适配机制，确保高效的小样本学习能力。在七个基准数据集上的实验表明，MSEF相比基线模型实现显著性能提升，平均降低31.8%的均方误差。代码已开源：<a href="https://github.com/One1sAll/MSEF%E3%80%82">https://github.com/One1sAll/MSEF。</a></p>
<div class="markdown-heading"><h2 class="heading-element">AgentFly：无需微调大语言模型即可优化智能体性能</h2><a id="user-content-agentfly无需微调大语言模型即可优化智能体性能" class="anchor" aria-label="Permalink: AgentFly：无需微调大语言模型即可优化智能体性能" href="#agentfly无需微调大语言模型即可优化智能体性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16153v1 公告类型：新论文<br>
摘要：本文提出一种新型自适应大语言模型（LLM）智能体学习范式，无需对底层LLM进行微调。现有方法往往依赖静态手工设计的反思流程而显得僵化，或需更新LLM模型参数的梯度计算而导致计算密集。相比之下，我们的方法通过基于记忆的在线强化学习实现低成本持续适应。我们将其形式化为记忆增强马尔可夫决策过程（M-MDP），配备神经案例选择策略以指导行动决策。过往经验存储于可微分或非参数化的情景记忆中，策略通过记忆重写机制根据环境反馈持续更新，而策略改进则通过高效记忆读取（检索）实现。我们在深度研究场景中实例化智能体模型AgentFly，在GAIA验证集上达到最高性能（87.88% Pass@3），测试集准确率达79.40%。在DeepResearcher数据集上取得66.6% F1值和80.4% PM值，优于当前基于训练的先进方法，而基于案例的记忆机制在分布外任务上带来4.7%至9.6%的绝对提升。该方法为开发通用LLM智能体提供了可扩展的高效路径，使其无需梯度更新即可实现持续实时学习，推动机器学习向开放式技能获取和深度研究场景迈进。代码已开源：<a href="https://github.com/Agent-on-the-Fly/AgentFly%E3%80%82">https://github.com/Agent-on-the-Fly/AgentFly。</a></p>
<div class="markdown-heading"><h2 class="heading-element">Z-Pruner：无需重新训练的大语言模型后训练剪枝以提升效率</h2><a id="user-content-z-pruner无需重新训练的大语言模型后训练剪枝以提升效率" class="anchor" aria-label="Permalink: Z-Pruner：无需重新训练的大语言模型后训练剪枝以提升效率" href="#z-pruner无需重新训练的大语言模型后训练剪枝以提升效率"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15828v1 公告类型：新成果<br>
摘要：近年来，大语言模型（LLMs）快速发展，在各类自然语言处理任务中展现出卓越性能。然而这种进步是以模型规模日益增大为代价的，这给部署、可扩展性和能效带来了重大挑战。为应对这些局限，训练后剪枝技术已成为一种有前景的解决方案，可在无需重新训练的情况下有效缩减模型规模并降低推理延迟。尽管优势明显，现有剪枝方法往往导致性能显著下降或需要计算成本高昂的微调。本研究提出Z-Pruner——一种创新的训练后剪枝方法，专为在无需任何重新训练的情况下诱导预训练大语言模型稀疏化而设计。与传统方法不同，Z-Pruner同时利用权重更新幅度和激活模式来更有效地识别并消除冗余参数。我们的方法具有模型无关性、高效性和易实施特点。我们在多种广泛使用的大语言模型架构（包括LLaMA-2、LLaMA-3和OPT）上，通过多样化标准语言基准测试对Z-Pruner进行评估。实验结果表明，Z-Pruner超越了需要密集权重更新的最先进剪枝方法，具体表现为获得了最低困惑度分数和最高的零样本准确率总体平均分。相关代码已公开于<a href="https://github.com/sazzadadib/Z-Pruner%E3%80%82">https://github.com/sazzadadib/Z-Pruner。</a></p>
<div class="markdown-heading"><h2 class="heading-element">高维数据的低维嵌入</h2><a id="user-content-高维数据的低维嵌入" class="anchor" aria-label="Permalink: 高维数据的低维嵌入" href="#高维数据的低维嵌入"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15929v1 公告类型：新成果<br>
摘要：高维数据的大规模集合已近乎普遍存在于从生物学到人文学科的众多学术领域与应用场景中。由于直接处理高维数据存在诸多挑战，当前对能够生成低维表示（即嵌入）以支持数据可视化、探索和分析的算法需求达到前所未有的高度。近年来，大量嵌入算法被开发出来，并在科研与工业界得到广泛应用。这一研究热潮催生出一个庞大而分散的研究领域，既面临技术挑战又存在基础理论争议，导致实践者缺乏关于如何有效运用现有方法的明确指导。为增强领域连贯性并推动未来研究，本综述详细批判性地梳理了最新进展，提炼出创建与使用低维嵌入的最佳实践准则，在多类数据集上评估主流方法，并探讨该领域尚存的挑战与开放性问题。</p>
<div class="markdown-heading"><h2 class="heading-element">强化学习并非万能亦非幻影：理解监督与强化学习在大型语言模型微调中的差异</h2><a id="user-content-强化学习并非万能亦非幻影理解监督与强化学习在大型语言模型微调中的差异" class="anchor" aria-label="Permalink: 强化学习并非万能亦非幻影：理解监督与强化学习在大型语言模型微调中的差异" href="#强化学习并非万能亦非幻影理解监督与强化学习在大型语言模型微调中的差异"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16546v1 公告类型：新成果<br>
摘要：从头开始训练大型语言模型（LLMs）日益不切实际，使得监督微调（SFT）和强化学习微调（RL-FT，如PPO）等后训练方法成为现代实践的核心。通过采用24点卡牌游戏的分布外（OOD）变体及新型频谱诊断技术，我们重新审视了这两个阶段如何重塑模型表征与OOD性能。核心发现如下：（1）RL-FT可大幅恢复SFT造成的OOD性能损失（如Llama-11B从8.97%提升至15.38%，Qwen-7B从17.09%提升至19.66%）。但当SFT导致严重过拟合和明显分布偏移时，RL-FT无法完全恢复OOD性能；（2）奇异向量方向偏移比奇异值幅度更重要，这种偏移集中体现在最大和最小奇异值对应的方向上，而主体频谱保持稳定；（3）低秩浅层恢复效果显著：仅恢复前20%奇异值或前25%网络层的奇异向量方向，即可实现70-80%的OOD性能恢复；（4）强SFT检查点有助于RL实现更好恢复，而过拟合检查点则难以被修正。这些发现调和了先前关于RL具有卓越OOD性能的报道：RL主要抵消SFT引发的方向漂移，而非寻找新解决方案。我们的频谱感知分析揭示了两种低成本恢复手段——低秩UV合并和浅层重置，可供实践者在昂贵RL微调前优先采用。</p>
<div class="markdown-heading"><h2 class="heading-element">GEM：一种面向有效下游适应的尺度感知与分布敏感稀疏微调框架</h2><a id="user-content-gem一种面向有效下游适应的尺度感知与分布敏感稀疏微调框架" class="anchor" aria-label="Permalink: GEM：一种面向有效下游适应的尺度感知与分布敏感稀疏微调框架" href="#gem一种面向有效下游适应的尺度感知与分布敏感稀疏微调框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：GEM在此作为专有名词保留，全称可理解为"可扩展高效微调框架"。翻译时突出其核心特性："尺度感知"指能自适应不同模型规模，"分布敏感"强调对数据分布的敏感性，"稀疏微调"表明采用参数高效的精调策略，整体体现该框架在下游任务适配中的高效性。）</p>
<p>arXiv:2508.16191v1 公告类型：新成果<br>
摘要：参数高效微调（PEFT）已成为将大型预训练模型适配到新任务的主流方法。现有PEFT方法通常仅更新少量参数并冻结其余部分以避免冗余计算，但由于其追求更新量的绝对规模而忽视参数原始尺度，导致模型行为改变有限。与之相反，我们通过最大化相对于各参数自身尺度的更新量，实现了更具意义的下游任务适配。本文提出梯度权重比与熵引导掩码（GEM）框架——一种参数尺度感知、分布敏感的稀疏微调方法。GEM优先选择那些更新量相对于预训练初始值具有显著比例的参数，同时根据参数值熵自适应决定每层需微调的参数量，从而在PEFT过程中实现计算资源的最优配置。我们在通用领域任务（GLUE和SuperGLUE）与专业领域任务（GSM8k和MBPP）上的实证研究表明，GEM仅需更新0.1%的模型参数，即可实现比全参数微调最高1.6%的精度提升。</p>
<div class="markdown-heading"><h2 class="heading-element">模块化嵌入重组用于增量学习</h2><a id="user-content-模块化嵌入重组用于增量学习" class="anchor" aria-label="Permalink: 模块化嵌入重组用于增量学习" href="#模块化嵌入重组用于增量学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16463v1 公告类型：新研究<br>
摘要：预训练视觉-语言模型（VLMs）的出现显著改变了持续学习（CL）领域，这主要得益于其零样本分类能力。这种能力使VLMs非常适合现实应用，即使面对训练时未见过的新类别，也无需调整即可实现强劲性能。然而，当下游任务与预训练领域差异较大时，微调仍然至关重要。以往的持续学习方法主要关注在下游任务增量微调过程中保持VLMs的零样本能力。我们更进一步，提出了一种将零样本能力从保持转化为增强的方法——模块化嵌入重组（MoDER）。该方法采用模块化框架，训练多个文本专家模块（每个模块专精于一个已见类别），并将其存储于基础枢纽中。在推理阶段，针对每个未见类别，我们查询该枢纽并组合检索到的专家模块，以合成能提升分类效果的优化原型。我们在两种主流零样本增量协议（Class-IL和MTIL）下的14个数据集中验证了方法的有效性，代码库已开源：<a href="https://github.com/aimagelab/mammoth%E3%80%82">https://github.com/aimagelab/mammoth。</a></p>
<div class="markdown-heading"><h2 class="heading-element">FraPPE：基于偏好的快速高效纯探索方法</h2><a id="user-content-frappe基于偏好的快速高效纯探索方法" class="anchor" aria-label="Permalink: FraPPE：基于偏好的快速高效纯探索方法" href="#frappe基于偏好的快速高效纯探索方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16487v1 公告类型：新成果<br>
摘要：基于偏好的纯探索（PrePEx）旨在以给定置信水平识别向量值（即多目标）赌博机中的帕累托最优臂集，其中奖励向量通过（给定的）偏好锥$\mathcal{C}$进行排序。尽管PrePEx及其变体已得到充分研究，但目前尚无计算高效的算法能够最优地追踪任意偏好锥下的现有下界。我们通过高效求解下界中的最小化与最大化问题成功填补了这一空白。首先，我们推导出下界的三个结构特性，从而实现了最小化问题的计算可处理简化。随后，我们采用Frank-Wolfe优化器加速下界中的最大化问题。这些技术共同将包含$K$个臂和$L$维奖励的赌博机实例的maxmin优化问题求解时间缩短至$\mathcal{O}(KL^{2})$，较现有研究实现显著加速。我们进一步证明所提出的PrePEx算法FraPPE能够渐近达到最优样本复杂度。最后，通过在合成数据集和真实数据集上的数值实验表明，FraPPE在现有算法中实现了识别精确帕累托集所需的最低样本复杂度。</p>
<div class="markdown-heading"><h2 class="heading-element">分块数据沙普利值：一种可扩展的机器学习数据集质量评估方法</h2><a id="user-content-分块数据沙普利值一种可扩展的机器学习数据集质量评估方法" class="anchor" aria-label="Permalink: 分块数据沙普利值：一种可扩展的机器学习数据集质量评估方法" href="#分块数据沙普利值一种可扩展的机器学习数据集质量评估方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16255v1 公告类型：新研究<br>
摘要：随着可用数据集的规模和多样性持续增长，评估数据质量已成为实现可靠高效机器学习分析的关键。现代博弈论方法中评估数据质量的数据沙普利值（Data Shapley）概念，可量化数据集中单个数据点的价值。然而，现有扩展NP难沙普利值计算的前沿方法在应用于大规模数据集时面临严峻挑战，限制了其实际应用。本研究提出一种数据沙普利值方法——分块数据沙普利值（C-DaSh），用于识别数据集中的高质量数据元组。C-DaSh通过将数据集可扩展地划分为可管理的块，并采用优化子集选择与单轮随机梯度下降来估算每个块的数据贡献，在保持高质量结果的同时大幅减少计算时间。我们在多样化现实世界分类和回归任务上对方法进行实证基准测试，证明C-DaSh在计算效率（实现80倍至2300倍加速）和低质量数据区域检测精度方面均优于现有沙普利值近似方法。该方法支持对大型表格数据集进行实际质量评估，适用于分类和回归全流程。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>