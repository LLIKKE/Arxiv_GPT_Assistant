<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">FireQ：面向大语言模型推理加速的快速INT4-FP8内核与RoPE感知量化技术</h2><a id="user-content-fireq面向大语言模型推理加速的快速int4-fp8内核与rope感知量化技术" class="anchor" aria-label="Permalink: FireQ：面向大语言模型推理加速的快速INT4-FP8内核与RoPE感知量化技术" href="#fireq面向大语言模型推理加速的快速int4-fp8内核与rope感知量化技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明如下）</p>
<ol>
<li>保留"FireQ"作为技术品牌名不译，维持技术术语一致性</li>
<li>"Fast INT4-FP8 Kernel"译为"快速INT4-FP8内核"，其中INT4(4位整数)和FP8(8位浮点)是计算机体系结构标准术语</li>
<li>"RoPE-aware"译为"RoPE感知"，RoPE(Rotary Position Embedding)是Transformer架构中的旋转位置编码技术</li>
<li>采用"面向...的"句式体现技术应用场景，将"LLM Inference Acceleration"处理为"大语言模型推理加速"，其中：
<ul>
<li>LLM(Large Language Model)采用行业通用译法"大语言模型"</li>
<li>"推理加速"比直译"推断加速"更符合中文AI领域术语习惯</li>
</ul>
</li>
<li>整体采用技术论文标题常见的名词短语结构，通过"与"字连接两个关键技术点，保持学术严谨性</li>
</ol>
<p>arXiv:2505.20839v1 公告类型：新研究<br>
摘要：随着大语言模型应用日益广泛，内存带宽限制严重制约了推理吞吐量，这促使后训练量化（PTQ）技术受到关注。本文提出FireQ——一个协同设计的PTQ框架与INT4-FP8矩阵乘法内核，可加速大模型所有线性层的推理。具体而言，FireQ将线性层权重和键值量化为INT4，激活值和查询量化为FP8，显著提升吞吐量。此外，我们为预填充阶段引入三阶段流水线设计，通过改进FlashAttention-3内核，有效缩短预填充阶段的首令牌生成时间。为最小化量化精度损失，我们分别针对线性层和注意力层开发了创新的异常值平滑技术：在线性层中，我们显式采用张量级缩放防止INT4量化的FP8缩放因子导致数值下溢，并通过通道级缩放补偿INT4的粗粒度缺陷；在注意力层中，我们结合旋转位置编码（RoPE）前后的缩放策略，解决了RoPE带来的量化挑战。FireQ显著优于现有最优方法，在Llama2-7B上前馈网络层推理速度达到QServe的1.68倍，在Llama3-8B上预填充阶段性能提升1.26倍，且精度损失可忽略不计。</p>
<div class="markdown-heading"><h2 class="heading-element">高效大型语言模型推理：基于神经块线性化的方法</h2><a id="user-content-高效大型语言模型推理基于神经块线性化的方法" class="anchor" aria-label="Permalink: 高效大型语言模型推理：基于神经块线性化的方法" href="#高效大型语言模型推理基于神经块线性化的方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.21077v1 公告类型：新研究<br>
摘要：基于Transformer架构的大语言模型（LLM）在推理时的高计算需求给实际部署带来了巨大挑战。为此，我们提出神经块线性化（NBL）框架，通过用线性最小均方误差估计器导出的线性近似替换自注意力层，显著加速Transformer模型的推理过程。该框架利用典型相关分析计算近似误差的理论上限，并以此作为替换准则，选择线性化误差最低的LLM层进行优化。NBL无需微调即可高效应用于预训练好的大语言模型。实验表明，NBL在多个推理基准测试中既能保持模型竞争力精度，又能实现显著的计算加速。例如，将NBL应用于DeepSeek-R1-Distill-Llama-8B模型的12个自注意力层时，推理速度提升32%的同时精度损失不足1%，这种灵活的方法为提升大语言模型推理效率提供了极具前景的解决方案。</p>
<p>（注：根据学术文献翻译规范，对技术术语如"Linear Minimum Mean Squared Error estimators"采用"线性最小均方误差估计器"的权威译法；"Canonical Correlation Analysis"保留为"典型相关分析"这一统计学标准译名；模型名称"DeepSeek-R1-Distill-Llama-8B"保持原貌以方便学术检索；通过拆分英文长句为中文短句结构，如将原文最后复合句分解为因果关系的分句表述，符合中文科技文献表达习惯。）</p>
<div class="markdown-heading"><h2 class="heading-element">硬件高效注意力机制助力快速解码</h2><a id="user-content-硬件高效注意力机制助力快速解码" class="anchor" aria-label="Permalink: 硬件高效注意力机制助力快速解码" href="#硬件高效注意力机制助力快速解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.21487v1 公告类型：新研究<br>
摘要：大批次长上下文场景下，LLM解码过程受限于从高带宽内存加载键值（KV）缓存的操作，这不仅增加了单令牌延迟，其顺序解码特性也制约了并行性。我们系统分析了算术强度、并行化与模型质量之间的相互作用，质疑当前架构是否充分挖掘了现代硬件的潜力。本研究重构注意力机制，通过提升内存每字节载入对应的计算量，在不牺牲并行扩展性的前提下实现硬件效率最大化。我们首先提出分组绑定注意力（GTA），这种简易变体通过合并复用键值状态，在保持模型质量的同时减少内存传输；继而引入分组潜在注意力（GLA），这种并行友好的潜在注意力机制配合底层优化，既能实现快速解码又能维持高模型质量。实验表明：GTA在质量上媲美分组查询注意力（GQA）的同时，KV缓存用量减少约50%；GLA则与多头潜在注意力（MLA）性能相当且更易分片。经优化的GLA内核在推测解码等场景中（当查询长度超过1时）比FlashMLA快达2倍。此外，通过减少单设备KV缓存载入量，GLA在在线服务基准测试中将端到端延迟降低、吞吐量提升最高达2倍。</p>
<div class="markdown-heading"><h2 class="heading-element">SageAttention2++：SageAttention2的高效实现升级版</h2><a id="user-content-sageattention2sageattention2的高效实现升级版" class="anchor" aria-label="Permalink: SageAttention2++：SageAttention2的高效实现升级版" href="#sageattention2sageattention2的高效实现升级版"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.21136v1 公告类型：新成果<br>
摘要：注意力机制的效率至关重要，因为其时间复杂度会随着序列长度呈二次方增长。SageAttention2通过量化技术加速注意力中的矩阵乘法（Matmul）来解决这一问题。为了进一步优化SageAttention2，我们提出采用FP8矩阵乘法以FP16累加的快速指令，该指令比SageAttention2原先使用的FP8矩阵乘法快2倍。实验表明，SageAttention2++在保持与SageAttention2相同注意力精度的同时，速度比FlashAttention提升3.9倍。这意味着SageAttention2++能有效加速语言、图像和视频生成等各类模型，且对端到端性能指标的影响可忽略不计。代码将在<a href="https://github.com/thu-ml/SageAttention%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/thu-ml/SageAttention开源。</a></p>
<p>（注：根据学术惯例，"arXiv"保留不译；"Matmul"作为专业术语采用英文缩写形式；"FP8"/"FP16"为浮点计算标准格式，保留英文缩写；项目名称"SageAttention2++"作为专有名词未翻译；URL链接保留原格式以确保可访问性）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>