<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">代码理解任务中知识蒸馏的实证研究</h2><a id="user-content-代码理解任务中知识蒸馏的实证研究" class="anchor" aria-label="Permalink: 代码理解任务中知识蒸馏的实证研究" href="#代码理解任务中知识蒸馏的实证研究"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>预训练语言模型（PLMs）已成为代码理解领域的强大工具。然而，由于计算强度高和推理延迟大，在大规模应用中部署这些PLM面临实际挑战。知识蒸馏（KD）作为一种前景广阔的模型压缩与加速技术，通过将大型教师模型的知识迁移至紧凑型学生模型，在保持教师模型大部分能力的同时实现高效推理，有效解决了这些局限性。尽管该技术在自然语言处理和计算机视觉领域已取得显著成功，但其在代码理解任务中的潜力仍未被充分探索。本文系统研究了知识蒸馏在代码理解任务中的有效性和应用方式。我们的研究涵盖两种主流KD方法（基于logit和基于特征的KD方法），在三个下游任务上对八个学生模型和两个不同领域的教师PLM进行了实验验证。实验结果表明，与标准微调相比，KD能持续为不同规模的学生模型带来显著性能提升。值得注意的是，代码专用PLM作为教师模型展现出更优效果。在所有KD方法中，最新的基于特征的方法表现最为卓越，使学生模型仅需5%参数量即可保留教师模型98%的性能。关于学生模型架构，实验表明与教师模型架构相似并不必然带来更好性能。我们进一步讨论了KD过程和推理阶段的效率与行为特征，总结了研究发现的实践意义，并指出了未来值得探索的方向。</p>
<div class="markdown-heading"><h2 class="heading-element">NiceWebRL：一个用于在强化学习环境中进行人类受试实验的Python库</h2><a id="user-content-nicewebrl一个用于在强化学习环境中进行人类受试实验的python库" class="anchor" aria-label="Permalink: NiceWebRL：一个用于在强化学习环境中进行人类受试实验的Python库" href="#nicewebrl一个用于在强化学习环境中进行人类受试实验的python库"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>我们推出NiceWebRL这一研究工具，使研究人员能够利用机器学习强化学习（RL）环境进行在线人类受试实验。NiceWebRL作为一个Python库，可将任何基于Jax的环境转换为在线交互界面，同时支持单智能体与多智能体环境。该工具使人工智能研究者能够将其算法与人类表现进行对比，助力认知科学家测试机器学习算法作为人类认知理论的可行性，并支持多智能体研究者开发人机协作算法。通过三项案例研究，我们展示了NiceWebRL在推动类人人工智能、人本兼容人工智能及辅助型人工智能发展方面的潜力：首项研究（类人AI）中，NiceWebRL支持开发新型认知强化学习模型，通过在网格世界和2D版Minecraft领域Craftax中与人类参与者进行对比测试；第二项研究（人本兼容AI）中，该工具助力开发新型多智能体强化学习算法，能在《 overcooked》游戏中泛化至人类合作伙伴；第三项研究（辅助型AI）则演示了如何通过NiceWebRL研究大型语言模型在包含数百万层级任务的XLand-Minigrid环境中辅助人类完成复杂任务。该开源库详见<a href="https://github.com/KempnerInstitute/nicewebrl%E3%80%82">https://github.com/KempnerInstitute/nicewebrl。</a></p>
<div class="markdown-heading"><h2 class="heading-element">关于神经平均的定义</h2><a id="user-content-关于神经平均的定义" class="anchor" aria-label="Permalink: 关于神经平均的定义" href="#关于神经平均的定义"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>平均神经网络究竟意味着什么？我们研究了一个问题：如何仅利用预训练模型的最终权重，在无法获取训练数据的情况下，从多个在不同数据分片上训练的模型中合成单一神经网络。在构建神经平均的定义时，我们借鉴了"模型汤"的洞见——这种方法通过聚合多个模型形成单一模型，同时提升泛化性能。本工作中，我们将模型汤重新阐释为一个更广泛框架的特例：用于神经平均的摊销模型集成（AME）。这种数据无关的元优化方法将模型差异视为伪梯度，用以指导神经权重的更新。我们证明这一视角不仅能复现模型汤，还能实现更具表现力和自适应性的集成策略。实验表明，AME产生的平均神经解决方案在性能上超越了单个专家模型和模型汤基线，尤其在分布外场景中表现突出。我们的研究结果提出了一种原则性且可推广的数据无关模型权重聚合方法，从某种意义上定义了如何实现神经平均。</p>
<div class="markdown-heading"><h2 class="heading-element">Hydra：一款拥有16亿参数的状态空间语言模型，具备稀疏注意力机制、专家混合架构与记忆功能</h2><a id="user-content-hydra一款拥有16亿参数的状态空间语言模型具备稀疏注意力机制专家混合架构与记忆功能" class="anchor" aria-label="Permalink: Hydra：一款拥有16亿参数的状态空间语言模型，具备稀疏注意力机制、专家混合架构与记忆功能" href="#hydra一款拥有16亿参数的状态空间语言模型具备稀疏注意力机制专家混合架构与记忆功能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>我们提出Hydra作为混合长上下文语言模型的架构方案，该方案在约16亿参数规模内融合了条件计算、长上下文记忆机制和稀疏专家混合系统。Hydra采用Mamba风格的结构化状态空间模型(SSM)主干网络，结合间歇性稀疏全局注意力机制、分块级MoE前馈路由以及双内存系统（工作内存+事实型PKM内存）。我们规范了组件接口标准，提供透明的参数与复杂度核算，并制定了分阶段课程学习策略以稳定激活各组件。配合技术规范，我们提供了示意性的玩具级原型测试数据（基于合成数据的数千万参数规模），其唯一目的在于验证实现可行性及定性扩展特性（例如长上下文吞吐量拐点与可控专家路由），而非宣称具备竞争性的全规模性能。我们明确界定了假设与开放风险（训练复杂度、内存利用率、专业化动态），并将Hydra定位为激发后续实证研究的蓝图而非完整系统。通过融合SSM效率、选择性稀疏注意力、MoE容量与可学习内存，Hydra勾勒出模块化输入自适应长上下文语言模型的发展路径；在目标规模上验证终端任务增益仍属未来工作。</p>
<div class="markdown-heading"><h2 class="heading-element">S3LoRA：基于安全频谱锐度导向的智能体规划器自适应剪枝方法</h2><a id="user-content-s3lora基于安全频谱锐度导向的智能体规划器自适应剪枝方法" class="anchor" aria-label="Permalink: S3LoRA：基于安全频谱锐度导向的智能体规划器自适应剪枝方法" href="#s3lora基于安全频谱锐度导向的智能体规划器自适应剪枝方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译在保持技术准确性的基础上，采用中文专业文献常用表述方式，将"Safe Spectral Sharpness-Guided Pruning"译为"基于安全频谱锐度导向的剪枝"，"Adaptation of Agent Planner"译为"智能体规划器自适应"，通过添加"方法"二字使术语更符合中文技术文献表达习惯。）</p>
<p>利用LoRA等参数高效微调（PEFT）技术对大语言模型（LLM）进行适配，已为基于LLM的智能体赋予了强大能力。然而这种适配可能无意中破坏安全对齐机制，导致不安全或不稳定的行为，尤其在智能体规划任务中更为明显。现有安全感知适配方法通常需要同时获取基础模型和指令微调模型的检查点，这在实际应用中往往难以实现，限制了其适用性。我们提出S3LoRA（安全谱锐度引导剪枝LoRA），这是一个轻量级、无需数据且与模型无关的框架，仅通过检查微调后的权重更新即可降低LoRA适配模型的安全风险。我们首先引入幅度感知球面归一化奇异值分解（MAS-SVD），在保留全局幅度信息的同时，有效分析LoRA更新的结构特性；进而设计谱锐度指数（SSI），通过锐度感知指标检测具有高度集中且潜在不安全更新的层级。通过事后剪裁这些风险层级，可在不牺牲任务性能的前提下降低风险。在智能体规划和语言生成任务上的大量实验及消融研究表明，S3LoRA能持续提升安全指标，同时保持或改善效用指标，并显著降低推理成本。这些结果证明S3LoRA是种实用且可扩展的解决方案，适用于在现实世界、资源受限及安全关键环境中安全部署基于LLM的智能体。</p>
<div class="markdown-heading"><h2 class="heading-element">词汇定制师：针对小型语言模型下游任务的动态词汇选择机制</h2><a id="user-content-词汇定制师针对小型语言模型下游任务的动态词汇选择机制" class="anchor" aria-label="Permalink: 词汇定制师：针对小型语言模型下游任务的动态词汇选择机制" href="#词汇定制师针对小型语言模型下游任务的动态词汇选择机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：译文采用意译方式，在保持专业性的同时增强可读性。"VocabTailor"译为"词汇定制师"既保留"tailor"的裁剪本意，又体现个性化定制概念；"Dynamic Vocabulary Selection"译为"动态词汇选择机制"通过增译"机制"二字使技术概念更完整；"Downstream Tasks"采用行业通用译法"下游任务"；"Small Language Models"译为"小型语言模型"准确传达参数规模较小的AI模型特性。）</p>
<p>小型语言模型（SLMs）在资源受限环境中具有计算优势，但内存限制仍是边缘设备部署的关键瓶颈。由于词汇量庞大，SLM内存占用的相当一部分来自词汇相关组件，尤其是嵌入层和语言建模头。现有静态词汇剪枝方法虽能降低内存使用，但其僵化的"一刀切"设计会导致预填充阶段的信息丢失且缺乏灵活性。本研究揭示了词汇缩减挑战背后的两大核心原则：一是词汇局部性原理——任何单次推理仅需少量词汇子集；二是SLM词汇相关组件间存在计算特性不对称。基于这些发现，我们提出VocabTailor：一种新型解耦动态词汇选择框架，通过嵌入层卸载机制解决内存约束，并为语言建模头设计混合式静态-动态词汇选择策略，实现词汇组件的按需加载。在多类下游任务的综合实验表明，VocabTailor能将词汇相关组件内存使用降低高达99%，且任务性能几乎无损，显著优于现有静态词汇剪枝方法。</p>
<div class="markdown-heading"><h2 class="heading-element">SemToken：面向高效长上下文语言建模的语义感知分词技术</h2><a id="user-content-semtoken面向高效长上下文语言建模的语义感知分词技术" class="anchor" aria-label="Permalink: SemToken：面向高效长上下文语言建模的语义感知分词技术" href="#semtoken面向高效长上下文语言建模的语义感知分词技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译采用技术文献常用命名规范，"Semantic-Aware"译为"语义感知"符合计算机领域术语习惯；"Efficient Long-Context"处理为"高效长上下文"保持技术准确性；整体采用"技术名称：功能描述"的标题结构，符合中文技术文档的表述方式。）</p>
<p>分词在语言建模中起着关键作用，但现有方法如字节对编码（BPE）或WordPiece仅基于频率统计进行操作，忽略了文本底层的语义结构。这导致语义冗余片段被过度切分，且未能充分利用上下文连贯性——在长上下文场景中尤为明显。本研究提出\textbf{SemToken}，一种语义感知的分词框架，可同时降低词汇冗余并提升计算效率。该框架首先通过轻量级编码器提取上下文语义嵌入，执行局部语义聚类以合并语义等价的词元；随后根据语义密度分配异构化粒度——在内容密集区域采用细粒度分词，在重复性或低信息熵片段进行粗粒度压缩。SemToken可与现代语言模型及注意力加速机制无缝集成。在WikiText-103和LongBench等长上下文语言建模基准测试中，该方案实现了最高$2.4\times$的词汇量压缩和$1.9\times$的加速效果，且困惑度与下游任务准确率几乎无衰减。我们的研究表明：语义结构为优化大语言模型的分词机制与计算效能提供了新的突破方向。</p>
<div class="markdown-heading"><h2 class="heading-element">集成感知、通信与计算：空中联邦边缘学习技术</h2><a id="user-content-集成感知通信与计算空中联邦边缘学习技术" class="anchor" aria-label="Permalink: 集成感知、通信与计算：空中联邦边缘学习技术" href="#集成感知通信与计算空中联邦边缘学习技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>本文研究了一种集成感知、通信与计算（ISCC）的空中联邦边缘学习（Air-FEEL）系统。在该系统中，边缘服务器协调多个边缘设备对目标进行无线感知，并利用感知数据协同训练机器学习模型以完成识别任务。系统采用空中计算（AirComp）技术实现边缘设备模型的一步式聚合。在此框架下，我们通过特别考虑训练数据采集时的无线感知噪声和空中模型聚合时的AirComp失真，从损失函数退化角度分析了支持ISCC的Air-FEEL系统的收敛性能。理论结果表明：感知、通信与计算过程需要竞争网络资源来共同决定收敛速率。基于该分析，我们在保证每轮训练时延和能耗预算的前提下，以最大化损失函数退化量为目标设计ISCC参数。核心挑战在于不同设备间感知、通信与计算过程的紧密耦合。为此，我们通过交替优化批量大小控制和网络资源分配，提出了一种低复杂度ISCC算法。研究发现：对于每个设备，当获取更大批量数据样本时应当减少感知功率消耗，反之亦然；在给定批量大小情况下，单个设备的最优计算速度是满足时延约束的最小速度值。基于人体动作识别任务的数值结果验证了理论收敛分析，表明所提ISCC算法能有效协调感知、通信与计算之间的批量控制与资源分配，从而提升学习性能。</p>
<div class="markdown-heading"><h2 class="heading-element">微控制器量化神经网络：方法、平台与应用全面综述</h2><a id="user-content-微控制器量化神经网络方法平台与应用全面综述" class="anchor" aria-label="Permalink: 微控制器量化神经网络：方法、平台与应用全面综述" href="#微控制器量化神经网络方法平台与应用全面综述"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在资源受限设备（如微控制器）上部署量化神经网络（QNN）时，如何平衡模型性能、计算复杂度与内存限制已成为关键挑战。微型机器学习（TinyML）通过融合机器学习算法、硬件加速和软件优化等领域的最新进展，有效解决了在嵌入式系统上高效运行深度神经网络的难题。本综述以硬件为中心视角介绍量化技术，系统性地梳理了用于加速嵌入式深度学习模型的核心量化方法，特别聚焦于模型性能与硬件能力之间的关键权衡关系。文章进一步评估了现有专门支持微控制器执行QNN的软件框架与硬件平台，并对快速发展的QNN部署领域当前面临的挑战进行分析，同时展望了未来具有前景的研究方向。</p>
<div class="markdown-heading"><h2 class="heading-element">EffiFusion-GAN：面向语音增强的高效融合生成对抗网络</h2><a id="user-content-effifusion-gan面向语音增强的高效融合生成对抗网络" class="anchor" aria-label="Permalink: EffiFusion-GAN：面向语音增强的高效融合生成对抗网络" href="#effifusion-gan面向语音增强的高效融合生成对抗网络"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>我们推出EffiFusion-GAN（高效融合生成对抗网络），这是一个轻量级但功能强大的语音增强模型。该模型在多尺度模块中集成深度可分离卷积，以高效捕捉多样化声学特征。采用具有双重归一化和残差优化的增强注意力机制，进一步提升了训练稳定性与收敛效率。此外，通过动态剪枝技术在保持性能的同时压缩模型体积，使该框架适用于资源受限环境。在公开数据集VoiceBank+DEMAND上的实验评估表明，EffiFusion-GAN取得了3.45的PESQ评分，在相同参数量设置下优于现有模型。</p>
<div class="markdown-heading"><h2 class="heading-element">Spark：支持查询感知的非结构化稀疏化与可恢复KV缓存通道剪枝</h2><a id="user-content-spark支持查询感知的非结构化稀疏化与可恢复kv缓存通道剪枝" class="anchor" aria-label="Permalink: Spark：支持查询感知的非结构化稀疏化与可恢复KV缓存通道剪枝" href="#spark支持查询感知的非结构化稀疏化与可恢复kv缓存通道剪枝"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此处采用意译处理技术术语：</p>
<ol>
<li>"Query-Aware"译为"查询感知"保持技术准确性</li>
<li>"Unstructured Sparsity"译为"非结构化稀疏化"符合机器学习领域术语规范</li>
<li>"Recoverable KV Cache Channel Pruning"采用扩展译法明确技术含义，其中：
<ul>
<li>"Recoverable"译为"可恢复"</li>
<li>"KV Cache"保留技术缩写"KV"（键值对）并明确译为"缓存"</li>
<li>"Channel Pruning"译为"通道剪枝"符合神经网络优化领域的标准译法）</li>
</ul>
</li>
</ol>
<p>大型语言模型（LLM）的长上下文推理正日益受到KV缓存瓶颈的制约：内存使用随序列长度线性增长，而注意力计算量呈二次方增长。现有方法通过时间轴上的KV缓存压缩策略（如令牌驱逐或合并）来降低内存和计算开销，但这些方法往往忽略了特征维度（即通道轴）上的细粒度重要性差异，从而限制了其有效平衡效率与模型精度的能力。实际上，我们观察到通道显著性在查询和位置间存在剧烈波动：某些特征通道对特定查询携带近乎零信息，而其他通道的相关性却突然激增。针对这一疏漏，我们提出无需训练的即插即用方法SPARK，通过在通道层级实施非结构化稀疏化来修剪KV缓存，并在注意力分数计算过程中动态恢复被修剪的条目。值得注意的是，该方法与现有KV压缩及量化技术正交兼容，可协同实现进一步加速。通过消除通道级冗余，SPARK能在相同内存预算下处理更长序列。对于等长序列，相比基于驱逐的方法，SPARK不仅保持或提升模型精度，还将KV缓存存储降低30%以上。即使在80%的激进修剪比例下，SPARK的性能衰减仍比基线驱逐方法低5%，展现出卓越的鲁棒性与有效性。代码已开源：<a href="https://github.com/Xnhyacinth/SparK%E3%80%82">https://github.com/Xnhyacinth/SparK。</a></p>
<div class="markdown-heading"><h2 class="heading-element">JEDI-linear：针对FPGA上喷注标记的快速高效图神经网络</h2><a id="user-content-jedi-linear针对fpga上喷注标记的快速高效图神经网络" class="anchor" aria-label="Permalink: JEDI-linear：针对FPGA上喷注标记的快速高效图神经网络" href="#jedi-linear针对fpga上喷注标记的快速高效图神经网络"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>图神经网络（GNN），特别是交互网络（IN），在欧洲核子研究中心高亮度大型强子对撞机（HL-LHC）的喷注标记任务中展现出卓越性能。然而其计算复杂度高且内存访问模式不规则，在硬件触发系统中部署于FPGA时面临严峻挑战——该系统对延迟和资源有着严格限制。本研究提出JEDI-linear新型GNN架构，通过共享变换和全局聚合消除显式成对交互，实现线性计算复杂度。为提升硬件效率，我们采用细粒度量化感知训练配合逐参数位宽优化，并通过分布式算法实现无乘法器的乘累加运算。评估结果表明：基于FPGA的JEDI-linear相比先进设计方案，延迟降低3.7至11.5倍，初始间隔最高减少150倍，LUT使用量最多降低6.2倍，同时具有更高模型精度且完全无需DSP模块（现有方案需消耗超8,700个DSP）。这是首个实现低于60纳秒延迟的交互式GNN，目前满足HL-LHC紧凑缪子线圈（CMS）一级触发系统要求。本研究通过实现精准、可扩展且资源高效的实时GNN推理，推动下一代触发系统发展。开源模板将进一步提升科学应用中的可复现性与推广性。</p>
<div class="markdown-heading"><h2 class="heading-element">合成自适应引导嵌入（SAGE）：一种新型知识蒸馏方法</h2><a id="user-content-合成自适应引导嵌入sage一种新型知识蒸馏方法" class="anchor" aria-label="Permalink: 合成自适应引导嵌入（SAGE）：一种新型知识蒸馏方法" href="#合成自适应引导嵌入sage一种新型知识蒸馏方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>模型蒸馏技术能够将知识从大规模模型迁移至紧凑型学生模型，助力在资源受限环境中的部署应用。然而传统蒸馏方法常面临计算开销大与泛化能力受限的问题。我们提出了一种新型自适应蒸馏框架，该框架可针对学生模型高损失区域动态增强训练数据。通过UMAP降维与近邻采样技术，我们的方法能识别嵌入空间中的薄弱区域，并生成针对性合成样本来引导学生模型学习。为进一步提升效率，我们设计了轻量级师生交互接口，绕过教师模型的输入层，直接在向量化表征上进行蒸馏。在标准自然语言处理基准测试中，我们的6600万参数学生模型仅需更少训练轮次即可持续达到或超越现有基线水平，在QNLI和SST-2数据集上分别取得91.2%和92.3%的准确率。这些成果彰显了基于损失感知的数据增强与向量化蒸馏技术在实现高效模型压缩方面的巨大潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">NVIDIA Nemotron Nano 2：精准高效的混合Mamba-Transformer推理模型</h2><a id="user-content-nvidia-nemotron-nano-2精准高效的混合mamba-transformer推理模型" class="anchor" aria-label="Permalink: NVIDIA Nemotron Nano 2：精准高效的混合Mamba-Transformer推理模型" href="#nvidia-nemotron-nano-2精准高效的混合mamba-transformer推理模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：采用技术领域常见的"模型"替代直译"推理模型"，保留"Mamba-Transformer"专业术语不译，通过添加"混合"明确架构特性，"精准高效"四字格提升专业文本的凝练度）</p>
<p>我们推出Nemotron-Nano-9B-v2混合架构语言模型，该模型融合Mamba与Transformer技术，旨在提升推理任务吞吐量的同时，实现同规模模型中领先的准确度。该模型基于Nemotron-H架构构建，通过将传统Transformer中的多数自注意力层替换为Mamba-2层，显著提升了生成推理所需长思维链时的推断速度。</p>
<p>我们采用FP8训练方案，首先使用20万亿token预训练出120亿参数的基座模型（Nemotron-Nano-12B-v2-Base），在对齐该基座模型后，运用Minitron策略进行压缩与蒸馏，最终实现在单张NVIDIA A10G GPU（22GiB显存，bfloat16精度）上处理高达128k token的推理能力。</p>
<p>与同规模现有模型（如Qwen3-8B）相比，Nemotron-Nano-9B-v2在推理基准测试中达到相当或更优的准确度，同时在8k输入16k输出的典型推理场景中实现最高6倍的推理吞吐量提升。我们将通过Hugging Face平台开放Nemotron-Nano-9B-v2、Nemotron-Nano12B-v2-Base和Nemotron-Nano-9B-v2-Base的模型权重，并公布大部分预训练与后训练数据集。</p>
<div class="markdown-heading"><h2 class="heading-element">IMC阵列上容错多位权重表示的行列混合分组方法</h2><a id="user-content-imc阵列上容错多位权重表示的行列混合分组方法" class="anchor" aria-label="Permalink: IMC阵列上容错多位权重表示的行列混合分组方法" href="#imc阵列上容错多位权重表示的行列混合分组方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>注：该翻译保持了技术术语的准确性，同时符合中文表达习惯。"Row-Column Hybrid Grouping"译为"行列混合分组"，"Fault-Resilient"译为"容错"，"Multi-Bit Weight Representation"译为"多位权重表示"，"IMC Arrays"保持专业缩写形式译为"IMC阵列"。整体采用技术文献常用的偏正结构命名方式，确保专业领域的表达规范性。</p>
<p>本文针对模拟内存计算（IMC）系统中限制其可扩展性和部署能力的两个关键挑战展开研究：由固定型故障（SAF）引发的计算不可靠性问题，以及现有故障缓解算法（即无故障算法FF）的高编译开销。为突破这些局限，我们首先提出了一种新颖的多比特权重表示技术——行列混合分组法，该方法通过引入跨行列的双重冗余机制，扩展了传统的列分组模式。这种结构冗余增强了容错能力，并能与现有故障缓解方案有效结合。其次，我们设计了将故障感知权重分解问题重构为整数线性规划（ILP）任务的编译流水线，借助现成求解器实现快速可扩展的编译过程。通过理论洞察识别出可简化解的特殊故障模式，进一步加速了计算过程。在卷积网络和小型语言模型上的实验结果表明，我们的方法相比现有基线实现了最高8个百分点的精度提升、150倍的编译速度提升以及2倍的能效增益。</p>
<div class="markdown-heading"><h2 class="heading-element">使用SparseLoCo实现通信高效的大型语言模型预训练</h2><a id="user-content-使用sparseloco实现通信高效的大型语言模型预训练" class="anchor" aria-label="Permalink: 使用SparseLoCo实现通信高效的大型语言模型预训练" href="#使用sparseloco实现通信高效的大型语言模型预训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>近年来，通信高效的分布式训练算法因其能在带宽受限环境（如跨数据中心和互联网）中有效支持大语言模型（LLM）训练而备受关注。尽管这些方法通过降低通信频率来优化传输，但仍通常需要传输完整的模型梯度副本——即使对于跨数据中心链路而言，这依然会形成通信瓶颈。此外，与朴素的AdamW DDP基线相比，这些方法的性能可能略有下降。虽然量化和误差反馈技术常被用于压缩伪梯度大小，但在LLM预训练场景中，现有方法始终未能有效结合稀疏化技术，且量化程度有限。本研究提出SparseLoCo——一种面向大语言模型的通信高效训练算法，通过巧妙结合Top-k稀疏化与量化技术，实现了高达1-3%稀疏度和2比特量化的极端压缩比，其性能甚至优于全精度DiLoCo。我们的核心发现是：外部动量可通过误差反馈与激进稀疏化进行本地近似，且稀疏聚合实际上能提升模型性能。通过一系列通信受限的LLM训练场景实证，我们证明SparseLoCo在性能表现和通信成本方面均能带来显著提升。</p>
<div class="markdown-heading"><h2 class="heading-element">StreamMem：面向流媒体视频理解的查询无关键值缓存内存</h2><a id="user-content-streammem面向流媒体视频理解的查询无关键值缓存内存" class="anchor" aria-label="Permalink: StreamMem：面向流媒体视频理解的查询无关键值缓存内存" href="#streammem面向流媒体视频理解的查询无关键值缓存内存"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>多模态大语言模型（MLLMs）在视觉语言推理方面取得显著进展，但其高效处理长视频的能力仍受限。尽管近期长上下文MLLMs有所突破，但存储和处理长视觉上下文的关键值（KV）缓存会产生巨大的内存和计算开销。现有视觉压缩方法需在压缩前编码整个视觉上下文，或需提前获取问题信息，这对于长视频理解和多轮对话场景并不实用。本研究提出StreamMem——一种面向流式视频理解的查询无关KV缓存记忆机制。该机制以流式方式编码新视频帧，通过视觉标记与通用查询标记间的注意力分数压缩KV缓存，同时维持固定大小的KV记忆库，从而在内存受限的长视频场景中实现高效问答。在三个长视频理解和两个流式视频问答基准测试中，StreamMem在查询无关KV缓存压缩方面达到最先进性能，并与查询感知压缩方法具有竞争力。</p>
<div class="markdown-heading"><h2 class="heading-element">让我们共同培育无偏见社群：通过新增链接引导图的公平性</h2><a id="user-content-让我们共同培育无偏见社群通过新增链接引导图的公平性" class="anchor" aria-label="Permalink: 让我们共同培育无偏见社群：通过新增链接引导图的公平性" href="#让我们共同培育无偏见社群通过新增链接引导图的公平性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>图神经网络（GNN）已在众多应用领域取得显著成功。然而，由于图结构中存在的偏见，图神经网络在公平性方面面临重大挑战。虽然原始用户图结构通常存在偏差，但通过引入新链接来引导现有结构向无偏状态发展具有广阔前景。这种基于新链接的公平性引导能够促进无偏见社区的形成，从而提升下游应用的公平性。针对这一问题，我们提出了名为FairGuide的创新框架。具体而言，为确保在公平性引导图上训练的下游任务公平性，我们引入了可微分社区检测任务作为伪下游任务。理论分析进一步证明，通过优化该伪任务中的公平性，能有效增强结构公平性，促进跨下游应用的公平性泛化。此外，FairGuide采用高效策略，利用从公平性引导目标中提取的元梯度来识别能显著提升结构公平性的新链接。大量实验结果表明，我们提出的方法在各种基于图的公平性任务中均展现出卓越的有效性和泛化能力。</p>
<div class="markdown-heading"><h2 class="heading-element">量化与扩散大语言模型的交汇：关于扩散大语言模型训练后量化的系统研究</h2><a id="user-content-量化与扩散大语言模型的交汇关于扩散大语言模型训练后量化的系统研究" class="anchor" aria-label="Permalink: 量化与扩散大语言模型的交汇：关于扩散大语言模型训练后量化的系统研究" href="#量化与扩散大语言模型的交汇关于扩散大语言模型训练后量化的系统研究"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>扩散大语言模型（dLLM）近期取得突破性进展，通过采用全局注意力机制和基于去噪的解码策略，为自然语言生成任务提供了自回归（AR）大模型之外的新选择。然而，由于这类模型参数量庞大且资源需求高，其在边缘设备上的部署仍面临挑战。虽然训练后量化（PTQ）技术已成为压缩自回归大模型的常用方法，但其在扩散模型上的适用性尚未得到充分探索。本研究首次系统性地探讨基于扩散的语言模型量化问题：我们首先发现激活值中存在异常大数值的离群点，这些离群点主导了动态范围分布，成为低比特量化的主要障碍——因为它们导致难以在保持多数数值精度的同时有效压缩模型。更重要的是，我们实施了最先进的PTQ方法，并在多任务类型和模型变体上展开全面评估。我们的分析框架围绕四个核心维度构建：比特宽度、量化方法、任务类别和模型类型。通过这种多视角评估，我们为不同配置下扩散大语言的量化特性提供了实用见解。希望本研究能为高效部署扩散大模型的后续研究奠定基础，所有代码和实验设置将开源以支持学界进一步探索。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>