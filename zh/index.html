<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">机密计算环境中的蒸馏大型语言模型助力系统级芯片设计</h2><a id="user-content-机密计算环境中的蒸馏大型语言模型助力系统级芯片设计" class="anchor" aria-label="Permalink: 机密计算环境中的蒸馏大型语言模型助力系统级芯片设计" href="#机密计算环境中的蒸馏大型语言模型助力系统级芯片设计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.16226v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）在电路设计任务中的应用日益广泛，这类模型通常经过多轮训练。无论是训练完成的模型还是相关训练数据，都被视为机密知识产权（IP），必须防止泄露。机密计算技术通过可信执行环境（TEEs）为保护数据和模型提供了可行方案，但现有TEE实现方案难以高效支持LLMs的高资源需求。本研究首次对基于TEE的机密计算环境（特别是采用英特尔信任域扩展TDX技术）中的LLMs进行了全面评估。我们在三种实验环境中进行测试：基于TEE的环境、纯CPU环境以及CPU-GPU混合环境，并以每秒生成token数作为性能指标。</p>
<p>我们的首要发现是蒸馏模型（如DeepSeek）凭借更少的参数量，在性能上超越其他模型，尤其适合资源受限设备。在量化模型方面（如4位量化Q4和8位量化Q8），其性能较FP16模型提升最高达3倍。研究结果表明，对于参数量较少的模型（如DeepSeek-r1-1.5B），TDX实现在安全环境中的计算执行效率优于纯CPU版本。我们进一步通过面向SoC设计任务的测试平台验证了这些结果，证实了轻量化LLMs在资源受限系统中高效部署的潜力，可有效支持半导体CAD应用。</p>
<div class="markdown-heading"><h2 class="heading-element">论稀疏自编码器在解释压缩模型中的可迁移性</h2><a id="user-content-论稀疏自编码器在解释压缩模型中的可迁移性" class="anchor" aria-label="Permalink: 论稀疏自编码器在解释压缩模型中的可迁移性" href="#论稀疏自编码器在解释压缩模型中的可迁移性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.15977v1 公告类型：新研究<br>
摘要：现代大型语言模型（LLM）因其规模庞大而面临推理效率挑战。为此，研究者提出了剪枝、量化等多种压缩方法，但压缩对模型可解释性的影响尚不明确。尽管目前已存在电路发现等多种模型解释方法，其中稀疏自编码器（SAE）在将模型激活空间分解为特征基向量方面表现尤为突出。本研究探索了原始模型与压缩模型在SAE解释上的差异。我们发现：基于原始模型训练的SAE仍能有效解释压缩模型，尽管其性能略逊于直接在压缩模型上训练的SAE；更值得注意的是，直接对原始SAE进行剪枝所获得的性能，与在剪枝后模型上重新训练的SAE相当。这一发现使我们能够规避SAE训练过程中的巨大计算成本。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语保留英文缩写并首次出现时标注全称（如SAE/Sparse Autoencoders）</li>
<li>"elusive"译为"尚不明确"体现学术文本的严谨性</li>
<li>"albeit with slight performance degradation"处理为"尽管其性能略逊于"，通过"略逊"准确传达轻微程度</li>
<li>被动语态转换为中文主动句式（如"have been proposed"译为"研究者提出了"）</li>
<li>长难句拆分重组（如最后两句通过分号连接保持逻辑连贯）</li>
<li>"mitigate the extensive training costs"意译为"规避...巨大计算成本"，更符合中文表达习惯）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">TorchAO：基于PyTorch原生框架的训练至服务全流程模型优化工具</h2><a id="user-content-torchao基于pytorch原生框架的训练至服务全流程模型优化工具" class="anchor" aria-label="Permalink: TorchAO：基于PyTorch原生框架的训练至服务全流程模型优化工具" href="#torchao基于pytorch原生框架的训练至服务全流程模型优化工具"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（翻译说明：1. 保留TorchAO专业术语不译以保持技术一致性 2. "PyTorch-Native"译为"基于PyTorch原生框架"突出技术特性 3. "Training-to-Serving"采用"训练至服务全流程"的意译方式体现端到端特性 4. 增补"工具"二字符合中文技术文档命名习惯 5. 整体采用技术文档标准的四字格+补充说明结构）</p>
<p>arXiv:2507.16099v1 公告类型：新成果<br>
摘要：我们推出TorchAO——一个基于PyTorch的原生模型优化框架，通过量化和稀疏化技术为AI模型提供从训练到服务的端到端工作流。该框架支持多种主流模型优化方法，包括FP8量化训练、量化感知训练（QAT）、训练后量化（PTQ）以及2:4稀疏化，并创新性地采用张量子类抽象来表示多种广泛使用且后端无关的低精度数据类型，如INT4、INT8、FP8、MXFP4、MXFP6和MXFP8。TorchAO在模型优化流程的每个环节（从预训练TorchTitan到微调TorchTune/Axolotl，再到服务部署HuggingFace/vLLM/SGLang/ExecuTorch）都与更广泛的生态系统深度集成，将原本碎片化的技术领域统一到单一工作流中。该框架已成功支撑量化版Llama 3.2 1B/3B与LlamaGuard3-8B模型的近期发布，项目开源地址为<a href="https://github.com/pytorch/ao/%E3%80%82">https://github.com/pytorch/ao/。</a></p>
<p>（注：根据技术文档翻译规范，对以下术语进行了标准化处理：</p>
<ol>
<li>"backend agnostic"译为"后端无关"而非字面直译</li>
<li>"tensor subclass abstraction"译为"张量子类抽象"以保持PyTorch术语体系一致性</li>
<li>产品名称TorchTitan/TorchTune等保留原名不译</li>
<li>长复合句拆分为符合中文表达习惯的短句结构）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>