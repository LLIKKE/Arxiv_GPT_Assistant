<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">稀疏注意力混合：基于内容的可学习稀疏注意力通过专家选择路由</h2><a id="user-content-稀疏注意力混合基于内容的可学习稀疏注意力通过专家选择路由" class="anchor" aria-label="Permalink: 稀疏注意力混合：基于内容的可学习稀疏注意力通过专家选择路由" href="#稀疏注意力混合基于内容的可学习稀疏注意力通过专家选择路由"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>近期大规模语言模型的进展凸显了自注意力机制中二次方计算成本的过高问题。尽管已有大量研究投入，但次二次方注意力方法在实际应用中仍存在性能不足的缺陷。我们提出假设：动态的、基于学习的内容稀疏化可以催生更高效的注意力机制。为此，我们提出稀疏注意力混合机制（MoSA）——一种受专家混合（MoE）架构中专家选择路由机制启发的新方法。MoSA为每个注意力头动态选择token，支持任意的稀疏注意力模式。通过从长度为T的序列中选择k个token，MoSA将每个注意力头的计算复杂度从O(T²)降至O(k²+T)。这使得在相同计算资源下能使用更多注意力头，从而提升专业化程度。实验表明，在所有测试的稀疏注意力变体中，MoSA是唯一能持续超越稠密基线模型的方案，在同等计算预算下有时能获得高达27%的困惑度提升。与稠密自注意力相比，MoSA还能降低资源消耗。即便使用未经内核优化的torch实现，在困惑度匹配的条件下，MoSA模型在时钟时间上更快、训练内存需求更低，且相比稠密Transformer基线能大幅减少KV缓存大小。</p>
<div class="markdown-heading"><h2 class="heading-element">突破低位优化器的极限：聚焦EMA动态机制</h2><a id="user-content-突破低位优化器的极限聚焦ema动态机制" class="anchor" aria-label="Permalink: 突破低位优化器的极限：聚焦EMA动态机制" href="#突破低位优化器的极限聚焦ema动态机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>模型规模的爆炸式增长导致训练/微调成本持续攀升，尤其对于需要维护相当于模型体积两倍辅助信息以实现最优收敛的状态化优化器而言，这种成本已变得令人望而却步。为此，我们提出了一种具有极轻量级状态负载的新型优化器，其核心技术在于超低精度量化。虽然前人研究已通过8位或4位量化取得了一定成果，但我们的方法首次实现了每个状态元素仅需3比特甚至2比特的超低精度运行。这一突破源于我们对两个关键挑战的识别与解决：无符号量化中导致状态动态停滞的信号湮灭问题，以及有符号量化中因梯度方差激增引发的错误下降方向问题。理论分析表明，针对前者需采用定制化的对数量化方案，针对后者则需要根据精度动态调整动量值。最终实现的SOLO优化器在训练7B参数模型时可节省约45GB内存，同时仅带来微小的精度损失。我们期待SOLO能助力突破算力资源瓶颈，从而推动基础研究实现更广泛的普惠化发展。</p>
<div class="markdown-heading"><h2 class="heading-element">Pack-PTQ：通过分组重构推进神经网络的训练后量化</h2><a id="user-content-pack-ptq通过分组重构推进神经网络的训练后量化" class="anchor" aria-label="Permalink: Pack-PTQ：通过分组重构推进神经网络的训练后量化" href="#pack-ptq通过分组重构推进神经网络的训练后量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>训练后量化（PTQ）已成为压缩复杂模型的重要解决方案，其优势在于仅需少量校准数据且无需端到端重新训练。然而现有PTQ方法多采用逐块重建策略，忽视了块间依赖性，在低比特量化时会出现显著精度下降。为突破这些局限，本文提出创新性方法Pack-PTQ：首先设计基于Hessian矩阵的自适应分组机制，将网络块划分为非重叠的量化组作为重建单元，既保持块间依赖关系又能精确估计量化参数；其次基于分组结构提出混合精度量化方案，根据各分组不同的灵敏度分配差异化比特位宽，从而进一步提升性能。通过在2D图像和3D点云分类任务上的大量实验（涵盖多种网络架构），本方法相较最先进的PTQ方案展现出显著优势。</p>
<p>（注：翻译过程中对技术术语进行了如下处理：</p>
<ol>
<li>"calibration dataset"译为"校准数据"而非字面直译，符合机器学习领域术语习惯</li>
<li>"Hessian-guided"译为"基于Hessian矩阵"以明确数学含义</li>
<li>"non-overlapping packs"译为"非重叠的量化组"通过添加"量化"二字明确技术场景</li>
<li>"state-of-the-art"采用"最先进的"标准译法</li>
<li>将原文两个长句拆分为符合中文表达习惯的短句结构，同时保持技术严谨性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">利用安全引导自压缩优化深度神经网络</h2><a id="user-content-利用安全引导自压缩优化深度神经网络" class="anchor" aria-label="Permalink: 利用安全引导自压缩优化深度神经网络" href="#利用安全引导自压缩优化深度神经网络"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在资源受限设备上部署深度神经网络，需要采用能审慎平衡模型规模压缩与性能保持的有效模型压缩策略。本研究提出了一种创新的安全驱动量化框架，通过利用保护集对神经网络权重进行系统性剪枝与量化，在保持精度的同时优化模型复杂度。该方案在卷积神经网络（CNN）和基于注意力的语言模型上进行了严格验证，证明了其跨不同架构范式的适用性。实验结果表明：相较于原始未量化模型，本框架在维持60%初始模型规模的同时，测试准确率最高可提升2.5%。与传统量化技术相比，该方法通过消除参数噪声并保留关键权重，不仅增强了模型泛化能力，还降低了方差，从而确保关键模型特征得以保留。这些发现充分印证了安全驱动量化作为一种稳健可靠的深度学习模型优化策略的有效性。本框架的实现代码与完整实验评估已在GitHub平台开源。</p>
<p>（注：根据学术文献翻译规范，对以下要点进行了专业化处理：</p>
<ol>
<li>"preservation sets"译为"保护集"符合机器学习领域术语习惯</li>
<li>"quantize"统一译为"量化"保持术语一致性</li>
<li>"variance"在统计学语境下译为"方差"</li>
<li>技术指标"2.5%"保留数字与百分号格式</li>
<li>GitHub直接保留原名符合技术文档惯例</li>
<li>长难句采用分切重组策略，如将"demonstrating..."独立成句译为"证明了..."</li>
<li>被动语态转换为中文主动表达，如"is rigorously evaluated"译为"进行了严格验证"）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">FineScope：基于SAE引导自数据培育的领域专用大语言模型精准剪枝技术</h2><a id="user-content-finescope基于sae引导自数据培育的领域专用大语言模型精准剪枝技术" class="anchor" aria-label="Permalink: FineScope：基于SAE引导自数据培育的领域专用大语言模型精准剪枝技术" href="#finescope基于sae引导自数据培育的领域专用大语言模型精准剪枝技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明如下：</p>
<ol>
<li>"Precision Pruning"译为"精准剪枝技术"，体现了模型优化的核心方法</li>
<li>"Domain-Specialized"译为"领域专用"，准确表达模型的专业化特性</li>
<li>"SAE-Guided"采用首字母缩略词SAE保留专业术语，补充"引导"说明其作用</li>
<li>"Self-Data Cultivation"译为"自数据培育"，既保持"自我生成数据"的原文含义，又符合中文技术文献表达习惯</li>
<li>整体采用"技术"作为中心词，使中文标题更符合学术论文命名规范）</li>
</ol>
<p>从头训练大型语言模型（LLM）需要消耗大量计算资源，这促使研究者开始关注开发既能保持高效又能实现强劲任务性能的小型领域专用模型。以LLaMA等中等规模模型作为领域适配起点时，这些模型在专业数据集测试中常出现准确率下降问题。我们提出FineScope框架，通过从预训练大模型中提取紧凑的领域优化模型来解决这一挑战。该框架基于稀疏自编码器（SAE）架构——其可解释特征表示能力启发了我们——从海量数据中提取领域特定子集。我们采用带领域约束的结构化剪枝技术，确保剪枝后的模型能保留目标领域核心知识。为进一步提升性能，这些剪枝模型会通过自数据蒸馏过程，利用SAE筛选的数据集恢复剪枝过程中丢失的关键领域信息。大量实验与消融研究表明，FineScope在领域特定任务中展现出极具竞争力的性能，超越多个最先进的大规模LLM。值得注意的是，当使用SAE筛选数据集微调时，剪枝模型能恢复绝大部分原始性能。此外，将该数据集应用于未经剪枝的预训练LLM微调时，同样能提升其领域准确率，这印证了我们方法的鲁棒性。相关代码将公开。</p>
<div class="markdown-heading"><h2 class="heading-element">FreqKV：面向上下文窗口高效扩展的频域键值压缩技术</h2><a id="user-content-freqkv面向上下文窗口高效扩展的频域键值压缩技术" class="anchor" aria-label="Permalink: FreqKV：面向上下文窗口高效扩展的频域键值压缩技术" href="#freqkv面向上下文窗口高效扩展的频域键值压缩技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>将专业术语"Frequency Domain"译为"频域"，符合信号处理领域的规范译法</li>
<li>"Key-Value Compression"采用"键值压缩"这一计算机领域的标准译法</li>
<li>通过增译"技术"二字，使中文标题更符合技术论文的命名习惯</li>
<li>使用"面向...的"结构替代原文介词"for"，更符合中文表达逻辑</li>
<li>"Efficient Context Window Extension"译为"高效扩展"作定语，保持技术术语简洁性</li>
<li>整体采用"主标题+副标题"结构，通过冒号分隔，既忠实原意又符合中文技术文献标题规范）</li>
</ol>
<p>扩展大型语言模型（LLM）的上下文窗口对于涉及长文本生成的应用至关重要。然而，键值（KV）缓存内存需求的线性增长，以及自注意力机制相对于序列长度的二次方复杂度，给微调和推理过程带来了重大挑战。现有方法在扩展到更长上下文时往往面临性能下降的问题。本文提出了一种新颖的上下文扩展方法，可同时优化微调与推理效率。我们的方法基于一个关键发现：在频域中，KV缓存的能量分布主要集中在低频分量。通过滤除高频分量，可以在最小化信息损失的前提下高效压缩KV缓存。基于这一洞察，我们提出了一种高效的频域压缩技术FreqKV，该技术可将不断增长的KV缓存迭代压缩至固定尺寸，适用于微调和推理全流程。FreqKV无需引入额外参数或修改模型架构。经过极少量微调后，LLM即可学会有效利用频域压缩后的有限缓存，从而实现高效的上下文窗口扩展。在多种长上下文语言建模与理解任务上的实验验证了该方法的效率与有效性。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>