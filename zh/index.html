<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">真伪辨识：通过模式感知推理实现可泛化的深度伪造检测</h2><a id="user-content-真伪辨识通过模式感知推理实现可泛化的深度伪造检测" class="anchor" aria-label="Permalink: 真伪辨识：通过模式感知推理实现可泛化的深度伪造检测" href="#真伪辨识通过模式感知推理实现可泛化的深度伪造检测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>深度伪造检测因现实场景中伪造内容的复杂性与动态演变特性，仍面临严峻挑战。然而，现有学术基准与工业实践存在显著差异：通常采用同质化训练源和低质量测试图像，这严重制约了现有检测器的实际部署。为弥合这一差距，我们推出HydraFake数据集——通过分层泛化测试模拟现实挑战。该数据集涵盖多样化的深度伪造技术和野外伪造样本，配备严格的训练与评估协议，涉及未知模型架构、新兴伪造技术和新型数据领域。基于此资源，我们提出Veritas检测器：一种基于多模态大语言模型（MLLM）的深度伪造检测系统。区别于传统思维链（CoT）方法，我们引入模式感知推理机制，融合"规划"与"自省"等关键推理模式以模拟人类取证流程。进一步设计两阶段训练管道，将深度伪造推理能力无缝内化至现有MLLM中。在HydraFake数据集上的实验表明：尽管现有检测器在跨模型场景中展现良好泛化性，但在未知伪造技术和数据领域表现不佳。我们的Veritas在不同OOD场景中均取得显著提升，并能提供透明可靠的检测结果输出。</p>
<div class="markdown-heading"><h2 class="heading-element">重新思考Transformer连接性：TLinFormer，通往精确、全上下文感知线性注意力的路径</h2><a id="user-content-重新思考transformer连接性tlinformer通往精确全上下文感知线性注意力的路径" class="anchor" aria-label="Permalink: 重新思考Transformer连接性：TLinFormer，通往精确、全上下文感知线性注意力的路径" href="#重新思考transformer连接性tlinformer通往精确全上下文感知线性注意力的路径"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Transformer架构已成为现代人工智能的基石，但其核心自注意力机制存在随序列长度呈二次方增长的复杂度瓶颈，严重限制了其在长序列任务中的应用。为应对这一挑战，现有线性注意力方法通常依赖与数据无关的核近似或受限的上下文选择，以牺牲模型性能为代价。本文回归连接主义的第一性原理，从信息流的拓扑结构出发，提出新型线性注意力架构——\textbf{TLinFormer}。通过重构神经元连接模式，TLinFormer在计算精确注意力分数并确保信息流感知完整历史上下文的同时，实现了严格的线性复杂度。该设计旨在弥合现有高效注意力方法与标准注意力之间普遍存在的性能差距。通过系列实验，我们系统评估了TLinFormer在长序列推理任务中相对标准Transformer基线的性能表现。结果表明，TLinFormer在\textbf{推理延迟}、\textbf{KV缓存效率}、\textbf{内存占用}和\textbf{整体加速比}等关键指标上均展现出压倒性优势。</p>
<div class="markdown-heading"><h2 class="heading-element">逆转咒语：通过秩一安全注入实现轻量级对齐增强</h2><a id="user-content-逆转咒语通过秩一安全注入实现轻量级对齐增强" class="anchor" aria-label="Permalink: 逆转咒语：通过秩一安全注入实现轻量级对齐增强" href="#逆转咒语通过秩一安全注入实现轻量级对齐增强"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>大型语言模型（LLMs）的安全对齐通常涉及通过调节内部表征来拒绝有害请求。最新研究表明，通过消融或移除模型内特定表征方向，这些安全机制可能被绕过。本文提出一种反向策略：秩一安全注入（ROSI）——一种白盒方法，通过永久性地将模型激活导向拒绝调解子空间来增强模型的安全对齐能力。ROSI作为一种无需微调的简单秩一权重修改方案，可应用于所有残差流写入矩阵。所需的安全方向可通过少量有害与无害指令对计算得出。实验表明，经Llama Guard 3评估，ROSI能持续提升安全拒绝率，同时在MMLU、HellaSwag和Arc等标准基准测试中保持模型性能。此外，ROSI还能通过放大"未审查"模型自身潜在的安全方向实现重新对齐，证明其可作为有效的最终安全处理程序。研究结果表明，定向可解释的权重调控是一种低成本、高效能的LLM安全增强机制，可对资源密集型的微调范式形成有力补充。</p>
<div class="markdown-heading"><h2 class="heading-element">CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型</h2><a id="user-content-cogvla通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型" class="anchor" aria-label="Permalink: CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型" href="#cogvla通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>基于预训练视觉语言模型（VLA）构建的近期视觉-语言-动作模型需要大量后训练，导致计算开销高昂，限制了可扩展性与部署能力。我们提出CogVLA——一种认知对齐的视觉-语言-动作框架，通过指令驱动路由与稀疏化技术同步提升效率与性能。该框架受人类多模态协调机制启发，采用三阶段渐进式架构：1）基于编码器-FiLM的聚合路由（EFA-Routing）将指令信息注入视觉编码器，选择性聚合压缩双流视觉标记，形成指令感知的潜在表征；2）基于LLM-FiLM的剪枝路由（LFP-Routing）在此紧凑视觉编码基础上，通过剪除指令无关的视觉接地标记，将动作意图引入语言模型，实现标记级稀疏化；3）为确保压缩后的感知输入仍能支持精准连贯的动作生成，我们提出视觉-语言-动作耦合注意力机制（CAtten），融合因果视觉语言注意力与双向动作并行解码。在LIBERO基准测试和真实机器人任务上的大量实验表明，CogVLA以97.4%和70.0%的成功率实现最先进性能，同时相比OpenVLA降低2.5倍训练成本并减少2.8倍推理延迟。CogVLA已在<a href="https://github.com/JiuTian-VL/CogVLA%E5%BC%80%E6%BA%90%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/JiuTian-VL/CogVLA开源发布。</a></p>
<div class="markdown-heading"><h2 class="heading-element">自适应训练与展开方法实现深度与精度可扩展的自组合神经算子</h2><a id="user-content-自适应训练与展开方法实现深度与精度可扩展的自组合神经算子" class="anchor" aria-label="Permalink: 自适应训练与展开方法实现深度与精度可扩展的自组合神经算子" href="#自适应训练与展开方法实现深度与精度可扩展的自组合神经算子"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在本研究中，我们提出了一种通过自组合提升神经算子效率与准确性的创新框架，兼具理论保证与实践优势。受数值偏微分方程（PDE）迭代求解方法的启发，我们通过重复应用单一神经算子模块构建特定神经算子，在不显式增加新模块的情况下逐步深化模型，从而提升模型容量。为高效训练这些模型，我们引入自适应训练与展开策略，在训练过程中逐步增加神经算子深度。该方法揭示了模型深度与精度的缩放规律，并通过自适应训练策略实现显著的计算效率提升。我们的架构在标准基准测试中实现了最先进（SOTA）性能，并在一项具有挑战性的高频超声计算机断层扫描（USCT）问题上验证了其有效性——采用多重网格启发式主干网络的结构在解析复杂波动现象方面展现出卓越性能。该框架为大规模数据驱动的科学机器学习应用提供了计算可处理、精确且可扩展的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">CoFormer：与异构边缘设备协同实现可扩展Transformer推理</h2><a id="user-content-coformer与异构边缘设备协同实现可扩展transformer推理" class="anchor" aria-label="Permalink: CoFormer：与异构边缘设备协同实现可扩展Transformer推理" href="#coformer与异构边缘设备协同实现可扩展transformer推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Transformer模型的卓越性能推动了智能应用在资源受限的边缘设备上的部署。然而，由于这类模型巨大的计算需求和资源消耗，如何为实时边缘系统提供高质量服务成为重大挑战。现有方案通常将Transformer计算任务卸载到其他设备，或直接在单个边缘设备上部署压缩模型，但这些策略要么产生显著通信开销，要么导致精度与效率间的次优权衡。为应对这些挑战，我们提出名为CoFormer的通用Transformer协同推理系统，其核心思想是利用Transformer的可分割性与可集成性：将现成的大型Transformer分解为多个小型模型进行分布式推理，并通过聚合中间结果生成最终输出。我们构建了优化问题以在异构硬件约束下最小化推理延迟和精度损失，提出DeBo算法先求解分解策略，再通过渐进式校准恢复模型性能。实验证明该系统能支持多种Transformer模型在异构边缘设备上运行，使大型Transformer模型的推理速度提升最高达3.1倍。值得注意的是，CoFormer实现了参数量达16亿的GPT2-XL模型在边缘设备上的高效推理，内存需求降低76.3%，同时能在保持满意推理性能的前提下降低约40%的能耗。</p>
<div class="markdown-heading"><h2 class="heading-element">LeMat-Traj：用于原子建模的可扩展统一材料轨迹数据集</h2><a id="user-content-lemat-traj用于原子建模的可扩展统一材料轨迹数据集" class="anchor" aria-label="Permalink: LeMat-Traj：用于原子建模的可扩展统一材料轨迹数据集" href="#lemat-traj用于原子建模的可扩展统一材料轨迹数据集"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>精确机器学习原子间势函数（MLIPs）的发展受到量子力学轨迹数据集碎片化可用性及格式不一致的限制——这些数据集源自密度泛函理论（DFT）计算，生成成本高昂却因格式、元数据和可访问性的差异难以整合。为此，我们推出LeMat-Traj精选数据集，聚合来自Materials Project、Alexandria和OQMD等大型数据库的超过1.2亿个原子构型。该数据集统一了数据表示规范，协调了不同DFT泛函（PBE、PBESol、SCAN、r2SCAN）的计算结果，并筛选出高质量构型，显著降低了训练可迁移且精确的MLIPs的门槛。</p>
<p>LeMat-Traj同时涵盖弛豫低能态与高能高受力结构，有效补充了分子动力学和主动学习数据集。通过使用LeMat-Traj对高受力数据预训练模型进行微调，我们在弛豫任务中的受力预测误差显著降低。我们还开发了模块化可扩展的开源库LeMaterial-Fetcher，为学界提供可复现框架，支持便捷集成新数据源并推动大规模材料数据集的持续演进。LeMat-Traj与LeMaterial-Fetcher已公开于<a href="https://huggingface.co/datasets/LeMaterial/LeMat-Traj" rel="nofollow">https://huggingface.co/datasets/LeMaterial/LeMat-Traj</a> 与 <a href="https://github.com/LeMaterial/lematerial-fetcher%E3%80%82">https://github.com/LeMaterial/lematerial-fetcher。</a></p>
<div class="markdown-heading"><h2 class="heading-element">FORGE：基于图嵌入的基础优化表示</h2><a id="user-content-forge基于图嵌入的基础优化表示" class="anchor" aria-label="Permalink: FORGE：基于图嵌入的基础优化表示" href="#forge基于图嵌入的基础优化表示"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>组合优化问题在科学与工程领域无处不在，但基于学习的加速求解方法通常需要解决大量难解优化实例以收集训练数据，导致显著的计算开销。现有方法需为每个下游任务针对特定问题分布训练专用模型，严重限制了其可扩展性与泛化能力。本研究提出Forge方法，通过在大规模多样化混合整数规划（MIP）实例集合上以无监督方式预训练向量量化图自编码器，且不依赖于问题解。向量量化过程生成的离散代码分配可作为表示优化实例的词汇表。我们在监督与无监督两种设置下评估该方法：无监督场景中，Forge嵌入能有效区分和聚类未见过的实例；监督场景中，通过微调Forge嵌入，单个模型即可预测多类问题分布中的热启动变量和割生成所需的整性间隙。这两类预测均有助提升业界领先商业优化求解器的性能。我们已在<a href="https://github.com/skadio/forge/">https://github.com/skadio/forge/</a> 开源代码与预训练权重，以促进实例级MIP嵌入的进一步研究与实践应用。</p>
<div class="markdown-heading"><h2 class="heading-element">医学图像分类中的双模型权重选择与自知识蒸馏</h2><a id="user-content-医学图像分类中的双模型权重选择与自知识蒸馏" class="anchor" aria-label="Permalink: 医学图像分类中的双模型权重选择与自知识蒸馏" href="#医学图像分类中的双模型权重选择与自知识蒸馏"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>我们提出了一种新颖的医学图像分类方法，该方法将双模型权重选择与自知识蒸馏（SKD）技术相结合。在实际医疗环境中，大规模模型的部署常受限于计算资源约束，这为其实际应用带来了重大挑战。因此，开发在保持计算效率的同时能达到与大规模模型相当性能的轻量级模型至关重要。为此，我们采用双模型权重选择策略：首先基于大型预训练模型的权重初始化两个轻量级模型，实现有效的知识迁移；随后对这些选定的模型应用自知识蒸馏技术，使其能够利用广泛的初始权重配置而无需承担过多额外计算成本，最后针对目标分类任务进行微调。通过将双模型权重选择与自知识蒸馏相结合，我们的方法克服了传统方案在紧凑模型中难以保留关键信息的局限性。在公开数据集（胸部X光图像、肺部CT扫描和脑部MRI扫描）上进行的大量实验表明，相较于现有方法，我们提出的方案具有更优越的性能和鲁棒性。</p>
<div class="markdown-heading"><h2 class="heading-element">阿玛迪乌斯：具备双向属性建模的自回归符号音乐模型</h2><a id="user-content-阿玛迪乌斯具备双向属性建模的自回归符号音乐模型" class="anchor" aria-label="Permalink: 阿玛迪乌斯：具备双向属性建模的自回归符号音乐模型" href="#阿玛迪乌斯具备双向属性建模的自回归符号音乐模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>现有的先进符号音乐生成模型主要采用自回归或分层自回归架构，将符号音乐建模为具有单向时间依赖性的属性标记序列，其前提是假设这些属性之间存在固定且严格的依赖结构。然而，我们观察到在这些模型中使用不同属性作为初始标记都能产生相当的性能表现。这表明音符属性本质上是一个并发且无序的集合，而非时间依赖序列。基于此发现，我们推出Amadeus——一种创新的符号音乐生成框架。该框架采用双层架构：用于音符序列的自回归模型和用于属性的双向离散扩散模型。为提升性能，我们提出音乐潜在空间可辨别性增强策略（MLSDES），通过引入对比学习约束来增强中间音乐表征的区分度。条件信息增强模块（CIEM）则通过注意力机制同步强化音符潜在向量表征，实现更精确的音符解码。我们在无条件生成和文本条件生成任务上进行了广泛实验，Amadeus在多项指标上显著超越现有最优模型，同时实现至少4倍的加速效果。此外，我们还验证了使用该模型实现免训练的细粒度音符属性控制的可行性。为探索Amadeus架构的性能上限，我们构建了迄今最大的开源符号音乐数据集AMD（Amadeus MIDI数据集），同时支持预训练与微调任务。</p>
<div class="markdown-heading"><h2 class="heading-element">算子学习的多项式混沌展开</h2><a id="user-content-算子学习的多项式混沌展开" class="anchor" aria-label="Permalink: 算子学习的多项式混沌展开" href="#算子学习的多项式混沌展开"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>算子学习（OL）已成为科学机器学习（SciML）中近似无限维函数空间映射的强大工具，其主要应用之一是学习偏微分方程（PDE）的解算子。尽管该领域的进展主要由深度神经网络方法（如深度算子网络DeepONet和傅里叶神经算子FNO）推动，但近期研究开始探索传统机器学习方法在OL中的应用。本文引入多项式混沌展开（PCE）作为OL方法——PCE长期以来广泛应用于不确定性量化（UQ）领域，最近在SciML背景下备受关注。针对OL任务，我们建立了数学框架使PCE能够在纯数据驱动和物理信息嵌入两种场景中实现算子近似，该框架将算子学习任务转化为求解PCE系数方程组的问题。此外，通过简单后处理PCE系数即可实现UQ，无需任何额外计算成本。我们将所提方法应用于多种PDE问题以验证其能力，数值结果表明该方法在OL和UQ任务中均表现优异，兼具卓越的数值精度与计算效率。</p>
<div class="markdown-heading"><h2 class="heading-element">CoCoL：一种面向多机器人系统的高效通信分散协作方法</h2><a id="user-content-cocol一种面向多机器人系统的高效通信分散协作方法" class="anchor" aria-label="Permalink: CoCoL：一种面向多机器人系统的高效通信分散协作方法" href="#cocol一种面向多机器人系统的高效通信分散协作方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>协同学习能提升多机器人系统在复杂任务中的性能与适应能力，但由于多机器人任务固有的高通信开销和数据异构性，该方法面临重大挑战。为此，我们提出CoCoL——一种专为具有异构本地数据集的多机器人系统设计的通信高效去中心化协同学习方法。该方法基于镜像下降框架，通过捕捉机器人目标函数之间的相似性实现近似牛顿型更新，显著提升通信效率，并采用非精确子问题求解降低计算成本。此外，梯度追踪机制的融入确保了算法对数据异构性的鲁棒性。在三个典型多机器人协同学习任务上的实验结果表明，CoCoL在保持顶尖精度的同时，能显著减少通信轮数和总带宽消耗。这些优势在涉及非独立同分布数据、流数据以及时变网络拓扑的挑战性场景中尤为突出。</p>
<div class="markdown-heading"><h2 class="heading-element">ASR命名实体纠错的生成式标注</h2><a id="user-content-asr命名实体纠错的生成式标注" class="anchor" aria-label="Permalink: ASR命名实体纠错的生成式标注" href="#asr命名实体纠错的生成式标注"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>端到端自动语音识别系统常因无法准确转录领域特定命名实体，导致下游任务出现灾难性错误。近年来涌现出诸多快速轻量的命名实体修正（NEC）模型，这些主要基于音素级编辑距离算法的模型已展现出显著成效。然而当误转录词与真实实体存在显著形态差异时，现有方法往往难以在识别假设中定位错误转录片段，从而限制了其应用范围。我们提出一种创新NEC方法，通过语音特征检索候选实体，并创新性地结合语音特征与候选实体设计生成式方案，用于标注ASR转录文本中的实体错误并用正确实体替换。该方法能有效应对词汇形态差异场景。我们使用开源及自建测试集进行验证，结果表明该NEC方法可显著提升实体准确率。我们将开源自建测试集及训练数据。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>