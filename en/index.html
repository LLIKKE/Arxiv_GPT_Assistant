<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 08/22/2025</h1><a id="user-content-personalized-daily-arxiv-papers-08222025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 08/22/2025" href="#personalized-daily-arxiv-papers-08222025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 13</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">HHNAS-AM: Hierarchical Hybrid Neural Architecture Search using Adaptive Mutation Policies</a>
<strong>Authors:</strong> Anurag Tripathi, Ajeet Kumar Singh, Rajsabi Surya, Aum Gupta, Sahiinii Lemaina Veikho, Dorien Herremans, Sudhir Bisane</p>
</li>
<li>
<p><a href="#link1">Rethinking the Potential of Layer Freezing for Efficient DNN Training</a>
<strong>Authors:</strong> Chence Yang, Ci Zhang, Lei Lu, Qitao Tan, Sheng Li, Ao Li, Xulong Tang, Shaoyi Huang, Jinzhen Wang, Guoming Li, Jundong Li, Xiaoming Zhai, Jin Lu, Geng Yuan</p>
</li>
<li>
<p><a href="#link2">Communication Efficient LLM Pre-training with SparseLoCo</a>
<strong>Authors:</strong> Amir Sarfi, Benjamin Th'erien, Joel Lidin, Eugene Belilovsky</p>
</li>
<li>
<p><a href="#link3">Inductive Domain Transfer In Misspecified Simulation-Based Inference</a>
<strong>Authors:</strong> Ortal Senouf, Antoine Wehenkel, C'edric Vincent-Cuaz, Emmanuel Abb'e, Pascal Frossard</p>
</li>
<li>
<p><a href="#link4">Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications</a>
<strong>Authors:</strong> Hamza A. Abushahla, Dara Varam, Ariel J. N. Panopio, Mohamed I. AlHajri</p>
</li>
<li>
<p><a href="#link5">Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory</a>
<strong>Authors:</strong> Siddharth Chaudhary, Bennett Browning</p>
</li>
<li>
<p><a href="#link6">Towards Source-Free Machine Unlearning</a>
<strong>Authors:</strong> Sk Miraj Ahmed, Umit Yigit Basaran, Dripta S. Raychaudhuri, Arindam Dutta, Rohit Kundu, Fahim Faisal Niloy, Basak Guler, Amit K. Roy-Chowdhury</p>
</li>
<li>
<p><a href="#link7">Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs</a>
<strong>Authors:</strong> Anastasis Kratsios, Ariel Neufeld, Philipp Schmocker</p>
</li>
<li>
<p><a href="#link8">S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner</a>
<strong>Authors:</strong> Shuang Ao, Gopal Rumchurn</p>
</li>
<li>
<p><a href="#link9">NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</a>
<strong>Authors:</strong> Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha</p>
</li>
<li>
<p><a href="#link10">DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization</a>
<strong>Authors:</strong> Jinning Yang, Wen Shi</p>
</li>
<li>
<p><a href="#link11">Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data</a>
<strong>Authors:</strong> Yonathan Guttel, Orit Moradov, Nachi Lieder, Asnat Greenstein-Messica</p>
</li>
<li>
<p><a href="#link12">Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation</a>
<strong>Authors:</strong> Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2508.14946" rel="nofollow">HHNAS-AM: Hierarchical Hybrid Neural Architecture Search using Adaptive Mutation Policies</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-hhnas-am-hierarchical-hybrid-neural-architecture-search-using-adaptive-mutation-policies-" class="anchor" aria-label="Permalink: 0. HHNAS-AM: Hierarchical Hybrid Neural Architecture Search using Adaptive Mutation Policies" href="#0-hhnas-am-hierarchical-hybrid-neural-architecture-search-using-adaptive-mutation-policies-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14946
<strong>Authors:</strong> Anurag Tripathi, Ajeet Kumar Singh, Rajsabi Surya, Aum Gupta, Sahiinii Lemaina Veikho, Dorien Herremans, Sudhir Bisane</p>
<p><strong>Abstract:</strong> arXiv:2508.14946v1 Announce Type: new  Abstract: Neural Architecture Search (NAS) has garnered significant research interest due to its capability to discover architectures superior to manually designed ones. Learning text representation is crucial for text classification and other language-related tasks. The NAS model used in text classification does not have a Hybrid hierarchical structure, and there is no restriction on the architecture structure, due to which the search space becomes very large and mostly redundant, so the existing RL models are not able to navigate the search space effectively. Also, doing a flat architecture search leads to an unorganised search space, which is difficult to traverse. For this purpose, we propose HHNAS-AM (Hierarchical Hybrid Neural Architecture Search with Adaptive Mutation Policies), a novel approach that efficiently explores diverse architectural configurations. We introduce a few architectural templates to search on which organise the search spaces, where search spaces are designed on the basis of domain-specific cues. Our method employs mutation strategies that dynamically adapt based on performance feedback from previous iterations using Q-learning, enabling a more effective and accelerated traversal of the search space. The proposed model is fully probabilistic, enabling effective exploration of the search space. We evaluate our approach on the database id (db_id) prediction task, where it consistently discovers high-performing architectures across multiple experiments. On the Spider dataset, our method achieves an 8% improvement in test accuracy over existing baselines.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2508.15033" rel="nofollow">Rethinking the Potential of Layer Freezing for Efficient DNN Training</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-rethinking-the-potential-of-layer-freezing-for-efficient-dnn-training-" class="anchor" aria-label="Permalink: 1. Rethinking the Potential of Layer Freezing for Efficient DNN Training" href="#1-rethinking-the-potential-of-layer-freezing-for-efficient-dnn-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15033
<strong>Authors:</strong> Chence Yang, Ci Zhang, Lei Lu, Qitao Tan, Sheng Li, Ao Li, Xulong Tang, Shaoyi Huang, Jinzhen Wang, Guoming Li, Jundong Li, Xiaoming Zhai, Jin Lu, Geng Yuan</p>
<p><strong>Abstract:</strong> arXiv:2508.15033v1 Announce Type: new  Abstract: With the growing size of deep neural networks and datasets, the computational costs of training have significantly increased. The layer-freezing technique has recently attracted great attention as a promising method to effectively reduce the cost of network training. However, in traditional layer-freezing methods, frozen layers are still required for forward propagation to generate feature maps for unfrozen layers, limiting the reduction of computation costs. To overcome this, prior works proposed a hypothetical solution, which caches feature maps from frozen layers as a new dataset, allowing later layers to train directly on stored feature maps. While this approach appears to be straightforward, it presents several major challenges that are severely overlooked by prior literature, such as how to effectively apply augmentations to feature maps and the substantial storage overhead introduced. If these overlooked challenges are not addressed, the performance of the caching method will be severely impacted and even make it infeasible. This paper is the first to comprehensively explore these challenges and provides a systematic solution. To improve training accuracy, we propose \textit{similarity-aware channel augmentation}, which caches channels with high augmentation sensitivity with a minimum additional storage cost. To mitigate storage overhead, we incorporate lossy data compression into layer freezing and design a \textit{progressive compression} strategy, which increases compression rates as more layers are frozen, effectively reducing storage costs. Finally, our solution achieves significant reductions in training cost while maintaining model accuracy, with a minor time overhead. Additionally, we conduct a comprehensive evaluation of freezing and compression strategies, providing insights into optimizing their application for efficient DNN training.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2508.15706" rel="nofollow">Communication Efficient LLM Pre-training with SparseLoCo</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-communication-efficient-llm-pre-training-with-sparseloco-" class="anchor" aria-label="Permalink: 2. Communication Efficient LLM Pre-training with SparseLoCo" href="#2-communication-efficient-llm-pre-training-with-sparseloco-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15706
<strong>Authors:</strong> Amir Sarfi, Benjamin Th'erien, Joel Lidin, Eugene Belilovsky</p>
<p><strong>Abstract:</strong> arXiv:2508.15706v1 Announce Type: new  Abstract: Communication-efficient distributed training algorithms have received considerable interest recently due to their benefits for training Large Language Models (LLMs) in bandwidth-constrained settings, such as across data centers and over the internet. Despite reducing communication frequency, these methods still typically require communicating a full copy of the model's gradients-resulting in a communication bottleneck even for cross-datacenter links. Furthermore, they can slightly degrade performance compared to a naive AdamW DDP baseline. While quantization and error feedback are often applied to reduce the pseudo-gradient's size, in the context of LLM pre-training, existing approaches have been unable to additionally leverage sparsification and have obtained limited quantization. In this work, we introduce SparseLoCo, a communication-efficient training algorithm for LLMs that effectively leverages Top-k sparsification and quantization to reach extreme compression ratios of up to 1-3% sparsity and 2-bit quantization while outperforming full-precision DiLoCo. Our key observations are that outer momentum can be locally approximated by an error feedback combined with aggressive sparsity and that sparse aggregation can actually improve model performance. We empirically demonstrate in a range of communication-constrained LLM training settings that SparseLoCo provides significant benefits in both performance and communication cost.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2508.15593" rel="nofollow">Inductive Domain Transfer In Misspecified Simulation-Based Inference</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-inductive-domain-transfer-in-misspecified-simulation-based-inference-" class="anchor" aria-label="Permalink: 3. Inductive Domain Transfer In Misspecified Simulation-Based Inference" href="#3-inductive-domain-transfer-in-misspecified-simulation-based-inference-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15593
<strong>Authors:</strong> Ortal Senouf, Antoine Wehenkel, C'edric Vincent-Cuaz, Emmanuel Abb'e, Pascal Frossard</p>
<p><strong>Abstract:</strong> arXiv:2508.15593v1 Announce Type: new  Abstract: Simulation-based inference (SBI) is a statistical inference approach for estimating latent parameters of a physical system when the likelihood is intractable but simulations are available. In practice, SBI is often hindered by model misspecification--the mismatch between simulated and real-world observations caused by inherent modeling simplifications. RoPE, a recent SBI approach, addresses this challenge through a two-stage domain transfer process that combines semi-supervised calibration with optimal transport (OT)-based distribution alignment. However, RoPE operates in a fully transductive setting, requiring access to a batch of test samples at inference time, which limits scalability and generalization. We propose here a fully inductive and amortized SBI framework that integrates calibration and distributional alignment into a single, end-to-end trainable model. Our method leverages mini-batch OT with a closed-form coupling to align real and simulated observations that correspond to the same latent parameters, using both paired calibration data and unpaired samples. A conditional normalizing flow is then trained to approximate the OT-induced posterior, enabling efficient inference without simulation access at test time. Across a range of synthetic and real-world benchmarks--including complex medical biomarker estimation--our approach matches or surpasses the performance of RoPE, as well as other standard SBI and non-SBI estimators, while offering improved scalability and applicability in challenging, misspecified environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2508.15008" rel="nofollow">Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-quantized-neural-networks-for-microcontrollers-a-comprehensive-review-of-methods-platforms-and-applications-" class="anchor" aria-label="Permalink: 4. Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications" href="#4-quantized-neural-networks-for-microcontrollers-a-comprehensive-review-of-methods-platforms-and-applications-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15008
<strong>Authors:</strong> Hamza A. Abushahla, Dara Varam, Ariel J. N. Panopio, Mohamed I. AlHajri</p>
<p><strong>Abstract:</strong> arXiv:2508.15008v1 Announce Type: new  Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained devices, such as microcontrollers, has introduced significant challenges in balancing model performance, computational complexity and memory constraints. Tiny Machine Learning (TinyML) addresses these issues by integrating advancements across machine learning algorithms, hardware acceleration, and software optimization to efficiently run deep neural networks on embedded systems. This survey presents a hardware-centric introduction to quantization, systematically reviewing essential quantization techniques employed to accelerate deep learning models for embedded applications. In particular, further emphasis is put on critical trade-offs among model performance and hardware capabilities. The survey further evaluates existing software frameworks and hardware platforms designed specifically for supporting QNN execution on microcontrollers. Moreover, we provide an analysis of the current challenges and an outline of promising future directions in the rapidly evolving domain of QNN deployment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2508.15099" rel="nofollow">Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-hydra-a-16b-parameter-state-space-language-model-with-sparse-attention-mixture-of-experts-and-memory-" class="anchor" aria-label="Permalink: 5. Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory" href="#5-hydra-a-16b-parameter-state-space-language-model-with-sparse-attention-mixture-of-experts-and-memory-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15099
<strong>Authors:</strong> Siddharth Chaudhary, Bennett Browning</p>
<p><strong>Abstract:</strong> arXiv:2508.15099v1 Announce Type: new  Abstract: We present Hydra as an architectural proposal for hybrid long-context language models that combine conditional computation, long-context memory mechanisms, and sparse mixture-of-experts within an approximately 1.6B parameter design envelope. Hydra integrates a Mamba-style Structured State Space Model (SSM) backbone with intermittent sparse global attention, chunk-level MoE feed-forward routing, and dual (workspace plus factual PKM) memories. We formalize the component interfaces, give transparent parameter and complexity accounting, and outline a staged curriculum intended to stably activate the parts. We accompany the specification with illustrative toy-scale prototype measurements (tens of millions of parameters on synthetic data) whose sole purpose is to demonstrate implementation feasibility and qualitative scaling behaviors (for example, long-context throughput crossover and controllable expert routing), not to claim competitive full-scale performance. We explicitly delineate assumptions and open risks (training complexity, memory utilization, specialization dynamics) and position Hydra as a blueprint to stimulate empirical follow-up rather than a finished system. By combining SSM efficiency, selective sparse attention, MoE capacity, and learnable memory, Hydra sketches a path toward modular, input-adaptive long-context language models; validating end-task gains at target scale remains future work.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2508.15127" rel="nofollow">Towards Source-Free Machine Unlearning</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-towards-source-free-machine-unlearning-" class="anchor" aria-label="Permalink: 6. Towards Source-Free Machine Unlearning" href="#6-towards-source-free-machine-unlearning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15127
<strong>Authors:</strong> Sk Miraj Ahmed, Umit Yigit Basaran, Dripta S. Raychaudhuri, Arindam Dutta, Rohit Kundu, Fahim Faisal Niloy, Basak Guler, Amit K. Roy-Chowdhury</p>
<p><strong>Abstract:</strong> arXiv:2508.15127v1 Announce Type: new  Abstract: As machine learning becomes more pervasive and data privacy regulations evolve, the ability to remove private or copyrighted information from trained models is becoming an increasingly critical requirement. Existing unlearning methods often rely on the assumption of having access to the entire training dataset during the forgetting process. However, this assumption may not hold true in practical scenarios where the original training data may not be accessible, i.e., the source-free setting. To address this challenge, we focus on the source-free unlearning scenario, where an unlearning algorithm must be capable of removing specific data from a trained model without requiring access to the original training dataset. Building on recent work, we present a method that can estimate the Hessian of the unknown remaining training data, a crucial component required for efficient unlearning. Leveraging this estimation technique, our method enables efficient zero-shot unlearning while providing robust theoretical guarantees on the unlearning performance, while maintaining performance on the remaining data. Extensive experiments over a wide range of datasets verify the efficacy of our method.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2508.14995" rel="nofollow">Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-generative-neural-operators-of-log-complexity-can-simultaneously-solve-infinitely-many-convex-programs-" class="anchor" aria-label="Permalink: 7. Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs" href="#7-generative-neural-operators-of-log-complexity-can-simultaneously-solve-infinitely-many-convex-programs-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.14995
<strong>Authors:</strong> Anastasis Kratsios, Ariel Neufeld, Philipp Schmocker</p>
<p><strong>Abstract:</strong> arXiv:2508.14995v1 Announce Type: new  Abstract: Neural operators (NOs) are a class of deep learning models designed to simultaneously solve infinitely many related problems by casting them into an infinite-dimensional space, whereon these NOs operate. A significant gap remains between theory and practice: worst-case parameter bounds from universal approximation theorems suggest that NOs may require an unrealistically large number of parameters to solve most operator learning problems, which stands in direct opposition to a slew of experimental evidence. This paper closes that gap for a specific class of {NOs}, generative {equilibrium operators} (GEOs), using (realistic) finite-dimensional deep equilibrium layers, when solving families of convex optimization problems over a separable Hilbert space $X$. Here, the inputs are smooth, convex loss functions on $X$, and outputs are the associated (approximate) solutions to the optimization problem defined by each input loss.   We show that when the input losses lie in suitable infinite-dimensional compact sets, our GEO can uniformly approximate the corresponding solutions to arbitrary precision, with rank, depth, and width growing only logarithmically in the reciprocal of the approximation error. We then validate both our theoretical results and the trainability of GEOs on three applications: (1) nonlinear PDEs, (2) stochastic optimal control problems, and (3) hedging problems in mathematical finance under liquidity constraints.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2508.15068" rel="nofollow">S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-s3lora-safe-spectral-sharpness-guided-pruning-in-adaptation-of-agent-planner-" class="anchor" aria-label="Permalink: 8. S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner" href="#8-s3lora-safe-spectral-sharpness-guided-pruning-in-adaptation-of-agent-planner-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15068
<strong>Authors:</strong> Shuang Ao, Gopal Rumchurn</p>
<p><strong>Abstract:</strong> arXiv:2508.15068v1 Announce Type: new  Abstract: Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning (PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based agents. However, these adaptations can unintentionally compromise safety alignment, leading to unsafe or unstable behaviors, particularly in agent planning tasks. Existing safety-aware adaptation methods often require access to both base and instruction-tuned model checkpoints, which are frequently unavailable in practice, limiting their applicability. We propose S3LoRA (Safe Spectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and model-independent framework that mitigates safety risks in LoRA-adapted models by inspecting only the fine-tuned weight updates. We first introduce Magnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes the structural properties of LoRA updates while preserving global magnitude information. We then design the Spectral Sharpness Index (SSI), a sharpness-aware metric to detect layers with highly concentrated and potentially unsafe updates. These layers are pruned post-hoc to reduce risk without sacrificing task performance. Extensive experiments and ablation studies across agent planning and language generation tasks show that S3LoRA consistently improves safety metrics while maintaining or improving utility metrics and significantly reducing inference cost. These results establish S3LoRA as a practical and scalable solution for safely deploying LLM-based agents in real-world, resource-constrained, and safety-critical environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2508.15693" rel="nofollow">NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-nicewebrl-a-python-library-for-human-subject-experiments-with-reinforcement-learning-environments-" class="anchor" aria-label="Permalink: 9. NiceWebRL: a Python library for human subject experiments with reinforcement learning environments" href="#9-nicewebrl-a-python-library-for-human-subject-experiments-with-reinforcement-learning-environments-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15693
<strong>Authors:</strong> Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha</p>
<p><strong>Abstract:</strong> arXiv:2508.15693v1 Announce Type: new  Abstract: We present NiceWebRL, a research tool that enables researchers to use machine reinforcement learning (RL) environments for online human subject experiments. NiceWebRL is a Python library that allows any Jax-based environment to be transformed into an online interface, supporting both single-agent and multi-agent environments. As such, NiceWebRL enables AI researchers to compare their algorithms to human performance, cognitive scientists to test ML algorithms as theories for human cognition, and multi-agent researchers to develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3 case studies that demonstrate its potential to help develop Human-like AI, Human-compatible AI, and Human-assistive AI. In the first case study (Human-like AI), NiceWebRL enables the development of a novel RL model of cognition. Here, NiceWebRL facilitates testing this model against human participants in both a grid world and Craftax, a 2D Minecraft domain. In our second case study (Human-compatible AI), NiceWebRL enables the development of a novel multi-agent RL algorithm that can generalize to human partners in the Overcooked domain. Finally, in our third case study (Human-assistive AI), we show how NiceWebRL can allow researchers to study how an LLM can assist humans on complex tasks in XLand-Minigrid, an environment with millions of hierarchical tasks. The library is available at <a href="https://github.com/KempnerInstitute/nicewebrl">https://github.com/KempnerInstitute/nicewebrl</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2508.15338" rel="nofollow">DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-diagecg-an-llm-driven-framework-for-diagnostic-reasoning-via-discretized-ecg-tokenization-" class="anchor" aria-label="Permalink: 10. DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization" href="#10-diagecg-an-llm-driven-framework-for-diagnostic-reasoning-via-discretized-ecg-tokenization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15338
<strong>Authors:</strong> Jinning Yang, Wen Shi</p>
<p><strong>Abstract:</strong> arXiv:2508.15338v1 Announce Type: new  Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet existing automated approaches often struggle to generalize across clinical tasks and offer limited support for open-ended reasoning. We present DiagECG, a novel framework that integrates time-series and language modeling by enabling large language models to process 12-lead ECG signals for clinical text generation tasks. Our approach discretizes continuous ECG embeddings into symbolic tokens using a lead-independent encoder and quantization module. These tokens are then used to extend the vocabulary of LLM, allowing the model to handle both ECG and natural language inputs in a unified manner. To bridge the modality gap, we pretrain the model on an autoregressive ECG forecasting task, enabling the LLM to model temporal dynamics using its native language modeling capabilities. Finally, we perform instruction tuning on both ECG question answering and diagnostic report generation. Without modifying the core model, DiagECG achieves strong performance across tasks while maintaining generalization to out-of-distribution settings. Extensive experiments demonstrate the effectiveness of each component and highlight the potential of integrating symbolic ECG representations into LLMs for medical reasoning.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2508.15369" rel="nofollow">Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-enhancing-forecasting-with-a-2d-time-series-approach-for-cohort-based-data-" class="anchor" aria-label="Permalink: 11. Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data" href="#11-enhancing-forecasting-with-a-2d-time-series-approach-for-cohort-based-data-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15369
<strong>Authors:</strong> Yonathan Guttel, Orit Moradov, Nachi Lieder, Asnat Greenstein-Messica</p>
<p><strong>Abstract:</strong> arXiv:2508.15369v1 Announce Type: new  Abstract: This paper introduces a novel two-dimensional (2D) time series forecasting model that integrates cohort behavior over time, addressing challenges in small data environments. We demonstrate its efficacy using multiple real-world datasets, showcasing superior performance in accuracy and adaptability compared to reference models. The approach offers valuable insights for strategic decision-making across industries facing financial and marketing forecasting challenges.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2508.15427" rel="nofollow">Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-lang2lift-a-framework-for-language-guided-pallet-detection-and-pose-estimation-integrated-in-autonomous-outdoor-forklift-operation-" class="anchor" aria-label="Permalink: 12. Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation" href="#12-lang2lift-a-framework-for-language-guided-pallet-detection-and-pose-estimation-integrated-in-autonomous-outdoor-forklift-operation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.15427
<strong>Authors:</strong> Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi</p>
<p><strong>Abstract:</strong> arXiv:2508.15427v1 Announce Type: new  Abstract: The logistics and construction industries face persistent challenges in automating pallet handling, especially in outdoor environments with variable payloads, inconsistencies in pallet quality and dimensions, and unstructured surroundings. In this paper, we tackle automation of a critical step in pallet transport: the pallet pick-up operation. Our work is motivated by labor shortages, safety concerns, and inefficiencies in manually locating and retrieving pallets under such conditions. We present Lang2Lift, a framework that leverages foundation models for natural language-guided pallet detection and 6D pose estimation, enabling operators to specify targets through intuitive commands such as "pick up the steel beam pallet near the crane." The perception pipeline integrates Florence-2 and SAM-2 for language-grounded segmentation with FoundationPose for robust pose estimation in cluttered, multi-pallet outdoor scenes under variable lighting. The resulting poses feed into a motion planning module for fully autonomous forklift operation. We validate Lang2Lift on the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet segmentation accuracy on a real-world test dataset. Timing and error analysis demonstrate the system's robustness and confirm its feasibility for deployment in operational logistics and construction environments. Video demonstrations are available at <a href="https://eric-nguyen1402.github.io/lang2lift.github.io/" rel="nofollow">https://eric-nguyen1402.github.io/lang2lift.github.io/</a></p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>