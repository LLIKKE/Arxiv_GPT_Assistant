<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">MNN-LLM：面向移动设备快速部署大型语言模型的通用推理引擎</h2><a id="user-content-mnn-llm面向移动设备快速部署大型语言模型的通用推理引擎" class="anchor" aria-label="Permalink: MNN-LLM：面向移动设备快速部署大型语言模型的通用推理引擎" href="#mnn-llm面向移动设备快速部署大型语言模型的通用推理引擎"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.10443v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）已在多项任务中展现出卓越性能，但其庞大规模导致推理过程中消耗大量计算资源，成本高昂。因此，边缘设备推理成为颇具前景的解决方案。边缘推理的主要挑战在于内存占用与推理速度。本文提出MNN-LLM框架，专为加速大语言模型在移动端部署而设计。该框架通过模型量化和DRAM-Flash混合存储技术应对LLMs的运行时特性，显著降低内存使用；基于移动端CPU指令集与GPU特性重组权重与输入数据，同时采用多核负载均衡、混合精度浮点运算及几何计算等策略提升性能。值得注意的是，MNN-LLM相比当前主流LLM专用框架最高可实现8.6倍的加速比。</p>
<p>（译文说明：</p>
<ol>
<li>专业术语保留英文缩写如LLMs/DRAM-Flash，首次出现时标注中文全称</li>
<li>技术概念如"quantization"译为"量化"符合计算机领域惯例</li>
<li>长句拆分重组，如将原文复合状语结构转换为分号连接的并列句式</li>
<li>被动语态转换为主动表述，如"are rearranged"译为"重组"</li>
<li>关键性能指标"8.6x speed increase"采用"8.6倍加速比"的行业标准表述</li>
<li>保持学术论文摘要的客观严谨风格，避免口语化表达）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">AWP：基于激活感知的权重剪枝与投影梯度下降量化</h2><a id="user-content-awp基于激活感知的权重剪枝与投影梯度下降量化" class="anchor" aria-label="Permalink: AWP：基于激活感知的权重剪枝与投影梯度下降量化" href="#awp基于激活感知的权重剪枝与投影梯度下降量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2506.10205v1 公告类型：新研究<br>
摘要：为解决大语言模型（LLMs）参数量庞大的问题，模型压缩技术（如量化和剪枝）常被采用，尤其在边缘设备上。本研究聚焦于逐层训练后量化和剪枝。通过建立激活感知权重剪枝与稀疏近似问题之间的联系，并受迭代硬阈值法（IHT）成功的启发，我们提出了一种基于投影梯度下降的统一方法——激活感知权重剪枝与量化联合优化（AWP）。实验表明，AWP在LLM剪枝与量化任务上超越了现有最优方法。同时，我们还为所提剪枝方法提供了理论收敛性保证。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"post-training quantization"译为"训练后量化"（业界通用译法）</li>
<li>"Iterative Hard Thresholding"保留英文缩写IHT但首次出现时给出全称"迭代硬阈值法"</li>
<li>"Projected gradient descent"译为"投影梯度下降"（数学优化领域标准译名）</li>
<li>保持"activation-aware"为"激活感知"的连贯性</li>
<li>将"state-of-the-art"译为"现有最优"以符合中文论文表述习惯）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>