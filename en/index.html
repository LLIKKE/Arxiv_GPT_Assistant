<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/07/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10072025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/07/2025" href="#personalized-daily-arxiv-papers-10072025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 46</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion</a>
<strong>Authors:</strong> Yanlong Wang, Hang Yu, Jian Xu, Fei Ma, Hongkang Zhang, Tongtong Feng, Zijian Zhang, Shao-Lun Huang, Danny Dongning Sun, Xiao-Ping Zhang</p>
</li>
<li>
<p><a href="#link1">Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs</a>
<strong>Authors:</strong> Raghav Sharma, Manan Mehta</p>
</li>
<li>
<p><a href="#link2">OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows</a>
<strong>Authors:</strong> John Nguyen, Marton Havasi, Tariq Berrada, Luke Zettlemoyer, Ricky T. Q. Chen</p>
</li>
<li>
<p><a href="#link3">Bridging the Gap Between Multimodal Foundation Models and World Models</a>
<strong>Authors:</strong> Xuehai He</p>
</li>
<li>
<p><a href="#link4">LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0</a>
<strong>Authors:</strong> Jinbo Wen, Jiawen Kang, Linfeng Zhang, Xiaoying Tang, Jianhang Tang, Yang Zhang, Zhaohui Yang, Dusit Niyato</p>
</li>
<li>
<p><a href="#link5">ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs</a>
<strong>Authors:</strong> Wonjun Kang, Kevin Galim, Seunghyuk Oh, Minjae Lee, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Yuezhou Hu, Hyung Il Koo, Nam Ik Cho, Kangwook Lee</p>
</li>
<li>
<p><a href="#link6">Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility</a>
<strong>Authors:</strong> Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang</p>
</li>
<li>
<p><a href="#link7">PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression</a>
<strong>Authors:</strong> Di Zhang</p>
</li>
<li>
<p><a href="#link8">KVComm: Enabling Efficient LLM Communication through Selective KV Sharing</a>
<strong>Authors:</strong> Xiangyu Shi, Marco Chiesa, Gerald Q. Maguire Jr., Dejan Kostic</p>
</li>
<li>
<p><a href="#link9">Rare Text Semantics Were Always There in Your Diffusion Transformer</a>
<strong>Authors:</strong> Seil Kang, Woojung Han, Dayun Ju, Seong Jae Hwang</p>
</li>
<li>
<p><a href="#link10">Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation</a>
<strong>Authors:</strong> Renrong Shao, Wei Zhang, Jun wang</p>
</li>
<li>
<p><a href="#link11">The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning</a>
<strong>Authors:</strong> Mayank Ravishankara, Varindra V. Persad Maharaj</p>
</li>
<li>
<p><a href="#link12">Expand Neurons, Not Parameters</a>
<strong>Authors:</strong> Linghao Kong, Inimai Subramanian, Yonadav Shavit, Micah Adler, Dan Alistarh, Nir Shavit</p>
</li>
<li>
<p><a href="#link13">Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention</a>
<strong>Authors:</strong> Haiquan Qiu, Quanming Yao</p>
</li>
<li>
<p><a href="#link14">Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation</a>
<strong>Authors:</strong> Yongfu Xue</p>
</li>
<li>
<p><a href="#link15">HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks</a>
<strong>Authors:</strong> Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson</p>
</li>
<li>
<p><a href="#link16">Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing</a>
<strong>Authors:</strong> Xuanhua Yin, Runkai Zhao, Weidong Cai</p>
</li>
<li>
<p><a href="#link17">Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models</a>
<strong>Authors:</strong> Tianao Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong Guo, Yulun Zhang</p>
</li>
<li>
<p><a href="#link18">UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</a>
<strong>Authors:</strong> Yizhuo Ding, Wanying Qu, Jiawei Geng, Wenqi Shao, Yanwei Fu</p>
</li>
<li>
<p><a href="#link19">CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models</a>
<strong>Authors:</strong> Dongqi Zheng, Wenjin Fu</p>
</li>
<li>
<p><a href="#link20">COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability</a>
<strong>Authors:</strong> Yizhuo Ding, Mingkang Chen, Qiuhua Liu, Fenghua Weng, Wanying Qu, Yue Yang, Yugang Jiang, Zuxuan Wu, Yanwei Fu, Wenqi Shao</p>
</li>
<li>
<p><a href="#link21">MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering</a>
<strong>Authors:</strong> Chenlu Ding, Jiancan Wu, Leheng Sheng, Fan Zhang, Yancheng Yuan, Xiang Wang, Xiangnan He</p>
</li>
<li>
<p><a href="#link22">ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering</a>
<strong>Authors:</strong> Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso</p>
</li>
<li>
<p><a href="#link23">PT$^2$-LLM: Post-Training Ternarization for Large Language Models</a>
<strong>Authors:</strong> Xianglong Yan, Chengzhu Bao, Zhiteng Li, Tianao Zhang, Kaicheng Yang, Haotong Qin, Ruobing Xie, Xingwu Sun, Yulun Zhang</p>
</li>
<li>
<p><a href="#link24">Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments</a>
<strong>Authors:</strong> Vinay Venkatesh, Vamsidhar R Kamanuru, Lav Kumar, Nikita Kothari</p>
</li>
<li>
<p><a href="#link25">SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size</a>
<strong>Authors:</strong> Junhao Xia, Ming Zhao, Limin Xiao, Xiujun Zhang</p>
</li>
<li>
<p><a href="#link26">Boomerang Distillation Enables Zero-Shot Model Size Interpolation</a>
<strong>Authors:</strong> Sara Kangaslahti, Nihal V. Nayak, Jonathan Geuter, Marco Fumero, Francesco Locatello, David Alvarez-Melis</p>
</li>
<li>
<p><a href="#link27">Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data</a>
<strong>Authors:</strong> Jiancheng Zhang, Yinglun Zhu</p>
</li>
<li>
<p><a href="#link28">Cross-Modal Content Optimization for Steering Web Agent Preferences</a>
<strong>Authors:</strong> Tanqiu Jiang, Min Bai, Nikolaos Pappas, Yanjun Qi, Sandesh Swamy</p>
</li>
<li>
<p><a href="#link29">Think Then Embed: Generative Context Improves Multimodal Embedding</a>
<strong>Authors:</strong> Xuanming Cui, Jianpeng Cheng, Hong-you Chen, Satya Narayan Shukla, Abhijeet Awasthi, Xichen Pan, Chaitanya Ahuja, Shlok Kumar Mishra, Qi Guo, Ser-Nam Lim, Aashu Singh, Xiangjun Fan</p>
</li>
<li>
<p><a href="#link30">Distributed Low-Communication Training with Decoupled Momentum Optimization</a>
<strong>Authors:</strong> Sasho Nedelkoski, Alexander Acker, Odej Kao, Soeren Becker, Dominik Scheinert</p>
</li>
<li>
<p><a href="#link31">Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation</a>
<strong>Authors:</strong> Guofu Xie, Chen Zhang, Xiao Zhang, Yunsheng Shi, Ting Yao, Jun Xu</p>
</li>
<li>
<p><a href="#link32">Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer</a>
<strong>Authors:</strong> Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Li Yang Ku, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Amaris Paryag, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Peter Pastor Sampedro, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, Ren'e Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou, Yuxiang Zhou</p>
</li>
<li>
<p><a href="#link33">More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models</a>
<strong>Authors:</strong> Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo</p>
</li>
<li>
<p><a href="#link34">Post-training quantization of vision encoders needs prefixing registers</a>
<strong>Authors:</strong> Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee</p>
</li>
<li>
<p><a href="#link35">Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval</a>
<strong>Authors:</strong> Mallikarjuna Tupakula</p>
</li>
<li>
<p><a href="#link36">Compressed Concatenation of Small Embedding Models</a>
<strong>Authors:</strong> Mohamed Ayoub Ben Ayad, Michael Dinzinger, Kanishka Ghosh Dastidar, Jelena Mitrovic, Michael Granitzer</p>
</li>
<li>
<p><a href="#link37">Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment</a>
<strong>Authors:</strong> Lingjie Yi, Raphael Douady, Chao Chen</p>
</li>
<li>
<p><a href="#link38">Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework</a>
<strong>Authors:</strong> Hao Gu, Vibhas Nair, Amrithaa Ashok Kumar, Jayvart Sharma, Ryan Lagasse</p>
</li>
<li>
<p><a href="#link39">ContextNav: Towards Agentic Multimodal In-Context Learning</a>
<strong>Authors:</strong> Honghao Fu, Yuan Ouyang, Kai-Wei Chang, Yiwei Wang, Zi Huang, Yujun Cai</p>
</li>
<li>
<p><a href="#link40">StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory</a>
<strong>Authors:</strong> Xinyuan Song, Guangji Bai, Liang Zhao</p>
</li>
<li>
<p><a href="#link41">ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context</a>
<strong>Authors:</strong> Huiwon Jang, Sihyun Yu, Heeseung Kwon, Hojin Jeon, Younggyo Seo, Jinwoo Shin</p>
</li>
<li>
<p><a href="#link42">GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction</a>
<strong>Authors:</strong> Zhuangzhi Gao, Hongyi Qin, He Zhao, Qinkai Yu, Feixiang Zhou, Eduard Shantsila, Uazman Alam, Alena Shantsila, Wahbi El-Bouri, Gregory Y. H. Lip, Yalin Zheng</p>
</li>
<li>
<p><a href="#link43">Staircase Streaming for Low-Latency Multi-Agent Inference</a>
<strong>Authors:</strong> Junlin Wang (Zach), Jue Wang (Zach), Zhen (Zach), Xu, Ben Athiwaratkun, Bhuwan Dhingra, Ce Zhang, James Zou</p>
</li>
<li>
<p><a href="#link44">Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices</a>
<strong>Authors:</strong> Congzheng Song, Xinyu Tang</p>
</li>
<li>
<p><a href="#link45">DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks</a>
<strong>Authors:</strong> Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.03244" rel="nofollow">VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-vifo-visual-feature-empowered-multivariate-time-series-forecasting-with-cross-modal-fusion-" class="anchor" aria-label="Permalink: 0. VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion" href="#0-vifo-visual-feature-empowered-multivariate-time-series-forecasting-with-cross-modal-fusion-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03244
<strong>Authors:</strong> Yanlong Wang, Hang Yu, Jian Xu, Fei Ma, Hongkang Zhang, Tongtong Feng, Zijian Zhang, Shao-Lun Huang, Danny Dongning Sun, Xiao-Ping Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.03244v1 Announce Type: new  Abstract: Large time series foundation models often adopt channel-independent architectures to handle varying data dimensions, but this design ignores crucial cross-channel dependencies. Concurrently, existing multimodal approaches have not fully exploited the power of large vision models (LVMs) to interpret spatiotemporal data. Additionally, there remains significant unexplored potential in leveraging the advantages of information extraction from different modalities to enhance time series forecasting performance. To address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO uniquely renders multivariate time series into image, enabling pre-trained LVM to extract complex cross-channel patterns that are invisible to channel-independent models. These visual features are then aligned and fused with representations from the time series modality. By freezing the LVM and training only 7.45% of its parameters, VIFO achieves competitive performance on multiple benchmarks, offering an efficient and effective solution for capturing cross-variable relationships in</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.03847" rel="nofollow">Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-small-language-models-for-agentic-systems-a-survey-of-architectures-capabilities-and-deployment-trade-offs-" class="anchor" aria-label="Permalink: 1. Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs" href="#1-small-language-models-for-agentic-systems-a-survey-of-architectures-capabilities-and-deployment-trade-offs-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03847
<strong>Authors:</strong> Raghav Sharma, Manan Mehta</p>
<p><strong>Abstract:</strong> arXiv:2510.03847v1 Announce Type: new  Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance.   Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.03506" rel="nofollow">OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-oneflow-concurrent-mixed-modal-and-interleaved-generation-with-edit-flows-" class="anchor" aria-label="Permalink: 2. OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows" href="#2-oneflow-concurrent-mixed-modal-and-interleaved-generation-with-edit-flows-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03506
<strong>Authors:</strong> John Nguyen, Marton Havasi, Tariq Berrada, Luke Zettlemoyer, Ricky T. Q. Chen</p>
<p><strong>Abstract:</strong> arXiv:2510.03506v1 Announce Type: new  Abstract: We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.03727" rel="nofollow">Bridging the Gap Between Multimodal Foundation Models and World Models</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-bridging-the-gap-between-multimodal-foundation-models-and-world-models-" class="anchor" aria-label="Permalink: 3. Bridging the Gap Between Multimodal Foundation Models and World Models" href="#3-bridging-the-gap-between-multimodal-foundation-models-and-world-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03727
<strong>Authors:</strong> Xuehai He</p>
<p><strong>Abstract:</strong> arXiv:2510.03727v1 Announce Type: new  Abstract: Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.04765" rel="nofollow">LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-lmm-incentive-large-multimodal-model-based-incentive-design-for-user-generated-content-in-web-30-" class="anchor" aria-label="Permalink: 4. LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0" href="#4-lmm-incentive-large-multimodal-model-based-incentive-design-for-user-generated-content-in-web-30-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04765
<strong>Authors:</strong> Jinbo Wen, Jiawen Kang, Linfeng Zhang, Xiaoying Tang, Jianhang Tang, Yang Zhang, Zhaohui Yang, Dusit Niyato</p>
<p><strong>Abstract:</strong> arXiv:2510.04765v1 Announce Type: new  Abstract: Web 3.0 represents the next generation of the Internet, which is widely recognized as a decentralized ecosystem that focuses on value expression and data ownership. By leveraging blockchain and artificial intelligence technologies, Web 3.0 offers unprecedented opportunities for users to create, own, and monetize their content, thereby enabling User-Generated Content (UGC) to an entirely new level. However, some self-interested users may exploit the limitations of content curation mechanisms and generate low-quality content with less effort, obtaining platform rewards under information asymmetry. Such behavior can undermine Web 3.0 performance. To this end, we propose \textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based contract-theoretic model to motivate users to generate high-quality UGC, thereby mitigating the adverse selection problem from information asymmetry. To alleviate potential moral hazards after contract selection, we leverage LMM agents to evaluate UGC quality, which is the primary component of the contract, utilizing prompt engineering techniques to improve the evaluation performance of LMM agents. Recognizing that traditional contract design methods cannot effectively adapt to the dynamic environment of Web 3.0, we develop an improved Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for optimal contract design. Simulation results demonstrate the superiority of the proposed MoE-based PPO algorithm over representative benchmarks in the context of contract design. Finally, we deploy the designed contract within an Ethereum smart contract framework, further validating the effectiveness of the proposed scheme.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.04767" rel="nofollow">ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-parallelbench-understanding-the-trade-offs-of-parallel-decoding-in-diffusion-llms-" class="anchor" aria-label="Permalink: 5. ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" href="#5-parallelbench-understanding-the-trade-offs-of-parallel-decoding-in-diffusion-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04767
<strong>Authors:</strong> Wonjun Kang, Kevin Galim, Seunghyuk Oh, Minjae Lee, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Yuezhou Hu, Hyung Il Koo, Nam Ik Cho, Kangwook Lee</p>
<p><strong>Abstract:</strong> arXiv:2510.04767v1 Announce Type: new  Abstract: While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.03358" rel="nofollow">Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-understanding-transformers-for-time-series-rank-structure-flow-of-ranks-and-compressibility-" class="anchor" aria-label="Permalink: 6. Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility" href="#6-understanding-transformers-for-time-series-rank-structure-flow-of-ranks-and-compressibility-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03358
<strong>Authors:</strong> Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang</p>
<p><strong>Abstract:</strong> arXiv:2510.03358v1 Announce Type: new  Abstract: Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly to models trained to other modalities. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data differ remarkably from those of text or vision. We show that time-series embeddings, unlike text or vision, exhibit sharply decaying singular value spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why ranks grow with depth. Guided by these theoretical and empirical results, we use these insights to compress Chronos, a large time series foundation model, achieving a reduction of $65%$ in inference time and $81%$ in memory, without loss of accuracy. Our findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.04205" rel="nofollow">PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-polykan-a-polyhedral-analysis-framework-for-provable-and-minimal-kan-compression-" class="anchor" aria-label="Permalink: 7. PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression" href="#7-polykan-a-polyhedral-analysis-framework-for-provable-and-minimal-kan-compression-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04205
<strong>Authors:</strong> Di Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.04205v1 Announce Type: new  Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability and a strong mathematical foundation. However, their parameter efficiency remains a significant challenge for practical deployment. This paper introduces PolyKAN, a novel theoretical framework for KAN compression that provides formal guarantees on both model size reduction and approximation error. By leveraging the inherent piecewise polynomial structure of KANs, we formulate the compression problem as one of optimal polyhedral region merging. We establish a rigorous polyhedral characterization of KANs, develop a complete theory of $\epsilon$-equivalent compression, and design an optimal dynamic programming algorithm that guarantees minimal compression under specified error bounds. Our theoretical analysis demonstrates that PolyKAN achieves provably minimal compression while maintaining strict error control, with polynomial-time complexity in all network parameters. The framework provides the first formal foundation for KAN compression with mathematical guarantees, opening new directions for efficient deployment of interpretable neural architectures.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.03346" rel="nofollow">KVComm: Enabling Efficient LLM Communication through Selective KV Sharing</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-kvcomm-enabling-efficient-llm-communication-through-selective-kv-sharing-" class="anchor" aria-label="Permalink: 8. KVComm: Enabling Efficient LLM Communication through Selective KV Sharing" href="#8-kvcomm-enabling-efficient-llm-communication-through-selective-kv-sharing-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03346
<strong>Authors:</strong> Xiangyu Shi, Marco Chiesa, Gerald Q. Maguire Jr., Dejan Kostic</p>
<p><strong>Abstract:</strong> arXiv:2510.03346v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.03886" rel="nofollow">Rare Text Semantics Were Always There in Your Diffusion Transformer</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-rare-text-semantics-were-always-there-in-your-diffusion-transformer-" class="anchor" aria-label="Permalink: 9. Rare Text Semantics Were Always There in Your Diffusion Transformer" href="#9-rare-text-semantics-were-always-there-in-your-diffusion-transformer-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03886
<strong>Authors:</strong> Seil Kang, Woojung Han, Dayun Ju, Seong Jae Hwang</p>
<p><strong>Abstract:</strong> arXiv:2510.03886v1 Announce Type: new  Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiT's outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.03375" rel="nofollow">Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-conditional-pseudo-supervised-contrast-for-data-free-knowledge-distillation-" class="anchor" aria-label="Permalink: 10. Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation" href="#10-conditional-pseudo-supervised-contrast-for-data-free-knowledge-distillation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03375
<strong>Authors:</strong> Renrong Shao, Wei Zhang, Jun wang</p>
<p><strong>Abstract:</strong> arXiv:2510.03375v1 Announce Type: new  Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at <a href="https://github.com/RoryShao/CPSC-DFKD.git">https://github.com/RoryShao/CPSC-DFKD.git</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.04141" rel="nofollow">The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-the-artificial-intelligence-cognitive-examination-a-survey-on-the-evolution-of-multimodal-evaluation-from-recognition-to-reasoning-" class="anchor" aria-label="Permalink: 11. The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning" href="#11-the-artificial-intelligence-cognitive-examination-a-survey-on-the-evolution-of-multimodal-evaluation-from-recognition-to-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04141
<strong>Authors:</strong> Mayank Ravishankara, Varindra V. Persad Maharaj</p>
<p><strong>Abstract:</strong> arXiv:2510.04141v1 Announce Type: new  Abstract: This survey paper chronicles the evolution of evaluation in multimodal artificial intelligence (AI), framing it as a progression of increasingly sophisticated "cognitive examinations." We argue that the field is undergoing a paradigm shift, moving from simple recognition tasks that test "what" a model sees, to complex reasoning benchmarks that probe "why" and "how" it understands. This evolution is driven by the saturation of older benchmarks, where high performance often masks fundamental weaknesses. We chart the journey from the foundational "knowledge tests" of the ImageNet era to the "applied logic and comprehension" exams such as GQA and Visual Commonsense Reasoning (VCR), which were designed specifically to diagnose systemic flaws such as shortcut learning and failures in compositional generalization. We then survey the current frontier of "expert-level integration" benchmarks (e.g., MMBench, SEED-Bench, MMMU) designed for today's powerful multimodal large language models (MLLMs), which increasingly evaluate the reasoning process itself. Finally, we explore the uncharted territories of evaluating abstract, creative, and social intelligence. We conclude that the narrative of AI evaluation is not merely a history of datasets, but a continuous, adversarial process of designing better examinations that, in turn, redefine our goals for creating truly intelligent systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.04500" rel="nofollow">Expand Neurons, Not Parameters</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-expand-neurons-not-parameters-" class="anchor" aria-label="Permalink: 12. Expand Neurons, Not Parameters" href="#12-expand-neurons-not-parameters-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04500
<strong>Authors:</strong> Linghao Kong, Inimai Subramanian, Yonadav Shavit, Micah Adler, Dan Alistarh, Nir Shavit</p>
<p><strong>Abstract:</strong> arXiv:2510.04500v1 Announce Type: new  Abstract: This work demonstrates how increasing the number of neurons in a network without increasing its number of non-zero parameters improves performance. We show that this gain corresponds with a decrease in interference between multiple features that would otherwise share the same neurons. To reduce such entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter Expansion (FPE): replace a neuron with multiple children and partition the parent's weights disjointly across them, so that each child inherits a non-overlapping subset of connections. On symbolic tasks, specifically Boolean code problems, clause-aligned FPE systematically reduces polysemanticity metrics and yields higher task accuracy. Notably, random splits of neuron weights approximate these gains, indicating that reduced collisions, not precise assignment, are a primary driver. Consistent with the superposition hypothesis, the benefits of FPE grow with increasing interference: when polysemantic load is high, accuracy improvements are the largest. Transferring these insights to real models (classifiers over CLIP embeddings and deeper multilayer networks) we find that widening networks while maintaining a constant non-zero parameter count consistently increases accuracy. These results identify an interpretability-grounded mechanism to leverage width against superposition, improving performance without increasing the number of non-zero parameters. Such a direction is well matched to modern accelerators, where memory movement of non-zero parameters, rather than raw compute, is the dominant bottleneck.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.04212" rel="nofollow">Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-why-low-precision-transformer-training-fails-an-analysis-on-flash-attention-" class="anchor" aria-label="Permalink: 13. Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention" href="#13-why-low-precision-transformer-training-fails-an-analysis-on-flash-attention-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04212
<strong>Authors:</strong> Haiquan Qiu, Quanming Yao</p>
<p><strong>Abstract:</strong> arXiv:2510.04212v1 Announce Type: new  Abstract: The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.03731" rel="nofollow">Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-optimizing-fine-tuning-through-advanced-initialization-strategies-for-low-rank-adaptation-" class="anchor" aria-label="Permalink: 14. Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation" href="#14-optimizing-fine-tuning-through-advanced-initialization-strategies-for-low-rank-adaptation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03731
<strong>Authors:</strong> Yongfu Xue</p>
<p><strong>Abstract:</strong> arXiv:2510.03731v1 Announce Type: new  Abstract: The rapid development of parameter-efficient fine-tuning methods has noticeably improved the efficiency of adapting large language models. Among these, LoRA has gained widespread popularity due to its strong balance of effectiveness and parameter efficiency. However, LoRA relies on initializing two low-rank matrices whose product is zero, which limits its ability to effectively activate and leverage the original model weights-creating a potential bottleneck for optimal performance. To address this limitation, we propose \textbf{IniLoRA}, a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights. Experimental results indicate that IniLoRA achieves better performance than LoRA across a range of models and tasks. Additionally, we introduce two variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct initialization methods to enhance performance further.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2510.04898" rel="nofollow">HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-hypervla-efficient-inference-in-vision-language-action-models-via-hypernetworks-" class="anchor" aria-label="Permalink: 15. HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks" href="#15-hypervla-efficient-inference-in-vision-language-action-models-via-hypernetworks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04898
<strong>Authors:</strong> Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson</p>
<p><strong>Abstract:</strong> arXiv:2510.04898v1 Announce Type: new  Abstract: Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at <a href="https://github.com/MasterXiong/HyperVLA">https://github.com/MasterXiong/HyperVLA</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2510.04670" rel="nofollow">Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-improving-multimodal-brain-encoding-model-with-dynamic-subject-awareness-routing-" class="anchor" aria-label="Permalink: 16. Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing" href="#16-improving-multimodal-brain-encoding-model-with-dynamic-subject-awareness-routing-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04670
<strong>Authors:</strong> Xuanhua Yin, Runkai Zhao, Weidong Cai</p>
<p><strong>Abstract:</strong> arXiv:2510.04670v1 Announce Type: new  Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic Framework for Multimodal fMRI Response Encoding), an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating. Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from upstream fusion, while MIND combines token-dependent Top-K sparse routing with a subject prior to personalize expert usage without sacrificing generality. Experiments across multiple multimodal backbones and subjects show consistent improvements over strong baselines, enhanced cross-subject generalization, and interpretable expert patterns that correlate with content type. The framework offers a simple attachment point for new encoders and datasets, enabling robust, plug-and-improve performance for naturalistic neuroimaging studies.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2510.03274" rel="nofollow">Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-quant-dllm-post-training-extreme-low-bit-quantization-for-diffusion-large-language-models-" class="anchor" aria-label="Permalink: 17. Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models" href="#17-quant-dllm-post-training-extreme-low-bit-quantization-for-diffusion-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03274
<strong>Authors:</strong> Tianao Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong Guo, Yulun Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.03274v1 Announce Type: new  Abstract: Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: <a href="https://github.com/ZTA2785/Quant-dLLM">https://github.com/ZTA2785/Quant-dLLM</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2510.03291" rel="nofollow">UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-unipruning-unifying-local-metric-and-global-feedback-for-scalable-sparse-llms-" class="anchor" aria-label="Permalink: 18. UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs" href="#18-unipruning-unifying-local-metric-and-global-feedback-for-scalable-sparse-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03291
<strong>Authors:</strong> Yizhuo Ding, Wanying Qu, Jiawei Geng, Wenqi Shao, Yanwei Fu</p>
<p><strong>Abstract:</strong> arXiv:2510.03291v1 Announce Type: new  Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: <a href="https://github.com/RainbowQTT/UniPruning">https://github.com/RainbowQTT/UniPruning</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2510.03298" rel="nofollow">CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-cafl-l-constraint-aware-federated-learning-with-lagrangian-dual-optimization-for-on-device-language-models-" class="anchor" aria-label="Permalink: 19. CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models" href="#19-cafl-l-constraint-aware-federated-learning-with-lagrangian-dual-optimization-for-on-device-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03298
<strong>Authors:</strong> Dongqi Zheng, Wenjin Fu</p>
<p><strong>Abstract:</strong> arXiv:2510.03298v1 Announce Type: new  Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual Optimization (CAFL-L), a principled extension of FedAvg that explicitly incorporates device-level resource constraints including energy, communication, memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to dynamically adapt training hyperparameters -- freezing depth, local steps, batch size, and communication compression -- while preserving training stability through token-budget preservation via gradient accumulation. Experiments on a character-level language model demonstrate that CAFL-L achieves superior constraint satisfaction compared to standard FedAvg (reducing memory usage by 20% and communication by 95%) while maintaining competitive validation performance, making it practical for deployment on resource-constrained edge devices.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2510.04196" rel="nofollow">COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-cosmo-rl-towards-trustworthy-lmrms-via-joint-safety-and-stability-" class="anchor" aria-label="Permalink: 20. COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability" href="#20-cosmo-rl-towards-trustworthy-lmrms-via-joint-safety-and-stability-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04196
<strong>Authors:</strong> Yizhuo Ding, Mingkang Chen, Qiuhua Liu, Fenghua Weng, Wanying Qu, Yue Yang, Yugang Jiang, Zuxuan Wu, Yanwei Fu, Wenqi Shao</p>
<p><strong>Abstract:</strong> arXiv:2510.04196v1 Announce Type: new  Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications, where they must be both useful and safe. Safety is especially challenging in multimodal settings: images and text can be combined to bypass guardrails, and single objective training can cause policy drift that yields over-refusal on benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed reinforcement learning framework that trains reasoning oriented LMRMs under multimodal, multitask, and multiobjective signals, and we release the resulting model, COSMO-R1. Our approach aims to let safety and capability grow together in one stable pipeline rather than competing during alignment. In experiments, COSMO-R1 improves safety while maintaining-and often improving multimodal reasoning and instruction following, shows stronger robustness to multimodal jailbreaks, and reduces unnecessary refusals. The framework also transfers across backbones with consistent gains. Ablations support the design choices, indicating a simple path to advancing safety and general capability together in LMRMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2510.04217" rel="nofollow">MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-mllmeraser-achieving-test-time-unlearning-in-multimodal-large-language-models-through-activation-steering-" class="anchor" aria-label="Permalink: 21. MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering" href="#21-mllmeraser-achieving-test-time-unlearning-in-multimodal-large-language-models-through-activation-steering-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04217
<strong>Authors:</strong> Chenlu Ding, Jiancan Wu, Leheng Sheng, Fan Zhang, Yancheng Yuan, Xiang Wang, Xiangnan He</p>
<p><strong>Abstract:</strong> arXiv:2510.04217v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities across vision-language tasks, yet their large-scale deployment raises pressing concerns about memorized private data, outdated knowledge, and harmful content. Existing unlearning approaches for MLLMs typically adapt training-based strategies such as gradient ascent or preference optimization, but these methods are computationally expensive, irreversible, and often distort retained knowledge. In this work, we propose MLLMEraser, an input-aware, training-free framework for test-time unlearning. Our approach leverages activation steering to enable dynamic knowledge erasure without parameter updates. Specifically, we construct a multimodal erasure direction by contrasting adversarially perturbed, knowledge-recall image-text pairs with knowledge-erasure counterparts, capturing both textual and visual discrepancies. To prevent unnecessary interference, we further design an input-aware steering mechanism that adaptively determines when and how the erasure direction should be applied, preserving utility on retained knowledge while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms state-of-the-art MLLM unlearning baselines, achieving stronger forgetting performance with lower computational cost and minimal utility degradation.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2510.04514" rel="nofollow">ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-chartagent-a-multimodal-agent-for-visually-grounded-reasoning-in-complex-chart-question-answering-" class="anchor" aria-label="Permalink: 22. ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering" href="#22-chartagent-a-multimodal-agent-for-visually-grounded-reasoning-in-complex-chart-question-answering-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04514
<strong>Authors:</strong> Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso</p>
<p><strong>Abstract:</strong> arXiv:2510.04514v1 Announce Type: new  Abstract: Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2510.03267" rel="nofollow">PT$^2$-LLM: Post-Training Ternarization for Large Language Models</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-pt2-llm-post-training-ternarization-for-large-language-models-" class="anchor" aria-label="Permalink: 23. PT$^2$-LLM: Post-Training Ternarization for Large Language Models" href="#23-pt2-llm-post-training-ternarization-for-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03267
<strong>Authors:</strong> Xianglong Yan, Chengzhu Bao, Zhiteng Li, Tianao Zhang, Kaicheng Yang, Haotong Qin, Ruobing Xie, Xingwu Sun, Yulun Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.03267v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency. However, its potential in the post-training quantization (PTQ) setting remains underexplored, due to the challenge of training-free parameter optimization and the quantization difficulty posed by outliers and dispersed weights. To address these issues, we propose PT$^2$-LLM, a post-training ternarization framework tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and (2) Activation-aware Grid Alignment (AGA), which further refines the ternary grid to better match full-precision outputs. In addition, we propose a plug-and-play Structural Similarity-based Reordering (SSR) strategy that leverages inter-column structural similarity to ease quantization and mitigate outlier effects, further enhancing overall performance. Extensive experiments demonstrate that PT$^2$-LLM delivers competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with lower memory cost, while also accelerating both prefill and decoding to achieve end-to-end speedup. The code and models will be available at <a href="https://github.com/XIANGLONGYAN/PT2-LLM">https://github.com/XIANGLONGYAN/PT2-LLM</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2510.03284" rel="nofollow">Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-edge-fit-federated-instruction-tuning-of-quantized-llms-for-privacy-preserving-smart-home-environments-" class="anchor" aria-label="Permalink: 24. Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments" href="#24-edge-fit-federated-instruction-tuning-of-quantized-llms-for-privacy-preserving-smart-home-environments-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03284
<strong>Authors:</strong> Vinay Venkatesh, Vamsidhar R Kamanuru, Lav Kumar, Nikita Kothari</p>
<p><strong>Abstract:</strong> arXiv:2510.03284v1 Announce Type: new  Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a scalable framework for Federated Instruction Tuning (FIT) of Large Language Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT framework combines federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA), mitigating the core issues of communication and computational overhead. We demonstrate this by filtering the general-purpose Databricks Dolly 15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable framework for decentralized LLM deployment on home compute gateways.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">25. <a href="https://arxiv.org/abs/2510.03275" rel="nofollow">SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size</a> <a id="user-content-link25"></a>
</h2><a id="user-content-25-sdq-llm-sigma-delta-quantization-for-1-bit-llms-of-any-size-" class="anchor" aria-label="Permalink: 25. SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size" href="#25-sdq-llm-sigma-delta-quantization-for-1-bit-llms-of-any-size-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03275
<strong>Authors:</strong> Junhao Xia, Ming Zhao, Limin Xiao, Xiujun Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.03275v1 Announce Type: new  Abstract: Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at <a href="https://github.com/Dreamlittlecat/LLM-Quant-Factory">https://github.com/Dreamlittlecat/LLM-Quant-Factory</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">26. <a href="https://arxiv.org/abs/2510.05064" rel="nofollow">Boomerang Distillation Enables Zero-Shot Model Size Interpolation</a> <a id="user-content-link26"></a>
</h2><a id="user-content-26-boomerang-distillation-enables-zero-shot-model-size-interpolation-" class="anchor" aria-label="Permalink: 26. Boomerang Distillation Enables Zero-Shot Model Size Interpolation" href="#26-boomerang-distillation-enables-zero-shot-model-size-interpolation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.05064
<strong>Authors:</strong> Sara Kangaslahti, Nihal V. Nayak, Jonathan Geuter, Marco Fumero, Francesco Locatello, David Alvarez-Melis</p>
<p><strong>Abstract:</strong> arXiv:2510.05064v1 Announce Type: new  Abstract: Large language models (LLMs) are typically deployed under diverse memory and compute constraints. Existing approaches build model families by training each size independently, which is prohibitively expensive and provides only coarse-grained size options. In this work, we identify a novel phenomenon that we call boomerang distillation: starting from a large base model (the teacher), one first distills down to a small student and then progressively reconstructs intermediate-sized models by re-incorporating blocks of teacher layers into the student without any additional training. This process produces zero-shot interpolated models of many intermediate sizes whose performance scales smoothly between the student and teacher, often matching or surpassing pretrained or distilled models of the same size. We further analyze when this type of interpolation succeeds, showing that alignment between teacher and student through pruning and distillation is essential. Boomerang distillation thus provides a simple and efficient way to generate fine-grained model families, dramatically reducing training cost while enabling flexible adaptation across deployment environments. The code and models are available at <a href="https://github.com/dcml-lab/boomerang-distillation">https://github.com/dcml-lab/boomerang-distillation</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">27. <a href="https://arxiv.org/abs/2510.03247" rel="nofollow">Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data</a> <a id="user-content-link27"></a>
</h2><a id="user-content-27-towards-multimodal-active-learning-efficient-learning-with-limited-paired-data-" class="anchor" aria-label="Permalink: 27. Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data" href="#27-towards-multimodal-active-learning-efficient-learning-with-limited-paired-data-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03247
<strong>Authors:</strong> Jiancheng Zhang, Yinglun Zhu</p>
<p><strong>Abstract:</strong> arXiv:2510.03247v1 Announce Type: new  Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in data-hungry deep learning. However, existing AL algorithms focus almost exclusively on unimodal data, overlooking the substantial annotation burden in multimodal learning. We introduce the first framework for multimodal active learning with unaligned data, where the learner must actively acquire cross-modal alignments rather than labels on pre-aligned pairs. This setting captures the practical bottleneck in modern multimodal pipelines such as CLIP and SigLIP, where unimodal features are easy to obtain but high-quality alignment is costly. We develop a new algorithm that combines uncertainty and diversity principles in a modality-aware design, achieves linear-time acquisition, and applies seamlessly to both pool-based and streaming-based settings. Extensive experiments on benchmark datasets demonstrate that our approach consistently reduces multimodal annotation cost while preserving performance; for instance, on the ColorSwap dataset it cuts annotation requirements by up to $40%$ without loss in accuracy.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">28. <a href="https://arxiv.org/abs/2510.03612" rel="nofollow">Cross-Modal Content Optimization for Steering Web Agent Preferences</a> <a id="user-content-link28"></a>
</h2><a id="user-content-28-cross-modal-content-optimization-for-steering-web-agent-preferences-" class="anchor" aria-label="Permalink: 28. Cross-Modal Content Optimization for Steering Web Agent Preferences" href="#28-cross-modal-content-optimization-for-steering-web-agent-preferences-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03612
<strong>Authors:</strong> Tanqiu Jiang, Min Bai, Nikolaos Pappas, Yanjun Qi, Sandesh Swamy</p>
<p><strong>Abstract:</strong> arXiv:2510.03612v1 Announce Type: new  Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes selection tasks like content recommendation or product ranking by combining multimodal perception with preference reasoning. Recent studies reveal that these agents are vulnerable against attackers who can bias selection outcomes through preference manipulations using adversarial pop-ups, image perturbations, or content tweaks. Existing work, however, either assumes strong white-box access, with limited single-modal perturbations, or uses impractical settings. In this paper, we demonstrate, for the first time, that joint exploitation of visual and textual channels yields significantly more powerful preference manipulations under realistic attacker capabilities. We introduce Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible modifications to an item's visual and natural language descriptions, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions. In contrast to prior studies that assume gradient access, or control over webpages, or agent memory, we adopt a realistic black-box threat setup: a non-privileged adversary can edit only their own listing's images and textual metadata, with no insight into the agent's model internals. We evaluate CPS on agents powered by state-of-the-art proprietary and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both movie selection and e-commerce tasks. Our results show that CPS is significantly more effective than leading baseline methods. For instance, our results show that CPS consistently outperforms baselines across all models while maintaining 70% lower detection rates, demonstrating both effectiveness and stealth. These findings highlight an urgent need for robust defenses as agentic systems play an increasingly consequential role in society.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">29. <a href="https://arxiv.org/abs/2510.05014" rel="nofollow">Think Then Embed: Generative Context Improves Multimodal Embedding</a> <a id="user-content-link29"></a>
</h2><a id="user-content-29-think-then-embed-generative-context-improves-multimodal-embedding-" class="anchor" aria-label="Permalink: 29. Think Then Embed: Generative Context Improves Multimodal Embedding" href="#29-think-then-embed-generative-context-improves-multimodal-embedding-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.05014
<strong>Authors:</strong> Xuanming Cui, Jianpeng Cheng, Hong-you Chen, Satya Narayan Shukla, Abhijeet Awasthi, Xichen Pan, Chaitanya Ahuja, Shlok Kumar Mishra, Qi Guo, Ser-Nam Lim, Aashu Singh, Xiangjun Fan</p>
<p><strong>Abstract:</strong> arXiv:2510.05014v1 Announce Type: new  Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">30. <a href="https://arxiv.org/abs/2510.03371" rel="nofollow">Distributed Low-Communication Training with Decoupled Momentum Optimization</a> <a id="user-content-link30"></a>
</h2><a id="user-content-30-distributed-low-communication-training-with-decoupled-momentum-optimization-" class="anchor" aria-label="Permalink: 30. Distributed Low-Communication Training with Decoupled Momentum Optimization" href="#30-distributed-low-communication-training-with-decoupled-momentum-optimization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03371
<strong>Authors:</strong> Sasho Nedelkoski, Alexander Acker, Odej Kao, Soeren Becker, Dominik Scheinert</p>
<p><strong>Abstract:</strong> arXiv:2510.03371v1 Announce Type: new  Abstract: The training of large models demands substantial computational resources, typically available only in data centers with high-bandwidth interconnects. However, reducing the reliance on high-bandwidth interconnects between nodes enables the use of distributed compute resources as an alternative to centralized data center training. Building on recent advances in distributed model training, we propose an approach that further reduces communication by combining infrequent synchronizations across distributed model replicas with gradient momentum compression. In particular, we treat the optimizer momentum as a signal and decompose the Nesterov momentum into high- and low-frequency components via the discrete cosine transform (DCT). Only the high-frequency components are synchronized across model replicas every $H$ steps. Empirically, our method achieves up to a $16\times$ reduction in communication compared to the baseline DiLoCo, and it generalizes across architectures, including transformer-based language models and convolutional neural networks for images. Overall, this work advances the feasibility of training large models on distributed nodes with low-bandwidth interconnects.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">31. <a href="https://arxiv.org/abs/2510.03782" rel="nofollow">Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation</a> <a id="user-content-link31"></a>
</h2><a id="user-content-31-merge-and-guide-unifying-model-merging-and-guided-decoding-for-controllable-multi-objective-generation-" class="anchor" aria-label="Permalink: 31. Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation" href="#31-merge-and-guide-unifying-model-merging-and-guided-decoding-for-controllable-multi-objective-generation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03782
<strong>Authors:</strong> Guofu Xie, Chen Zhang, Xiao Zhang, Yunsheng Shi, Ting Yao, Jun Xu</p>
<p><strong>Abstract:</strong> arXiv:2510.03782v1 Announce Type: new  Abstract: Adapting to diverse user needs at test time is a key challenge in controllable multi-objective generation. Existing methods are insufficient: merging-based approaches provide indirect, suboptimal control at the parameter level, often disregarding the impacts of multiple objectives. While decoding-based guidance is more direct, it typically requires aggregating logits from multiple expert models, incurring significant space overhead and relying heavily on individual model capacity. To address these issues, we introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model merging for guided decoding. We first identify a critical compatibility problem between the guidance and base models. In Stage 1, MAGE resolves this by dynamically constructing a more robust base model, merging a series of backbone models that account for multiple objectives. In Stage 2, we merge explicit and implicit value models into a unified guidance proxy, which then steers the decoding of the base model from Stage 1. Our analysis empirically validates Linear Mode Connectivity (LMC) in value models, explores the relationship between model merging and prediction ensembling, and demonstrates the enhanced controllability afforded by our approach. Extensive experiments show that our method outperforms existing approaches, achieving superior controllability, Pareto-optimal performance, and enhanced adaptability.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">32. <a href="https://arxiv.org/abs/2510.03342" rel="nofollow">Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer</a> <a id="user-content-link32"></a>
</h2><a id="user-content-32-gemini-robotics-15-pushing-the-frontier-of-generalist-robots-with-advanced-embodied-reasoning-thinking-and-motion-transfer-" class="anchor" aria-label="Permalink: 32. Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer" href="#32-gemini-robotics-15-pushing-the-frontier-of-generalist-robots-with-advanced-embodied-reasoning-thinking-and-motion-transfer-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03342
<strong>Authors:</strong> Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Li Yang Ku, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Amaris Paryag, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Peter Pastor Sampedro, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, Ren'e Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou, Yuxiang Zhou</p>
<p><strong>Abstract:</strong> arXiv:2510.03342v1 Announce Type: new  Abstract: General-purpose robots need a deep understanding of the physical world, advanced reasoning, and general and dexterous control. This report introduces the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5, a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER 1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together three major innovations. First, Gemini Robotics 1.5 features a novel architecture and a Motion Transfer (MT) mechanism, which enables it to learn from heterogeneous, multi-embodiment robot data and makes the VLA more general. Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal reasoning process in natural language. This enables the robot to "think before acting" and notably improves its ability to decompose and execute complex, multi-step tasks, and also makes the robot's behavior more interpretable to the user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for embodied reasoning, i.e., for reasoning capabilities that are critical for robots, such as visual and spatial understanding, task planning, and progress estimation. Together, this family of models takes us a step towards an era of physical agents-enabling robots to perceive, think and then act so they can solve complex multi-step tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">33. <a href="https://arxiv.org/abs/2510.04532" rel="nofollow">More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models</a> <a id="user-content-link33"></a>
</h2><a id="user-content-33-more-than-meets-the-eye-uncovering-the-reasoning-planning-disconnect-in-training-vision-language-driving-models-" class="anchor" aria-label="Permalink: 33. More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models" href="#33-more-than-meets-the-eye-uncovering-the-reasoning-planning-disconnect-in-training-vision-language-driving-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04532
<strong>Authors:</strong> Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo</p>
<p><strong>Abstract:</strong> arXiv:2510.04532v1 Announce Type: new  Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end autonomy by first producing natural-language reasoning and then predicting trajectory planning. However, whether planning is causally driven by this reasoning remains a critical but unverified assumption. To investigate this, we build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan. Our data generation process converts sensors and annotations into structured inputs and, crucially, separates priors from to-be-reasoned signals, enabling clean information ablations. Using DriveMind, we train representative VLM agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately, indicate a consistent causal disconnect in reasoning-planning: removing ego/navigation priors causes large drops in planning scores, whereas removing CoT produces only minor changes. Attention analysis further shows that planning primarily focuses on priors rather than the CoT. Based on this evidence, we propose the Reasoning-Planning Decoupling Hypothesis, positing that the training-yielded reasoning is an ancillary byproduct rather than a causal mediator. To enable efficient diagnosis, we also introduce a novel, training-free probe that measures an agent's reliance on priors by evaluating its planning robustness against minor input perturbations. In summary, we provide the community with a new dataset and a diagnostic tool to evaluate the causal fidelity of future models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">34. <a href="https://arxiv.org/abs/2510.04547" rel="nofollow">Post-training quantization of vision encoders needs prefixing registers</a> <a id="user-content-link34"></a>
</h2><a id="user-content-34-post-training-quantization-of-vision-encoders-needs-prefixing-registers-" class="anchor" aria-label="Permalink: 34. Post-training quantization of vision encoders needs prefixing registers" href="#34-post-training-quantization-of-vision-encoders-needs-prefixing-registers-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04547
<strong>Authors:</strong> Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee</p>
<p><strong>Abstract:</strong> arXiv:2510.04547v1 Announce Type: new  Abstract: Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Post-training quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\textit{RegCache}$, a training-free algorithm to mitigate outliers in vision encoders, enabling quantization with significantly smaller accuracy drops. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">35. <a href="https://arxiv.org/abs/2510.03309" rel="nofollow">Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval</a> <a id="user-content-link35"></a>
</h2><a id="user-content-35-thin-bridges-for-drug-text-alignment-lightweight-contrastive-learning-for-target-specific-drug-retrieval-" class="anchor" aria-label="Permalink: 35. Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval" href="#35-thin-bridges-for-drug-text-alignment-lightweight-contrastive-learning-for-target-specific-drug-retrieval-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03309
<strong>Authors:</strong> Mallikarjuna Tupakula</p>
<p><strong>Abstract:</strong> arXiv:2510.03309v1 Announce Type: new  Abstract: Multimodal foundation models hold promise for drug discovery and biomedical applications, but most existing approaches rely on heavy pretraining or large scale multimodal corpora. We investigate whether thin contrastive bridges, lightweight projection heads over frozen unimodal encoders can align chemical and textual representations without training a full multimodal model. Using paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with biomedical sentence embeddings through dual linear projections trained with a contrastive objective. To better handle drugs sharing the same therapeutic target, we incorporate hard negative weighting and a margin loss. Evaluation under scaffold based splits, which require generalization across disjoint chemical cores, demonstrates that our approach achieves non-trivial cross modal alignment and substantially improves within target discrimination compared to frozen baselines. These results suggest that thin bridges offer a compute efficient alternative to large scale multimodal pretraining, enabling scaffold aware drug text alignment and target specific retrieval in precision medicine.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">36. <a href="https://arxiv.org/abs/2510.04626" rel="nofollow">Compressed Concatenation of Small Embedding Models</a> <a id="user-content-link36"></a>
</h2><a id="user-content-36-compressed-concatenation-of-small-embedding-models-" class="anchor" aria-label="Permalink: 36. Compressed Concatenation of Small Embedding Models" href="#36-compressed-concatenation-of-small-embedding-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04626
<strong>Authors:</strong> Mohamed Ayoub Ben Ayad, Michael Dinzinger, Kanishka Ghosh Dastidar, Jelena Mitrovic, Michael Granitzer</p>
<p><strong>Abstract:</strong> arXiv:2510.04626v1 Announce Type: new  Abstract: Embedding models are central to dense retrieval, semantic search, and recommendation systems, but their size often makes them impractical to deploy in resource-constrained environments such as browsers or edge devices. While smaller embedding models offer practical advantages, they typically underperform compared to their larger counterparts. To bridge this gap, we demonstrate that concatenating the raw embedding vectors of multiple small models can outperform a single larger baseline on standard retrieval benchmarks. To overcome the resulting high dimensionality of naive concatenation, we introduce a lightweight unified decoder trained with a Matryoshka Representation Learning (MRL) loss. This decoder maps the high-dimensional joint representation to a low-dimensional space, preserving most of the original performance without fine-tuning the base models. We also show that while concatenating more base models yields diminishing gains, the robustness of the decoder's representation under compression and quantization improves. Our experiments show that, on a subset of MTEB retrieval tasks, our concat-encode-quantize pipeline recovers 89% of the original performance with a 48x compression factor when the pipeline is applied to a concatenation of four small embedding models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">37. <a href="https://arxiv.org/abs/2510.03268" rel="nofollow">Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment</a> <a id="user-content-link37"></a>
</h2><a id="user-content-37-decrypt-modality-gap-in-multimodal-contrastive-learning-from-convergent-representation-to-pair-alignment-" class="anchor" aria-label="Permalink: 37. Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment" href="#37-decrypt-modality-gap-in-multimodal-contrastive-learning-from-convergent-representation-to-pair-alignment-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03268
<strong>Authors:</strong> Lingjie Yi, Raphael Douady, Chao Chen</p>
<p><strong>Abstract:</strong> arXiv:2510.03268v1 Announce Type: new  Abstract: Multimodal contrastive learning (MCL) aims to embed data from different modalities in a shared embedding space. However, empirical evidence shows that representations from different modalities occupy completely separate regions of embedding space, a phenomenon referred to as the modality gap. Moreover, experimental findings on how the size of the modality gap influences downstream performance are inconsistent. These observations raise two key questions: (1) What causes the modality gap? (2) How does it affect downstream tasks? To address these questions, this paper introduces the first theoretical framework for analyzing the convergent optimal representations of MCL and the modality alignment when training is optimized. Specifically, we prove that without any constraint or under the cone constraint, the modality gap converges to zero. Under the subspace constraint (i.e., representations of two modalities fall into two distinct hyperplanes due to dimension collapse), the modality gap converges to the smallest angle between the two hyperplanes. This result identifies \emph{dimension collapse} as the fundamental origin of the modality gap. Furthermore, our theorems demonstrate that paired samples cannot be perfectly aligned under the subspace constraint. The modality gap influences downstream performance by affecting the alignment between sample pairs. We prove that, in this case, perfect alignment between two modalities can still be achieved via two ways: hyperplane rotation and shared space projection.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">38. <a href="https://arxiv.org/abs/2510.03282" rel="nofollow">Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework</a> <a id="user-content-link38"></a>
</h2><a id="user-content-38-discovering-transformer-circuits-via-a-hybrid-attribution-and-pruning-framework-" class="anchor" aria-label="Permalink: 38. Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework" href="#38-discovering-transformer-circuits-via-a-hybrid-attribution-and-pruning-framework-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03282
<strong>Authors:</strong> Hao Gu, Vibhas Nair, Amrithaa Ashok Kumar, Jayvart Sharma, Ryan Lagasse</p>
<p><strong>Abstract:</strong> arXiv:2510.03282v1 Announce Type: new  Abstract: Interpreting language models often involves circuit analysis, which aims to identify sparse subnetworks, or circuits, that accomplish specific tasks. Existing circuit discovery algorithms face a fundamental trade-off: attribution patching is fast but unfaithful to the full model, while edge pruning is faithful but computationally expensive. This research proposes a hybrid attribution and pruning (HAP) framework that uses attribution patching to identify a high-potential subgraph, then applies edge pruning to extract a faithful circuit from it. We show that HAP is 46% faster than baseline algorithms without sacrificing circuit faithfulness. Furthermore, we present a case study on the Indirect Object Identification task, showing that our method preserves cooperative circuit components (e.g. S-inhibition heads) that attribution patching methods prune at high sparsity. Our results show that HAP could be an effective approach for improving the scalability of mechanistic interpretability research to larger models. Our code is available at <a href="https://anonymous.4open.science/r/HAP-circuit-discovery" rel="nofollow">https://anonymous.4open.science/r/HAP-circuit-discovery</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">39. <a href="https://arxiv.org/abs/2510.04560" rel="nofollow">ContextNav: Towards Agentic Multimodal In-Context Learning</a> <a id="user-content-link39"></a>
</h2><a id="user-content-39-contextnav-towards-agentic-multimodal-in-context-learning-" class="anchor" aria-label="Permalink: 39. ContextNav: Towards Agentic Multimodal In-Context Learning" href="#39-contextnav-towards-agentic-multimodal-in-context-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04560
<strong>Authors:</strong> Honghao Fu, Yuan Ouyang, Kai-Wei Chang, Yiwei Wang, Zi Huang, Yujun Cai</p>
<p><strong>Abstract:</strong> arXiv:2510.04560v1 Announce Type: new  Abstract: Recent advances demonstrate that multimodal large language models (MLLMs) exhibit strong multimodal in-context learning (ICL) capabilities, enabling them to adapt to novel vision-language tasks from a few contextual examples. However, existing ICL approaches face challenges in reconciling scalability with robustness across diverse tasks and noisy contextual examples: manually selecting examples produces clean contexts but is labor-intensive and task-specific, while similarity-based retrieval improves scalability but could introduce irrelevant or structurally inconsistent samples that degrade ICL performance. To address these limitations, we propose ContextNav, the first agentic framework that integrates the scalability of automated retrieval with the quality and adaptiveness of human-like curation, enabling noise-robust and dynamically optimized contextualization for multimodal ICL. ContextNav unifies context management and noise-robust contextualization within a closed-loop workflow driven by graph-based orchestration. Specifically, it builds a resource-aware multimodal embedding pipeline, maintains a retrievable vector database, and applies agentic retrieval and structural alignment to construct noise-resilient contexts. An Operational Grammar Graph (OGG) further supports adaptive workflow planning and optimization, enabling the agent to refine its operational strategies based on downstream ICL feedback. Experimental results demonstrate that ContextNav achieves state-of-the-art performance across various datasets, underscoring the promise of agentic workflows for advancing scalable and robust contextualization in multimodal ICL.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">40. <a href="https://arxiv.org/abs/2510.03246" rel="nofollow">StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory</a> <a id="user-content-link40"></a>
</h2><a id="user-content-40-structprune-structured-global-pruning-asymptotics-with-mathcalosqrtn-gpu-memory-" class="anchor" aria-label="Permalink: 40. StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory" href="#40-structprune-structured-global-pruning-asymptotics-with-mathcalosqrtn-gpu-memory-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03246
<strong>Authors:</strong> Xinyuan Song, Guangji Bai, Liang Zhao</p>
<p><strong>Abstract:</strong> arXiv:2510.03246v1 Announce Type: new  Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning achieves strong performance but requires $\mathcal{O}(N)$ memory, which is infeasible for billion-parameter models. Local pruning reduces GPU memory usage to that of a single layer by pruning layers independently, but it neglects inter-layer dependencies and often leads to suboptimal performance in high-sparsity regimes. Unlike unstructured pruning, structured pruning produces regular sparsity patterns that align well with GPU kernels and library optimizations, making it more hardware-efficient. However, structured pruning typically relies on global pruning, since structured patterns are more prone to severe performance degradation under local optimization. To jointly achieve structured pruning and the memory efficiency of local pruning, we propose a divide-and-conquer strategy that decomposes the global pruning problem into coordinated subproblems across different modules, each of which fits within limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an ADMM-based framework that integrates structured sparsity into the pruning process, combining the memory efficiency of local pruning with the hardware compatibility of structured methods. We derive a closed-form analytical solution for structured pruning masks that provides an explicit rule for layer-wise sparsity allocation, and further develop an energy-based asymptotic framework yielding a softmax-form allocation scheme that simplifies optimization while adapting to heterogeneous layer importance. Experiments demonstrate that STRUPRUNE matches the perplexity of global structured pruning while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$, enabling practical deployment at the billion-parameter scale.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">41. <a href="https://arxiv.org/abs/2510.04246" rel="nofollow">ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context</a> <a id="user-content-link41"></a>
</h2><a id="user-content-41-contextvla-vision-language-action-model-with-amortized-multi-frame-context-" class="anchor" aria-label="Permalink: 41. ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context" href="#41-contextvla-vision-language-action-model-with-amortized-multi-frame-context-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04246
<strong>Authors:</strong> Huiwon Jang, Sihyun Yu, Heeseung Kwon, Hojin Jeon, Younggyo Seo, Jinwoo Shin</p>
<p><strong>Abstract:</strong> arXiv:2510.04246v1 Announce Type: new  Abstract: Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">42. <a href="https://arxiv.org/abs/2510.04281" rel="nofollow">GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction</a> <a id="user-content-link42"></a>
</h2><a id="user-content-42-grok-from-quantitative-biomarkers-to-qualitative-diagnosis-via-a-grounded-mllm-with-knowledge-guided-instruction-" class="anchor" aria-label="Permalink: 42. GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction" href="#42-grok-from-quantitative-biomarkers-to-qualitative-diagnosis-via-a-grounded-mllm-with-knowledge-guided-instruction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04281
<strong>Authors:</strong> Zhuangzhi Gao, Hongyi Qin, He Zhao, Qinkai Yu, Feixiang Zhou, Eduard Shantsila, Uazman Alam, Alena Shantsila, Wahbi El-Bouri, Gregory Y. H. Lip, Yalin Zheng</p>
<p><strong>Abstract:</strong> arXiv:2510.04281v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse data modalities, but current medical adaptations such as LLaVA-Med often fail to fully exploit the synergy between color fundus photography (CFP) and optical coherence tomography (OCT), and offer limited interpretability of quantitative biomarkers. We introduce GROK, a grounded multimodal large language model that jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of ocular and systemic disease. GROK comprises three core modules: Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning, which together establish a quantitative-to-qualitative diagnostic chain of thought, mirroring real clinical reasoning when producing detailed lesion annotations. To evaluate our approach, we introduce the Grounded Ophthalmic Understanding benchmark, which covers six disease categories and three tasks: macro-level diagnostic classification, report generation quality, and fine-grained clinical assessment of the generated chain of thought. Experiments show that, with only LoRA (Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK outperforms comparable 7B and 32B baselines on both report quality and fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are publicly available in the GROK repository.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">43. <a href="https://arxiv.org/abs/2510.05059" rel="nofollow">Staircase Streaming for Low-Latency Multi-Agent Inference</a> <a id="user-content-link43"></a>
</h2><a id="user-content-43-staircase-streaming-for-low-latency-multi-agent-inference-" class="anchor" aria-label="Permalink: 43. Staircase Streaming for Low-Latency Multi-Agent Inference" href="#43-staircase-streaming-for-low-latency-multi-agent-inference-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.05059
<strong>Authors:</strong> Junlin Wang (Zach), Jue Wang (Zach), Zhen (Zach), Xu, Ben Athiwaratkun, Bhuwan Dhingra, Ce Zhang, James Zou</p>
<p><strong>Abstract:</strong> arXiv:2510.05059v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) opened up new directions for leveraging the collective expertise of multiple LLMs. These methods, such as Mixture-of-Agents, typically employ additional inference steps to generate intermediate outputs, which are then used to produce the final response. While multi-agent inference can enhance response quality, it can significantly increase the time to first token (TTFT), posing a challenge for latency-sensitive applications and hurting user experience. To address this issue, we propose staircase streaming for low-latency multi-agent inference. Instead of waiting for the complete intermediate outputs from previous steps, we begin generating the final response as soon as we receive partial outputs from these steps. Experimental results demonstrate that staircase streaming reduces TTFT by up to 93% while maintaining response quality.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">44. <a href="https://arxiv.org/abs/2510.03425" rel="nofollow">Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices</a> <a id="user-content-link44"></a>
</h2><a id="user-content-44-memory-efficient-backpropagation-for-fine-tuning-llms-on-resource-constrained-mobile-devices-" class="anchor" aria-label="Permalink: 44. Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices" href="#44-memory-efficient-backpropagation-for-fine-tuning-llms-on-resource-constrained-mobile-devices-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.03425
<strong>Authors:</strong> Congzheng Song, Xinyu Tang</p>
<p><strong>Abstract:</strong> arXiv:2510.03425v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even for a subset of parameters such as LoRA\textemdash can be much more memory-consuming than inference and is often deemed impractical for resource-constrained mobile devices. Alternative methods, such as zeroth-order optimization (ZO), can greatly reduce the memory footprint but come at the cost of significantly slower model convergence (10$\times$ to 100$\times$ more steps than backpropagation). We propose a memory-efficient implementation of backpropagation (MeBP) on mobile devices that provides better trade-off between memory usage and compute time, while converging faster and achieving better performance than the ZO baseline. We verify the effectiveness of MeBP on an iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B parameters, can be fine-tuned using less than 1GB of memory. We release an example of the MeBP implementation at <a href="https://github.com/apple/ml-mebp">https://github.com/apple/ml-mebp</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">45. <a href="https://arxiv.org/abs/2510.04331" rel="nofollow">DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks</a> <a id="user-content-link45"></a>
</h2><a id="user-content-45-doran-stabilizing-weight-decomposed-low-rank-adaptation-via-noise-injection-and-auxiliary-networks-" class="anchor" aria-label="Permalink: 45. DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks" href="#45-doran-stabilizing-weight-decomposed-low-rank-adaptation-via-noise-injection-and-auxiliary-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.04331
<strong>Authors:</strong> Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho</p>
<p><strong>Abstract:</strong> arXiv:2510.04331v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) and Mixture of Experts (MoE) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research introduces a novel multimodal large model that integrates text, language, and vision modalities. The goal is to advance the performance and generalization capabilities of the model by establishing a new approach for training and fusing these modalities effectively, rather than focusing on incremental optimizations.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>