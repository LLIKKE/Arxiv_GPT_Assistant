<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">机器人视觉-语言-动作模型：现实世界应用回顾</h2><a id="user-content-机器人视觉-语言-动作模型现实世界应用回顾" class="anchor" aria-label="Permalink: 机器人视觉-语言-动作模型：现实世界应用回顾" href="#机器人视觉-语言-动作模型现实世界应用回顾"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07077v1宣布类型：新摘要：随着人们越来越多地努力利用大型语言模型（LLM）和视觉语言模型（VLM）的进步来实现机器人技术，视觉-语言-动作（VLA）模型最近受到了广泛关注。通过大规模地统一传统上单独研究的视觉、语言和动作数据，VLA模型旨在学习在不同任务、对象、体现和环境中推广的策略。这种概括能力预计将使机器人能够以最少或没有额外的特定任务数据来解决新颖的下游任务，从而促进更灵活和可扩展的现实世界部署。与之前狭隘地关注动作表示或高级模型架构的调查不同，这项工作提供了全面的全栈审查，集成了VLA系统的软件和硬件组件。特别是，本文对VLA进行了系统性回顾，涵盖了它们的策略和架构转型、架构和构建模块、特定于模式的处理技术和学习范式。此外，为了支持在现实世界的机器人应用程序中部署VGA，我们还审查了常用的机器人平台、数据收集策略、公开可用的数据集、数据增强方法和评估基准。在这项全面的调查中，本文旨在为机器人界将VLA应用于现实世界的机器人系统提供实用指导。按培训方法、评估方法、模式和数据集分类的所有参考文献均可在我们项目网站上的表格中找到：<a href="https://vla-survey.github.io%E3%80%82" rel="nofollow">https://vla-survey.github.io。</a></p>
<div class="markdown-heading"><h2 class="heading-element">指南：嵌入物的引导蒸馏和蒸馏</h2><a id="user-content-指南嵌入物的引导蒸馏和蒸馏" class="anchor" aria-label="Permalink: 指南：嵌入物的引导蒸馏和蒸馏" href="#指南嵌入物的引导蒸馏和蒸馏"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06502v1宣布类型：新摘要：蒸馏等数学效率技术（\cite{hinton 2015 stival}）对于在不增加服务成本的情况下提高模型质量非常有用，前提是在培训期间为较小的学生模型提供更大的教师模型可供学习。标准的蒸馏方法仅限于迫使学生与教师的输出相匹配。考虑到与训练大型模型相关的成本，我们认为我们应该从教师模型中提取更多有用的信息，而不是仅仅让学生与教师的输出相匹配。   本文介绍了\guide（嵌入式的引导蒸馏和蒸馏）。\guide可以被认为是一种蒸馏技术，迫使学生在参数空间中与老师匹配。使用\guide，我们显示，当使用在大约20 B美元代币上训练的大型学生模型（400 M- 1B参数）时，师生质量差距减少了25- 26%。我们还提出了一项彻底的分析，证明\guide可以与知识提炼相结合，并进行近乎加法改进。此外，我们表明，单独应用\guide比单独应用知识提炼可以带来更好的模型质量。   最重要的是，\guide不引入训练或推理费用，因此我们的方法中的任何模型质量收益几乎都是免费的。</p>
<div class="markdown-heading"><h2 class="heading-element">带上苹果，而不是沙发：指定人工智能命令中不相关上下文对VLA模型的影响</h2><a id="user-content-带上苹果而不是沙发指定人工智能命令中不相关上下文对vla模型的影响" class="anchor" aria-label="Permalink: 带上苹果，而不是沙发：指定人工智能命令中不相关上下文对VLA模型的影响" href="#带上苹果而不是沙发指定人工智能命令中不相关上下文对vla模型的影响"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07067v1宣布类型：新摘要：视觉语言动作（VLA）模型广泛应用于并行人工智能中，使机器人能够解释和执行语言指令。然而，它们对现实世界场景中自然语言变异性的鲁棒性尚未得到彻底研究。在这项工作中，我们对语言扰动下最先进的VLA模型的鲁棒性进行了一项新颖的系统研究。具体来说，我们评估了两种类型的指令噪音下的模型性能：（1）人类生成的重述和（2）添加不相关的上下文。我们根据不相关的上下文的长度以及与机器人命令的语义和词汇接近度进一步将其分为两组。在这项研究中，我们观察到随着上下文大小的扩大而一致的性能下降。我们还证明，该模型可以对随机上下文表现出相对的鲁棒性，性能下降在10%以内，而相同长度的语义和词汇相似的上下文可以引发约50%的质量下降。人类对指令的解释导致下降近20%。为了缓解这个问题，我们提出了一个基于LLM的过滤框架，可以从有噪的输入中提取核心命令。简化我们的过滤步骤可以使模型在噪音条件下恢复高达98.5%的原始性能。</p>
<div class="markdown-heading"><h2 class="heading-element">TIGeR：机器人视觉语言模型中的工具集成几何推理</h2><a id="user-content-tiger机器人视觉语言模型中的工具集成几何推理" class="anchor" aria-label="Permalink: TIGeR：机器人视觉语言模型中的工具集成几何推理" href="#tiger机器人视觉语言模型中的工具集成几何推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07181v1宣布类型：新摘要：视觉语言模型（VLM）在空间推理方面表现出了非凡的能力，但它们从根本上仍然局限于定性精度，并且缺乏现实世界机器人所需的计算精度。当前的方法未能利用来自深度传感器和相机校准的指标线索，而是将几何问题减少到模式识别任务，而模式识别任务无法提供机器人操纵所需的厘米级准确度。我们提出了TIGeR（工具集成几何推理），这是一种新颖的框架，可以将VLM从感知估计器转换到几何计算机，使它们能够通过外部工具生成和执行精确的几何计算。TIGeR不是试图将复杂的几何操作内化在神经网络中，而是使模型能够识别几何推理要求、合成适当的计算代码并调用专门的库进行精确计算。为了支持这一范式，我们引入了TIGeR-300 K，这是一个全面的面向工具调用的数据集，涵盖点转换、姿态估计、轨迹生成和空间兼容性验证，并包含工具调用序列和中间计算。通过将监督微调（SFT）和强化微调（RFT）与我们提出的分层奖励设计相结合的两阶段训练管道，TIGeR在几何推理基准上实现了SOTA性能，同时在现实世界的机器人操纵任务中展示了厘米级的精确度。</p>
<div class="markdown-heading"><h2 class="heading-element">零激发量化的敏锐度感知数据生成</h2><a id="user-content-零激发量化的敏锐度感知数据生成" class="anchor" aria-label="Permalink: 零激发量化的敏锐度感知数据生成" href="#零激发量化的敏锐度感知数据生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07018v1宣布类型：新摘要：零镜头量化旨在从预训练的全精度模型中学习量化模型，而无需访问原始的真实训练数据。零激发量化方法的共同想法是生成合成数据以量化全精度模型。虽然众所周知，低清晰度的深度神经网络具有更好的概括能力，但之前的零镜头量化作品都没有将量化模型的清晰度作为生成训练数据的标准。本文介绍了一种新颖的方法，该方法在合成数据生成中考虑量化模型清晰度以增强概括性。具体来说，我们首先证明，在某些假设下，可以通过最大化在合成和真实验证数据上计算的重建损失梯度之间的梯度匹配来实现清晰度最小化。然后，我们通过使用每个生成的样本与其邻居之间的梯度匹配来逼近它，从而避免了没有真实验证集的梯度匹配问题。对CIFAR-100和ImageNet数据集的实验评估证明了所提出的方法在低位量化设置中优于最先进技术。</p>
<div class="markdown-heading"><h2 class="heading-element">通过单令牌数字嵌入实现语言模型中的高效算术能力</h2><a id="user-content-通过单令牌数字嵌入实现语言模型中的高效算术能力" class="anchor" aria-label="Permalink: 通过单令牌数字嵌入实现语言模型中的高效算术能力" href="#通过单令牌数字嵌入实现语言模型中的高效算术能力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06824v1宣布类型：新摘要：为了推动科学和工程领域的进步，大型语言模型（LLM）必须能够处理大量数字数据并有效地解决长时间计算。目前，这只能通过使用外部工具或广泛的推理链来实现，要么限制LLM的数字直觉，要么限制他们可以解决的问题的长度。我们表明，前沿LLM甚至需要大量的推理令牌来解决基本的计算，而他们将单个数字分解为多个令牌的令牌化策略加剧了这一问题。这激发了对高效且有效的单令牌号码编码的需求。我们为此类编码引入了一组需求，并表明现有方法无法满足它们。为了解决这些缺点，我们提出了BitTokens，这是一种新颖的代币化策略，使用其IEEE 754二进制浮点表示将任何数字嵌入到单个代币中。通过广泛的实验，我们表明，我们的BitTokens甚至允许小型语言模型学习几乎完美解决基本算术运算的算法。这种新获得的效率可能会扩大语言模型可以解决的问题的长度和复杂性。</p>
<div class="markdown-heading"><h2 class="heading-element">TrackVLA++：在VLA模型中释放推理和记忆能力以实现同步视觉跟踪</h2><a id="user-content-trackvla在vla模型中释放推理和记忆能力以实现同步视觉跟踪" class="anchor" aria-label="Permalink: TrackVLA++：在VLA模型中释放推理和记忆能力以实现同步视觉跟踪" href="#trackvla在vla模型中释放推理和记忆能力以实现同步视觉跟踪"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07134v1公告类型：新摘要：目标视觉跟踪（GPT）是支撑实际应用（例如伴侣机器人、引导机器人和服务助理）的基本能力，其中持续跟踪移动目标至关重要。最近的进步使得在复杂和非结构化的场景中实现了语言引导的跟踪。然而，现有的方法缺乏明确的空间推理和有效的时间记忆，导致在严重阻塞或存在相似外观干扰物的情况下失败。为了应对这些挑战，我们提出了TrackVLA++，这是一种新型的视觉-语言-动作（VLA）模型，它通过两个关键模块（空间推理机制和目标识别记忆（TIM））增强了具体视觉跟踪。推理模块引入了一种名为Polar-CoT的思想链范式，它推断目标的相对位置，并将其编码为用于动作预测的紧凑极坐标标记。在这些空间先验的指导下，TIM采用门控更新策略来保留长视野目标记忆，确保时空一致性并减轻扩展遮挡期间的目标丢失。大量实验表明，TrackVLA++在以自我为中心和多摄像机设置的公共基准测试中都实现了最先进的性能。在具有挑战性的EVT-Bench DT分割中，TrackVLA++分别超过了之前的领先方法5.1和12。此外，TrackVLA++具有强大的零拍摄泛化能力，能够在动态和遮挡场景中实现强大的真实世界跟踪。</p>
<div class="markdown-heading"><h2 class="heading-element">马尔科夫思想家</h2><a id="user-content-马尔科夫思想家" class="anchor" aria-label="Permalink: 马尔科夫思想家" href="#马尔科夫思想家"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06557v1宣布类型：新摘要：强化学习（RL）最近已成为训练产生长思维链（LongCoT）的推理LLM的有力秘诀。然而，标准的RL“思维环境”（其中状态是提示加上所有先前的推理令牌）使状态无限，并迫使基于注意力的政策随着思想的延长而进行二次计算。我们重新审视环境本身。我们提出了马尔可夫思维，一种范式，在这种范式中，政策的进步推理，而条件不变的大小状态，脱钩的思维长度从上下文大小。作为直接结果，这产生具有恒定内存的线性计算。我们用Delethink实例化了这个想法，Delethink是一个RL环境，它将推理结构化为固定大小的块。在每个块中，模型照常思考;在边界处，环境重置上下文并通过短暂的结转重新恢复提示。通过RL，策略学会在每个块的结尾附近编写足以在重置后无缝继续推理的文本状态。在这种环境中训练的R1-Distill 1.5B模型在8 K代币块中推理，但考虑最多24 K代币，与用24 K预算训练的LongCoT-RL相当或超过。通过测试时间扩展，Delethink继续改善LongCoT稳定期的情况。线性计算的影响是巨大的：我们根据经验估计，平均思维长度为96 K，LongCoT-RL的成本为27 H100个月，而Delethink的成本为7个月。RL初始化时的分析表明，现成的推理模型（1.5B-120 B）经常在不同基准上对马尔科夫轨迹进行零射击采样，提供使RL大规模有效的积极样本。我们的结果表明，重新设计思维环境是一个强大的杠杆：它可以实现非常长的推理，而无需二次开销，并为高效、可扩展的推理LLM开辟了一条道路。</p>
<div class="markdown-heading"><h2 class="heading-element">SaFeR-VLM：走向多模式模型中的安全意识细粒度推理</h2><a id="user-content-safer-vlm走向多模式模型中的安全意识细粒度推理" class="anchor" aria-label="Permalink: SaFeR-VLM：走向多模式模型中的安全意识细粒度推理" href="#safer-vlm走向多模式模型中的安全意识细粒度推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06871v1宣布类型：新摘要：多模式大型推理模型（MLRM）展示了令人印象深刻的跨模式推理，但经常在对抗性或不安全的提示下放大安全风险，我们称这种现象为\textit{Reasoning Tax}。现有的防御措施主要作用于输出级别，不限制推理过程，从而使模型面临隐性风险。在本文中，我们提出了SaFeR-VLM，这是一个安全对齐的强化学习框架，它将安全性直接嵌入到多模式推理中。该框架集成了四个组件：（I）QI-Safe-10 K，一个精心策划的数据集，强调安全关键和推理敏感的案例;（II）安全意识的推出，不安全的一代会经历反思和纠正，而不是被抛弃;（III）具有多维加权标准的结构化奖励建模以及对幻觉和矛盾的明确惩罚;和（IV）GRPO优化，它强化了安全和正确的轨迹。这种统一的设计将安全性从被动保障转变为主动推理驱动器，从而实现可扩展和可推广的安全意识推理。SaFeR-VLM进一步证明了对显性和隐性风险的鲁棒性，支持动态和可解释的安全决策，而不仅仅是表面过滤。SaFeR-VLM-3B在六个基准测试中的安全性和实用性分别达到了70.13美元和78.97美元的平均性能，超过了同等规模和10倍以上的型号，如Skywork-R1 V3 - 38 B、Qwen2.5VL-72 B和GLM4.5V-106 B。值得注意的是，SaFeR-VLM-7 B受益于其增加的规模，在安全指标上分别超过GPT-5-mini和Gemini-2.5-Flash\num{6.47}和\num{16.76}点，实现了这一改进，而没有任何有用性能的下降。我们的代码可在<a href="https://github.com/HarveyYi/SaFeR-VLM%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HarveyYi/SaFeR-VLM上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">通过经验动态形式提升有效停止LLM生成</h2><a id="user-content-通过经验动态形式提升有效停止llm生成" class="anchor" aria-label="Permalink: 通过经验动态形式提升有效停止LLM生成" href="#通过经验动态形式提升有效停止llm生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06478v1宣布类型：新摘要：我们引入了Sequential-EDFL（经验动态形式提升），将随时有效的顺序测试应用于语言模型生成停止。我们的方法使用自规范化的逻辑伯恩斯坦e流程来跟踪信息提升（完整模型和故意削弱的“骨架”基线之间的log似然比），该流程提供正式的增量级别错误控制，无论停止时间如何。我们通过在线均值估计处理未知的中心化，通过混合e过程组合多个参数，并支持分布漂移下的自适应重置。在六个基准测试中，与顺序基线相比，Sequential-EDFL将生成量减少了22-28%，同时以12%的计算费用维持增量级控制。我们引入了自动化骨架（提炼子模型、随机逻辑），并在骨架家族中展示了稳健性。使用轻量级正确性门（句子边界+验证器）组成EDFL可以提高结束任务的正确性，同时仅通过延迟停止来保留随时有效的保证。我们的证书控制信息的充分性，而不是事实的正确性--即使有门，10.9%的停止序列仍然不正确（没有门，13.2-22.7%）。EDFL充当第一级过滤器，可将验证负担减少83%，而不是作为安全关键域的独立解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">专家指导：软路由混合专家的可证明特征学习动态</h2><a id="user-content-专家指导软路由混合专家的可证明特征学习动态" class="anchor" aria-label="Permalink: 专家指导：软路由混合专家的可证明特征学习动态" href="#专家指导软路由混合专家的可证明特征学习动态"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.07205v1宣布类型：新摘要：专家混合（MoE）架构已成为现代人工智能系统的基石。特别是，教育部将输入动态地传递给专业专家，这些专家的输出通过加权总和进行汇总。尽管应用广泛，但对MoE训练动态的理论理解仍然局限于单独的专家路由器优化或仅限于具有精心构建的数据集的顶级路由场景。本文通过为学生-教师框架中与非线性路由器和专家联合训练软路由MoE模型提供收敛保证来推进MoE理论。我们证明，适度的参数化，学生网络经历了一个功能学习阶段，路由器的学习过程是“指导”的专家，恢复教师的参数。此外，我们证明了训练后的修剪可以有效地消除冗余神经元，然后是一个可证明收敛的微调过程，达到全局最优。据我们所知，我们的分析是第一个为理解MoE架构的优化格局带来新颖见解的分析。</p>
<div class="markdown-heading"><h2 class="heading-element">LLM中的注意力下沉和压缩谷是同一枚硬币的两面</h2><a id="user-content-llm中的注意力下沉和压缩谷是同一枚硬币的两面" class="anchor" aria-label="Permalink: LLM中的注意力下沉和压缩谷是同一枚硬币的两面" href="#llm中的注意力下沉和压缩谷是同一枚硬币的两面"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06477v1宣布类型：新摘要：注意力下沉和压缩谷作为大型语言模型中的两种令人困惑的现象引起了人们的广泛关注，但一直是孤立研究的。在这项工作中，我们呈现了注意力下沉和压缩谷之间令人惊讶的联系，并将两者追溯到残余流中大规模激活的形成。我们从理论上证明，大规模激活必然会产生代表性压缩，并为由此产生的熵减少建立界限。通过多个模型（410 M-120 B参数）的实验，我们证实，当序列触发代币在中层发展极端激活规范时，压缩谷和注意力下沉同时出现。有针对性的消融研究验证了我们的理论预测。这种统一的观点促使我们提出信息流的混合-压缩-精炼理论，试图解释LLM如何通过大规模激活控制注意力和代表性压缩来深入组织计算。具体来说，我们假设基于Transformer的LLM分三个不同的阶段处理令牌：（1）早期层中的广泛混合，（2）中间层中的压缩计算有限混合，以及（3）后期层中的选择性细化。我们的框架有助于解释为什么嵌入任务在中间层表现最好，而生成任务则受益于全深度处理，从而澄清了任务相关表示的差异。</p>
<div class="markdown-heading"><h2 class="heading-element">RLininf-VLA：统一高效的VLA+RL培训框架</h2><a id="user-content-rlininf-vla统一高效的vlarl培训框架" class="anchor" aria-label="Permalink: RLininf-VLA：统一高效的VLA+RL培训框架" href="#rlininf-vla统一高效的vlarl培训框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06710v1宣布类型：新摘要：视觉和语言基础模型的最新进展显着推进了多模式理解、推理和生成，激发了人们通过视觉-语言-动作（VLA）模型将此类能力扩展到具体环境的兴趣。然而，大多数VLA模型仍然是通过监督式微调（SFT）进行训练的，由于误差积累，该模型很难在分布变化下进行概括。强化学习（RL）通过交互直接优化任务性能，提供了一种有希望的替代方案，但现有的尝试仍然支离破碎，并且缺乏一个统一的平台来进行模型架构和算法设计之间的公平和系统比较。为了解决这一差距，我们引入了RLininf-VLA，这是一个统一且高效的框架，用于VLA模型的可扩展RL训练。该系统采用高度灵活的资源分配设计，解决了RL+VLA训练中集成渲染、训练和推理的挑战。特别是，对于GOP并行模拟器，RLininf-VLA实现了一种新颖的混合细粒度管道分配模式，实现了1.61x-1.88x的训练加速。通过统一的接口，RLininf-VLA无缝支持各种VLA架构（例如，OpenVLA、OpenVLA-OFT）、多个RL算法（例如，PPO、GRPO）和各种模拟器（例如，ManiSkill，伦敦）。在模拟中，统一模型在130个LIBERO任务中实现了98.11%，在25个ManiSkill任务中实现了97.66%。除了经验表现之外，我们的研究还提炼了一套将RL应用于VLA培训的最佳实践，并揭示了这种集成中新出现的模式。此外，我们在现实世界的Franka机器人上进行了初步部署，其中RL训练的策略比使用SFT训练的策略表现出更强的概括性。我们设想RLininf-VLA作为加速和标准化体现智能研究的基础。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>