<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">旋转、裁剪与分区：通过整合旋转与可学习的非均匀量化器，迈向W2A4KV4量化之路</h2><a id="user-content-旋转裁剪与分区通过整合旋转与可学习的非均匀量化器迈向w2a4kv4量化之路" class="anchor" aria-label="Permalink: 旋转、裁剪与分区：通过整合旋转与可学习的非均匀量化器，迈向W2A4KV4量化之路" href="#旋转裁剪与分区通过整合旋转与可学习的非均匀量化器迈向w2a4kv4量化之路"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.15779v1 公告类型：新研究  摘要：我们提出了旋转、裁剪与分区（RCP），一种量化感知训练（QAT）方法，首次实现了W2A4KV4配置（2位权重、4位激活、4位键值缓存）下大型语言模型（LLMs）的极致压缩。RCP通过定量分析随机旋转对2位权重量化的影响，将最新的旋转技术与一种新颖的非均匀权重量化器设计相结合。我们的权重量化器采用了可学习直接分区（LDP），通过引入可学习参数，直接与LLM权重共同学习非均匀区间。此外，我们还展示了一个专门支持非均匀W2A4上GEMV操作的GPU内核。实验表明，RCP能够将LLaMA-2-7B压缩至W2A4KV4，仅损失2.84 WikiText2 ppl，并减少5.29倍的内存占用。更重要的是，RCP能够量化面向移动端的挑战性模型LLaMA-3.2以及特定领域的WizardCoder-7B和MetaMath-7B，且未出现如收敛失败或重复等关键问题。代码将在blind_review处提供。</p>
<div class="markdown-heading"><h2 class="heading-element">BigMac：一种通信高效的专家混合模型结构，旨在实现快速训练与推理</h2><a id="user-content-bigmac一种通信高效的专家混合模型结构旨在实现快速训练与推理" class="anchor" aria-label="Permalink: BigMac：一种通信高效的专家混合模型结构，旨在实现快速训练与推理" href="#bigmac一种通信高效的专家混合模型结构旨在实现快速训练与推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.16927v1 公告类型：新研究  摘要：混合专家（Mixture-of-Experts, MoE）结构通过仅需次线性增长的计算资源，扩展了基于Transformer的大型语言模型（LLMs）并提升了其性能。最近，提出了一种细粒度的DeepSeekMoE结构，能在不降低性能的前提下进一步提高MoE的计算效率。然而，MoE引入的全对全（All-to-All）通信已成为瓶颈，尤其是对于细粒度结构，它通常涉及并激活更多专家，从而导致更重的通信开销。本文提出了一种名为BigMac的新型MoE结构，它同样具有细粒度特性，但通信效率更高。BigMac的创新主要在于我们摒弃了细粒度MoE使用的“通信-降维-升维-通信”（CDAC）方式，这种方式导致全对全通信总是在最高维度进行。相反，BigMac设计了一种高效的“降维-通信-通信-升维”（DCCA）方式。具体来说，我们在专家的入口和出口分别添加了降维和升维投影，使得通信能在非常低的维度上进行。此外，为了适应DCCA，我们重新设计了小型专家的结构，确保BigMac中的专家具备足够的复杂性来处理令牌。实验结果显示，在相同专家数量和相似总参数量的情况下，BigMac实现了与细粒度MoEs相当甚至更优的模型质量。同样重要的是，在包括Megatron、Tutel和DeepSpeed-Inference在内的最先进AI计算框架上，BigMac将训练端到端延迟最多减少了3.09倍，并将推理吞吐量最多提升了3.11倍。</p>
<div class="markdown-heading"><h2 class="heading-element">基于专家混合模型（MoE）的大型语言模型（LLMs）压缩中的Delta解压技术</h2><a id="user-content-基于专家混合模型moe的大型语言模型llms压缩中的delta解压技术" class="anchor" aria-label="Permalink: 基于专家混合模型（MoE）的大型语言模型（LLMs）压缩中的Delta解压技术" href="#基于专家混合模型moe的大型语言模型llms压缩中的delta解压技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.17298v1 公告类型：新研究<br>
摘要：大型语言模型（LLMs）中的专家混合（Mixture-of-Experts, MoE）架构虽然表现出色，但面临着存储和内存需求过高的难题。为解决这些问题，我们提出了$D^2$-MoE，一种新的增量解压缩压缩器，用于减少MoE LLMs的参数。基于对专家多样性的观察，我们将它们的权重分解为共享的基础权重和独特的增量权重。具体而言，我们的方法首先利用Fisher信息矩阵将每个专家的权重合并到基础权重中，以捕捉共享成分。接着，通过奇异值分解（SVD）利用增量权重的低秩特性进行压缩。最后，我们为基权重引入了一种半动态结构化剪枝策略，结合静态和动态冗余分析，在保持输入适应性的同时进一步减少参数。通过这种方式，$D^2$-MoE成功地将MoE LLMs压缩至高压缩比，且无需额外训练。大量实验证明了我们方法的优越性，在Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs上，压缩率为40%至60%时，性能提升超过13%，优于其他压缩器。代码可在<a href="https://github.com/lliai/D2MoE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lliai/D2MoE获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">CipherPrune：高效且可扩展的私有Transformer推理</h2><a id="user-content-cipherprune高效且可扩展的私有transformer推理" class="anchor" aria-label="Permalink: CipherPrune：高效且可扩展的私有Transformer推理" href="#cipherprune高效且可扩展的私有transformer推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.16782v1 公告类型：新摘要<br>
摘要：利用密码学协议进行私有Transformer推理为隐私保护机器学习提供了有前景的解决方案；然而，它仍面临显著的运行时开销（效率问题）和处理长令牌输入时的挑战（可扩展性问题）。我们观察到，Transformer的操作复杂度与输入令牌数量呈二次方增长，因此减少输入令牌长度至关重要。值得注意的是，每个令牌的重要性不同，许多输入包含冗余令牌。此外，先前依赖高次多项式近似非线性激活的私有推理方法计算成本高昂。因此，对不太重要的令牌降低多项式次数可以显著加速私有推理。基于这些观察，我们提出了\textit{CipherPrune}，一个高效且可扩展的私有推理框架，包含一个安全的加密令牌剪枝协议、一个多项式降次协议以及相应的Transformer网络优化。在协议层面，加密令牌剪枝以渐进、逐层的方式自适应地从加密输入中移除不重要的令牌。此外，加密多项式降次在剪枝后为不太重要的令牌分配较低次数的多项式，无需解密即可提升效率。在网络层面，我们通过基于梯度的搜索引入协议感知的网络优化，以最大化剪枝阈值和多项式降次条件，同时保持所需的准确性。我们的实验表明，与之前的方法相比，CipherPrune将128令牌输入的私有Transformer推理执行开销减少了约$6.1\times$，512令牌输入减少了约$10.6\times$，且准确率仅略有下降。代码公开于<a href="https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference%E3%80%82">https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference。</a></p>
<div class="markdown-heading"><h2 class="heading-element">修剪作为一种防御手段：减少大型语言模型中的记忆现象</h2><a id="user-content-修剪作为一种防御手段减少大型语言模型中的记忆现象" class="anchor" aria-label="Permalink: 修剪作为一种防御手段：减少大型语言模型中的记忆现象" href="#修剪作为一种防御手段减少大型语言模型中的记忆现象"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.15796v1 公告类型：新研究  摘要：大型语言模型已被证明能够记忆其训练数据中的大量内容，并在适当提示下重现这些信息。本研究探讨了简单剪枝技术对此行为的影响。我们的发现表明，剪枝能有效减少大型语言模型中的记忆程度，展现了其作为防御成员推理攻击基础方法的潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">稳定SPAM：如何在4位训练中比16位Adam更稳定</h2><a id="user-content-稳定spam如何在4位训练中比16位adam更稳定" class="anchor" aria-label="Permalink: 稳定SPAM：如何在4位训练中比16位Adam更稳定" href="#稳定spam如何在4位训练中比16位adam更稳定"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.17055v1 公告类型：新论文  摘要：本文全面评估了几种近期提出的用于4位训练的优化器，发现低比特精度放大了对学习率的敏感性，并常常导致梯度范数不稳定，进而在较高学习率下引发发散。其中，SPAM作为一种新近的优化器，具备动量重置和尖峰感知梯度裁剪特性，在不同比特级别上表现最佳，但在稳定梯度范数方面存在困难，需谨慎调整学习率。针对这些局限，我们提出了Stable-SPAM，它融合了增强的梯度归一化和裁剪技术。具体而言，Stable-SPAM（1）通过追踪历史最大值自适应更新尖峰梯度的裁剪阈值；（2）基于历史$l_2$范数统计对整个梯度矩阵进行归一化；以及（3）继承SPAM的动量重置机制，定期重置Adam的一阶和二阶矩，减轻尖峰梯度的累积。大量实验表明，Stable-SPAM在4位大语言模型训练中有效稳定了梯度范数，相较于Adam和SPAM展现出更优性能。值得注意的是，使用Stable-SPAM训练的4位LLaMA-1B模型，其困惑度比使用Adam训练的BF16 LLaMA-1B模型高出最多$2$。此外，当两者均在4位下训练时，Stable-SPAM达到与Adam相同的损失，而所需训练步数仅为后者的一半。代码已发布于<a href="https://github.com/TianjinYellow/StableSPAM.git%E3%80%82">https://github.com/TianjinYellow/StableSPAM.git。</a></p>
<div class="markdown-heading"><h2 class="heading-element">构建压缩策略的通用误差理论分析框架</h2><a id="user-content-构建压缩策略的通用误差理论分析框架" class="anchor" aria-label="Permalink: 构建压缩策略的通用误差理论分析框架" href="#构建压缩策略的通用误差理论分析框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.15802v1 公告类型：新研究  摘要：深度学习模型参数规模和计算复杂度的指数增长给高效部署带来了重大挑战。现有压缩方法的核心问题在于，模型的不同层对压缩程度的容忍度存在显著差异。例如，与最后一层相比，模型的第一层通常能够承受更高的压缩率而不影响性能。因此，关键挑战在于如何分配各层的压缩级别，以在最小化性能损失的同时最大化参数减少。针对这一挑战，我们提出了压缩误差理论（Compression Error Theory, CET）框架，旨在为每一层确定最优压缩级别。以量化为例，CET利用微分扩展和代数几何将量化误差的二次形式重构为椭球体和双曲抛物面，并利用其几何结构定义误差子空间。为了识别性能损失最小的误差子空间，通过对几何空间进行正交分解，CET将误差子空间的优化过程转化为一个互补问题。最终的理论分析表明，沿主轴构建量化子空间能实现最小的性能下降。通过实验验证，CET在压缩的同时能极大保留性能。具体而言，在ResNet-34模型上，CET实现了近11倍的参数压缩，同时性能甚至超越了原模型。</p>
<div class="markdown-heading"><h2 class="heading-element">压缩比例定律：统一稀疏性与量化</h2><a id="user-content-压缩比例定律统一稀疏性与量化" class="anchor" aria-label="Permalink: 压缩比例定律：统一稀疏性与量化" href="#压缩比例定律统一稀疏性与量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.16440v1 公告类型：新研究  摘要：我们探讨了不同的压缩技术——如权重和激活量化，以及权重稀疏化——如何影响大规模语言模型（LLMs）在预训练过程中的扩展行为。基于先前研究表明权重稀疏化在扩展定律中作为模型大小的恒定乘数，我们证明了这种“有效参数”的扩展模式同样适用于量化。具体而言，我们发现仅对权重进行量化能实现显著的参数效率乘数，而同时对权重和激活进行全量化在较低比特宽度下则显示出收益递减。我们的结果表明，不同的压缩技术可以在一个共同的扩展定律框架下统一起来，从而使得这些方法的原理性比较和组合成为可能。</p>
<div class="markdown-heading"><h2 class="heading-element">自动联合结构化剪枝与量化：高效神经网络训练与压缩</h2><a id="user-content-自动联合结构化剪枝与量化高效神经网络训练与压缩" class="anchor" aria-label="Permalink: 自动联合结构化剪枝与量化：高效神经网络训练与压缩" href="#自动联合结构化剪枝与量化高效神经网络训练与压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.16638v1 公告类型：新摘要<br>
摘要：结构化剪枝和量化是用于缩减深度神经网络（DNNs）规模的基本技术，通常独立应用。通过联合优化同时应用这些技术，有可能生成更小、高质量的模型。然而，现有的联合方案并未广泛采用，原因在于：(1) 工程难度（复杂的多阶段流程），(2) 黑箱优化（需要大量超参数调优以控制整体压缩效果），以及 (3) 架构泛化能力不足。针对这些局限，我们提出了GETA框架，它能自动且高效地在任何DNN上执行联合结构化剪枝和量化感知训练。GETA引入了三项关键创新：(i) 量化感知依赖图（QADG），为通用量化感知DNN构建剪枝搜索空间，(ii) 部分投影随机梯度法，确保满足逐层比特约束，以及 (iii) 一种新的联合学习策略，融入了剪枝与量化之间可解释的关系。我们在卷积神经网络和Transformer架构上进行了数值实验，结果表明，与现有的联合剪枝和量化方法相比，我们的方法实现了具有竞争力（往往更优）的性能。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>