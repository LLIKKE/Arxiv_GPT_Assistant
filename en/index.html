<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/15/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10152025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/15/2025" href="#personalized-daily-arxiv-papers-10152025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 28</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference</a>
<strong>Authors:</strong> Yihua Cheng, Yuhan Liu, Jiayi Yao, Yuwei An, Xiaokun Chen, Shaoting Feng, Yuyang Huang, Samuel Shen, Kuntai Du, Junchen Jiang</p>
</li>
<li>
<p><a href="#link1">Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey</a>
<strong>Authors:</strong> Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi</p>
</li>
<li>
<p><a href="#link2">RAG-Anything: All-in-One RAG Framework</a>
<strong>Authors:</strong> Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang</p>
</li>
<li>
<p><a href="#link3">Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models</a>
<strong>Authors:</strong> Junhyuck Kim, Ethan Ewer, Taehong Moon, Jongho Park, Dimitris Papailiopoulos</p>
</li>
<li>
<p><a href="#link4">Vision-LLMs for Spatiotemporal Traffic Forecasting</a>
<strong>Authors:</strong> Ning Yang, Hengyu Zhong, Haijun Zhang, Randall Berry</p>
</li>
<li>
<p><a href="#link5">Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware</a>
<strong>Authors:</strong> Lion Mueller, Alberto Garcia-Ortiz, Ardalan Najafi, Adam Fuks, Lennart Bamberg</p>
</li>
<li>
<p><a href="#link6">LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences</a>
<strong>Authors:</strong> Wenbo Wu, Qingyi Si, Xiurui Pan, Ye Wang, Jie Zhang</p>
</li>
<li>
<p><a href="#link7">PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models</a>
<strong>Authors:</strong> Lancheng Zou, Shuo Yin, Zehua Pei, Tsung-Yi Ho, Farzan Farnia, Bei Yu</p>
</li>
<li>
<p><a href="#link8">CacheClip: Accelerating RAG with Effective KV Cache Reuse</a>
<strong>Authors:</strong> Bin Yang, Qiuyu Leng, Jun Zeng, Zhenhua Wu</p>
</li>
<li>
<p><a href="#link9">PatentVision: A multimodal method for drafting patent applications</a>
<strong>Authors:</strong> Ruo Yang, Sai Krishna Reddy Mudhiganti, Manali Sharma</p>
</li>
<li>
<p><a href="#link10">Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization</a>
<strong>Authors:</strong> Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim</p>
</li>
<li>
<p><a href="#link11">Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</a>
<strong>Authors:</strong> Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li</p>
</li>
<li>
<p><a href="#link12">ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning</a>
<strong>Authors:</strong> Hanyang Chen, Mark Zhao, Rui Yang, Qinwei Ma, Ke Yang, Jiarui Yao, Kangrui Wang, Hao Bai, Zhenhailong Wang, Rui Pan, Mengchao Zhang, Jose Barreiros, Aykut Onol, ChengXiang Zhai, Heng Ji, Manling Li, Huan Zhang, Tong Zhang</p>
</li>
<li>
<p><a href="#link13">Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding</a>
<strong>Authors:</strong> Payel Bhattacharjee, Fengwei Tian, Meiyu Zhong, Guangyi Zhang, Osvaldo Simeone, Ravi Tandon</p>
</li>
<li>
<p><a href="#link14">AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs</a>
<strong>Authors:</strong> Gunho Park, Jeongin Bae, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</p>
</li>
<li>
<p><a href="#link15">Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models</a>
<strong>Authors:</strong> Mingyang Lyu, Yinqian Sun, Erliang Lin, Huangrui Li, Ruolin Chen, Feifei Zhao, Yi Zeng</p>
</li>
<li>
<p><a href="#link16">Hierarchical LoRA MoE for Efficient CTR Model Scaling</a>
<strong>Authors:</strong> Zhichen Zeng, Mengyue Hang, Xiaolong Liu, Xiaoyi Liu, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Zhining Liu, Siyang Yuan, Chaofei Yang, Yiqun Liu, Hang Yin, Jiyan Yang, Hanghang Tong</p>
</li>
<li>
<p><a href="#link17">ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces</a>
<strong>Authors:</strong> Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar</p>
</li>
<li>
<p><a href="#link18">ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning</a>
<strong>Authors:</strong> Jinyang Zhang, Yue Fang, Hongxin Ding, Weibin Liao, Muyang Ye, Xu Chu, Junfeng Zhao, Yasha Wang</p>
</li>
<li>
<p><a href="#link19">Reflection-Based Task Adaptation for Self-Improving VLA</a>
<strong>Authors:</strong> Baicheng Li, Dong Wu, Zike Yan, Xinchen Liu, Zecui Zeng, Lusong Li, Hongbin Zha</p>
</li>
<li>
<p><a href="#link20">Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form</a>
<strong>Authors:</strong> Lorenzo Nikiforos, Charalampos Antoniadis, Luciano Prono, Fabio Pareschi, Riccardo Rovatti, Gianluca Setti</p>
</li>
<li>
<p><a href="#link21">ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding</a>
<strong>Authors:</strong> Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou</p>
</li>
<li>
<p><a href="#link22">FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management</a>
<strong>Authors:</strong> Kahou Tam, Chunlin Tian, Li Li, Haikai Zhao, ChengZhong Xu</p>
</li>
<li>
<p><a href="#link23">Reasoning-Enhanced Large Language Models for Molecular Property Prediction</a>
<strong>Authors:</strong> Jiaxi Zhuang, Yaorui Shi, Jue Hou, Yunong He, Mingwei Ye, Mingjun Xu, Yuming Su, Linfeng Zhang, Linfeng Zhang, Guolin Ke, Hengxing Cai</p>
</li>
<li>
<p><a href="#link24">Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks</a>
<strong>Authors:</strong> Yuxiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, Jitao Sang</p>
</li>
<li>
<p><a href="#link25">QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs</a>
<strong>Authors:</strong> Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen</p>
</li>
<li>
<p><a href="#link26">MC#: Mixture Compressor for Mixture-of-Experts Large Models</a>
<strong>Authors:</strong> Wei Huang, Yue Liao, Yukang Chen, Jianhui Liu, Haoru Tan, Si Liu, Shiming Zhang, Shuicheng Yan, Xiaojuan Qi</p>
</li>
<li>
<p><a href="#link27">Neural Weight Compression for Language Models</a>
<strong>Authors:</strong> Jegwang Ryu, Minkyu Kim, Seungjun Shin, Hee Min Choi, Dokwan Oh, Jaeho Lee</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.09665" rel="nofollow">LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-lmcache-an-efficient-kv-cache-layer-for-enterprise-scale-llm-inference-" class="anchor" aria-label="Permalink: 0. LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference" href="#0-lmcache-an-efficient-kv-cache-layer-for-enterprise-scale-llm-inference-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09665
<strong>Authors:</strong> Yihua Cheng, Yuhan Liu, Jiayi Yao, Yuwei An, Xiaokun Chen, Shaoting Feng, Yuyang Huang, Samuel Shen, Kuntai Du, Junchen Jiang</p>
<p><strong>Abstract:</strong> arXiv:2510.09665v1 Announce Type: new  Abstract: Today's LLM inference systems treat individual engines and queries independently for simplicity, but this causes significant resource inefficiencies. While there are proposals to avoid redundant computation by reusing KV caches across queries and to increase GPU utilization by disaggregating a single query to different engines, their promises cannot be realized without efficiently offloading and communicating KV cache across LLM inference engines and queries.   We present LMCache, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) and shares the KV caches across engines and queries. LMCache exposes KV caches in the LLM engine interface, effectively transforming LLM engines from individual token processors to a collection of engines with KV cache as the storage and communication medium. In particular, it supports both cache offloading (prefix reuse across queries) and prefill-decode disaggregation (cross-engine cache transfer). LMCache's high performance and wide adoption stem from the following contributions: highly optimized KV cache data movement with performance optimizations including batched data movement operations, compute and I/O pipelining; a modular KV cache connector component, decoupling LMCache from the rapid evolution of inference engines; a first-class control API, such as pinning, lookup, cleanup, movement, and compression, for flexible cache orchestration across GPU, CPU, storage, and network layers. Evaluation shows that combining LMCache with vLLM achieves up to 15x improvement in throughput across diverse workloads. With a growing community, LMCache has seen dramatic growth in adoption by enterprise inference systems, which provides valuable lessons for future KV caching solutions. The source code of LMCache is at: <a href="https://github.com/LMCache/LMCache">https://github.com/LMCache/LMCache</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.12178" rel="nofollow">Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-evolution-of-metas-llama-models-and-parameter-efficient-fine-tuning-of-large-language-models-a-survey-" class="anchor" aria-label="Permalink: 1. Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey" href="#1-evolution-of-metas-llama-models-and-parameter-efficient-fine-tuning-of-large-language-models-a-survey-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.12178
<strong>Authors:</strong> Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi</p>
<p><strong>Abstract:</strong> arXiv:2510.12178v1 Announce Type: new  Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.12323" rel="nofollow">RAG-Anything: All-in-One RAG Framework</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-rag-anything-all-in-one-rag-framework-" class="anchor" aria-label="Permalink: 2. RAG-Anything: All-in-One RAG Framework" href="#2-rag-anything-all-in-one-rag-framework-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.12323
<strong>Authors:</strong> Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang</p>
<p><strong>Abstract:</strong> arXiv:2510.12323v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: <a href="https://github.com/HKUDS/RAG-Anything">https://github.com/HKUDS/RAG-Anything</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.10964" rel="nofollow">Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-not-all-bits-are-equal-scale-dependent-memory-optimization-strategies-for-reasoning-models-" class="anchor" aria-label="Permalink: 3. Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models" href="#3-not-all-bits-are-equal-scale-dependent-memory-optimization-strategies-for-reasoning-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10964
<strong>Authors:</strong> Junhyuck Kim, Ethan Ewer, Taehong Moon, Jongho Park, Dimitris Papailiopoulos</p>
<p><strong>Abstract:</strong> arXiv:2510.10964v1 Announce Type: new  Abstract: While 4-bit quantization has emerged as a memory-optimal choice for non-reasoning models and zero-shot tasks across scales, we show that this universal prescription fails for reasoning models, where the KV cache rather than model size can dominate memory. Through systematic experiments across 1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent trade-off: models with an effective size below 8-bit 4B parameters achieve better accuracy by allocating memory to more weights rather than longer generation, while larger models achieve better accuracy by allocating memory to longer generations. This scale threshold also determines when parallel scaling becomes memory-efficient and whether KV cache eviction outperforms KV quantization. Our findings show that memory optimization for LLMs cannot be scale-agnostic, while providing principled guidelines: for small reasoning models, prioritize model capacity over test-time compute, while for larger ones, maximize test-time compute. Our results suggest that optimizing reasoning models for deployment requires fundamentally different strategies from those established for non-reasoning models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.11282" rel="nofollow">Vision-LLMs for Spatiotemporal Traffic Forecasting</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-vision-llms-for-spatiotemporal-traffic-forecasting-" class="anchor" aria-label="Permalink: 4. Vision-LLMs for Spatiotemporal Traffic Forecasting" href="#4-vision-llms-for-spatiotemporal-traffic-forecasting-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11282
<strong>Authors:</strong> Ning Yang, Hengyu Zhong, Haijun Zhang, Randall Berry</p>
<p><strong>Abstract:</strong> arXiv:2510.11282v1 Announce Type: new  Abstract: Accurate spatiotemporal traffic forecasting is a critical prerequisite for proactive resource management in dense urban mobile networks. While Large Language Models (LLMs) have shown promise in time series analysis, they inherently struggle to model the complex spatial dependencies of grid-based traffic data. Effectively extending LLMs to this domain is challenging, as representing the vast amount of information from dense geographical grids can be inefficient and overwhelm the model's context. To address these challenges, we propose ST-Vision-LLM, a novel framework that reframes spatiotemporal forecasting as a vision-language fusion problem. Our approach leverages a Vision-LLM visual encoder to process historical global traffic matrices as image sequences, providing the model with a comprehensive global view to inform cell-level predictions. To overcome the inefficiency of LLMs in handling numerical data, we introduce an efficient encoding scheme that represents floating-point values as single tokens via a specialized vocabulary, coupled with a two-stage numerical alignment fine-tuning process. The model is first trained with Supervised Fine-Tuning (SFT) and then further optimized for predictive accuracy using Group Relative Policy Optimization (GRPO), a memory-efficient reinforcement learning method. Evaluations on real-world mobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing methods by 15.6% in long-term prediction accuracy and exceeds the second-best baseline by over 30.04% in cross-domain few-shot scenarios. Our extensive experiments validate the model's strong generalization capabilities across various data-scarce environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.11484" rel="nofollow">Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-rescaling-aware-training-for-efficient-deployment-of-deep-learning-models-on-full-integer-hardware-" class="anchor" aria-label="Permalink: 5. Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware" href="#5-rescaling-aware-training-for-efficient-deployment-of-deep-learning-models-on-full-integer-hardware-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11484
<strong>Authors:</strong> Lion Mueller, Alberto Garcia-Ortiz, Ardalan Najafi, Adam Fuks, Lennart Bamberg</p>
<p><strong>Abstract:</strong> arXiv:2510.11484v1 Announce Type: new  Abstract: Integer AI inference significantly reduces computational complexity in embedded systems. Quantization-aware training (QAT) helps mitigate accuracy degradation associated with post-training quantization but still overlooks the impact of integer rescaling during inference, which is a hardware costly operation in integer-only AI inference. This work shows that rescaling cost can be dramatically reduced post-training, by applying a stronger quantization to the rescale multiplicands at no model-quality loss. Furthermore, we introduce Rescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling multiplicands. Experiments show that even with 8x reduced rescaler widths, the full accuracy is preserved through minimal incremental retraining. This enables more energy-efficient and cost-efficient AI inference for resource-constrained embedded systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.11292" rel="nofollow">LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-louiskv-efficient-kv-cache-retrieval-for-long-input-output-sequences-" class="anchor" aria-label="Permalink: 6. LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences" href="#6-louiskv-efficient-kv-cache-retrieval-for-long-input-output-sequences-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11292
<strong>Authors:</strong> Wenbo Wu, Qingyi Si, Xiurui Pan, Ye Wang, Jie Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.11292v1 Announce Type: new  Abstract: While Key-Value (KV) cache succeeds in reducing redundant computations in auto-regressive models, it introduces significant memory overhead, limiting its practical deployment in long-sequence scenarios. Existing KV retrieval methods mitigate this by dynamically retaining only a subset of KV entries on the GPU. However, they still suffer from notable efficiency and accuracy bottlenecks due to per-token retrieval and coarse-grained page-level KV management, especially in long-output reasoning scenarios. With the emergence of large reasoning models, efficiently handling such scenarios has become increasingly important. To address this issue, we present two key observations: (1) critical KVs exhibit strong temporal locality during decoding, and (2) these KVs exhibit distinct distribution patterns across the input prompt and generated output. Building on these observations, we propose LouisKV, an efficient KV cache retrieval framework designed for various long-sequence scenarios. Specifically, LouisKV introduces a semantic-aware retrieval strategy leveraging temporal locality to trigger retrieval only at semantic boundaries, drastically reducing computation and data transfer overhead. LouisKV also designs a decoupled, fine-grained management scheme that tailors differentiated strategies for input and output sequences to create retrieval units that better match the model's attention patterns, enabling precise identification of critical KVs. Furthermore, to boost efficiency, LouisKV incorporates several kernel-level optimizations, including custom Triton and CUDA kernels to accelerate the KV clustering and retrieval. Evaluations show that LouisKV achieves up to 4.7$\times$ speedup over state-of-the-art KV retrieval methods while maintaining near-lossless accuracy across diverse long-sequence tasks, including long-input short-output, short-input long-output, and long-input long-output scenarios.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.10136" rel="nofollow">PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-permllm-learnable-channel-permutation-for-nm-sparse-large-language-models-" class="anchor" aria-label="Permalink: 7. PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models" href="#7-permllm-learnable-channel-permutation-for-nm-sparse-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10136
<strong>Authors:</strong> Lancheng Zou, Shuo Yin, Zehua Pei, Tsung-Yi Ho, Farzan Farnia, Bei Yu</p>
<p><strong>Abstract:</strong> arXiv:2510.10136v1 Announce Type: new  Abstract: Channel permutation is a powerful technique for enhancing the accuracy of N:M sparse models by reordering the channels of weight matrices to prioritize the retention of important weights. However, traditional channel permutation methods rely on handcrafted quality metrics, which often fail to accurately capture the true impact of pruning on model performance. To address this limitation, we propose PermLLM, a novel post-training pruning framework that introduces learnable channel permutation (LCP) for N:M sparsity. LCP leverages Sinkhorn normalization to transform discrete permutation matrices into differentiable soft permutation matrices, enabling end-to-end optimization. Additionally, PermLLM incorporates an efficient block-wise channel permutation strategy, which significantly reduces the number of learnable parameters and computational complexity. PermLLM seamlessly integrates with existing one-shot pruning methods to adaptively optimize channel permutations, effectively mitigating pruning-induced errors. Extensive experiments on the LLaMA series, Qwen, and OPT models demonstrate that PermLLM achieves superior performance in optimizing N:M sparse models. The code is available at <a href="https://github.com/lanchengzou/PermLLM">https://github.com/lanchengzou/PermLLM</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.10129" rel="nofollow">CacheClip: Accelerating RAG with Effective KV Cache Reuse</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-cacheclip-accelerating-rag-with-effective-kv-cache-reuse-" class="anchor" aria-label="Permalink: 8. CacheClip: Accelerating RAG with Effective KV Cache Reuse" href="#8-cacheclip-accelerating-rag-with-effective-kv-cache-reuse-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10129
<strong>Authors:</strong> Bin Yang, Qiuyu Leng, Jun Zeng, Zhenhua Wu</p>
<p><strong>Abstract:</strong> arXiv:2510.10129v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) systems suffer from severe time-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV cache reuse methods face a fundamental trade-off: prefix caching requires identical prefixes that rarely occur in RAG scenarios, while direct precomputation sacrifices quality due to missing inter-chunk attention and repeated attention sinks. Recent methods like APE and CacheBlend partially address these issues but remain inadequate for robust RAG applications. This paper presents CacheClip, a novel framework that achieves both fast TTFT and high generation quality. Our key insight is that small auxiliary LLMs exhibit similar last-layer attention distributions to primary LLMs (the target model for generation), enabling efficient identification of tokens critical for restoring inter-chunk attention, thereby significantly improving response quality on cross-chunk reasoning tasks. CacheClip integrates three techniques: (1) auxiliary-model-guided token selection for selective KV cache recomputation, where the auxiliary model is finetuned to improve selection accuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3) grouping strategy to maintain local coherence during partial KV cache updates. Experiments show CacheClip retains up to 94.8% and 85.0% of full-attention performance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2% and 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM inference by up to 1.92x in prefill time, providing a practical solution to the efficiency-quality trade-off in RAG systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.09762" rel="nofollow">PatentVision: A multimodal method for drafting patent applications</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-patentvision-a-multimodal-method-for-drafting-patent-applications-" class="anchor" aria-label="Permalink: 9. PatentVision: A multimodal method for drafting patent applications" href="#9-patentvision-a-multimodal-method-for-drafting-patent-applications-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09762
<strong>Authors:</strong> Ruo Yang, Sai Krishna Reddy Mudhiganti, Manali Sharma</p>
<p><strong>Abstract:</strong> arXiv:2510.09762v1 Announce Type: new  Abstract: Patent drafting is complex due to its need for detailed technical descriptions, legal compliance, and visual elements. Although Large Vision Language Models (LVLMs) show promise across various tasks, their application in automating patent writing remains underexplored. In this paper, we present PatentVision, a multimodal framework that integrates textual and visual inputs such as patent claims and drawings to generate complete patent specifications. Built on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned vision language models with domain specific training tailored to patents. Experiments reveal it surpasses text only methods, producing outputs with greater fidelity and alignment with human written standards. Its incorporation of visual data allows it to better represent intricate design features and functional connections, leading to richer and more precise results. This study underscores the value of multimodal techniques in patent automation, providing a scalable tool to reduce manual workloads and improve consistency. PatentVision not only advances patent drafting but also lays the groundwork for broader use of LVLMs in specialized areas, potentially transforming intellectual property management and innovation processes.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.10028" rel="nofollow">Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-efficient-onboard-vision-language-inference-in-uav-enabled-low-altitude-economy-networks-via-llm-enhanced-optimization-" class="anchor" aria-label="Permalink: 10. Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization" href="#10-efficient-onboard-vision-language-inference-in-uav-enabled-low-altitude-economy-networks-via-llm-enhanced-optimization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10028
<strong>Authors:</strong> Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim</p>
<p><strong>Abstract:</strong> arXiv:2510.10028v1 Announce Type: new  Abstract: The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled a variety of applications, including aerial surveillance, environmental sensing, and semantic data collection. To support these scenarios, unmanned aerial vehicles (UAVs) equipped with onboard vision-language models (VLMs) offer a promising solution for real-time multimodal inference. However, ensuring both inference accuracy and communication efficiency remains a significant challenge due to limited onboard resources and dynamic network conditions. In this paper, we first propose a UAV-enabled LAENet system model that jointly captures UAV mobility, user-UAV communication, and the onboard visual question answering (VQA) pipeline. Based on this model, we formulate a mixed-integer non-convex optimization problem to minimize task latency and power consumption under user-specific accuracy constraints. To solve the problem, we design a hierarchical optimization framework composed of two parts: (i) an Alternating Resolution and Power Optimization (ARPO) algorithm for resource allocation under accuracy constraints, and (ii) a Large Language Model-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV trajectory optimization. The large language model (LLM) serves as an expert in refining reward design of reinforcement learning in an offline fashion, introducing no additional latency in real-time decision-making. Numerical results demonstrate the efficacy of our proposed framework in improving inference performance and communication efficiency under dynamic LAENet conditions.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.12276" rel="nofollow">Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-spatial-forcing-implicit-spatial-representation-alignment-for-vision-language-action-model-" class="anchor" aria-label="Permalink: 11. Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model" href="#11-spatial-forcing-implicit-spatial-representation-alignment-for-vision-language-action-model-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.12276
<strong>Authors:</strong> Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li</p>
<p><strong>Abstract:</strong> arXiv:2510.12276v1 Announce Type: new  Abstract: Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators.We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision.Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at <a href="https://spatial-forcing.github.io/" rel="nofollow">https://spatial-forcing.github.io/</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.12693" rel="nofollow">ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-era-transforming-vlms-into-embodied-agents-via-embodied-prior-learning-and-online-reinforcement-learning-" class="anchor" aria-label="Permalink: 12. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning" href="#12-era-transforming-vlms-into-embodied-agents-via-embodied-prior-learning-and-online-reinforcement-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.12693
<strong>Authors:</strong> Hanyang Chen, Mark Zhao, Rui Yang, Qinwei Ma, Ke Yang, Jiarui Yao, Kangrui Wang, Hao Bai, Zhenhailong Wang, Rui Pan, Mengchao Zhang, Jose Barreiros, Aykut Onol, ChengXiang Zhai, Heng Ji, Manling Li, Huan Zhang, Tong Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.12693v1 Announce Type: new  Abstract: Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present \textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, \textit{Embodied Prior Learning}, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4% on EB-ALFRED and 19.4% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.09942" rel="nofollow">Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-conformal-sparsification-for-bandwidth-efficient-edge-cloud-speculative-decoding-" class="anchor" aria-label="Permalink: 13. Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding" href="#13-conformal-sparsification-for-bandwidth-efficient-edge-cloud-speculative-decoding-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09942
<strong>Authors:</strong> Payel Bhattacharjee, Fengwei Tian, Meiyu Zhong, Guangyi Zhang, Osvaldo Simeone, Ravi Tandon</p>
<p><strong>Abstract:</strong> arXiv:2510.09942v1 Announce Type: new  Abstract: Edge-cloud speculative decoding (SD) accelerates inference by having a cloud-based large language model (LLM) that verifies draft tokens generated by a resource-constrained small language model (SLM) at the edge. A central bottleneck is the limited bandwidth of the edge-cloud link, which necessitates efficient compression of draft token distributions. We first derive an information-theoretic bound that decomposes the token rejection rate into contributions from SLM-LLM distribution mismatch and from quantization distortion. Guided by this analysis, we propose the Sparse Quantize-and-Sample SD (SQS-SD) framework, which exploits distributional sparsity through structured sparsification and lattice-based quantization. Within this framework, K-SQS applies fixed top-K truncation, while C-SQS adaptively adjusts the retained token set via online conformal prediction to ensure bounded deviation from the dense distribution. Empirical results confirm that both approaches improve end-to-end latency and rejection rates in complimentary operating regimes.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.10467" rel="nofollow">AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-anybcq-hardware-efficient-flexible-binary-coded-quantization-for-multi-precision-llms-" class="anchor" aria-label="Permalink: 14. AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs" href="#14-anybcq-hardware-efficient-flexible-binary-coded-quantization-for-multi-precision-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10467
<strong>Authors:</strong> Gunho Park, Jeongin Bae, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</p>
<p><strong>Abstract:</strong> arXiv:2510.10467v1 Announce Type: new  Abstract: The deployment of large language models (LLMs) is increasingly constrained by memory and latency bottlenecks, motivating the need for quantization techniques that flexibly balance accuracy and efficiency. Recent work has introduced multi-precision models, which enable inference at multiple precisions within a single model depending on runtime constraints. To support such flexibility, quantized weights are often stored as bit-planes, where hardware efficiency improves when the compute operates directly at the bit-plane level and activates only the precision required by each request. In this work, we present AnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded Quantization (BCQ) that supports direct bit-plane operations. By representing weights as binary bit-planes with corresponding scale factors, AnyBCQ enables bit-plane-level computation and maps naturally to accelerator-friendly, bit-parallel arithmetic. Our progressive precision expansion mechanism incrementally refines scaling factors while reusing previously assigned binary codes, yielding monotonic improvements in accuracy as additional bits are enabled. We further co-design a specialized kernel that exploits the BCQ structure to support dynamic per-request precision selection with negligible overhead. Experiments on recent LLMs demonstrate that AnyBCQ significantly narrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains competitive at higher precision, and achieves throughput gains of up to 3.0x over half precision and 1.2x over state-of-the-art multi-precision methods. By aligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a practical foundation for multi-precision LLM deployment across diverse service-level objectives.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2510.09976" rel="nofollow">Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-reinforcement-fine-tuning-of-flow-matching-policies-for-vision-language-action-models-" class="anchor" aria-label="Permalink: 15. Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models" href="#15-reinforcement-fine-tuning-of-flow-matching-policies-for-vision-language-action-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09976
<strong>Authors:</strong> Mingyang Lyu, Yinqian Sun, Erliang Lin, Huangrui Li, Ruolin Chen, Feifei Zhao, Yi Zeng</p>
<p><strong>Abstract:</strong> arXiv:2510.09976v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\pi_0$ have shown strong generalization by leveraging large-scale demonstrations, yet their performance is still fundamentally constrained by the quality and coverage of supervised data. Reinforcement learning (RL) provides a promising path for improving and fine-tuning VLAs through online interaction. However, conventional policy gradient methods are computationally infeasible in the context of flow-matching based models due to the intractability of the importance sampling process, which requires explicit computation of policy ratios. To overcome this limitation, we propose Flow Policy Optimization (FPO) algorithm, which reformulates importance sampling by leveraging per-sample changes in the conditional flow-matching objective. Furthermore, FPO achieves stable and scalable online reinforcement fine-tuning of the $\pi_0$ model by integrating structure-aware credit assignment to enhance gradient efficiency, clipped surrogate objectives to stabilize optimization, multi-step latent exploration to encourage diverse policy updates, and a Q-ensemble mechanism to provide robust value estimation. We evaluate FPO on the LIBERO benchmark and the ALOHA simulation task against supervised, preference-aligned, diffusion-based, autoregressive online RL, and $\pi_0$-FAST baselines, observing consistent improvements over the imitation prior and strong alternatives with stable learning under sparse rewards. In addition, ablation studies and analyses of the latent space dynamics further highlight the contributions of individual components within FPO, validating the effectiveness of the proposed computational modules and the stable convergence of the conditional flow-matching objective during online RL.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2510.10432" rel="nofollow">Hierarchical LoRA MoE for Efficient CTR Model Scaling</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-hierarchical-lora-moe-for-efficient-ctr-model-scaling-" class="anchor" aria-label="Permalink: 16. Hierarchical LoRA MoE for Efficient CTR Model Scaling" href="#16-hierarchical-lora-moe-for-efficient-ctr-model-scaling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10432
<strong>Authors:</strong> Zhichen Zeng, Mengyue Hang, Xiaolong Liu, Xiaoyi Liu, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Zhining Liu, Siyang Yuan, Chaofei Yang, Yiqun Liu, Hang Yin, Jiyan Yang, Hanghang Tong</p>
<p><strong>Abstract:</strong> arXiv:2510.10432v1 Announce Type: new  Abstract: Deep models have driven significant advances in click-through rate (CTR) prediction. While vertical scaling via layer stacking improves model expressiveness, the layer-by-layer sequential computation poses challenges to efficient scaling. Conversely, horizontal scaling through Mixture of Experts (MoE) achieves efficient scaling by activating a small subset of experts in parallel, but flat MoE layers may struggle to capture the hierarchical structure inherent in recommendation tasks. To push the Return-On-Investment (ROI) boundary, we explore the complementary strengths of both directions and propose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic scaling in a parameter-efficient manner. Specifically, HiLoMoE employs lightweight rank-1 experts for parameter-efficient horizontal scaling, and stacks multiple MoE layers with hierarchical routing to enable combinatorially diverse expert compositions. Unlike conventional stacking, HiLoMoE routes based on prior layer scores rather than outputs, allowing all layers to execute in parallel. A principled three-stage training framework ensures stable optimization and expert diversity. Experiments on four public datasets show that HiLoMoE achieving better performance-efficiency tradeoff, achieving an average AUC improvement of 0.20% in AUC and 18.5% reduction in FLOPs compared to the non-MoE baseline.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2510.11168" rel="nofollow">ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-elmo-efficiency-via-low-precision-and-peak-memory-optimization-in-large-output-spaces-" class="anchor" aria-label="Permalink: 17. ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces" href="#17-elmo-efficiency-via-low-precision-and-peak-memory-optimization-in-large-output-spaces-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11168
<strong>Authors:</strong> Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar</p>
<p><strong>Abstract:</strong> arXiv:2510.11168v1 Announce Type: new  Abstract: Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2510.10071" rel="nofollow">ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-adept-continual-pretraining-via-adaptive-expansion-and-dynamic-decoupled-tuning-" class="anchor" aria-label="Permalink: 18. ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning" href="#18-adept-continual-pretraining-via-adaptive-expansion-and-dynamic-decoupled-tuning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10071
<strong>Authors:</strong> Jinyang Zhang, Yue Fang, Hongxin Ding, Weibin Liao, Muyang Ye, Xu Chu, Junfeng Zhao, Yasha Wang</p>
<p><strong>Abstract:</strong> arXiv:2510.10071v1 Announce Type: new  Abstract: Conventional continual pretraining (CPT) for large language model (LLM) domain adaptation often suffers from catastrophic forgetting and limited domain capacity. Existing strategies adopt layer expansion, introducing additional trainable parameters to accommodate new knowledge. However, the uniform expansion and updates still entangle general and domain learning, undermining its effectiveness. Our pilot studies reveal that LLMs exhibit functional specialization, where layers and units differentially encode general-critical capabilities, suggesting that parameter expansion and optimization should be function-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled Tuning for continual pretraining, a two-stage framework for domain-adaptive CPT. ADEPT first performs General-Competence Guided Selective Layer Expansion, duplicating layers least critical for the general domain to increase representational capacity while minimizing interference with general knowledge. It then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter units within expanded layers according to their general-domain importance and assigning asymmetric learning rates to balance knowledge injection and retention. Experiments on mathematical and medical benchmarks show that ADEPT outperforms full-parameter CPT by up to 5.76% on the general domain and 5.58% on the target domain with only 15% of parameters tuned and less than 50% training time. Ablation studies, theoretical analysis, and extended investigations further demonstrate the necessity of targeted expansion and decoupled optimization, providing new principles for efficient and robust domain-adaptive CPT. Our code is open-sourced at <a href="https://github.com/PuppyKnightUniversity/ADEPT">https://github.com/PuppyKnightUniversity/ADEPT</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2510.12710" rel="nofollow">Reflection-Based Task Adaptation for Self-Improving VLA</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-reflection-based-task-adaptation-for-self-improving-vla-" class="anchor" aria-label="Permalink: 19. Reflection-Based Task Adaptation for Self-Improving VLA" href="#19-reflection-based-task-adaptation-for-self-improving-vla-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.12710
<strong>Authors:</strong> Baicheng Li, Dong Wu, Zike Yan, Xinchen Liu, Zecui Zeng, Lusong Li, Hongbin Zha</p>
<p><strong>Abstract:</strong> arXiv:2510.12710v1 Announce Type: new  Abstract: Pre-trained Vision-Language-Action (VLA) models represent a major leap towards general-purpose robots, yet efficiently adapting them to novel, specific tasks in-situ remains a significant hurdle. While reinforcement learning (RL) is a promising avenue for such adaptation, the process often suffers from low efficiency, hindering rapid task mastery. We introduce Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation without human intervention. Our framework establishes a self-improving loop where the agent learns from its own experience to enhance both strategy and execution.   The core of our framework is a dual-pathway architecture that addresses the full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway enables rapid learning by using the VLM's causal reasoning to automatically synthesize a targeted, dense reward function from failure analysis. This provides a focused learning signal that significantly accelerates policy exploration. However, optimizing such proxy rewards introduces a potential risk of "reward hacking," where the agent masters the reward function but fails the actual task. To counteract this, our second pathway, Success-Driven Quality-Guided SFT, grounds the policy in holistic success. It identifies and selectively imitates high-quality successful trajectories, ensuring the agent remains aligned with the ultimate task goal. This pathway is strengthened by a conditional curriculum mechanism to aid initial exploration.   We conduct experiments in challenging manipulation tasks. The results demonstrate that our framework achieves faster convergence and higher final success rates compared to representative baselines. Our work presents a robust solution for creating self-improving agents that can efficiently and reliably adapt to new environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2510.09696" rel="nofollow">Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-vanishing-contributions-a-unified-approach-to-smoothly-transition-neural-models-into-compressed-form-" class="anchor" aria-label="Permalink: 20. Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form" href="#20-vanishing-contributions-a-unified-approach-to-smoothly-transition-neural-models-into-compressed-form-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09696
<strong>Authors:</strong> Lorenzo Nikiforos, Charalampos Antoniadis, Luciano Prono, Fabio Pareschi, Riccardo Rovatti, Gianluca Setti</p>
<p><strong>Abstract:</strong> arXiv:2510.09696v1 Announce Type: new  Abstract: The increasing scale of deep neural networks has led to a growing need for compression techniques such as pruning, quantization, and low-rank decomposition. While these methods are very effective in reducing memory, computation and energy consumption, they often introduce severe accuracy degradation when applied directly. We introduce Vanishing Contributions (VCON), a general approach for smoothly transitioning neural models into compressed form. Rather than replacing the original network directly with its compressed version, VCON executes the two in parallel during fine-tuning. The contribution of the original (uncompressed) model is progressively reduced, while that of the compressed model is gradually increased. This smooth transition allows the network to adapt over time, improving stability and mitigating accuracy degradation. We evaluate VCON across computer vision and natural language processing benchmarks, in combination with multiple compression strategies. Across all scenarios, VCON leads to consistent improvements: typical gains exceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus provides a generalizable method that can be applied to the existing compression techniques, with evidence of consistent gains across multiple benchmarks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2510.11498" rel="nofollow">ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-relook-vision-grounded-rl-with-a-multimodal-llm-critic-for-agentic-web-coding-" class="anchor" aria-label="Permalink: 21. ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding" href="#21-relook-vision-grounded-rl-with-a-multimodal-llm-critic-for-agentic-web-coding-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11498
<strong>Authors:</strong> Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou</p>
<p><strong>Abstract:</strong> arXiv:2510.11498v1 Announce Type: new  Abstract: While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2510.11400" rel="nofollow">FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-fedhybrid-breaking-the-memory-wall-of-federated-learning-via-hybrid-tensor-management-" class="anchor" aria-label="Permalink: 22. FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management" href="#22-fedhybrid-breaking-the-memory-wall-of-federated-learning-via-hybrid-tensor-management-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11400
<strong>Authors:</strong> Kahou Tam, Chunlin Tian, Li Li, Haikai Zhao, ChengZhong Xu</p>
<p><strong>Abstract:</strong> arXiv:2510.11400v1 Announce Type: new  Abstract: Federated Learning (FL) emerges as a new learning paradigm that enables multiple devices to collaboratively train a shared model while preserving data privacy. However, one fundamental and prevailing challenge that hinders the deployment of FL on mobile devices is the memory limitation. This paper proposes \textit{FedHybrid}, a novel framework that effectively reduces the memory footprint during the training process while guaranteeing the model accuracy and the overall training progress. Specifically, \textit{FedHybrid} first selects the participating devices for each training round by jointly evaluating their memory budget, computing capability, and data diversity. After that, it judiciously analyzes the computational graph and generates an execution plan for each selected client in order to meet the corresponding memory budget while minimizing the training delay through employing a hybrid of recomputation and compression techniques according to the characteristic of each tensor. During the local training process, \textit{FedHybrid} carries out the execution plan with a well-designed activation compression technique to effectively achieve memory reduction with minimum accuracy loss. We conduct extensive experiments to evaluate \textit{FedHybrid} on both simulation and off-the-shelf mobile devices. The experiment results demonstrate that \textit{FedHybrid} achieves up to a 39.1% increase in model accuracy and a 15.5$\times$ reduction in wall clock time under various memory budgets compared with the baselines.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2510.10248" rel="nofollow">Reasoning-Enhanced Large Language Models for Molecular Property Prediction</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-reasoning-enhanced-large-language-models-for-molecular-property-prediction-" class="anchor" aria-label="Permalink: 23. Reasoning-Enhanced Large Language Models for Molecular Property Prediction" href="#23-reasoning-enhanced-large-language-models-for-molecular-property-prediction-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10248
<strong>Authors:</strong> Jiaxi Zhuang, Yaorui Shi, Jue Hou, Yunong He, Mingwei Ye, Mingjun Xu, Yuming Su, Linfeng Zhang, Linfeng Zhang, Guolin Ke, Hengxing Cai</p>
<p><strong>Abstract:</strong> arXiv:2510.10248v1 Announce Type: new  Abstract: Molecular property prediction is crucial for drug discovery and materials science, yet existing approaches suffer from limited interpretability, poor cross-task generalization, and lack of chemical reasoning capabilities. Traditional machine learning models struggle with task transferability, while specialized molecular language models provide little insight into their decision-making processes. To address these limitations, we propose \textbf{MPPReasoner}, a multimodal large language model that incorporates chemical reasoning for molecular property prediction. Our approach, built upon Qwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to enable comprehensive molecular understanding. We develop a two-stage training strategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning trajectories generated through expert knowledge and multiple teacher models, followed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR employs verifiable, rule-based rewards that systematically evaluate chemical principle application, molecular structure analysis, and logical consistency through computational verification. Extensive experiments across 8 datasets demonstrate significant performance improvements, with MPPReasoner outperforming the best baselines by 7.91% and 4.53% on in-distribution and out-of-distribution tasks respectively. MPPReasoner exhibits exceptional cross-task generalization and generates chemically sound reasoning paths that provide valuable insights into molecular property analysis, substantially enhancing both interpretability and practical utility for chemists. Code is available at <a href="https://anonymous.4open.science/r/MPPReasoner-12687" rel="nofollow">https://anonymous.4open.science/r/MPPReasoner-12687</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2510.12635" rel="nofollow">Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-memory-as-action-autonomous-context-curation-for-long-horizon-agentic-tasks-" class="anchor" aria-label="Permalink: 24. Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks" href="#24-memory-as-action-autonomous-context-curation-for-long-horizon-agentic-tasks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.12635
<strong>Authors:</strong> Yuxiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, Jitao Sang</p>
<p><strong>Abstract:</strong> arXiv:2510.12635v1 Announce Type: new  Abstract: Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, we reframe working memory management as a learnable, intrinsic capability. We propose a novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of a unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of a continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose a new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the model's intrinsic capabilities.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">25. <a href="https://arxiv.org/abs/2510.11696" rel="nofollow">QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs</a> <a id="user-content-link25"></a>
</h2><a id="user-content-25-qerl-beyond-efficiency----quantization-enhanced-reinforcement-learning-for-llms-" class="anchor" aria-label="Permalink: 25. QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs" href="#25-qerl-beyond-efficiency----quantization-enhanced-reinforcement-learning-for-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11696
<strong>Authors:</strong> Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen</p>
<p><strong>Abstract:</strong> arXiv:2510.11696v1 Announce Type: new  Abstract: We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">26. <a href="https://arxiv.org/abs/2510.10962" rel="nofollow">MC#: Mixture Compressor for Mixture-of-Experts Large Models</a> <a id="user-content-link26"></a>
</h2><a id="user-content-26-mc-mixture-compressor-for-mixture-of-experts-large-models-" class="anchor" aria-label="Permalink: 26. MC#: Mixture Compressor for Mixture-of-Experts Large Models" href="#26-mc-mixture-compressor-for-mixture-of-experts-large-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10962
<strong>Authors:</strong> Wei Huang, Yue Liao, Yukang Chen, Jianhui Liu, Haoru Tan, Si Liu, Shiming Zhang, Shuicheng Yan, Xiaojuan Qi</p>
<p><strong>Abstract:</strong> arXiv:2510.10962v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and vision-language models (VLMs) by increasing capacity through sparse activation. However, preloading all experts into memory and activating multiple experts per input introduces significant computational and memory overhead, making the expert module a major contributor to model size and inference cost. To address this, we propose MC# (Mixture-Compressor-sharp), a framework that combines static quantization and dynamic expert pruning by leveraging the significance of experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce storage and loading costs, we introduce Pre-Loading Mixed-Precision Quantization (PMQ), which optimizes bit allocation via linear programming, balancing expert importance and quantization error for a Pareto-optimal trade-off between size and performance. To reduce runtime computation, Online Top-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a subset of experts per token, enabling fine-grained control over activation. By combining PMQ's static bit-width optimization with OTP's dynamic routing, MC# achieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC# achieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7% accuracy drop across five multimodal benchmarks. Additionally, OTP reduces expert activation over 20% with less than 1% performance degradation, demonstrating strong potential for efficient MoE-based model deployment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">27. <a href="https://arxiv.org/abs/2510.11234" rel="nofollow">Neural Weight Compression for Language Models</a> <a id="user-content-link27"></a>
</h2><a id="user-content-27-neural-weight-compression-for-language-models-" class="anchor" aria-label="Permalink: 27. Neural Weight Compression for Language Models" href="#27-neural-weight-compression-for-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11234
<strong>Authors:</strong> Jegwang Ryu, Minkyu Kim, Seungjun Shin, Hee Min Choi, Dokwan Oh, Jaeho Lee</p>
<p><strong>Abstract:</strong> arXiv:2510.11234v1 Announce Type: new  Abstract: The efficient storage and transmission of language model weights is becoming increasingly important, as their scale and adoption continue to grow. However, as our understanding of this new data modality is limited, designing a good compression algorithm for language model weights heavily relies on manual, trial-and-error approaches. In this paper, we propose a learned compression framework that trains neural codecs directly from pretrained language model weights. Unlike conventional data (e.g., images), language model weights pose unique challenges: the sizes and shapes of weight tensors vary significantly, and the reconstruction quality must be judged by downstream model predictions rather than na"ive MSE loss. To address this, we introduce Neural Weight Compression (NWC), a novel autoencoder-based neural codec tailored to model weight compression. The proposed method inherits the advantages of autoencoder-based codecs while incorporating three technical components: (1) column-wise tensor chunking and normalization; (2) an importance-aware training loss; (3) an inference-time error compensation mechanism guided by model outputs. Experiments on open-weight language models show that NWC achieves competitive or state-of-the-art accuracy-compression tradeoffs, with particularly strong results at 4-6 bit precisions where accuracy remains nearly on par with FP16 models.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) and Mixture of Experts (MoE) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research introduces a novel multimodal large model that integrates text, language, and vision modalities. The goal is to advance the performance and generalization capabilities of the model by establishing a new approach for training and fusing these modalities effectively, rather than focusing on incremental optimizations.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>