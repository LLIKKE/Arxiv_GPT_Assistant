<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">分布式学习中通信压缩与拜占庭鲁棒性的协同优化</h2><a id="user-content-分布式学习中通信压缩与拜占庭鲁棒性的协同优化" class="anchor" aria-label="Permalink: 分布式学习中通信压缩与拜占庭鲁棒性的协同优化" href="#分布式学习中通信压缩与拜占庭鲁棒性的协同优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17129v1 公告类型：新研究<br>
摘要：分布式学习（DL）能够基于分散数据实现可扩展的模型训练，但仍面临拜占庭故障和高通信成本的双重挑战。虽然这两个问题各自已被广泛研究，但它们的交互作用却较少被探讨。现有研究表明，简单地将通信压缩与拜占庭鲁棒聚合相结合会降低对故障节点（或工作节点）的容错能力。目前最先进的算法Byz-DASHA-PAGE[29]利用动量方差缩减方案来缓解压缩噪声对拜占庭鲁棒性的不利影响。我们提出了一种新算法RoSDHB，该算法将经典的Polyak动量与新型协调压缩机制相结合。我们证明，在标准的(G,B)-梯度差异性异构模型下，RoSDHB的性能与Byz-DASHA-PAGE相当，同时依赖更少的假设。特别地，我们仅假设诚实工作节点平均损失函数的Lipschitz平滑性，而文献[29]还需额外假设有界全局Hessian方差这一特殊平滑条件。在基准图像分类任务上的实证结果表明，RoSDHB在实现强鲁棒性的同时显著节约了通信成本。</p>
<div class="markdown-heading"><h2 class="heading-element">一种人机协同方法提升预测性业务流程监控的公平性</h2><a id="user-content-一种人机协同方法提升预测性业务流程监控的公平性" class="anchor" aria-label="Permalink: 一种人机协同方法提升预测性业务流程监控的公平性" href="#一种人机协同方法提升预测性业务流程监控的公平性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17477v1 公告类型：新成果<br>
摘要：预测性流程监控使组织能够对运行中的业务流程实例主动作出反应和干预。针对未完成的流程实例，系统会生成关于结果、下一活动或剩余时间的预测。这通过强大的机器学习模型实现，这些模型已展现出卓越的预测性能。然而，这些模型的数据驱动特性使其容易从数据中发现不公平、有偏见或不道德的规律。此类规律会导致基于所谓敏感属性（如流程参与者的性别或年龄）产生带有偏见的预测。先前研究已识别出该问题，并提出通过完全移除流程实例中的敏感属性来缓解偏差的解决方案。但敏感属性在同一流程实例中可能同时存在公平与不公平的使用场景，例如医疗流程中，治疗方案决策可基于性别因素，而患者接收决策则不应考虑性别。本文提出一种新颖的模型无关方法，用于识别和修正预测性业务流程监控模型中的偏见决策，即使同一敏感属性同时存在公平与不公平的使用情况。该方法采用人机协同机制，通过对从原始预测模型提炼出的决策树模型进行简单修改，来区分公平与不公平决策。实验结果表明，在存在偏见数据的情况下，所提方法在公平性与准确性之间实现了良好平衡。所有源代码和数据均公开于<a href="https://doi.org/10.5281/zenodo.15387576%E3%80%82" rel="nofollow">https://doi.org/10.5281/zenodo.15387576。</a></p>
<div class="markdown-heading"><h2 class="heading-element">预算思考者：通过控制令牌实现预算感知的大语言模型推理能力增强</h2><a id="user-content-预算思考者通过控制令牌实现预算感知的大语言模型推理能力增强" class="anchor" aria-label="Permalink: 预算思考者：通过控制令牌实现预算感知的大语言模型推理能力增强" href="#预算思考者通过控制令牌实现预算感知的大语言模型推理能力增强"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17196v1 公告类型：新成果<br>
摘要：大型语言模型（LLMs）近期通过增加测试时计算来提升推理能力，这一策略虽有效，却会带来显著延迟和资源消耗，限制了其在实时受限或成本敏感场景中的应用。本文提出BudgetThinker——一种创新框架，使LLMs具备预算感知推理能力，可精确控制思维链长度。我们设计了一种在推理过程中周期性插入特殊控制标记的方法，持续向模型反馈剩余标记预算。该方案结合完整的两阶段训练流程：首先通过监督微调（SFT）使模型适应预算约束，随后采用基于课程学习的强化学习（RL）阶段，利用长度感知奖励函数同步优化准确率与预算遵循度。实验表明，在具有挑战性的数学基准测试中，BudgetThinker在不同推理预算下均显著超越强基线模型。该方法为开发高效可控的LLM推理提供了可扩展的解决方案，使先进模型更适用于资源受限和实时环境部署。</p>
<div class="markdown-heading"><h2 class="heading-element">消息传递在节点分类中的局限性：类瓶颈如何限制信噪比</h2><a id="user-content-消息传递在节点分类中的局限性类瓶颈如何限制信噪比" class="anchor" aria-label="Permalink: 消息传递在节点分类中的局限性：类瓶颈如何限制信噪比" href="#消息传递在节点分类中的局限性类瓶颈如何限制信噪比"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17822v1 公告类型：新成果<br>
摘要：消息传递神经网络（MPNN）在节点分类任务中表现出强大能力，但在异配性（同类节点连接度低）和图结构瓶颈制约下存在性能局限。我们提出一个统一统计框架，通过MPNN表征的信噪比（SNR）揭示异配性与瓶颈效应之间的内在关联。该信噪比模型将性能分解为特征相关参数与特征无关的敏感度：证明类别信号敏感度受高阶同配性（经典同配性在多跳邻域中的推广）约束，并揭示低阶同配性在局部表现为结构瓶颈与类别标签的交互作用（类别瓶颈）。通过对图集合的理论分析，我们进一步将瓶颈效应定量分解为"欠抵达"（深度不足导致信号无法传递）和"过挤压"（广度不足导致信号传输路径过少），并给出闭式表达式。研究证明，最大化高阶同配性的最优图结构由单类别和双类别二分簇的互不相交联合构成。基于此提出BRIDGE算法——一种基于图集合重构的优化方法，在合成基准测试中实现全同配性区间接近完美的分类精度，在现实基准测试中显著提升性能。该算法通过消除MPNN通常表现欠佳的"中同配性陷阱"，超越了现有主流图重构技术。我们公开了算法代码的框架既提供评估MPNN性能的诊断工具，也通过原理性图修改为性能提升提供简单有效的方法。</p>
<div class="markdown-heading"><h2 class="heading-element">锚定专家混合模型（Anchor-MoE）：面向概率回归的均值锚定专家混合方法</h2><a id="user-content-锚定专家混合模型anchor-moe面向概率回归的均值锚定专家混合方法" class="anchor" aria-label="Permalink: 锚定专家混合模型（Anchor-MoE）：面向概率回归的均值锚定专家混合方法" href="#锚定专家混合模型anchor-moe面向概率回归的均值锚定专家混合方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译在保持专业术语准确性的同时，采用中文常见的学术表达方式：</p>
<ol>
<li>"Mean-Anchored" 译为"均值锚定"既保留统计学中"均值"概念，又体现"锚定"的技术意象</li>
<li>"Mixture of Experts" 采用学界通用译法"专家混合模型"</li>
<li>通过增译"方法"二字使中文表述更完整，符合技术文献标题规范</li>
<li>使用破折号连接主副标题，保持原文的层级关系）</li>
</ol>
<p>arXiv:2508.16802v1 公告类型：新成果<br>
摘要：不确定性下的回归问题是科学与工程领域的核心课题。我们提出锚点混合专家模型（Anchor-MoE），该模型可同时处理概率回归与点回归。为简化实现，我们采用调优后的梯度提升模型提供锚点均值，但任何现成的点回归器均可作为锚点使用。锚点预测值被映射到潜在空间中，通过可学习的度量窗口核函数进行局部性评分，软路由器将每个样本分配至少量混合密度网络专家；专家系统生成异方差校正和预测方差。我们通过最小化负对数似然进行训练，并在独立校准集上对预测均值进行事后线性映射以提升点预测精度。理论方面，假设回归函数满足α阶Hölder光滑性且具有固定Lipschitz单位分解权重（有限重叠），证明Anchor-MoE可达最小最大最优L²风险率O(N^{-2α/(2α+d)})。此外，CRPS测试泛化 gap 以Õ(√((log(Mh)+P+K)/N))缩放，其对Mh呈对数依赖，对P和K呈平方根缩放。在有限重叠路由下，K可替换为k，潜在维度的影响被吸收至P中。在均值方差一致有界条件下，测试NLL同样具有Õ(√((log(Mh)+P+K)/N))的缩放特性（至常数项）。实证表明，在标准UCI回归任务中，Anchor-MoE在RMSE和NLL指标上持续匹配或超越强基准NGBoost；在多个数据集上创造了我们基准测试套件中概率回归的新最优结果。代码详见<a href="https://github.com/BaozhuoSU/Probabilistic_Regression%E3%80%82">https://github.com/BaozhuoSU/Probabilistic_Regression。</a></p>
<div class="markdown-heading"><h2 class="heading-element">模仿物理学家的视角：以视觉语言模型为核心的物理公式发现方法</h2><a id="user-content-模仿物理学家的视角以视觉语言模型为核心的物理公式发现方法" class="anchor" aria-label="Permalink: 模仿物理学家的视角：以视觉语言模型为核心的物理公式发现方法" href="#模仿物理学家的视角以视觉语言模型为核心的物理公式发现方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17380v1 公告类型：新成果<br>
摘要：从现实世界的观测数据中自动发现物理定律是人工智能领域的重大挑战。现有方法依赖符号回归或大语言模型，仅限于单模态数据，忽视了物理学家不可或缺的丰富视觉化运动现象表征。这种"感官剥夺"严重削弱了它们解释动态现象中固有时空模式的能力。为弥补这一缺陷，我们提出多模态模型VIPER-R1，通过视觉归纳进行物理方程推理以发现基础符号公式。该模型融合视觉感知、轨迹数据和符号推理，模拟科学发现过程：通过运动结构归纳（MSI）课程进行训练，使用监督微调解读运动相轨迹图，并遵循因果思维链（C-CoT）构建假设；继而通过奖励引导的符号校准（RGSC）以强化学习优化公式结构。在推理阶段，训练完成的VIPER-R1作为智能体运作：先提出高置信度的符号假设，再主动调用外部符号回归工具执行符号残差重整（SR²）——这一最终步骤类似于物理学家的扰动分析，使理论模型与实证数据相契合。为支持本研究，我们构建了包含5,000个实例的多模态数据集PhysSymbol。实验表明，VIPER-R1在准确性和可解释性上持续超越最先进的视觉语言模型基线，能更精确地发现物理定律。项目页面：<a href="https://jiaaqiliu.github.io/VIPER-R1/" rel="nofollow">https://jiaaqiliu.github.io/VIPER-R1/</a></p>
<div class="markdown-heading"><h2 class="heading-element">FlowVLA：以视觉思维链进行动态思考</h2><a id="user-content-flowvla以视觉思维链进行动态思考" class="anchor" aria-label="Permalink: FlowVLA：以视觉思维链进行动态思考" href="#flowvla以视觉思维链进行动态思考"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18269v1 公告类型：新研究<br>
摘要：多数视觉-语言-动作（VLA）模型依赖通过下一帧预测训练的内部世界模型。然而，这种方法在物理推理方面存在局限，因其将静态外观与动态运动混为一谈，常导致不合理的视觉预测和低效的策略学习。为解决这些问题，我们提出视觉思维链（Visual CoT）：一种预训练框架，促使模型在预测场景外观前先推理其演化过程。基于此理念，我们构建了FlowVLA模型——该模型仅在生成编码运动动态的中间光流表示（$f_t$）后，才预测未来帧（$v_{t+1}$）。这种"$v_t \rightarrow f_t \rightarrow v_{t+1}$"的推理过程通过单一自回归Transformer实现，引导模型学习解耦的动态表征。实验表明，FlowVLA不仅能生成连贯的视觉预测，还显著提升了策略学习效率。在具有挑战性的机器人操作基准测试中，该方法以大幅提升的样本效率实现了最先进性能，为世界建模奠定了更严谨的理论基础。项目页面：<a href="https://irpn-lab.github.io/FlowVLA/" rel="nofollow">https://irpn-lab.github.io/FlowVLA/</a></p>
<p>（注：翻译严格遵循了术语统一性，将"optical flow"译为专业术语"光流"，"autoregressive Transformer"保留技术特征译为"自回归Transformer"，同时将长难句拆解为符合中文表达习惯的短句结构。）</p>
<div class="markdown-heading"><h2 class="heading-element">Transformer上的模块感知参数高效机器遗忘</h2><a id="user-content-transformer上的模块感知参数高效机器遗忘" class="anchor" aria-label="Permalink: Transformer上的模块感知参数高效机器遗忘" href="#transformer上的模块感知参数高效机器遗忘"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17233v1 公告类型：新成果<br>
摘要：Transformer已成为支撑众多预训练大模型的基础架构，这些模型在不同应用领域取得了显著成功。机器遗忘技术专注于高效消除特定数据影响以符合隐私监管要求，其通过仅针对影响关键参数进行更新展现出巨大潜力。然而，现有参数高效遗忘方法大多采用模块无关的设计思路，往往难以准确识别这些关键参数，导致Transformer的遗忘性能不佳。本文提出{\tt MAPE-Unlearn}——一种模块感知的参数高效机器遗忘方法，该方法通过可学习的双掩码机制精确定位Transformer中注意力头和前馈网络层的影响关键参数。这些掩码的学习目标基于遗忘任务的核心要求设计，并通过采用预热启动的贪婪搜索高效算法进行优化。在多种Transformer模型和数据集上的大量实验证明了{\tt MAPE-Unlearn}在遗忘任务中的有效性与鲁棒性。</p>
<div class="markdown-heading"><h2 class="heading-element">MoE-Inference-Bench：混合专家大型语言与视觉模型的性能评估</h2><a id="user-content-moe-inference-bench混合专家大型语言与视觉模型的性能评估" class="anchor" aria-label="Permalink: MoE-Inference-Bench：混合专家大型语言与视觉模型的性能评估" href="#moe-inference-bench混合专家大型语言与视觉模型的性能评估"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17467v1 公告类型：新成果<br>
摘要：混合专家（MoE）模型通过实现海量参数规模同时保持计算效率，推动了大语言模型（LLMs）和视觉语言模型（VLMs）的扩展。然而，MoE模型在推理阶段引入了若干挑战，包括专家间负载不均衡以及额外的路由计算开销。为应对这些挑战并充分发挥MoE优势，系统性地评估硬件加速技术至关重要。我们提出MoE-Inference-Bench——一个涵盖多场景的MoE性能综合研究框架。我们分析了批处理大小、序列长度以及关键MoE超参数（如前馈网络维度与专家数量）对吞吐量的影响，并在英伟达H100 GPU上评估了剪枝、融合MoE操作、推测解码、量化及多种并行化策略等优化技术。评估对象包括Mixtral、DeepSeek、OLMoE和Qwen系列的MoE模型。研究结果揭示了不同配置间的性能差异，为MoE模型的高效部署提供了重要参考依据。</p>
<div class="markdown-heading"><h2 class="heading-element">候选清单模型：面向离散变量生成的简化SimplexDiffusion方法</h2><a id="user-content-候选清单模型面向离散变量生成的简化simplexdiffusion方法" class="anchor" aria-label="Permalink: 候选清单模型：面向离散变量生成的简化SimplexDiffusion方法" href="#候选清单模型面向离散变量生成的简化simplexdiffusion方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：SimplexDiffusion作为专业术语保留英文形式，以保持技术准确性；"Streamlined"译为"简化"体现模型优化特性；"Discrete Variable Generation"采用"离散变量生成"这一标准学术译法）</p>
<p>arXiv:2508.17345v1 公告类型：新成果<br>
摘要：离散变量的生成建模虽具挑战性，但对自然语言处理和生物序列设计应用至关重要。我们提出短列表模型（SLM），这是一种基于单纯形的新颖扩散模型，其灵感来源于渐进式候选剪枝策略。SLM通过操作单纯形质心降低生成复杂度并提升可扩展性。此外，该模型融合了灵活的无分类器引导实现机制，显著增强了无条件生成性能。在DNA启动子与增强子设计、蛋白质设计、字符级及大词汇量语言建模等任务上的大量实验表明，SLM具有竞争优势和强大潜力。代码已开源：<a href="https://github.com/GenSI-THUAIR/SLM">https://github.com/GenSI-THUAIR/SLM</a></p>
<div class="markdown-heading"><h2 class="heading-element">神经算法推理器赋能的大语言模型应用于多智能体路径规划</h2><a id="user-content-神经算法推理器赋能的大语言模型应用于多智能体路径规划" class="anchor" aria-label="Permalink: 神经算法推理器赋能的大语言模型应用于多智能体路径规划" href="#神经算法推理器赋能的大语言模型应用于多智能体路径规划"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17971v1 公告类型：新成果<br>
摘要：大型语言模型（LLM）的发展与应用已证明基础模型能用于解决多种任务，但其在多智能体路径规划（MAPF）任务中的表现尚不理想，相关研究也较为有限。MAPF是一个需要同时兼顾规划与多智能体协调的复杂问题。为提升LLM在MAPF任务中的性能，我们提出创新框架LLM-NAR，通过神经算法推理器（NAR）为LLM提供MAPF决策支持。该框架包含三个核心组件：面向MAPF的LLM模块、基于图神经网络的预训练NAR模块，以及交叉注意力机制。本研究首次提出利用神经算法推理器将GNN与地图信息融合应用于MAPF，从而引导LLM实现更优性能。LLM-NAR可灵活适配多种LLM模型，仿真与真实环境实验均表明，该方法在解决MAPF问题方面显著优于现有基于LLM的方案。</p>
<div class="markdown-heading"><h2 class="heading-element">元认知R1：赋能大型推理模型实现元认知能力</h2><a id="user-content-元认知r1赋能大型推理模型实现元认知能力" class="anchor" aria-label="Permalink: 元认知R1：赋能大型推理模型实现元认知能力" href="#元认知r1赋能大型推理模型实现元认知能力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17291v1 公告类型：新成果<br>
摘要：大型推理模型（LRMs）在复杂任务上展现出卓越能力，呈现出类人的涌现式思维模式。然而我们发现其存在根本性局限：当前LRMs缺乏专属的元认知系统——这种人类认知中"对思考的思考"的核心能力缺失导致其涌现能力存在三重缺陷：不可控（非自适应推理）、不可靠（中间错误）且不灵活（缺乏明确方法论）。为弥补这一空白，我们提出Meta-R1系统框架，为LRMs赋予显式元认知能力。该框架借鉴认知科学原理，将推理过程解耦为对象级与元级双组件，通过级联架构协调主动规划、在线调控和自适应早停机制。在三大挑战性基准测试与八个竞争基线的对比实验中，Meta-R1展现出三大特性：（I）高性能：以最高27.3%的优势超越现有最佳方法；（II）令牌高效：相较原始版本将令牌消耗降低至15.7%~32.7%，效率提升最高达14.8%；（III）强迁移性：在不同数据集和模型骨干上均保持稳健性能。</p>
<div class="markdown-heading"><h2 class="heading-element">PowerChain：运用智能AI工作流自动化配电网分析</h2><a id="user-content-powerchain运用智能ai工作流自动化配电网分析" class="anchor" aria-label="Permalink: PowerChain：运用智能AI工作流自动化配电网分析" href="#powerchain运用智能ai工作流自动化配电网分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17094v1 公告类型：新成果<br>
摘要：随着电气化和脱碳进程的加速，配电网（DG）的运营与规划日趋复杂，亟需通过先进的计算分析来保障电网的可靠性与韧性。当前最先进的配电网分析依赖于由复杂模型、功能模块和数据管道组成的分散式工作流，这些工作流不仅需要专业知识支撑，更难以实现自动化。许多小型公用事业公司和合作社因缺乏大型研发团队，无法规模化应用先进分析技术。为填补这一空白，我们开发了一种新型智能体AI系统——PowerChain，通过自动化智能体编排与大语言模型（LLM）函数调用机制，解决未知的配电网分析任务。该系统接收自然语言查询后，基于专家构建的电力系统函数库语义指引，以及精选的已知专家生成工作流-查询对照参考集，动态生成并执行有序的领域感知函数序列。实验结果表明，在处理真实公用事业数据的复杂未知配电网分析任务时，PowerChain能基于GPT-5和开源Qwen模型生成专家级的工作流方案。</p>
<div class="markdown-heading"><h2 class="heading-element">CALR：高效大语言模型层压缩的校正自适应低秩分解方法</h2><a id="user-content-calr高效大语言模型层压缩的校正自适应低秩分解方法" class="anchor" aria-label="Permalink: CALR：高效大语言模型层压缩的校正自适应低秩分解方法" href="#calr高效大语言模型层压缩的校正自适应低秩分解方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16680v1 公告类型：新成果<br>
摘要：大语言模型（LLM）因其庞大的参数量和计算需求，在部署时面临重大挑战。模型压缩技术对于使这些模型在资源受限环境中具备实用性至关重要。一种主流的压缩策略是通过奇异值分解（SVD）进行低秩因子分解，通过近似权重矩阵来减少模型参数。然而，标准SVD方法专注于最小化矩阵重构误差，往往导致模型功能性能显著下降。这种性能退化源于现有方法未能充分补偿压缩过程中损失的功能信息。为弥补这一不足，我们提出可校正自适应低秩分解（CALR）——一种双组件压缩方案。CALR将SVD压缩层的主路径与并行的、可学习的低秩校正模块相结合，该模块经显式训练以恢复功能残差误差。我们在SmolLM2-135M、Qwen3-0.6B和Llama-3.2-1B上的实验表明，CALR能减少26.93%至51.77%的参数规模，同时保持原模型59.45%至90.42%的性能，持续优于LaCo、ShortGPT和LoSparse等方法。CALR的成功证明，将功能信息损失视为可学习信号是一种高效的压缩范式。该方法能创建显著更小、更高效的大语言模型，推动其在实际应用中的可及性与落地部署。</p>
<div class="markdown-heading"><h2 class="heading-element">超世代：一款高效超高分辨率视频生成系统，具备草图绘制与平铺功能</h2><a id="user-content-超世代一款高效超高分辨率视频生成系统具备草图绘制与平铺功能" class="anchor" aria-label="Permalink: 超世代：一款高效超高分辨率视频生成系统，具备草图绘制与平铺功能" href="#超世代一款高效超高分辨率视频生成系统具备草图绘制与平铺功能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17756v1 公告类型：新研究<br>
摘要：扩散模型近期在生成任务（如图像与视频生成）中取得显著成功，各领域对高质量内容（如2K/4K视频）的需求正快速增长。然而，由于需要大量重新训练以及极高的计算和内存成本，在现有标准分辨率（如720p）平台上生成超高清视频仍具挑战。为此，我们推出SuperGen——一种基于分块的高效超高清视频生成框架。该框架采用无需重新训练的创新型分块算法，可成功支持多种分辨率而不增加训练负担，同时显著降低内存占用和计算复杂度。此外，SuperGen融合了针对分块设计的自适应区域感知缓存策略，通过利用去噪步骤和空间区域间的冗余性加速视频生成。该框架还集成缓存引导的通信最小化分块并行技术，以提升吞吐量并降低延迟。评估结果表明，SuperGen在各类基准测试中既能实现高输出质量，又能获得最大性能增益。</p>
<div class="markdown-heading"><h2 class="heading-element">一种基于适当评分用于分类及其他领域的不确定性量化新框架</h2><a id="user-content-一种基于适当评分用于分类及其他领域的不确定性量化新框架" class="anchor" aria-label="Permalink: 一种基于适当评分用于分类及其他领域的不确定性量化新框架" href="#一种基于适当评分用于分类及其他领域的不确定性量化新框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18001v1 公告类型：新成果<br>
摘要：在本篇博士论文中，我们提出了一种基于恰当评分（proper scores）的机器学习不确定性量化新框架。不确定性量化是构建可信赖且可靠的机器学习应用的重要基石。通常，不确定性量化方法具有问题特定性，其解决方案和洞见难以在不同任务间直接迁移。恰当评分是通过预测目标分布来最小化的损失函数，由于其高度普适的定义，可应用于回归、分类乃至生成建模任务。我们提出了若干理论成果，将认知不确定性、偶然不确定性及模型校准与恰当评分相关联，从而构建出通用且广泛适用的框架。通过函数型布雷格曼散度，我们实现了严格恰当评分的一般性偏差-方差分解。特别地，我们采用基于核函数的恰当评分——核评分（kernel score），在图像、音频和自然语言生成等多个领域评估基于样本的生成模型，其中包括一种优于现有基线的大型语言模型不确定性估计新方法。此外，我们将校准-锐度分解推广至分类任务之外，由此推导出恰当校准误差的定义。继而提出分类任务中恰当校准误差的新估计量，以及基于风险的新方法来比较平方校准误差的不同估计量。最后，我们对另一种基于核函数的恰当评分——核球面评分（kernel spherical score）进行分解，实现对生成式图像模型更细粒度、可解释性更强的评估。</p>
<div class="markdown-heading"><h2 class="heading-element">Tri-Accel：面向高效GPU利用的曲率感知精度自适应与内存弹性优化方案</h2><a id="user-content-tri-accel面向高效gpu利用的曲率感知精度自适应与内存弹性优化方案" class="anchor" aria-label="Permalink: Tri-Accel：面向高效GPU利用的曲率感知精度自适应与内存弹性优化方案" href="#tri-accel面向高效gpu利用的曲率感知精度自适应与内存弹性优化方案"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译采用技术文档常用命名风格，保留核心术语"Tri-Accel"的音译特征，将复合技术概念拆解为符合中文技术文献表达习惯的并列结构，同时通过"面向...的方案"的句式完整传达技术目标。）</p>
<p>arXiv:2508.16905v1 公告类型：新研究<br>
摘要：深度神经网络日益面临优化成本的瓶颈，包括GPU内存和计算时间两方面。现有加速技术（如混合精度、二阶方法和批量大小缩放）通常被孤立使用。我们提出Tri-Accel——一种统一优化框架，能在训练过程中协同适配三种加速策略及自适应参数：（1）精度自适应更新：根据曲率和梯度方差动态分配层级混合精度；（2）稀疏二阶信号：利用Hessian/Fisher稀疏模式指导精度和步长决策；（3）内存弹性批量缩放：根据显存可用性实时调整批量大小。在CIFAR-10数据集使用ResNet-18和EfficientNet-B0的测试中，Tri-Accel相比FP32基线实现训练时间减少9.9%、内存使用降低13.3%，同时准确率提升1.1个百分点。在CIFAR-10/100的测试表明，该方法具有自适应学习行为，随着系统学会更有效分配资源，训练效率会逐步提升。相比静态混合精度训练，Tri-Accel在标准硬件上将内存占用量从0.35GB降至0.31GB的同时保持78.1%的准确率。该框架通过定制Triton内核实现，其硬件感知适配能力可在无需手动调参的情况下实现自动优化，适用于多种计算环境的部署。这项工作展示了如何将算法自适应性与硬件感知相结合，提升资源受限场景的可扩展性，为边缘设备和成本敏感型云部署提供更高效的神经网络训练方案。</p>
<div class="markdown-heading"><h2 class="heading-element">《盒式结构中的学习机制：自研表征中作为可共享路由器的密钥》</h2><a id="user-content-盒式结构中的学习机制自研表征中作为可共享路由器的密钥" class="anchor" aria-label="Permalink: 《盒式结构中的学习机制：自研表征中作为可共享路由器的密钥》" href="#盒式结构中的学习机制自研表征中作为可共享路由器的密钥"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17032v1 公告类型：新成果<br>
摘要：长上下文大语言模型（LLM）推理的一个瓶颈是关键值缓存（KV cache）的线性增长。近期研究提出了CARTRIDGE方法，该方法利用离线计算训练出比完整文档所需小得多的KV缓存（推理时内存使用量最高可减少40倍）。本文首次对学习型CARTRIDGE键值缓存结构进行机制性探索：我们提出(1) CARTRIDGE键充当压缩语料库中稳定、可共享的检索路由器；(2) 大部分学习型压缩发生在CARTRIDGE值向量内。我们通过跨任务、模型系列和模型规模的实验验证了路由理论，例如在不同任务间消融学习型CARTRIDGE键向量仅导致轻微性能损失。最后，我们提出一种名为采样块初始化（SCI）的改进初始化方法，该方法能比文献已展示结果更快实现CARTRIDGE收敛。本研究为CARTRIDGE训练优化的更广泛实证研究奠定了基础，这对进一步扩展规模至关重要。</p>
<div class="markdown-heading"><h2 class="heading-element">推测性安全感知解码</h2><a id="user-content-推测性安全感知解码" class="anchor" aria-label="Permalink: 推测性安全感知解码" href="#推测性安全感知解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17739v1 公告类型：新成果<br>
摘要：尽管已投入大量努力使大语言模型（LLMs）与人类价值观及安全规则对齐，但利用特定漏洞的越狱攻击仍不断涌现，这表明需要通过增强现有LLMs的安全属性来防御此类攻击。然而，大模型的调优过程日益消耗资源，且难以保证性能一致性。我们提出推测式安全感知解码（SSD），一种轻量级的解码时方法，既能赋予LLMs所需的安全属性，又可加速推理过程。该方法假设存在一个具备目标安全属性的小型语言模型。SSD在解码过程中整合推测采样技术，通过计算小模型与组合模型之间的匹配率来量化越狱风险。这使得SSD能够根据模型能力差异动态切换解码策略，优先保障实用性或安全性。最终输出token通过融合原始模型和小型模型概率分布的新分布进行采样。实验结果表明，SSD成功为大模型注入目标安全属性，同时保持对良性查询的有效响应能力。此外，得益于推测采样设计，SSD还显著提升了推理速度。</p>
<div class="markdown-heading"><h2 class="heading-element">探索使用LoRA和DoRA高效学习小型BERT网络的方法</h2><a id="user-content-探索使用lora和dora高效学习小型bert网络的方法" class="anchor" aria-label="Permalink: 探索使用LoRA和DoRA高效学习小型BERT网络的方法" href="#探索使用lora和dora高效学习小型bert网络的方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17586v1 公告类型：新研究<br>
摘要：尽管大语言模型（LLM）已经彻底改变了人工智能领域，但其微调过程需要极高的计算成本，这使得GPU资源有限的小型企业和研究团队难以参与前沿研究。Hu等人与Liu等人分别提出了低秩适应（LoRA）和权重分解低秩适应（DoRA）方法，作为高效解决LLM微调计算挑战的方案，在GPT-3和RoBERTa等模型上实现了显著的速度提升和内存节省。本研究旨在扩展原始LoRA与DoRA论文的范畴，通过将这两种方法应用于小规模语言模型minBERT进行效率与性能基准测试。我们的研究发现：结合自动混合精度（AMP）技术，定制化的LoRA和DoRA配置能在保持性能的同时显著提升训练效率；尽管minBERT的参数规模远小于GPT-3，但实验结果验证了语言模型的梯度更新即使在小模型空间中也具有内在低秩特性——秩1分解仅导致可忽略的性能损失。此外，凭借我们高效实现的minBERT框架，我们探索了多种架构、自定义损失函数和超参数，最终训练出可同时执行情感分析、复述检测和相似度评分的最优集成多任务minBERT模型。</p>
<div class="markdown-heading"><h2 class="heading-element">SafeBimanual：基于扩散的安全双手操作轨迹优化方法</h2><a id="user-content-safebimanual基于扩散的安全双手操作轨迹优化方法" class="anchor" aria-label="Permalink: SafeBimanual：基于扩散的安全双手操作轨迹优化方法" href="#safebimanual基于扩散的安全双手操作轨迹优化方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18268v1 公告类型：新研究<br>
摘要：双手操作技术已广泛应用于家庭服务与制造业，其通过协调配合实现复杂任务的完成。近年来基于扩散模型的策略学习方法在双手操作动作分布建模方面展现出优异性能，但忽视了物理安全约束，导致可能损害机器人及物体的危险行为。为此，我们提出名为SafeBimanual的测试时轨迹优化框架，适用于任何预训练的扩散式双手操作策略，通过施加安全约束避免危险行为并提升成功率。具体而言，我们针对不同双机械臂协作模式（包括避免物体撕裂、机械臂与物体碰撞等）设计多样化安全约束代价函数，通过引导扩散去噪过程的采样来优化机械臂轨迹。此外，采用视觉语言模型（VLM）调度代价函数，通过指定关键点及对应关联关系，在整个操作过程中动态生成最优安全约束。SafeBimanual在RoboTwin的8个模拟任务中展现卓越性能：相比最先进的扩散方法，成功率提升13.7%，不安全交互减少18.8%。在4个真实世界任务中的大量实验进一步验证其实际价值，成功率提高32.5%。</p>
<div class="markdown-heading"><h2 class="heading-element">DR-CircuitGNN：基于GPU的异构电路图神经网络训练加速方案</h2><a id="user-content-dr-circuitgnn基于gpu的异构电路图神经网络训练加速方案" class="anchor" aria-label="Permalink: DR-CircuitGNN：基于GPU的异构电路图神经网络训练加速方案" href="#dr-circuitgnn基于gpu的异构电路图神经网络训练加速方案"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：在保持专业术语准确性的基础上，采用"训练加速方案"的表述既完整传达了"Training Acceleration"的技术内涵，又符合中文技术文献的命名习惯。GPU保留英文缩写形式符合行业惯例，同时通过"基于"的介词结构明确技术实现载体。）</p>
<p>arXiv:2508.16769v1 公告类型：新成果<br>
摘要：集成电路设计规模与复杂度的持续增长，使电子设计自动化（EDA）面临日益严峻的挑战。由于电路可自然表示为图结构，图神经网络（GNN）已成为辅助EDA设计的重要工具。尽管GNN为电路分析提供了基础框架，但其往往难以完整捕捉EDA设计的复杂性。异构图神经网络（HGNN）通过同时捕获拓扑关系和几何特征，能更精准地解析EDA电路图。然而，由于其采用串行模块化消息传递机制，增强的表征能力伴随着更高的计算复杂度与处理成本，形成显著性能瓶颈。本文提出DR-CircuitGNN——通过利用行稀疏感知动态ReLU算法，并在异构图消息传递过程中优化稀疏矩阵乘法（SpMM）内核，实现针对EDA电路图数据集HGNN训练的快速GPU内核设计。为进一步提升性能，我们提出并行优化策略：通过多线程CPU初始化与多CUDA流GPU内核执行并发处理独立子图，最大化CPU-GPU协同效率。实验表明，在三种典型CircuitNet设计（小/中/大规模）上，本方法在前向传播与反向传播环节分别较SOTA实现最高3.51倍与4.09倍加速。在完整规模CircuitNet及采样Mini-CircuitNet数据集上，我们的并行设计相较官方DGL实现的cuSPARSE方案取得最高2.71倍加速，且相关性分数与错误率影响可忽略不计。</p>
<div class="markdown-heading"><h2 class="heading-element">UM3：无监督地图到地图匹配</h2><a id="user-content-um3无监督地图到地图匹配" class="anchor" aria-label="Permalink: UM3：无监督地图到地图匹配" href="#um3无监督地图到地图匹配"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16874v1 公告类型：新成果<br>
摘要：地图到地图匹配是实现异构空间数据对齐的关键任务，但由于缺乏真实对应关系、节点特征稀疏以及可扩展性需求，该任务仍面临挑战。本文提出一种基于图结构的无监督框架，通过三大创新突破这些难题：首先，我们的方法采用无监督学习，无需训练数据，这对于难以获取标注训练样本的大规模地图数据至关重要；其次，我们引入伪坐标系统捕捉每个地图内部节点的相对空间布局，既增强特征区分度又实现尺度不变学习；第三，我们设计了自适应平衡特征与几何相似度的机制，以及几何一致性损失函数，确保对噪声或不完整坐标数据的鲁棒性。在实现层面，为处理大规模地图，我们开发了基于瓦片的后处理流程，采用重叠区域和多数投票机制，在保持边界连贯性的同时实现并行处理。在真实数据集上的实验表明，本方法在匹配任务中达到最先进精度，尤其在高噪声和大规模场景下显著超越现有方法。该框架为地图对齐提供了可扩展的实用解决方案，为传统方法提供了更鲁棒高效的替代路径。</p>
<div class="markdown-heading"><h2 class="heading-element">生成式特征插补——一种容错语义通信技术</h2><a id="user-content-生成式特征插补一种容错语义通信技术" class="anchor" aria-label="Permalink: 生成式特征插补——一种容错语义通信技术" href="#生成式特征插补一种容错语义通信技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17957v1 公告类型：新成果<br>
摘要：语义通信（SemCom）作为一种前景广阔的范式，通过利用人工智能（AI）提取并传输源数据的底层含义，有望在第六代（6G）网络中实现前所未有的通信效率。然而，在数字系统中部署语义通信面临新的挑战，特别是在确保对可能扭曲语义关键内容的传输误差具有鲁棒性方面。为解决这一问题，本文提出了一种名为生成式特征填补的新型框架，包含三项关键技术。首先，我们引入了一种空间误差集中化分组策略，通过根据特征元素的信道映射进行编码，使特征失真在空间上集中，这一特性对后续技术的有效性和降低复杂度至关重要。其次，基于该策略，我们提出了一种生成式特征填补方法，利用扩散模型高效重建因数据包丢失而缺失的特征。最后，我们开发了语义感知功率分配方案，通过根据每个数据包的语义重要性分配传输功率，实现不等差错保护。实验结果表明，在块衰落信道条件下，所提框架优于深度联合信源信道编码（DJSCC）和JPEG2000等传统方法，实现了更高的语义准确度和更低的 Learned Perceptual Image Patch Similarity（LPIPS）分数。</p>
<div class="markdown-heading"><h2 class="heading-element">大型多关系图的有效聚类方法</h2><a id="user-content-大型多关系图的有效聚类方法" class="anchor" aria-label="Permalink: 大型多关系图的有效聚类方法" href="#大型多关系图的有效聚类方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17388v1 公告类型：新成果<br>
摘要：多关系图（MRG）是一种用于建模现实对象（即节点）间多样化交互/关系的表达性数据结构，已广泛应用于众多场景。给定包含N个节点的MRG图G，将其节点集划分为K个互不相交的聚类（MRGC）是分析MRG的基础任务，近年来备受关注。然而现有MRGC解决方案大多存在明显局限：或由于异构图结构与属性融合的低效性导致结果质量严重受损，或因采用复杂昂贵的深度学习模型而难以处理包含数百万节点和数十亿边的大规模MRG。</p>
<p>本文提出DEMM与DEMM+两种高效MRGC方法以解决上述问题。具体而言，我们的算法基于新颖的两阶段优化目标：前者通过优化专为MRG设计的多关系狄利克雷能量来获取高质量节点特征向量，后者在节点亲和图上最小化聚类结果的狄利克雷能量。特别地，DEMM+通过一系列精心设计的优化措施，在基础方法DEMM上实现了显著的可扩展性和效率提升。核心技术贡献包括：（i）高效构建节点特征向量的近似求解器；（ii）基于理论支撑的问题转化技术，通过巧妙设计实现线性时间复杂度聚类，无需显式构造N×N稠密亲和矩阵。此外，我们通过非平凡适配使DEMM+能够处理无属性MRG。</p>
<p>通过在11个真实MRG数据集上与20个基线方法的对比实验表明，DEMM+在基于真实标签评估的聚类质量方面持续优于现有方法，同时通常具有显著的速度优势。</p>
<div class="markdown-heading"><h2 class="heading-element">凝时于参：通过储层诱导特征扩展与固定随机动力实现参数高效的时间序列变换器</h2><a id="user-content-凝时于参通过储层诱导特征扩展与固定随机动力实现参数高效的时间序列变换器" class="anchor" aria-label="Permalink: 凝时于参：通过储层诱导特征扩展与固定随机动力实现参数高效的时间序列变换器" href="#凝时于参通过储层诱导特征扩展与固定随机动力实现参数高效的时间序列变换器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：译文采用学术论文标题常见的凝练风格，通过"凝时于参"四字浓缩核心概念，既保留"Frozen in Time"的时间冻结意象，又体现参数高效的特点。"储层诱导特征扩展"准确对应"Reservoir-Induced Feature Expansion"的技术术语，"固定随机动力"则精准翻译"Fixed Random Dynamics"的动力学概念，整体符合中文科技文献的表述规范。）</p>
<p>arXiv:2508.18130v1 公告类型：新成果<br>
摘要：Transformer虽是序列建模的事实首选，但其二次自注意力机制与弱时序偏置特性，使长程预测既昂贵又脆弱。我们提出FreezeTST——一种轻量级混合模型，通过将冻结随机特征（储层）块与标准可训练Transformer层交错组合。冻结块以零优化成本为网络注入丰富非线性记忆能力，可训练层则通过自注意力机制学习查询该记忆。这一设计既削减了可训练参数量，又缩短了实际训练时间，同时保持推理复杂度不变。在七项标准长程预测基准测试中，FreezeTST持续匹配或超越Informer、Autoformer、PatchTST等专用变体，且计算量显著降低。我们的研究表明，将储层计算原理嵌入Transformer架构，为高效长时序预测提供了一条简洁而原理清晰的路径。</p>
<div class="markdown-heading"><h2 class="heading-element">优化腿式机器人的抓取能力：深度学习在运动操控中的应用</h2><a id="user-content-优化腿式机器人的抓取能力深度学习在运动操控中的应用" class="anchor" aria-label="Permalink: 优化腿式机器人的抓取能力：深度学习在运动操控中的应用" href="#优化腿式机器人的抓取能力深度学习在运动操控中的应用"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17466v1 公告类型：新论文<br>
摘要：四足机器人已成为高效多能的移动平台，在传统轮式机器人可能失效的复杂非结构化地形中展现出卓越的导航能力。为这类机器人配备机械臂后，可进一步获得运动操控的高级能力，从而在工业自动化到搜救任务等多个领域执行复杂的物理交互任务。然而，在动态场景中实现精准且自适应的抓取仍是重大挑战，这一问题常因需要大量现实世界校准和预编程抓取配置而加剧。本文提出一种深度学习框架，旨在增强配备机械臂的四足机器人的抓取能力，重点提升其精度与适应性。我们的方法采用仿真到现实的训练策略，最大限度减少对物理数据收集的依赖。我们在Genesis仿真环境中开发了数据生成流程，针对常见物体生成合成抓取尝试数据集。通过从多视角模拟数千次交互，我们创建了像素级标注的抓取质量图作为模型训练的真实基准。该数据集用于训练采用类U-Net架构的定制卷积神经网络，可处理来自机载RGB与深度相机的多模态输入（包括RGB图像、深度图、分割掩码和表面法向图）。训练后的模型能输出抓取质量热力图以识别最优抓取点。我们在四足机器人上完整验证了该框架：系统成功执行了全流程运动操控任务——自主导航至目标物体，通过传感器感知环境，使用模型预测最优抓取姿态，并完成精准抓取。本研究表明，结合先进传感技术的仿真训练可为物体操控提供可扩展且高效的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">可转移归一化流的摊销采样</h2><a id="user-content-可转移归一化流的摊销采样" class="anchor" aria-label="Permalink: 可转移归一化流的摊销采样" href="#可转移归一化流的摊销采样"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18175v1 公告类型：新成果<br>
摘要：分子构象的高效平衡采样始终是计算化学与统计推断领域的核心挑战。传统方法如分子动力学或马尔可夫链蒙特卡洛本质上缺乏摊销特性——每个目标系统的采样计算成本都需全额支付。生成模型的广泛成功激发了通过算法学习突破这一局限的研究兴趣。尽管在单系统训练中表现与传统方法相当，但迄今学习式采样器在跨系统迁移能力上仍显不足。我们通过提出Prose模型证明深度学习能够设计可扩展且可迁移的采样器：这是一个拥有2.8亿参数的全原子可迁移标准化流模型，基于长度达8个残基的肽分子动力学轨迹库训练而成。Prose可对任意肽系统进行零样本无关联提案采样，实现了以往难以达成的跨序列长度迁移能力，同时保持标准化流的高效似然评估特性。通过大量实证评估，我们证明了Prose作为多种采样算法提案机制的有效性，发现基于重要性采样的简单微调程序在未知四肽系统上表现优于序贯蒙特卡洛等传统方法。我们开源了Prose代码库、模型权重及训练数据集，以进一步推动摊销采样方法与微调目标的研究。</p>
<div class="markdown-heading"><h2 class="heading-element">量子图注意力网络：一种用于图学习的新型量子多头注意力机制</h2><a id="user-content-量子图注意力网络一种用于图学习的新型量子多头注意力机制" class="anchor" aria-label="Permalink: 量子图注意力网络：一种用于图学习的新型量子多头注意力机制" href="#量子图注意力网络一种用于图学习的新型量子多头注意力机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17630v1 公告类型：新成果<br>
摘要：我们提出量子图注意力网络（QGAT），这是一种将变分量子电路集成到注意力机制中的混合图神经网络。其核心在于采用强纠缠量子电路与振幅编码的节点特征，以实现富有表现力的非线性交互。与经典多头注意力机制分别计算每个头不同，QGAT利用单一量子电路同步生成多个注意力系数。这种量子并行性促进了头间参数共享，显著降低计算开销和模型复杂度。经典投影权重与量子电路参数通过端到端方式联合优化，确保对学习任务的灵活适配。实证结果表明，QGAT在捕捉复杂结构依赖性和提升归纳场景泛化能力方面具有显著效果，凸显其在化学、生物学及网络分析等领域实现可扩展量子增强学习的潜力。此外，实验证实量子嵌入增强了模型对特征噪声与结构噪声的鲁棒性，表明其在处理现实世界噪声数据方面具有优势。QGAT的模块化设计也确保其能直接集成至现有架构，可轻松增强经典基于注意力的模型。</p>
<div class="markdown-heading"><h2 class="heading-element">框架：针对对抗性机器学习威胁的综合风险评估框架</h2><a id="user-content-框架针对对抗性机器学习威胁的综合风险评估框架" class="anchor" aria-label="Permalink: 框架：针对对抗性机器学习威胁的综合风险评估框架" href="#框架针对对抗性机器学习威胁的综合风险评估框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17405v1 公告类型：新成果<br>
摘要：随着机器学习（ML）系统的广泛采用，其安全性问题日益受到关注，对抗性机器学习（AML）技术的出现利用了ML系统的根本性漏洞，这使得对基于ML的系统进行全面风险评估变得尤为迫切。传统风险评估框架虽能评估常规网络安全风险，却难以应对AML威胁带来的独特挑战。现有AML威胁评估方法主要关注技术层面的攻击鲁棒性，忽略了部署环境、系统依赖关系和攻击可行性等关键现实因素。此前尝试进行的全面AML风险评估仅限于特定领域解决方案，难以跨不同系统应用。针对这些局限性，我们提出FRAME——首个全面自动化框架，用于评估各类基于ML的系统的AML风险。FRAME包含一种新颖的风险评估方法，通过系统化评估三个关键维度来量化AML风险：目标系统的部署环境、多样化AML技术特性以及来自先前研究的实证洞察。该框架整合了可行性评分机制和基于LLM的系统定制化评估功能，并建立了首个全面结构化的AML攻击数据集以实现情境感知风险评估。从工程应用视角看，FRAME提供的可操作结果专为系统所有者设计，仅需具备系统技术知识而无需AML专业知识。我们在六个真实场景中验证了该框架，评估结果显示其具有卓越的准确性且与AML专家分析高度吻合。FRAME能帮助组织优先处理AML风险，为现实环境中安全部署人工智能提供支撑。</p>
<div class="markdown-heading"><h2 class="heading-element">GateTS：通过注意力启发的专家混合路由实现多功能高效预测</h2><a id="user-content-gatets通过注意力启发的专家混合路由实现多功能高效预测" class="anchor" aria-label="Permalink: GateTS：通过注意力启发的专家混合路由实现多功能高效预测" href="#gatets通过注意力启发的专家混合路由实现多功能高效预测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17515v1 公告类型：新成果<br>
摘要：在现实世界系统（如能源市场、水文监测、零售需求与物联网监控）中，精确的单变量预测仍是迫切需求——这些场景中的信号往往具有间歇性特征，且预测跨度同时涵盖短期与长期维度。虽然Transformer和混合专家（MoE）架构在时间序列预测中日益受到青睐，但核心难题依然存在：MoE模型通常需要同时依赖主预测损失和辅助负载均衡损失进行复杂训练，还需精细的路由/温度参数调节，这阻碍了其实际应用。本文提出一种简化训练流程的单变量时间序列预测架构，能有效兼顾长短期预测（包括间歇性模式）。我们的方法将稀疏MoE计算与一种新颖的注意力机制启发的门控设计相结合，替代了传统的单层softmax路由器。通过大量实证评估，我们证明该门控设计能自然促进专家利用率均衡，且在无需传统MoE实现中辅助负载均衡损失的情况下，仍能获得更优的预测精度。该模型仅需PatchTST等最先进Transformer模型的一小部分参数即可实现更佳性能。此外，跨多数据集的实验证实：采用新门控机制的MoE架构在长短周期预测中均比LSTM模型更具计算效率，可实现低成本推理。这些结果凸显了我们的方法在预测精度与计算效率并重的实际时间序列预测应用中的潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">交易组：具备自我反思与数据合成能力的多智能体交易系统</h2><a id="user-content-交易组具备自我反思与数据合成能力的多智能体交易系统" class="anchor" aria-label="Permalink: 交易组：具备自我反思与数据合成能力的多智能体交易系统" href="#交易组具备自我反思与数据合成能力的多智能体交易系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17565v1 公告类型：新研究<br>
摘要：大型语言模型（LLM）的最新进展推动了基于智能体的金融应用发展，尤其在情感分析、财报解读和股票预测领域表现突出。然而现有系统普遍存在智能体间协作不足、缺乏结构化自省机制，以及难以获取高质量领域特异性训练后数据（如包含市场环境和智能体决策的交易活动数据）等问题。这些数据对智能体理解市场动态、提升决策质量和促进有效协作至关重要。我们推出TradingGroup多智能体交易系统，通过自省式架构和端到端数据合成管道解决上述局限。该系统包含新闻情感分析、财报解读、股票趋势预测、交易风格适配等专业智能体，并由一个集成所有信号与风格偏好的交易决策智能体最终生成买入、卖出或持有决策。特别设计了面向股票预测、风格适配和决策智能体的自省机制，可提炼历史成败经验用于类似场景的推理，并配备动态风险管理模型提供可配置的止盈止损机制。此外，系统内置自动化数据合成与标注管道，可生成高质量训练后数据以持续提升智能体性能。在五个真实股票数据集上的回测实验表明，TradingGroup的表现显著优于基于规则、机器学习、强化学习及现有LLM的交易策略。</p>
<div class="markdown-heading"><h2 class="heading-element">通过Bagging、Boosting与统计集成提升基于Transformer的基础模型在时间序列预测中的性能</h2><a id="user-content-通过baggingboosting与统计集成提升基于transformer的基础模型在时间序列预测中的性能" class="anchor" aria-label="Permalink: 通过Bagging、Boosting与统计集成提升基于Transformer的基础模型在时间序列预测中的性能" href="#通过baggingboosting与统计集成提升基于transformer的基础模型在时间序列预测中的性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16641v1 公告类型：新成果<br>
摘要：时间序列基础模型（TSFMs），如Lag-Llama、TimeGPT、Chronos、MOMENT、UniTS和TimesFM，在时间序列预测、异常检测、分类和填补任务中展现出强大的泛化能力和零样本性能。尽管具备这些优势，但在实际运营数据部署时，其预测仍存在方差偏大、领域特异性偏差以及不确定性量化有限等问题。本文研究了一系列基于统计与集成学习的增强技术，包括基于自助法的装袋策略、回归型堆叠集成、预测区间构建、统计残差建模以及迭代误差反馈机制，以提升模型的鲁棒性与准确性。以比利时电力短期负荷预测数据集为案例，我们证明所提出的混合方法在多个预测维度上持续优于独立基础模型：回归集成实现了最低均方误差；自助聚合显著减少长上下文误差；残差建模修正系统性偏差；最终生成的预测区间在覆盖率接近名义值的同时，区间宽度随上下文长度增加而收缩。结果表明，将统计推理与现代基础模型相结合，能为现实世界时间序列应用带来准确性、可靠性和可解释性的显著提升。</p>
<div class="markdown-heading"><h2 class="heading-element">机器人布料展开抓取选择数据集与基准：ICRA 2024布料竞赛</h2><a id="user-content-机器人布料展开抓取选择数据集与基准icra-2024布料竞赛" class="anchor" aria-label="Permalink: 机器人布料展开抓取选择数据集与基准：ICRA 2024布料竞赛" href="#机器人布料展开抓取选择数据集与基准icra-2024布料竞赛"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：ICRA为IEEE International Conference on Robotics and Automation的缩写，中文标准译名为"IEEE机器人与自动化国际会议"，此处保留英文缩写形式符合学术惯例）</p>
<p>arXiv:2508.16749v1 公告类型：新研究<br>
摘要：机器人布料操作领域长期缺乏标准化基准和共享数据集来评估与比较不同方法。为此，我们创建了一个基准测试平台，并组织了ICRA 2024布料操作竞赛——这是首个专注于空中机器人布料展开抓取位姿选择的对抗性评估。11支多元化的参赛团队利用我们公开发布的真实机器人布料展开尝试数据集，采用多种方法设计其展开方案。赛后我们还新增了176组竞赛评估试验，最终形成涵盖34件衣物的679次展开演示数据集。竞赛结果分析揭示了抓取成功率与覆盖范围之间的权衡关系，手工设计方法的惊人成效，以及竞赛表现与先前研究之间的显著差异，凸显了独立于实验室环境评估在机器人布料操作中的重要性。该数据集为开发评估抓取选择方法（特别是基于学习的方法）提供了宝贵资源。我们希望本基准测试、数据集及竞赛成果能为未来基准建设奠定基础，推动数据驱动的机器人布料操作进一步发展。数据集与基准代码详见：<a href="https://airo.ugent.be/cloth_competition%E3%80%82" rel="nofollow">https://airo.ugent.be/cloth_competition。</a></p>
<div class="markdown-heading"><h2 class="heading-element">AdapSNE：面向边缘DNN训练的自适应烟花优化与熵引导数据集采样方法</h2><a id="user-content-adapsne面向边缘dnn训练的自适应烟花优化与熵引导数据集采样方法" class="anchor" aria-label="Permalink: AdapSNE：面向边缘DNN训练的自适应烟花优化与熵引导数据集采样方法" href="#adapsne面向边缘dnn训练的自适应烟花优化与熵引导数据集采样方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译在保持专业术语准确性的同时，采用符合中文技术文献命名习惯的四段式结构，其中：</p>
<ol>
<li>"Adaptive"译为"自适应"体现算法特性</li>
<li>"Fireworks-Optimized"采用学界通用译法"烟花优化"</li>
<li>"Entropy-Guided"译为"熵引导"保持信息论术语规范</li>
<li>补充"方法"二字使中文表述更完整，符合学术翻译惯例）</li>
</ol>
<p>arXiv:2508.16647v1 公告类型：新成果<br>
摘要：在边缘设备上直接训练深度神经网络（DNN）正获得越来越多关注，因其能为领域自适应和隐私保护等挑战提供有前景的解决方案。然而传统DNN训练通常需要大规模数据集，这对边缘设备（尤其是新兴的大语言模型任务）造成巨大负担。为应对这一挑战，研究者提出了名为NMS（近内存采样）的无DNN方法（即不依赖DNN的数据集采样技术）。该方法先对数据集进行降维，然后在降维空间执行样本采样，避免了基于DNN方法的架构偏差，从而获得更好的泛化能力。但当前最先进的NMS方法存在两个局限：（1）搜索方法与困惑度误差函数的非单调特性不匹配，导致降维表示中出现异常值；（2）关键参数（即目标困惑度）依赖经验选择，引入随意性并导致采样不均。这两个问题会造成样本代表性偏差，进而降低训练精度。为此我们提出AdapSNE方法：集成高效的非单调搜索方法——烟花算法（FWA）来抑制异常值，采用熵引导优化实现均匀采样，从而确保训练样本的代表性并提升训练精度。为降低烟花算法搜索和熵引导优化迭代计算带来的边缘端成本，我们设计了具有定制数据流和时分复用特性的加速器，显著降低了设备端训练的能耗和芯片面积。</p>
<div class="markdown-heading"><h2 class="heading-element">TiKMiX：将数据影响力融入语言模型预训练的动态混合策略</h2><a id="user-content-tikmix将数据影响力融入语言模型预训练的动态混合策略" class="anchor" aria-label="Permalink: TiKMiX：将数据影响力融入语言模型预训练的动态混合策略" href="#tikmix将数据影响力融入语言模型预训练的动态混合策略"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17677v1 公告类型：新研究<br>
摘要：语言模型预训练阶段采用的数据混合策略是其最终性能的基石。然而，静态混合策略并非最优选择，因为模型对不同数据领域的学习偏好会在训练过程中动态变化。关键在于，如何以计算高效的方式观测这些持续演变的偏好仍是重大挑战。为此，我们提出TiKMiX方法，能够根据模型动态变化的偏好实时调整数据混合比例。该方法创新性地引入"群体影响力"指标——一种评估数据领域对模型影响程度的高效度量标准，从而将数据混合问题转化为寻找影响力最大化的最优分布。我们通过两种方案实现这一目标：采用直接优化方式的TiKMiX-D，以及使用回归模型预测更优混合比例的TiKMiX-M。我们在高达1万亿token规模上训练了不同参数量的模型：TiKMiX-D仅需消耗REGMIX等前沿方法20%的计算资源即可实现更优性能；TiKMiX-M在9个下游基准测试中平均性能提升达2%。实验表明，模型的数据偏好会随训练进程和规模发生演变，我们证实基于群体影响力（这些偏好的直接度量指标）动态调整数据混合比例，能有效改善静态比例导致的数据消化不足问题，从而显著提升模型性能。</p>
<div class="markdown-heading"><h2 class="heading-element">利用位串树在大规模数据集上进行学习</h2><a id="user-content-利用位串树在大规模数据集上进行学习" class="anchor" aria-label="Permalink: 利用位串树在大规模数据集上进行学习" href="#利用位串树在大规模数据集上进行学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17083v1 公告类型：新成果<br>
摘要：本论文发展了相似性保持哈希、分类与癌症基因组学的计算方法。传统基于空间划分的哈希方法依赖二叉搜索树（BST），但其指数级增长与稀疏性影响效率。为此，我们提出压缩式倒排哈希表二叉搜索树（ComBI），通过降低内存需求实现快速近似最近邻搜索。在多达十亿样本的数据集上，ComBI实现了0.90精确度，相比多索引哈希方法加速4至296倍，并在单细胞RNA序列搜索中超越Cellfishing.jl性能2至13倍。基于哈希结构，我们进一步提出引导随机森林（GRAF）——一种融合全局与局部划分的树集成分类器，在连接决策树与提升方法的同时降低泛化误差。在115个数据集测试中，GRAF展现出具有竞争力的准确度，其无监督变体（uGRAF）可支持引导哈希与重要性采样。我们证明GRAF与ComBI能用于评估样本级可分类性，从而实现癌症患者生存期的可扩展预测。针对突变解释的挑战，我们开发了密码子切换连续表示（CRCS）深度学习框架，将遗传变化嵌入数值向量。CRCS可在无需匹配正常样本的情况下识别体细胞突变、发现驱动基因并对肿瘤突变评分，其生存预测能力在膀胱癌、肝癌和脑癌中得到验证。这些方法共同为大规模数据分析和生物医学应用提供了高效、可扩展且可解释的工具集。</p>
<div class="markdown-heading"><h2 class="heading-element">随心驾驶：基于多头扩散模型的策略级运动规划</h2><a id="user-content-随心驾驶基于多头扩散模型的策略级运动规划" class="anchor" aria-label="Permalink: 随心驾驶：基于多头扩散模型的策略级运动规划" href="#随心驾驶基于多头扩散模型的策略级运动规划"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16947v1 公告类型：新成果<br>
摘要：自动驾驶运动规划领域的最新进展催生了能够生成高质量轨迹的模型。然而，现有规划器大多在监督训练后固定策略，导致驾驶行为虽稳定但僵化，难以体现人类偏好或适应动态的指令驱动需求。本研究提出一种基于扩散模型的多头轨迹规划器（M-diffusion planner）。在早期训练阶段，所有输出头共享权重以学习生成高质量轨迹；借助扩散模型的概率特性，我们随后应用群体相对策略优化（GRPO）对预训练模型进行微调，实现多样化的策略特异性行为。推理阶段引入大语言模型（LLM）指导策略选择，无需切换模型即可实现动态的指令感知规划。闭环仿真表明，经过后训练的规划器在保持强劲规划能力的同时，于nuPlan val14基准测试中达到最先进（SOTA）性能。开环实验结果进一步显示，生成轨迹具有明显多样性，可有效满足多模态驾驶行为需求。代码及相关实验将在论文录用后公开。</p>
<div class="markdown-heading"><h2 class="heading-element">OVITA：开放词汇可解释轨迹自适应</h2><a id="user-content-ovita开放词汇可解释轨迹自适应" class="anchor" aria-label="Permalink: OVITA：开放词汇可解释轨迹自适应" href="#ovita开放词汇可解释轨迹自适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17260v1 公告类型：新成果<br>
摘要：在非结构化环境中与非专业用户协同作业时，使机器人轨迹能动态适应情境变化和用户偏好至关重要。自然语言使用户能够以交互方式表达这些调整需求。我们提出OVITA——一个可解释、开放词汇的语言驱动框架，专为根据人类指令在动态新颖场景中调整机器人轨迹而设计。OVITA通过整合多个预训练大语言模型（LLMs），将用户指令融合至运动规划器生成的或通过演示学习的轨迹中。该框架采用LLM生成的代码作为适应策略，使用户能够调整单个路径点，从而实现灵活控制。另一个充当代码解释器的LLM消除了对专家用户的依赖，使交互过程更直观。通过在异构机器人平台（包括KUKA IIWA机械臂、Clearpath Jackal地面机器人和CrazyFlie无人机）上开展涉及时空变化的多任务广泛仿真与真实环境测试，验证了所提出OVITA框架的有效性与重要意义。</p>
<div class="markdown-heading"><h2 class="heading-element">WISCA：一种通过权重缩放优化大语言模型训练的轻量级模型迁移方法</h2><a id="user-content-wisca一种通过权重缩放优化大语言模型训练的轻量级模型迁移方法" class="anchor" aria-label="Permalink: WISCA：一种通过权重缩放优化大语言模型训练的轻量级模型迁移方法" href="#wisca一种通过权重缩放优化大语言模型训练的轻量级模型迁移方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16676v1 公告类型：新成果<br>
摘要：Transformer架构逐渐主导了大语言模型（LLM）领域。当前基于Transformer的大语言模型训练优化研究主要集中在架构修改或优化器调整，但这些方法缺乏对训练过程中权重模式的系统性优化。权重模式指神经网络中权重参数的分布与相对量级关系。为此，我们提出名为WISCA的权重缩放方法，通过战略性改进神经网络权重模式（无需改变网络结构）来提升训练效率和模型质量。该方法在保持模型输出的前提下重新缩放权重，间接优化模型训练轨迹。实验表明，WISCA显著提升了收敛质量（以泛化能力和损失减少为衡量指标），尤其在采用分组查询注意力（GQA）架构的LLM和LoRA微调任务中效果突出。实证结果显示，在零样本验证任务上平均提升5.6%，在多架构训练任务中困惑度平均降低2.12%。</p>
<div class="markdown-heading"><h2 class="heading-element">AdLoCo：自适应批处理技术显著提升大型语言模型的通信效率与收敛性能。</h2><a id="user-content-adloco自适应批处理技术显著提升大型语言模型的通信效率与收敛性能" class="anchor" aria-label="Permalink: AdLoCo：自适应批处理技术显著提升大型语言模型的通信效率与收敛性能。" href="#adloco自适应批处理技术显著提升大型语言模型的通信效率与收敛性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18182v1 公告类型：新成果<br>
摘要：扩展大规模语言模型（LLMs）的分布式训练不仅需要算法创新，还必须高效利用异构硬件资源。虽然现有方法（如DiLoCo）已展现出良好效果，但在动态工作负载下往往难以充分挖掘计算集群的潜力。为此，我们提出一种三阶段方法，融合多实例训练（MIT）、自适应批量DiLoCo和切换机制。MIT允许单个节点并行运行多个轻量级训练流（各含不同模型实例），并通过知识融合合并成果，从而提升吞吐量并减少空闲时间。自适应批量DiLoCo动态调整本地批次大小以平衡计算与通信，显著降低同步延迟。切换机制则通过梯度积累的无缝介入，在自适应批次超出硬件友好范围时进一步稳定训练过程。这些创新共同提升了收敛速度与系统效率。我们还从理论上估算了采用本方法训练模型达到完全收敛所需的通信次数。</p>
<div class="markdown-heading"><h2 class="heading-element">基于大语言模型的智能体推理框架：从方法到场景的综述</h2><a id="user-content-基于大语言模型的智能体推理框架从方法到场景的综述" class="anchor" aria-label="Permalink: 基于大语言模型的智能体推理框架：从方法到场景的综述" href="#基于大语言模型的智能体推理框架从方法到场景的综述"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17692v1 公告类型：新研究<br>
摘要：大型语言模型（LLM）内在推理能力的最新进展催生了基于LLM的智能体系统，这些系统在各类自动化任务中展现出接近人类水平的表现。然而，尽管这些系统在使用LLM方面存在相似性，但不同的智能体系统推理框架以不同方式引导和组织推理过程。本综述提出一种系统性分类法，对智能体推理框架进行解构分析，并通过比较它们在不同场景下的应用，探讨这些框架如何主导框架级推理。具体而言，我们提出统一的形式化语言，将智能体推理系统进一步划分为单智能体方法、基于工具的方法和多智能体方法。随后，全面综述其在科学发现、医疗健康、软件工程、社会模拟和经济学等关键应用场景中的实践。我们还分析了各框架的特征特点，并总结不同的评估策略。本综述旨在为研究界提供全景视角，促进对不同智能体推理框架的优势、适用场景及评估实践的理解。</p>
<div class="markdown-heading"><h2 class="heading-element">关于Stiefel流形上LoRA的黎曼优化</h2><a id="user-content-关于stiefel流形上lora的黎曼优化" class="anchor" aria-label="Permalink: 关于Stiefel流形上LoRA的黎曼优化" href="#关于stiefel流形上lora的黎曼优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17901v1 公告类型：新研究<br>
摘要：尽管大型语言模型（LLM）功能强大，但其庞大的规模带来了显著的微调挑战。参数高效微调（PEFT）方法（如LoRA）虽提供了解决方案，却存在关键的优化器效率问题——尤其是使用AdamW时LoRA的$B$矩阵存在基冗余，这从根本上限制了性能。我们通过将$B$矩阵优化置于Stiefel流形上解决了该问题，施加显式正交约束以实现近乎完美的正交性和完全有效秩。这种几何方法显著提升了参数效率和表示能力。我们的Stiefel优化器在LoRA和DoRA的多个基准测试中持续超越AdamW，证明几何约束是释放LoRA有效微调LLM全部潜能的关键。</p>
<div class="markdown-heading"><h2 class="heading-element">模块化均值流：迈向稳定且可扩展的一步生成建模</h2><a id="user-content-模块化均值流迈向稳定且可扩展的一步生成建模" class="anchor" aria-label="Permalink: 模块化均值流：迈向稳定且可扩展的一步生成建模" href="#模块化均值流迈向稳定且可扩展的一步生成建模"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17426v1 公告类型：新成果<br>
摘要：单步生成建模旨在通过单次函数评估生成高质量数据样本，相比传统的扩散模型或基于流的模型显著提升效率。本研究提出模块化平均流（MMF）——一种灵活且具有理论依据的时间平均速度场学习方法。该方法基于连接瞬时速度与平均速度的微分恒等式推导出一系列损失函数，并引入梯度调制机制，在不牺牲表达力的前提下实现稳定训练。我们进一步提出课程式预热策略，实现从粗粒度监督到完全可微分训练的平滑过渡。MMF框架统一并推广了现有基于一致性和流匹配的方法，同时避免了昂贵的高阶导数计算。在图像合成和轨迹建模任务上的实验结果表明，MMF在样本质量、收敛鲁棒性和泛化能力方面均具有竞争力，尤其在低数据或分布外场景下表现突出。</p>
<div class="markdown-heading"><h2 class="heading-element">PerPilot：通过记忆与探索实现基于视觉语言模型的移动代理个性化</h2><a id="user-content-perpilot通过记忆与探索实现基于视觉语言模型的移动代理个性化" class="anchor" aria-label="Permalink: PerPilot：通过记忆与探索实现基于视觉语言模型的移动代理个性化" href="#perpilot通过记忆与探索实现基于视觉语言模型的移动代理个性化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18040v1 公告类型：新成果<br>
摘要：基于视觉语言模型（VLM）的移动智能体在协助用户执行指令驱动任务方面展现出巨大潜力。然而，这类智能体通常难以处理个性化指令——即包含模糊性、用户特定上下文的指令，这一挑战在以往研究中长期被忽视。本文明确定义了个性化指令，并推出PerInstruct数据集，这是一个新颖的人工标注数据集，涵盖多移动场景下的多样化个性化指令。针对现有移动智能体个性化能力有限的问题，我们进一步提出PerPilot——一个由大语言模型（LLM）驱动的即插即用框架，使移动智能体能够自主感知、理解并执行个性化用户指令。PerPilot通过两种互补机制识别个性化元素并自主完成指令：基于记忆的检索和基于推理的探索。实验结果表明，PerPilot能以最少的用户干预有效处理个性化任务，并在持续使用中逐步提升性能，凸显了个性化感知推理对下一代移动智能体的重要意义。数据集与代码已开源：<a href="https://github.com/xinwang-nwpu/PerPilot">https://github.com/xinwang-nwpu/PerPilot</a></p>
<div class="markdown-heading"><h2 class="heading-element">谜题：在配备异构处理器的移动设备上调度多个深度学习模型</h2><a id="user-content-谜题在配备异构处理器的移动设备上调度多个深度学习模型" class="anchor" aria-label="Permalink: 谜题：在配备异构处理器的移动设备上调度多个深度学习模型" href="#谜题在配备异构处理器的移动设备上调度多个深度学习模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17764v1 公告类型：新成果<br>
摘要：随着深度学习模型日益部署于移动设备，现代移动设备通过集成深度学习专用加速器来应对增长的计算需求，从而加剧了硬件异构性。然而，现有针对这些处理器的深度学习工作负载调度研究存在显著局限：多数研究聚焦单模型场景而非现实多模型场景，忽视不同硬件/软件配置带来的性能差异，且难以实现精准的执行时间预估。为应对这些挑战，我们提出一种基于遗传算法的新方法，通过将神经网络划分为多个子图，实现异构处理器上的多深度学习网络调度。该方法融合三种不同类型的染色体（分区/映射/优先级探索），并采用设备在环性能分析与评估以实现精确的执行时间预估。基于此方法，我们的系统Puzzle在包含九个前沿网络的随机生成场景中进行广泛评估，展现出卓越性能。结果表明，在满足同等实时性要求的前提下，Puzzle相比两种启发式基线方案（NPU专用和最优映射）平均可分别支持3.7倍和2.2倍更高的请求频率。</p>
<div class="markdown-heading"><h2 class="heading-element">HiCL：海马启发的持续学习</h2><a id="user-content-hicl海马启发的持续学习" class="anchor" aria-label="Permalink: HiCL：海马启发的持续学习" href="#hicl海马启发的持续学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16651v1 公告类型：新成果<br>
摘要：我们提出HiCL——一种新颖的海马体启发的双记忆持续学习架构，通过借鉴海马神经回路元素来缓解灾难性遗忘。该系统通过类网格细胞层对输入进行编码，随后经由具有top-k稀疏度的齿状回启发模块进行稀疏模式分离。情景记忆痕迹保存在类CA3自联想记忆中。任务特异性处理通过齿状回门控的专家混合机制动态管理：基于归一化稀疏齿状回表征与通过在线指数移动平均计算的任务特异性原型之间的余弦相似度，将输入路由至相应专家。这种兼具生物学基础与数学原理的门控策略实现了可微分、可扩展的任务路由（无需独立门控网络），并增强了模型在连续多任务学习中的适应性与效率。皮层输出通过任务间相似度加权的弹性权重巩固进行整合。关键的是，我们采用存储模式的优先回放机制以强化关键历史经验。在标准持续学习基准测试中，该架构有效减少了任务干扰，以更低计算成本实现了接近最先进水平的持续学习性能。</p>
<div class="markdown-heading"><h2 class="heading-element">用于蒸馏的Sig-DEG：让扩散模型更快速、更轻便</h2><a id="user-content-用于蒸馏的sig-deg让扩散模型更快速更轻便" class="anchor" aria-label="Permalink: 用于蒸馏的Sig-DEG：让扩散模型更快速、更轻便" href="#用于蒸馏的sig-deg让扩散模型更快速更轻便"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16939v1 公告类型：新研究<br>
摘要：扩散模型在生成建模领域已取得最先进的成果，但在推理阶段仍存在计算密集的问题，通常需要数千个离散化步骤。为此，我们提出Sig-DEG（基于签名的微分方程生成器），这是一种用于蒸馏预训练扩散模型的新型生成器，能够在粗时间分辨率下通用逼近反向扩散过程。受随机微分方程（SDE）高阶近似的启发，Sig-DEG利用部分签名高效概括子区间内的布朗运动，并采用循环结构实现对SDE解的精确全局逼近。蒸馏过程被构建为监督学习任务，通过训练Sig-DEG在粗时间网格上匹配高分辨率扩散模型的输出。在推理阶段，由于部分签名项可被精确模拟而无需细粒度布朗路径，Sig-DEG能够实现快速生成。实验表明，Sig-DEG在将推理步骤减少一个数量级的同时，仍保持了具有竞争力的生成质量。我们的研究成果凸显了基于签名的近似方法在高效生成建模中的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">MoE-Beyond：基于学习的边缘设备专家激活预测</h2><a id="user-content-moe-beyond基于学习的边缘设备专家激活预测" class="anchor" aria-label="Permalink: MoE-Beyond：基于学习的边缘设备专家激活预测" href="#moe-beyond基于学习的边缘设备专家激活预测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17137v1 公告类型：新成果<br>
摘要：在边缘设备上部署大规模混合专家（MoE）模型面临内存限制的重大挑战。虽然MoE架构通过每次推理仅激活部分专家实现了计算资源的高效利用，但在资源受限环境中需要精细的内存管理才能高效运行。传统基于启发式的专家缓存策略（如MoE-Infinity）随着模型参数规模扩大难以维持高缓存命中率。本研究提出MoE-Beyond——一种基于学习的专家激活预测器，通过自回归解码过程预测专家激活状态。我们将该任务构建为多标签序列预测问题，基于从LDJnr-Puffin数据集[5]提取的6600万条专家激活轨迹，使用DeepSeek-V2-Chat-Lite MoE训练轻量级Transformer模型。该预测器在WebGLM-QA数据集[6]的未见提示词上展现出卓越泛化能力，实现97.5%的准确率和86.6%的F1分数。仿真结果表明，当仅有10%的专家可存入GPU缓存时，MoE-Beyond将GPU缓存命中率从17%提升至72%，显著优于启发式基线方法。</p>
<div class="markdown-heading"><h2 class="heading-element">强化学习引导的超启发式超参数优化：实现基于脉冲神经网络的公平且可解释的金融欺诈检测</h2><a id="user-content-强化学习引导的超启发式超参数优化实现基于脉冲神经网络的公平且可解释的金融欺诈检测" class="anchor" aria-label="Permalink: 强化学习引导的超启发式超参数优化：实现基于脉冲神经网络的公平且可解释的金融欺诈检测" href="#强化学习引导的超启发式超参数优化实现基于脉冲神经网络的公平且可解释的金融欺诈检测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16915v1 公告类型：新研究<br>
摘要：家庭银行系统的日益普及加剧了网络欺诈风险，这要求欺诈检测机制不仅需具备高准确性，还应满足公平性与可解释性。虽然人工智能模型在该领域展现出潜力，但仍面临关键局限：包括计算效率低下、脉冲神经网络（SNN）可解释性挑战，以及基于超启发式强化学习（RL）的超参数优化存在复杂性和收敛不稳定性。为解决这些问题，我们提出了一种创新框架，整合了群体编码皮层脉冲网络（CSNPC）与强化引导的超启发式脉冲系统优化器（RHOSS）。CSNPC作为受生物启发的脉冲神经网络，采用群体编码实现鲁棒分类；而RHOSS利用Q学习在公平性和召回率约束下动态选择底层启发式方法进行超参数优化。该系统嵌入模块化脉冲网络训练与解释监督框架（MoSSTI），结合可解释人工智能（XAI）技术——特别是基于显著性的归因和脉冲活动分析——以提升透明度。在银行账户欺诈（BAF）数据集套件上的评估显示，我们的模型在严格5%误报率（FPR）下实现90.8%的召回率，优于最先进的脉冲与非脉冲模型，同时在关键人口统计属性上保持超过98%的预测公平性。可解释性模块进一步证实显著性归因与脉冲动力学特征吻合，验证了模型的可解释能力。这些结果表明，将群体编码脉冲神经网络与强化引导的超启发式方法相结合，有望在实际金融应用中实现公平、透明且高性能的欺诈检测。</p>
<div class="markdown-heading"><h2 class="heading-element">在GPU上训练基于Mamba的状态空间模型的行为特性分析</h2><a id="user-content-在gpu上训练基于mamba的状态空间模型的行为特性分析" class="anchor" aria-label="Permalink: 在GPU上训练基于Mamba的状态空间模型的行为特性分析" href="#在gpu上训练基于mamba的状态空间模型的行为特性分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17679v1 公告类型：新研究<br>
摘要：基于Mamba的状态空间模型（SSM）已成为替代普遍存在的Transformer架构的有力候选者。尽管Transformer具有强大的表达能力，但其注意力机制计算所需的二次复杂度是阻碍序列长度增加时性能扩展的主要障碍。SSM通过为视频、文本生成和图结构等不同领域提供新颖的模型架构，有效降低了自注意力机制的计算复杂度需求。因此，在GPU微架构设计阶段表征这些新兴工作负载的行为并理解其需求至关重要。本研究评估了基于Mamba的SSM，并深入分析了其在GPU训练过程中的行为特征。我们构建了一个涵盖不同模型架构的代表性工作负载测试集，并借此分析基于Mamba的SSM在GPU上运行的架构级影响。这项工作为持续提升此类模型性能的潜在优化方向提供了新的见解。</p>
<div class="markdown-heading"><h2 class="heading-element">标量场的拓扑感知神经插值</h2><a id="user-content-标量场的拓扑感知神经插值" class="anchor" aria-label="Permalink: 标量场的拓扑感知神经插值" href="#标量场的拓扑感知神经插值"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17995v1 公告类型：新研究<br>
摘要：本文提出了一种针对时变标量场的拓扑感知神经插值方案。给定一个持续同调图的时间序列以及对应标量场的稀疏时间采样（称为关键帧），我们的插值方法旨在"逆转"非关键帧图表，以生成相应缺失数据的合理估计。为此，我们采用了一种神经架构，该架构基于关键帧样本学习从时间值到对应标量场的映射关系，并可靠地将这种关系扩展到非关键帧时间步。我们证明了通过利用输入图表的特定拓扑损失来增强该架构，能够同时改善非关键帧时间步的几何和拓扑重建。在查询时，给定需要插值的输入时间值，我们的方法通过网络对时间输入的单次前向传播即可瞬时生成输出。在二维和三维时变数据集上的插值实验表明，无论是在数据拟合还是拓扑拟合方面，我们的方法均优于参考插值方案。</p>
<div class="markdown-heading"><h2 class="heading-element">自适应K稀疏自编码器：面向可解释大语言模型表征的动态稀疏性分配策略</h2><a id="user-content-自适应k稀疏自编码器面向可解释大语言模型表征的动态稀疏性分配策略" class="anchor" aria-label="Permalink: 自适应K稀疏自编码器：面向可解释大语言模型表征的动态稀疏性分配策略" href="#自适应k稀疏自编码器面向可解释大语言模型表征的动态稀疏性分配策略"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17320v1 公告类型：新研究<br>
摘要：理解大型语言模型（LLMs）的内部表征仍然是可解释性研究的核心挑战。稀疏自编码器（SAEs）通过将激活分解为可解释特征提供了有前景的解决方案，但现有方法依赖固定稀疏度约束，无法适应输入复杂性。我们提出自适应Top K稀疏自编码器（AdaptiveK），这是一种创新框架，能根据每个输入的语义复杂度动态调整稀疏度水平。通过线性探针技术，我们证明上下文复杂度在线性编码于LLM表征中，并利用这一信号指导训练过程中的特征分配。在三个语言模型（Pythia-70M、Pythia-160M和Gemma-2-2B）上的实验表明，这种复杂度驱动的自适应方法在重建保真度、解释方差和余弦相似度指标上显著优于固定稀疏度方法，同时消除了大量超参数调优的计算负担。</p>
<div class="markdown-heading"><h2 class="heading-element">固定权重变换器中的上下文算法仿真</h2><a id="user-content-固定权重变换器中的上下文算法仿真" class="anchor" aria-label="Permalink: 固定权重变换器中的上下文算法仿真" href="#固定权重变换器中的上下文算法仿真"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17550v1 公告类型：新研究<br>
摘要：我们证明，具有冻结权重的最小Transformer架构能够通过上下文提示模拟广泛类型的算法。具体而言，对于任何可通过固定权重注意力头（例如单步梯度下降或线性/岭回归）实现的算法，存在一种提示，能够驱动双层softmax注意力模块以任意精度复现该算法的输出。这一保证甚至可扩展至单头注意力层（必要时使用更长提示），实现架构最小化。我们的核心思想是构建能将算法参数编码为令牌表示的提示，通过制造显著的点积差距迫使softmax注意力遵循预期计算。该构造无需前馈层且无需参数更新，所有适配仅通过提示完成。这些发现建立了上下文学习与算法模拟之间的直接联系，并为大型Transformer作为提示可编程算法库提供了简单机制。研究结果揭示了GPT风格基础模型如何仅通过提示即可切换算法，在现代Transformer模型中确立了一种算法通用性的表现形式。</p>
<div class="markdown-heading"><h2 class="heading-element">分层强化学习中嵌套选项生成的多层抽象（MANGO）</h2><a id="user-content-分层强化学习中嵌套选项生成的多层抽象mango" class="anchor" aria-label="Permalink: 分层强化学习中嵌套选项生成的多层抽象（MANGO）" href="#分层强化学习中嵌套选项生成的多层抽象mango"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17751v1  公告类型：新成果<br>
摘要：本文提出MANGO（多层抽象嵌套选项生成）这一新型分层强化学习框架，旨在解决长期稀疏奖励环境中的挑战。该框架通过将复杂任务分解为多层抽象结构，每层定义抽象状态空间并采用选项机制将轨迹模块化为宏动作。这些选项在层级间嵌套传递，实现学习动作的高效复用与样本利用率的提升。MANGO创新性地引入层内策略指导智能体在抽象状态空间中的转移，并通过任务动作整合奖励函数等任务特定组件。在程序化生成的网格环境中进行的实验表明，相较于标准强化学习方法，该框架在样本效率和泛化能力方面均有显著提升。通过使智能体的跨层决策过程透明化，MANGO还增强了模型可解释性，这对安全关键型和工业应用尤为重要。未来工作将探索抽象结构与抽象动作的自动发现、连续/模糊环境的自适应机制，以及更鲁棒的多层训练策略。</p>
<div class="markdown-heading"><h2 class="heading-element">通过多模态自监督框架实现场景无关的可通行性标注与估计</h2><a id="user-content-通过多模态自监督框架实现场景无关的可通行性标注与估计" class="anchor" aria-label="Permalink: 通过多模态自监督框架实现场景无关的可通行性标注与估计" href="#通过多模态自监督框架实现场景无关的可通行性标注与估计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18249v1 公告类型：新研究<br>
摘要：可通行性估计对于实现机器人在多样化地形与环境中的导航至关重要。虽然近期自监督学习方法取得了显著成果，但这些方法往往难以准确捕捉不可通行区域的特征。此外，现有研究多集中于单一模态，忽视了融合异构传感模态所带来的互补优势。为解决这些局限性，我们提出了一种多模态自监督框架用于可通行性标注与估计。首先，我们的标注流程整合足印、激光雷达和相机数据作为视觉基础模型的提示，生成同时考虑语义与几何线索的可通行性标签。随后利用这些标签，训练一个双流网络以解耦方式联合学习不同模态，增强其识别多样化可通行模式的能力。此外，我们引入基于稀疏激光雷达的监督机制以降低伪标签带来的噪声干扰。最终，在城市、越野和校园环境中的大量实验证明了本方法的有效性：所提出的自动标注方法在多样化数据集上持续达到约88%的交并比（IoU）；与现有自监督先进方法相比，我们的多模态可通行性估计网络在所有评估数据集上均实现1.6-3.5%的IoU提升，展现出更优且稳定的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">注意力层融入低维残差子空间</h2><a id="user-content-注意力层融入低维残差子空间" class="anchor" aria-label="Permalink: 注意力层融入低维残差子空间" href="#注意力层融入低维残差子空间"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16929v1 公告类型：新研究<br>
摘要：尽管普遍认为Transformer模型在高维隐藏空间中运行，但我们发现注意力输出实际上被限制在一个惊人的低维子空间内——约60%的方向占据了99%的方差，这一现象由注意力输出投影矩阵引起，并在不同模型家族和数据集中持续存在。关键的是，我们发现这种低秩结构是稀疏字典学习中普遍存在的"死亡特征"问题的根本原因，它导致随机初始化的特征与激活空间内在几何结构不匹配。基于此洞见，我们提出一种面向稀疏自编码器（SAE）的子空间约束训练方法，将特征方向初始化为激活的活跃子空间。该方法在包含100万个特征的注意力输出SAE中，将死亡特征比例从87%降至1%以下，并可进一步推广至其他稀疏字典学习方法。我们的发现既为注意力几何机制提供了新见解，也为改进大语言模型中稀疏字典学习提供了实用工具。</p>
<div class="markdown-heading"><h2 class="heading-element">交通预测的检索增强时空框架</h2><a id="user-content-交通预测的检索增强时空框架" class="anchor" aria-label="Permalink: 交通预测的检索增强时空框架" href="#交通预测的检索增强时空框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16623v1 公告类型：新成果<br>
摘要：交通预测是现代智能交通系统的基石，也是时空预测领域的核心任务。尽管先进的时空图神经网络（STGNNs）与预训练模型在交通预测方面取得显著进展，仍存在两大挑战：(i) 建模复杂时空依赖时上下文感知能力有限；(ii) 因异质性模式导致细粒度时空节点预测精度不足。受检索增强生成（RAG）技术启发，我们提出RAST框架——通过将检索增强机制与时空建模相结合来应对这些挑战的通用解决方案。该框架包含三大核心设计：1）解耦编码器与查询生成器，分别捕获解耦的空间与时间特征，并通过残差融合构建融合查询；2）时空检索库与检索器，用于维护并检索向量化的细粒度模式；3）通用骨干预测器，可灵活适配预训练STGNN或简易MLP预测器。在六个真实交通网络（含大规模数据集）上的实验表明，RAST在保持计算效率的同时实现了卓越的预测性能。</p>
<div class="markdown-heading"><h2 class="heading-element">通过保留类无关知识与双粒度表示实现少样本类增量故障诊断</h2><a id="user-content-通过保留类无关知识与双粒度表示实现少样本类增量故障诊断" class="anchor" aria-label="Permalink: 通过保留类无关知识与双粒度表示实现少样本类增量故障诊断" href="#通过保留类无关知识与双粒度表示实现少样本类增量故障诊断"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16634v1 公告类型：新成果<br>
摘要：少样本类增量故障诊断（FSC-FD）旨在仅用少量样本持续学习新故障类别而不遗忘旧知识，对工业系统具有重要意义。然而这一挑战性任务会加剧旧知识灾难性遗忘和新数据稀缺导致的过拟合问题。为此，本文提出基于双粒度表征的创新框架——双粒度引导网络（DGGN）。该框架将特征学习显式解耦为两个并行分支：1）细粒度表征流采用新型多阶交互聚合模块，从有限新样本中提取判别性类特定特征；2）粗粒度表征流专注于建模并保留所有故障类型共享的通用类无关知识。通过多语义交叉注意力机制动态融合双流表征，以稳定的粗粒度知识引导细粒度特征学习，从而抑制过拟合并缓解特征冲突。为进一步缓解灾难性遗忘，设计了边界感知样本优先级策略，并采用解耦平衡随机森林分类器应对数据失衡导致的决策边界偏差。在TEP基准和真实MFF数据集上的实验表明，DGGN在诊断性能与稳定性方面均优于最先进的FSC-FD方法。代码已开源：<a href="https://github.com/MentaY/DGGN">https://github.com/MentaY/DGGN</a></p>
<div class="markdown-heading"><h2 class="heading-element">AQ-PCDSys：面向自主太空探索的自适应量化行星陨石坑检测系统</h2><a id="user-content-aq-pcdsys面向自主太空探索的自适应量化行星陨石坑检测系统" class="anchor" aria-label="Permalink: AQ-PCDSys：面向自主太空探索的自适应量化行星陨石坑检测系统" href="#aq-pcdsys面向自主太空探索的自适应量化行星陨石坑检测系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18025v1 公告类型：新成果<br>
摘要：自主行星探测任务极度依赖实时精准的环境感知能力以实现导航与危险规避。然而，在计算资源受限的行星探测平台硬件上部署深度学习模型仍存在重大挑战。本文提出自适应量化行星陨石坑检测系统（AQ-PCDSys），这是一个专为空间探测任务中计算受限环境下的实时星载部署而设计的新型框架。该系统创新性地融合了通过量化感知训练（QAT）优化的量化神经网络（QNN）架构与自适应多传感器融合（AMF）模块：QNN架构在保持高精度的同时，显著优化了模型大小与推理延迟，适用于太空探测任务的实时星载部署；AMF模块通过自适应加权机制（AWM），在特征层级智能融合光学影像（OI）与数字高程模型（DEM）数据，能根据行星环境条件动态优先选择最相关可靠的传感器模态，从而提升跨行星多样化地形的检测鲁棒性。结合专为高效检测多尺度陨石坑而设计的多尺度检测头，AQ-PCDSys为行星陨石坑检测提供了计算高效、可靠精准的解决方案——这一关键能力将助推新一代自主行星着陆、导航与科学探测任务的发展。</p>
<div class="markdown-heading"><h2 class="heading-element">将分布鲁棒优化与实际深度学习需求对齐</h2><a id="user-content-将分布鲁棒优化与实际深度学习需求对齐" class="anchor" aria-label="Permalink: 将分布鲁棒优化与实际深度学习需求对齐" href="#将分布鲁棒优化与实际深度学习需求对齐"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16734v1 公告类型：新成果<br>
摘要：虽然传统深度学习（DL）优化方法对所有训练样本一视同仁，但分布鲁棒优化（DRO）能够自适应地为不同样本分配重要性权重。然而，DRO与当前深度学习实践之间存在显著差距。现代深度学习优化器需要具备自适应性和处理随机梯度的能力，因为这些方法展现出卓越性能。此外，在实际应用中，优化方法不仅应支持对单个样本的权重分配，还应支持对对象组（例如同一类别的所有样本）的权重分配。本文旨在通过引入ALSO——自适应损失缩放优化器——来弥合这一差距。该算法针对改进的DRO目标函数设计，能够处理样本组的权重分配问题。我们证明了所提出算法在非凸目标（深度学习模型的典型情况）上的收敛性。从表格深度学习到拆分学习任务的多领域实验评估表明，ALSO在性能上优于传统优化器和现有DRO方法。</p>
<div class="markdown-heading"><h2 class="heading-element">TreePO：通过启发式树形建模弥合策略优化效果与推理效率之间的鸿沟</h2><a id="user-content-treepo通过启发式树形建模弥合策略优化效果与推理效率之间的鸿沟" class="anchor" aria-label="Permalink: TreePO：通过启发式树形建模弥合策略优化效果与推理效率之间的鸿沟" href="#treepo通过启发式树形建模弥合策略优化效果与推理效率之间的鸿沟"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.17445v1 公告类型：新成果<br>
摘要：基于强化学习的大语言模型对齐技术近期取得显著突破，在解决复杂推理问题方面表现出色，但存在策略内推演成本高昂且推理路径多样性探索不足的局限。本研究提出TreePO方法，其核心是采用自引导推演算法将序列生成视为树状搜索过程。通过动态树采样策略和定长分段解码机制，该方法利用局部不确定性保证分支扩展。通过分摊公共前缀的计算量并及早剪枝低价值路径，TreePO在保持或增强探索多样性的同时显著降低了单次更新的计算负担。主要创新包括：(1) 分段采样算法通过连续分段减轻KV缓存压力，配合早停机制生成新分支；(2) 基于树的段级优势估计兼顾全局与局部近端策略优化；(3) 对概率与质量驱动的动态发散及回退策略的有效性分析。我们在多项推理基准测试中实证验证了TreePO的性能提升：对于已训练模型，采样设计的GPU耗时节约22%至43%；对现有模型，轨迹级采样计算量降低达40%，词元级计算量减少35%。在实现推理效率"免费提升"的同时，TreePO为基于强化学习的后训练提供了以更少样本和算力实现扩展的实用路径。项目主页详见<a href="https://m-a-p.ai/TreePO%E3%80%82" rel="nofollow">https://m-a-p.ai/TreePO。</a></p>
<div class="markdown-heading"><h2 class="heading-element">量化对大型语言模型效果的影响解析</h2><a id="user-content-量化对大型语言模型效果的影响解析" class="anchor" aria-label="Permalink: 量化对大型语言模型效果的影响解析" href="#量化对大型语言模型效果的影响解析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.16785v1 公告类型：新研究<br>
摘要：量化技术为在资源受限环境中部署大语言模型（LLM）提供了实用解决方案，但其对模型内部表示的影响尚未得到充分研究，这引发了关于量化模型可靠性的疑问。本研究采用多种可解释性技术探究量化如何影响模型及神经元行为。我们分析了多种LLM在4位和8位量化下的表现，发现量化对模型校准的影响总体较小。神经元激活分析表明，无论是否量化，死亡神经元（即在数据集中激活值始终接近零的神经元）数量保持稳定。在神经元对预测贡献方面，较小的全精度模型表现出较少的显著神经元，而较大模型通常具有更多显著神经元（Llama-2-7B除外）。量化对神经元冗余的影响因模型而异。总体而言，我们的研究结果表明量化效果可能因模型和任务而异，但并未观察到任何可能阻碍量化作为可靠模型压缩技术使用的剧烈变化。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>