<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">MoEQuant：通过专家均衡采样与亲和力引导增强混合专家大语言模型的量化性能</h2><a id="user-content-moequant通过专家均衡采样与亲和力引导增强混合专家大语言模型的量化性能" class="anchor" aria-label="Permalink: MoEQuant：通过专家均衡采样与亲和力引导增强混合专家大语言模型的量化性能" href="#moequant通过专家均衡采样与亲和力引导增强混合专家大语言模型的量化性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"MoEQuant" 作为专有技术名词保留不译，符合技术领域术语惯例</li>
<li>"Enhancing Quantization" 译为"增强...量化性能"，通过增补"性能"使中文表达更完整</li>
<li>"Mixture-of-Experts" 采用学界通用译法"混合专家"</li>
<li>"Large Language Models" 译为"大语言模型"，使用行业标准译法</li>
<li>"Expert-Balanced Sampling" 译为"专家均衡采样"，准确传达平衡采样的核心概念</li>
<li>"Affinity Guidance" 译为"亲和力引导"，保留affinity在机器学习中的专业含义</li>
<li>整体采用"通过..."的句式结构，清晰体现技术方法的实现路径</li>
<li>使用中文技术论文标题惯用的动宾结构，保持学术严谨性）</li>
</ol>
<p>arXiv:2505.03804v1 公告类型：新研究<br>
摘要：混合专家（Mixture-of-Experts，MoE）大语言模型通过动态路由和稀疏激活机制提升效率与可扩展性，在降低计算成本的同时实现了更高性能。然而，这类模型面临显著的内存开销问题，限制了其实际部署与广泛应用。训练后量化（Post-training Quantization，PTQ）作为大语言模型压缩的常用方法，在应用于MoE模型时会出现严重精度下降与泛化性能衰减。本文探究了MoE的稀疏动态特性对量化的影响，发现两大核心挑战：（1）专家间不平衡——样本在专家间分布不均，导致低频使用专家的校准不足且存在偏差；（2）专家内不平衡——源于MoE独特的聚合机制，使得不同样本与其分配专家间的关联程度存在差异。为此，我们提出专为MoE大语言模型设计的量化框架MoEQuant，其包含两项创新技术：1）专家平衡自采样（EBSS），通过利用词元累积概率和专家平衡指标作为引导因子，高效构建具有均衡专家分布的校准集；2）亲和力引导量化（AGQ），将专家与样本间的亲和关系纳入量化过程，从而精准评估单个样本对MoE层内不同专家的影响。实验表明，MoEQuant在4比特量化下为DeepSeekMoE-16B模型带来显著性能提升（HumanEval基准准确率提高超10分），同时有效提升效率。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语统一处理："sparse activation"译为"稀疏激活"，"calibration set"译为"校准集"</li>
<li>长句拆分重构：将原文复合句按中文表达习惯分解为多个短句，如"which leverage..."处理为独立分句</li>
<li>被动语态转化："are achieved"转译为主动式"实现了"</li>
<li>概念显化处理："affinities"译为"亲和关系"以准确传达机器学习语境</li>
<li>技术术语保留：MoE/EBSS/AGQ等缩写首次出现时标注全称</li>
<li>数据呈现优化："more than 10 points"译为"超10分"符合中文科技论文表述规范）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">RWKVQuant：采用标量与矢量量化代理引导混合方案对RWKV家族模型进行量化</h2><a id="user-content-rwkvquant采用标量与矢量量化代理引导混合方案对rwkv家族模型进行量化" class="anchor" aria-label="Permalink: RWKVQuant：采用标量与矢量量化代理引导混合方案对RWKV家族模型进行量化" href="#rwkvquant采用标量与矢量量化代理引导混合方案对rwkv家族模型进行量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.03803v1 公告类型：新研究<br>
摘要：RWKV是一种性能媲美Transformer的现代循环神经网络架构，但在部署到资源受限设备时仍面临挑战。训练后量化（PTQ）作为减小模型体积和降低推理延迟的关键技术，已在Transformer模型中广泛应用。然而该技术应用于RWKV时会出现显著的性能下降。本文通过研究揭示了RWKV固有的两大关键约束：（1）非线性算子阻碍了基于平滑和旋转量化的参数融合，引入了额外计算开销；（2）大量均匀分布的权重对基于聚类的量化方法构成挑战，导致精度下降。为此，我们提出RWKVQuant——专为RWKV模型设计的PTQ框架，包含两项创新技术：（1）通过评估权重均匀性及识别异常值，能自适应选择不同量化策略的粗细粒度代理机制；（2）针对RWKV中逐元素乘法运算，提升基于聚类的量化方法性能的码本优化算法。实验表明，RWKVQuant可将RWKV-6-14B模型量化为约3比特，在精度损失小于1%的同时实现2.14倍的加速。</p>
<p>（注：根据学术论文摘要的文体特点，翻译时特别注意：</p>
<ol>
<li>专业术语的准确对应，如"Post Training Quantization"译为行业通用术语"训练后量化"</li>
<li>长句拆分重构，如将原文复合从句转换为符合中文表达习惯的短句结构</li>
<li>技术概念的本土化表达，如"coarse-to-fine proxy"意译为"粗细粒度代理机制"以保持可读性</li>
<li>数量单位的规范处理，如"3-bit"译为"3比特"符合计算机领域术语规范）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">通过自适应秩与位宽高效微调量化模型</h2><a id="user-content-通过自适应秩与位宽高效微调量化模型" class="anchor" aria-label="Permalink: 通过自适应秩与位宽高效微调量化模型" href="#通过自适应秩与位宽高效微调量化模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.03802v1 公告类型：新研究<br>
摘要：QLoRA通过巧妙结合低位数量化与LoRA技术，实现了大语言模型（LLM）内存友好的微调。近期基于SVD的方法通过持续迭代更新来初始化LoRA矩阵以适配量化误差，但普遍未能稳定提升性能。动态混合精度是持续提升量化模型微调性能的自然思路，但既有方法往往单独优化低秩子空间或量化组件，未考虑二者的协同效应。为此，我们提出\textbf{QR-Adaptor}——一种无需梯度的统一策略，利用部分校准数据联合搜索每层的量化组件与低秩空间秩数，从而持续提升模型性能。QR-Adaptor并不最小化量化误差，而是将精度与秩分配视作由实际下游性能和内存占用指导的离散优化问题。相比最先进（SOTA）的量化LoRA微调方法，我们的方案在GSM8K上实现了4.89%的准确率提升，某些情况下甚至超越16位微调模型的表现，同时保持4比特设置的内存占用量。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"low-bit quantization"译为"低位数量化"以保持技术精确性</li>
<li>"gradient-free"译为"无需梯度"符合机器学习领域表述习惯</li>
<li>"state-of-the-art"采用学界通用译法"最先进"并保留"SOTA"缩写</li>
<li>数学符号\textbf{}保留原格式以符合arXiv论文排版要求</li>
<li>"GSM8K"作为专有数据集名称未翻译）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">设备端LLM赋能情境感知的Wi-Fi漫游</h2><a id="user-content-设备端llm赋能情境感知的wi-fi漫游" class="anchor" aria-label="Permalink: 设备端LLM赋能情境感知的Wi-Fi漫游" href="#设备端llm赋能情境感知的wi-fi漫游"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.04174v1 公告类型：新论文<br>
摘要：无线漫游是动态移动环境中保持无缝连接的关键性难题。传统基于阈值或启发式方案常因失效导致"粘滞切换"或"过度切换"。我们首次提出设备端大语言模型（LLM）的跨层应用：通过在应用层进行高层推理，实时触发物理层/MAC层栈的执行动作。该LLM主要解决两大任务：（i）上下文感知的AP选择——通过结构化提示融合位置、时间等环境线索，选择最优BSSID；（ii）动态阈值调整——模型自适应决策漫游时机。为满足边缘硬件严格的延迟与资源限制，我们采用思维链提示、参数高效微调和量化三重优化。室内外数据集实验表明，本方案超越传统启发式方法与深度强化学习基线，在漫游稳定性和信号质量间实现优异平衡。这些发现印证了应用层LLM推理对未来边缘系统中底层无线控制的巨大潜力。</p>
<p>（注：根据学术论文摘要的文体特点，翻译时着重处理了以下要点：</p>
<ol>
<li>专业术语统一："BSSID"保留英文缩写，"PHY/MAC stack"译为"物理层/MAC层栈"</li>
<li>技术概念准确表达："chain-of-thought prompting"采用学界通用译法"思维链提示"</li>
<li>长句拆分重构：将原文复合长句按中文表达习惯分解为多个短句</li>
<li>被动语态转化："are applied"等被动式转为主动表述"我们采用"</li>
<li>学术语气保持：使用"该LLM"、"本方案"等符合学术规范的指代方式）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">大型语言模型压缩：全局秩与稀疏性优化</h2><a id="user-content-大型语言模型压缩全局秩与稀疏性优化" class="anchor" aria-label="Permalink: 大型语言模型压缩：全局秩与稀疏性优化" href="#大型语言模型压缩全局秩与稀疏性优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.03801v1 公告类型：新研究<br>
摘要：低秩与稀疏复合近似是压缩大语言模型（LLM）的自然思路，但现有方法面临两大影响性能的核心挑战：一是低秩矩阵与稀疏矩阵间的交互协作问题，二是不同网络层冗余度差异显著导致的权重分配难题。为此，我们提出一种具有全局秩与稀疏度优化能力的双阶段LLM压缩方法。值得注意的是，整体优化空间极为庞大，全面优化在计算上不可行。因此第一阶段采用鲁棒主成分分析将LLM权重矩阵分解为低秩分量和稀疏分量，二者分别生成包含目标矩阵的低维空间与稀疏空间。第二阶段提出概率化全局优化技术，在上述两个空间中联合识别最优低秩与稀疏结构。本方法的突出优势在于能自动感知不同层的冗余特征，并智能调控稀疏与低秩组件的协同关系。大量实验表明，该方法在稀疏化与复合近似任务上显著超越现有最优技术。</p>
<p>（说明：翻译过程中进行了以下专业处理：</p>
<ol>
<li>将"composite approximation"译为专业术语"复合近似"</li>
<li>"robust principal component analysis"采用学界通用译法"鲁棒主成分分析"</li>
<li>将技术描述"probabilistic global optimization"转化为符合中文论文习惯的"概率化全局优化"</li>
<li>通过"智能调控"等措辞准确传达"manage the interaction"的技术内涵</li>
<li>保持"state-of-the-art"在学术文献中的标准译法"现有最优技术"</li>
<li>对长难句进行合理切分，如将原文最后复合句拆分为两个中文短句，确保可读性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">分组序列化排列旋转：免费优化量化旋转变换</h2><a id="user-content-分组序列化排列旋转免费优化量化旋转变换" class="anchor" aria-label="Permalink: 分组序列化排列旋转：免费优化量化旋转变换" href="#分组序列化排列旋转免费优化量化旋转变换"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>注：翻译时尽量保持了术语的准确性和专业性：</p>
<ol>
<li>"Grouped Sequency-arranged"译为"分组序列化排列"，其中"sequency"在信号处理领域通常译为"序列性/序列化"</li>
<li>"Rotation"在数学变换语境下译为"旋转"而非"轮换"</li>
<li>"Optimizing...for Free"采用意译为"免费优化"，突出技术方案的成本优势特性</li>
<li>整体采用技术论文标题常见的名词短语结构，通过冒号分层主副标题</li>
<li>保留了"Quantization"在数字信号处理中的专业译法"量化"</li>
</ol>
<p>arXiv:2505.03810v1 公告类型：新研究<br>
摘要：大型语言模型（LLMs）因高昂计算成本面临部署挑战，虽然训练后量化（PTQ）提供了解决方案，但现有基于旋转的方法在极低比特位宽（如2比特）下表现欠佳。我们提出了一种无需训练的新颖方法，通过构建改进的旋转矩阵解决当前技术的局限性。核心创新包括：采用按序数排列的沃尔什-哈达玛变换，通过聚类相似频率分量来降低量化误差（相比标准哈达玛矩阵），显著提升性能；进一步提出分组序数旋转（GSR）技术，使用包含小型沃尔什矩阵的块对角矩阵，有效隔离异常值影响，在不需任何训练的情况下达到与基于优化的方法相当的性能。我们的方法在推理任务和WikiText-2数据集上的困惑度（PPL）指标均表现优异，即使应用于现有学习型旋转技术之上也能进一步提升效果。</p>
<p>（注：根据学术论文摘要的文体特点，翻译时进行了以下处理：</p>
<ol>
<li>技术术语采用国内计算机领域通用译法（如"Post-Training Quantization"译为"训练后量化"）</li>
<li>长句拆分重组以符合中文表达习惯（如原文第二句拆分为两个分句）</li>
<li>专业概念保留英文缩写（如PPL）同时首次出现标注中文全称</li>
<li>被动语态转换为主动表述（如"is proposed"译为"提出"）</li>
<li>关键创新点通过分号连接保持逻辑紧凑性）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>