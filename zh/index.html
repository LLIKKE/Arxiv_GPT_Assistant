<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">TokenFlow：通过抢占式调度在请求突发下提供响应式LLM文本流服务</h2><a id="user-content-tokenflow通过抢占式调度在请求突发下提供响应式llm文本流服务" class="anchor" aria-label="Permalink: TokenFlow：通过抢占式调度在请求突发下提供响应式LLM文本流服务" href="#tokenflow通过抢占式调度在请求突发下提供响应式llm文本流服务"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02758v1宣布类型：新摘要：实时LLM交互需要流传输令牌生成，文本令牌逐步生成并交付给用户，同时平衡两个目标：响应性（即，到第一个令牌的时间较短）和稳定的生成（即，需要时间间隔）。标准LLM服务系统受到非抢占式请求调度和反应式存储器管理造成的灵活性的影响，导致资源利用率低和请求突发下的请求处理并行性低。因此，我们提出TokenFlow，这是一种新型的LLM服务系统，通过抢先请求调度和主动密钥-值（KV）缓存管理增强了文本流性能。TokenFlow根据实时令牌缓冲区占用率和令牌消耗率动态地对请求进行优先级排序，同时在后台的图形处理器和中央处理器内存之间主动传输KV缓存，并将I/O与计算重叠，以最大限度地减少请求抢占费用。跨多个图形处理器（RTX 4090、A6000、H200）对Llama 3 -8B和Qwen 2.5 - 32 B进行的广泛实验表明，TokenFlow的有效吞吐量提高了82.5%（考虑到实际用户消耗），同时将P99 TTFT降低高达80.2%，而不会降低总体令牌吞吐量。</p>
<div class="markdown-heading"><h2 class="heading-element">评估动态训练后量化中灾难性失败的可能性</h2><a id="user-content-评估动态训练后量化中灾难性失败的可能性" class="anchor" aria-label="Permalink: 评估动态训练后量化中灾难性失败的可能性" href="#评估动态训练后量化中灾难性失败的可能性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02457v1宣布类型：新摘要：训练后量化（PTQ）最近已成为一种有效工具，通过以较低的精确度表示神经网络的权重和激活来降低其计算复杂性和内存使用。虽然这种范式在降低计算和存储成本方面取得了巨大成功，但根据推理中经历的输入的分布，性能有可能大幅下降。当考虑在安全关键环境中的可能部署时，重要的是要调查潜在性能下降的程度，以及输入分布的哪些特征可能会导致这种下降。在这项工作中，我们探索了由动态PTQ产生的极端故障的想法，并制定了知识蒸馏和强化学习任务来学习网络和比特宽度策略对，以便根据最坏情况的潜力来分析量化下的灾难性故障。我们的结果证实了这种“有害”的网络策略对的存在，几个实例表明性能准确性下降了10-65%，而“稳健”的对应者的准确性下降了&lt;2%。通过系统性实验和分析，我们还对脆弱性最高的点进行了初步探索。虽然我们的结果代表了理解PTQ引入的失败案例的第一步，但我们的研究结果最终强调了在现实世界的部署场景中需要谨慎。我们希望这项工作鼓励对稳健性进行更严格的检查，并更加强调深度学习更广泛领域未来工作的安全考虑。</p>
<div class="markdown-heading"><h2 class="heading-element">状态空间模型训练中压缩的奇怪案例</h2><a id="user-content-状态空间模型训练中压缩的奇怪案例" class="anchor" aria-label="Permalink: 状态空间模型训练中压缩的奇怪案例" href="#状态空间模型训练中压缩的奇怪案例"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02823v1宣布类型：新摘要：状态空间模型（SSMS）旨在有效地处理长序列建模任务，提供并行训练和快速推理。它们的核心是维持隐藏状态的循环动态系统，更新成本随着状态维度而缩放。一个关键的设计挑战是在最大化表现力和限制计算负担之间取得正确的平衡。控制理论，更具体地说，汉克尔奇异值分析，为每个状态的能量测量提供了一个有效的框架，以及将原始系统平衡截断到具有性能保证的较小表示。利用汉克尔矩阵的特征值稳定性属性，我们在训练期间将此镜头应用于RSM，其中仅识别和保留高影响力的维度。我们的方法适用于线性时不变的STM，例如线性循环单元，但也可以扩展到选择性模型。实验表明，训练中的缩减可以显着加速优化，同时保留表达能力，压缩模型保留了直接在较小维度上训练的模型所丢失的任务关键型结构。换句话说，在训练期间开始变大并缩小的ASM可以实现计算效率，同时保持更高的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">HyperAdaLoRA：通过超网络在训练期间加速LoRA排名分配，而不牺牲性能</h2><a id="user-content-hyperadalora通过超网络在训练期间加速lora排名分配而不牺牲性能" class="anchor" aria-label="Permalink: HyperAdaLoRA：通过超网络在训练期间加速LoRA排名分配，而不牺牲性能" href="#hyperadalora通过超网络在训练期间加速lora排名分配而不牺牲性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02630v1宣布类型：新摘要：参数高效微调（PEFT），特别是低等级自适应（LoRA），已成为微调大型语言模型（LLM）同时减少计算和内存负担的一种有前途的方法。然而，LoRA假设每个增量矩阵都有统一的rank \texttit {r}，而不考虑不同模块和层之间权重矩阵的不同重要性。AdaLoRA利用奇异值分解（DID）来参数化更新，并利用奇异值的修剪来引入动态排名分配，从而增强适应性。但在训练过程中，经常会遇到收敛速度慢、计算负担高的问题。为了解决这个问题，我们提出了HyperAdaLoRA，这是一种新颖的框架，通过利用超网络加速AdaLoRA的融合。HyperAdaLoRA没有直接优化奇异值分解$（P，\Lambda，Q）$的组件，而是采用基于注意力机制的超网络来动态生成这些参数。通过修剪生成奇异值的超网络的输出，实现动态排名分配。对各种数据集和模型的全面实验表明，我们的方法可以在不牺牲性能的情况下实现更快的收敛。此外，对其他基于LoRA的方法的进一步扩展实验验证了我们方法的广泛适用性。</p>
<div class="markdown-heading"><h2 class="heading-element">缓解多模式推理中的模式不平衡</h2><a id="user-content-缓解多模式推理中的模式不平衡" class="anchor" aria-label="Permalink: 缓解多模式推理中的模式不平衡" href="#缓解多模式推理中的模式不平衡"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02608v1宣布类型：新摘要：部署在现实世界任务（例如计算机使用代理）中的基础模型（FM）必须集成不同的模式。FM在执行联合推理、同时推理多个模式方面有多好，特别是当模式相互交互并相互关联以形成跨模式上下文时？为了更好地理解这个问题，我们研究了跨模式冲突的FM：跨模式呈现相互冲突的证据的场景。这使我们能够检查FM是否优先考虑一种模式而不是另一种模式，或者共同考虑和解冲突。我们的实验表明，FM在90%的情况下可以识别由单一模式组成的单模式上下文中的冲突，但当证据在不同模式之间分开时，这一比例低至3%--类似的观察结果在跨语言上下文中也成立，由多种语言组成。我们将这种失败归因于跨模式注意力不平衡，表明FM在注意力得分方面表现出极端不对称，不成比例地优先考虑某些模式。我们表明，跨模式注意力不平衡并不能通过简单地盲目扩大多模式或多语言数据集来消除，因为它们缺乏明确需要跨模式推理的训练示例。我们证明，即使是在每个训练实例中显式组合多种模式的简单且可扩展的方法，也可以显着减少注意力不平衡。注意力不平衡的减少直接转化为多个视觉语言基准测试的下游性能的提高。我们的研究结果强调了系统性地解决跨模式上下文以构建可靠的基础模型的重要性。</p>
<div class="markdown-heading"><h2 class="heading-element">面向安全可解释网格集成EV的多模态大型语言模型框架</h2><a id="user-content-面向安全可解释网格集成ev的多模态大型语言模型框架" class="anchor" aria-label="Permalink: 面向安全可解释网格集成EV的多模态大型语言模型框架" href="#面向安全可解释网格集成ev的多模态大型语言模型框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02592v1宣布类型：新摘要：电动汽车（EV）融入智能电网为增强交通系统和能源网络提供了独特的机会。然而，确保驾驶员、车辆和周围环境之间安全且可解释的互动仍然是一个严峻的挑战。本文提出了一个基于多模式大型语言模型（LLM）的框架来处理多模式传感器数据（例如对象检测、语义分割和车辆遥感）并为驾驶员生成自然语言警报。该框架使用从城市道路上行驶的仪表车辆收集的现实世界数据进行验证，确保其适用于现实世界场景。通过结合视觉感知（YOLOv 8）、地理编码定位和CAN巴士遥感，该框架将原始传感器数据和驾驶员理解联系起来，从而在城市驾驶场景中实现更安全、更明智的决策。使用真实数据的案例研究证明了该框架在为关键情况（例如接近行人、骑自行车的人和其他车辆）生成上下文感知警报方面的有效性。本文强调了LLM作为电动汽车辅助工具的潜力，通过实现可扩展的车队协调、电动汽车负载预测和交通感知能源规划，使交通系统和电力网络受益。   指数术语-电动汽车、视觉感知、大型语言模型、YOLOv 8、语义分段、CAN接口、提示工程、智能电网。</p>
<div class="markdown-heading"><h2 class="heading-element">空间关系的多峰函数载体</h2><a id="user-content-空间关系的多峰函数载体" class="anchor" aria-label="Permalink: 空间关系的多峰函数载体" href="#空间关系的多峰函数载体"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02528v1宣布类型：新摘要：大型多模式模型（LSYS）从有限的多模式演示中展示了令人印象深刻的上下文学习能力，但支持此类任务学习的内部机制仍然不透明。在大型语言模型先前工作的基础上，我们表明视觉语言模型OpenFlamingo-4 B中的一小部分注意力头负责传输空间关系的表示。这些注意力头的激活（称为功能载体）可以被提取和操纵，以改变LMM在关系任务上的性能。首先，我们使用合成和真实图像数据集，应用因果中介分析来识别对关系预测产生强烈影响的注意力，并提取可提高推理时零射准确性的多峰函数载体。我们进一步证明，可以用适量的训练数据来微调这些多模式函数载体，同时保持LMM参数冻结，以显着优于上下文学习基线。最后，我们证明了特定关系的函数载体可以线性组合，以解决涉及新颖且未经训练的空间关系的类比问题，凸显了这种方法的强大概括能力。我们的结果表明，LSYS将空间关系知识编码在本地化的内部结构中，可以系统地提取和优化这些知识，从而促进我们对模型模块性的理解，并增强对LSYS中关系推理的控制。</p>
<div class="markdown-heading"><h2 class="heading-element">Litespark技术报告：高效率，节能的LLM培训框架</h2><a id="user-content-litespark技术报告高效率节能的llm培训框架" class="anchor" aria-label="Permalink: Litespark技术报告：高效率，节能的LLM培训框架" href="#litespark技术报告高效率节能的llm培训框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02483v1宣布类型：新摘要：训练大型语言模型（LLM）受到训练时间长和能源消耗大的困扰，现代模型需要数月的计算和千兆瓦时的电力。鉴于这些挑战，我们引入了Litesark，这是一种新型的预训练框架，通过对Transformer注意力和MLP层进行有针对性的优化来解决这些低效率问题。我们的方法将架构改进与算法增强相结合，以最大限度地提高模型FLOP利用率（MFU），同时保持与标准Transformer实现的兼容性。使用SlimPajama-627 B数据集对3B和30 B参数Llama模型进行全面基准测试，证明了巨大的性能提升：多节点H200图形处理器集群中训练吞吐量提高了2x-6倍，能耗降低了55%-83%。这些优化与模型和硬件无关，能够广泛适用于Transformer架构，并扩展到训练后阶段，包括监督式微调和直接偏好优化。</p>
<div class="markdown-heading"><h2 class="heading-element">CHARD：通过设备云协作定制混合精度设备上模型以进行顺序推荐</h2><a id="user-content-chard通过设备云协作定制混合精度设备上模型以进行顺序推荐" class="anchor" aria-label="Permalink: CHARD：通过设备云协作定制混合精度设备上模型以进行顺序推荐" href="#chard通过设备云协作定制混合精度设备上模型以进行顺序推荐"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2510.03038v1 Announce Type: new  Abstract: With the advancement of mobile device capabilities, deploying reranking models directly on devices has become feasible, enabling real-time contextual recommendations. When migrating models from cloud to devices, resource heterogeneity inevitably necessitates model compression. Recent quantization methods show promise for efficient deployment, yet they overlook device-specific user interests, resulting in compromised recommendation accuracy. While on-device finetuning captures personalized user preference, it imposes additional computational burden through local retraining. To address these challenges, we propose a framework for \underline{\textbf{C}}ustomizing \underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for sequential \underline{\textbf{R}}ecommendation with \underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging channel-wise mixed-precision quantization to simultaneously achieve personalization and resource-adaptive deployment. CHORD distributes randomly initialized models across heterogeneous devices and identifies user-specific critical parameters through auxiliary hypernetwork modules on the cloud. Our parameter sensitivity analysis operates across multiple granularities (layer, filter, and element levels), enabling precise mapping from user profiles to quantization strategy. Through on-device mixed-precision quantization, CHORD delivers dynamic model adaptation and accelerated inference without backpropagation, eliminating costly retraining cycles. We minimize communication overhead by encoding quantization strategies using only 2 bits per channel instead of 32-bit weights. Experiments on three real-world datasets with two popular backbones (SASRec and Caser) demonstrate the accuracy, efficiency, and adaptivity of CHORD.</p>
<div class="markdown-heading"><h2 class="heading-element">MM-Nav：通过多专家学习实现鲁棒视觉导航的多视图VLA模型</h2><a id="user-content-mm-nav通过多专家学习实现鲁棒视觉导航的多视图vla模型" class="anchor" aria-label="Permalink: MM-Nav：通过多专家学习实现鲁棒视觉导航的多视图VLA模型" href="#mm-nav通过多专家学习实现鲁棒视觉导航的多视图vla模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03142v1宣布类型：新摘要：视觉导航政策被广泛认为是一个有前途的方向，因为它通过使用以自我为中心的视觉观察进行导航来模仿人类。然而，视觉观察的光学信息很难像LiDART点云或深度图那样进行显式建模，这随后需要智能模型和大规模数据。为此，我们建议利用视觉-语言-动作（VLA）模型的智能，以师生的方式从合成专家数据中学习多样化的导航能力。具体来说，我们将VLA模型MM-Nav实现为基于预训练的大型语言模型和视觉基础模型的多视图VLA（具有360个观察）。对于大规模导航数据，我们从三名强化学习（RL）专家那里收集专家数据，他们在三个具有挑战性的、针对不同导航能力的量身定制环境中接受了特权深度信息培训：到达、挤压和避开。我们使用从RL专家在线收集的数据迭代训练VLA模型，其中训练率根据个人能力的性能动态平衡。通过在合成环境中的大量实验，我们证明了我们的模型具有很强的概括能力。此外，我们发现我们的学生VLA模型优于RL教师，证明了整合多种能力的协同效应。广泛的现实世界实验进一步证实了我们方法的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">许多零计算专家的混合：高速量化理论的视角</h2><a id="user-content-许多零计算专家的混合高速量化理论的视角" class="anchor" aria-label="Permalink: 许多零计算专家的混合：高速量化理论的视角" href="#许多零计算专家的混合高速量化理论的视角"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03151v1宣布类型：新摘要：本文使用经典的高速率量化理论来为回归任务的专家混合（MoE）模型提供新的见解。我们的MoE是通过将输入空间分割为区域来定义的，每个区域都有一个单参数专家，该专家充当一个在推理时进行零计算的常数预测器。受高速率量化理论假设的启发，我们假设专家的数量足够大，使得他们的输入空间区域非常小。这使我们能够研究MoE模型类的逼近误差：（i）对于一维输入，我们制定测试误差及其最小化分割和专家;（ii）对于多维输入，我们制定测试误差的上界并研究其最小化。此外，我们考虑从训练数据集中学习专家参数，给定输入空间分割，并制定它们的统计学习属性。这使我们从理论和经验上表明MoE学习中的逼近和估计误差之间的权衡如何取决于专家数量。</p>
<div class="markdown-heading"><h2 class="heading-element">压缩还是不压缩？突破无损GenAI模型的前沿通过指数浓度来衡量压缩</h2><a id="user-content-压缩还是不压缩突破无损genai模型的前沿通过指数浓度来衡量压缩" class="anchor" aria-label="Permalink: 压缩还是不压缩？突破无损GenAI模型的前沿通过指数浓度来衡量压缩" href="#压缩还是不压缩突破无损genai模型的前沿通过指数浓度来衡量压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02676v1宣布类型：新摘要：生成式人工智能（GenAI）模型扩展到数千亿个参数，使得低精度计算对于高效部署至关重要。我们认为，基本的解决方案在于开发低精度浮点格式，这种格式本质上可以提供数字稳定性、内存节省和硬件效率，而无需去量化负担。在本文中，我们对GenAI权重中的指数集中现象进行了理论和实证研究：指数在架构和模式中始终表现出低的熵。我们表明，这自然产生于随机梯度下降引起的$\Alpha$-稳定分布，并且我们证明了指数的熵的紧界。我们的分析确定了FP4.67附近的理论压缩极限，这激励了实用的FP 8格式的设计。在这些见解的基础上，我们提出了指数集中的FP 8（ECF 8），这是一个无损压缩框架，具有信息感知编码和图形处理器优化解码。在参数高达671 B的LLM和DiT上进行的实验表明，可节省高达26.9%的内存和177.1%的吞吐量加速，并具有完美的无损计算，即模型输出没有偏差。我们的结果将指数集中度确立为训练模型的统计定律，并为FP 8时代的无损低精度浮点设计开辟了一条原则性道路。</p>
<div class="markdown-heading"><h2 class="heading-element">ARMs：具有即插即用攻击的多模态模型自适应红队Agent</h2><a id="user-content-arms具有即插即用攻击的多模态模型自适应红队agent" class="anchor" aria-label="Permalink: ARMs：具有即插即用攻击的多模态模型自适应红队Agent" href="#arms具有即插即用攻击的多模态模型自适应红队agent"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02677v1公告类型：新摘要：随着视觉语言模型（VLM）的日益突出，其多模态界面也引入了新的安全漏洞，使安全评估变得具有挑战性和关键性。现有的红队工作要么局限于一组狭窄的对抗模式，要么严重依赖于人工工程，缺乏对新兴的现实世界VLM漏洞的可扩展探索。为了弥合这一差距，我们提出了ARM，这是一种自适应的红色团队代理，可以系统地对VLM进行全面的风险评估。给定目标有害行为或风险定义，ARM会通过推理增强的多步骤编排自动优化不同的红色团队策略，以有效地从目标VLM中引出有害输出。我们提出了11种新颖的多模式攻击策略，涵盖了VLM的各种对抗模式（例如，推理劫持、上下文伪装），并通过模型上下文协议（HCP）将17种红色团队算法集成到ARM中。为了平衡攻击的多样性和有效性，我们设计了一个具有epsilon-great攻击探索算法的分层内存。针对基于实例和策略的基准的广泛实验表明，ARM实现了SOTA攻击成功率，平均超过基线52.1%，在Claude-4-Sonnet上超过90%。我们表明，ARM生成的红色团队实例的多样性明显更高，揭示了VLM中新出现的漏洞。利用ARM，我们构建ARMS-Bench，这是一个大规模多模式安全数据集，包括超过3万个跨越51个不同风险类别的红色团队实例，基于现实世界的多模式威胁和监管风险。使用ARMs-Bench进行安全微调，大大提高了VLM的稳健性，同时保留了其通用性，为改进多模式安全性以应对新出现的威胁提供了可操作的指导。</p>
<div class="markdown-heading"><h2 class="heading-element">剖析变形金刚：绿色人工智能的清晰视角</h2><a id="user-content-剖析变形金刚绿色人工智能的清晰视角" class="anchor" aria-label="Permalink: 剖析变形金刚：绿色人工智能的清晰视角" href="#剖析变形金刚绿色人工智能的清晰视角"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.02810v1宣布类型：新摘要：大型语言模型（LLM）的迅速采用引发了重大的环境问题。与培训的一次性成本不同，LLM推理在全球范围内持续发生，现在主导了人工智能的能源足迹。然而，由于缺乏细粒度的测量方法，大多数可持续发展研究只报告了粗略的模型级指标，更多地将能源效率视为事后考虑，而不是主要目标。我们首次对Transformer架构核心组件的推理能量进行了细粒度的经验分析。我们提出了一种新颖的方法，即通过重复采样的子级能量评估（ClearAR），以克服微秒级组件执行和毫秒级能量传感器监控之间的时间不匹配。使用Clearar，我们评估了跨越四种不同架构类型的15个模型，并始终将组件能量方差保持在9.5%以下，同时将模型总能量的90%以上作为单个组件捕获。我们的实证分析表明，注意力块每次浮点操作（FLOP）消耗的能量明显更多，这表明能源消耗与FLOP计数不成比例一致。这表明仅靠FLOP无法捕捉组件层面的真实能源成本。我们的研究结果建立了详细的组件级能源基线，并提供了见解，作为通过组件级优化构建节能Transformer模型的第一步。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>