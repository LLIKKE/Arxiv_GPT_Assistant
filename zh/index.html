<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">通过激活模式聚类的大型语言模型稀疏性预测方法</h2><a id="user-content-通过激活模式聚类的大型语言模型稀疏性预测方法" class="anchor" aria-label="Permalink: 通过激活模式聚类的大型语言模型稀疏性预测方法" href="#通过激活模式聚类的大型语言模型稀疏性预测方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.14179v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）表现出显著的激活稀疏性，即对于给定输入仅部分神经元被激活。虽然这种稀疏性为降低计算成本提供了机会，但要有效利用它，需要以可扩展的方式预测激活模式。然而，由于现代大语言模型中神经元数量庞大，直接在神经元级别进行预测的计算成本极高。为实现激活稀疏性的高效预测与利用，我们提出了一种基于聚类的激活模式压缩框架。该方法不单独处理每个神经元，而是将相似的激活模式分组为少量代表性聚类簇。我们的方法实现了高达79.34%的聚类精确度，在保持困惑度（PPL）分数仅轻微下降的同时，优于标准二元聚类方法。当聚类簇数量足够大时，该方法可获得低至12.49的PPL分数，证明其在保持模型质量同时降低计算开销的有效性。通过预测聚类分配而非单个神经元状态，未来模型可从预计算的聚类中心高效推断激活模式。我们详细阐述了聚类算法，分析了其在捕捉有效激活结构方面的性能，并论证了其提升稀疏计算效率的潜力。这种基于聚类的框架为未来激活模式预测研究奠定了基础，为大规模语言模型的高效推理开辟了新途径。</p>
<div class="markdown-heading"><h2 class="heading-element">催化剂：一种用于结构化剪枝的新型正则化器，附带参数空间的辅助扩展</h2><a id="user-content-催化剂一种用于结构化剪枝的新型正则化器附带参数空间的辅助扩展" class="anchor" aria-label="Permalink: 催化剂：一种用于结构化剪枝的新型正则化器，附带参数空间的辅助扩展" href="#催化剂一种用于结构化剪枝的新型正则化器附带参数空间的辅助扩展"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.14170v1 公告类型：新论文<br>
摘要：结构化剪枝旨在通过移除完整的滤波器或通道来减小深度神经网络的规模与计算成本。传统正则化方法（如L1、Group Lasso及其变体）会导致基于幅度的剪枝决策偏差——即幅度较小的滤波器更易被剪除。此外，这些方法常产生紧贴剪枝决策边界的零边际结果，使得滤波器幅度的微小扰动就可能逆转剪枝决策。本文首先精确界定了保持模型性能的剪枝操作所需满足的代数条件，并利用该条件在扩展参数空间中通过辅助催化剂变量构建了一种新型正则化器。所提出的催化剂正则化（Catalyst regularization）能确保各滤波器获得公平的剪枝机会：理论上可证明其完全不受幅度偏差影响，且通过保留与剪除滤波器间幅度值的宽边际分岔实现鲁棒的剪枝行为。这些理论特性自然转化为实际效能，催化剂剪枝算法（Catalyst Pruning）的实证验证表明：在多数据集与多模型上的剪枝效果均优于当前最先进的滤波器剪枝方法，同时证实了该算法具有理论预测的鲁棒性与公平性剪枝特征。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语处理："structured pruning"译为"结构化剪枝"，"filters/channels"统一译为"滤波器/通道"，"regularizer"译为"正则化器/方法"</li>
<li>技术概念显化："magnitude-biased"译为"基于幅度的偏差"，"wide-margin bifurcation"译为"宽边际分岔"既保留数学意象又确保可读性</li>
<li>长句拆分重构：将原文复合从句按中文表达习惯分解为多个短句，如将"such that..."结构转换为破折号解释说明</li>
<li>被动语态转化："are likely to be pruned"译为"更易被剪除"符合中文主动表达习惯</li>
<li>学术风格保持：使用"界定""构建""证实"等学术动词，保持论文摘要的严谨性</li>
<li>理论-实践衔接："naturally lead to"译为"自然转化为"准确传达因果关系）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">IPPRO：基于重要性且带有投影偏移的修剪方法，适用于与规模无关的结构化剪枝</h2><a id="user-content-ippro基于重要性且带有投影偏移的修剪方法适用于与规模无关的结构化剪枝" class="anchor" aria-label="Permalink: IPPRO：基于重要性且带有投影偏移的修剪方法，适用于与规模无关的结构化剪枝" href="#ippro基于重要性且带有投影偏移的修剪方法适用于与规模无关的结构化剪枝"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.14171v1 公告类型：新论文<br>
摘要：随着神经网络压缩方法需求的增长，基于重要性判定的结构化剪枝方法（包括重要性评估法）正被广泛研究。传统幅度重要性及诸多现代关联判定标准往往限制剪枝决策的灵活性——只要较小幅度的滤波器未被剪除，较大幅度的滤波器即使冗余也难以被移除。本文提出一种创新剪枝策略：通过将滤波器投影至射影空间，挑战幅度参数的主导地位，使每个滤波器获得公平的剪枝机会。随后通过观察梯度下降过程中滤波器是否向原点移动，量化其可剪枝性。基于此构建PROscore新型重要性评分，进而提出IPPRO框架——首个对幅度参数不敏感的基于重要性的结构化剪枝方法。实验结果表明，采用射影空间的重要性标准能实现接近无损的剪枝效果，显著降低剪枝过程中的性能损失，并在微调后展现优异性能。本研究打破了剪枝领域"以大小论重要性"的迷思，从理论与实证双重维度拓展了基于重要性的剪枝技术边界。</p>
<p>（注：翻译过程中对以下术语进行了专业处理：</p>
<ol>
<li>"projective space"译为"射影空间"符合数学规范</li>
<li>"magnitude-indifference"创造性译为"对幅度参数不敏感"以保持技术准确性</li>
<li>"near-lossless pruning"译为"接近无损的剪枝"体现工程术语特征</li>
<li>将原文被动语态转换为中文主动表述（如"are actively studied"→"正被广泛研究"）</li>
<li>保留专业缩写"IPPRO"及数学概念"PROscore"的英文原名）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">通过软系数优化实现深度神经网络中特定应用组件感知的结构化剪枝</h2><a id="user-content-通过软系数优化实现深度神经网络中特定应用组件感知的结构化剪枝" class="anchor" aria-label="Permalink: 通过软系数优化实现深度神经网络中特定应用组件感知的结构化剪枝" href="#通过软系数优化实现深度神经网络中特定应用组件感知的结构化剪枝"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.14882v1 公告类型：新研究<br>
摘要：深度神经网络（DNNs）虽具有显著的通用性与性能优势，但其广泛应用常受限于高模型复杂度与计算需求。剪枝等模型压缩技术已成为解决这些挑战的有效方案，但确保压缩过程中保持应用场景特定的性能特征仍至关重要。在结构化剪枝（即移除结构连贯的元件组时），传统重要性度量标准往往难以维持这些关键性能属性。本研究提出一个增强的重要性度量框架，不仅能缩减模型规模，还能显式地兼顾应用特定的性能约束。我们采用多策略确定各组的最佳剪枝强度，实现压缩量与任务性能的平衡。该方法在用于MNIST图像重建的自编码器上进行评估，实验结果表明：通过满足应用特定标准，所提方案能有效保留任务相关性能，即使经过大幅剪枝后仍可维持模型的可用性。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"pruning"统一译为"剪枝"（计算机领域标准译法）</li>
<li>"autoencoder"译为"自编码器"（保留专业术语一致性）</li>
<li>长难句采用分译法处理（如原文第二句拆分为两个中文句子）</li>
<li>被动语态转换为主动句式（如"are preserved"译为"保持"）</li>
<li>补充衔接词"即"增强逻辑性，符合中文表达习惯）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">LaCache：阶梯式键值缓存助力大语言模型高效长上下文建模</h2><a id="user-content-lacache阶梯式键值缓存助力大语言模型高效长上下文建模" class="anchor" aria-label="Permalink: LaCache：阶梯式键值缓存助力大语言模型高效长上下文建模" href="#lacache阶梯式键值缓存助力大语言模型高效长上下文建模"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.14204v1 公告类型：新研究<br>
摘要：大型语言模型（LLMs）的最新进展激发了人们对众多应用场景的兴趣，这些场景需要强大的长程处理能力，以应对冗长的输入上下文并持续生成扩展输出。随着序列长度的增加，LLMs中的键值对（KV）数量急剧上升，形成显著的效率瓶颈。本文提出一种名为LaCache的新型KV缓存优化范式，这是一种无需重新训练即可实现LLMs高效精准生成推理的方法。LaCache使LLMs能够同时解决长程建模中的两大关键挑战：在保持强大长程能力的同时，实现持续生成而不触发内存溢出（OOM）。</p>
<p>具体而言，LaCache融合了两项核心创新：（1）阶梯式KV缓存模式——不仅在各层内部按序存储KV对（从左至右），还实现了跨层存储（从浅层到深层），在固定存储容量下为捕捉长程依赖提供了扩展空间，从而增强长程处理能力；（2）迭代压缩机制——基于令牌距离动态压缩历史缓存，在固定缓存容量内为新令牌释放空间。这种动态压缩策略使模型在受限缓存条件下能更有效地进行持续生成。</p>
<p>在多种任务、基准测试和LLM模型上的实验一致验证了LaCache提升LLMs长程能力的有效性。我们的代码已开源：<a href="https://github.com/GATECH-EIC/LaCache%E3%80%82">https://github.com/GATECH-EIC/LaCache。</a></p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理："generative inference"译为"生成推理"，"out-of-memory"保留英文缩写"OOM"并添加括号注释，技术概念如"KV pairs"保持"键值对"的通用译法。长句按中文表达习惯拆分，并采用"阶梯式""迭代"等工程术语保持专业性的同时确保可读性。）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>