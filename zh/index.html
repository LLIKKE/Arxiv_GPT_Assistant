<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">结构化稀疏性和权重自适应内存修剪和计算高效Whisper模型</h2><a id="user-content-结构化稀疏性和权重自适应内存修剪和计算高效whisper模型" class="anchor" aria-label="Permalink: 结构化稀疏性和权重自适应内存修剪和计算高效Whisper模型" href="#结构化稀疏性和权重自适应内存修剪和计算高效whisper模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12666v1宣布类型：新摘要：Whisper模型在语音识别方面取得了显着的进步;但它们的大尺寸仍然是在资源有限的边缘设备上部署的瓶颈。本文提出了一个框架来设计微调的Whisper变体，以解决上述问题。结构化稀疏性是通过稀疏组LANSO惩罚作为损失正规化器来强制执行的，以减少浮点操作（FLOP）的数量。进一步，提出了一种感知权重统计的修剪算法。我们还设计了用于WER评估的自定义文本规范器。在Common Voice 11.0印地语数据集上，我们在不降低WER的情况下获得（a）模型参数减少35.4%，Whisper-small上的内存消耗减少14.25%，FLOP减少18.5%，以及（b）模型参数减少31%，内存消耗减少15.29%，Whisper-medium上的FLOP减少16.95%;并且，（c）通过修剪多18.7%的参数并减少12.31的WER，大大优于最新的基于迭代幅度修剪的方法。</p>
<div class="markdown-heading"><h2 class="heading-element">重新思考知识蒸馏：具有负不对称回报的数据依赖调节器</h2><a id="user-content-重新思考知识蒸馏具有负不对称回报的数据依赖调节器" class="anchor" aria-label="Permalink: 重新思考知识蒸馏：具有负不对称回报的数据依赖调节器" href="#重新思考知识蒸馏具有负不对称回报的数据依赖调节器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12615v1宣布类型：新摘要：当根据学生的准确性和损失来判断时，知识蒸馏通常被认为是一种压缩机制，但人们对其功能影响知之甚少。在这项工作中，我们从功能的角度量化了知识蒸馏的压缩能力以及由此产生的知识转移，将压缩与架构简化脱钩，从而改善了对知识蒸馏的理解。我们采用假设测试、控制和随机控制提炼来了解跨数据模式的知识转移机制。为了严格测试我们分析的广度和局限性，我们探索了多种蒸馏变体并分析了不同模型尺寸的蒸馏缩放定律。我们的研究结果表明，虽然在某些模式和架构中存在统计上显着的知识转移，但这种转移的程度并没有预期那么明显，即使在旨在最大限度地提高知识共享的条件下也是如此。值得注意的是，在重大知识转移的情况下，我们发现负面知识向学生的持续且严重的不对称转移，从而引发了知识提炼应用中的安全问题。在12个实验设置、9个架构和7个数据集中，我们的研究结果表明，知识蒸馏的功能与其说是压缩机制，不如说是一种依赖数据的调节器，具有负的不对称回报。</p>
<div class="markdown-heading"><h2 class="heading-element">谨慎的重量衰减</h2><a id="user-content-谨慎的重量衰减" class="anchor" aria-label="Permalink: 谨慎的重量衰减" href="#谨慎的重量衰减"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12402v1宣布类型：新摘要：我们引入了谨慎权重衰减（CWD），这是一种一行、优化器不可知的修改，它仅将权重衰减应用于符号与优化器更新一致的参数坐标。与隐式优化正规化或约束目标的标准脱钩衰变不同，CWD保留了原始损失并允许二层解释：它在到达稳态流时引发滑动模式行为，允许它搜索未修改目标的局部帕累托最优稳定点。在实践中，CWD是AdamW、Lion和Muon等优化器的临时更改，不需要新的超参数或额外的调整。对于语言模型预训练和ImageNet分类，CWD在百万到十亿参数规模上持续提高最终损失和准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">CARVQ：具有用于LLM嵌入压缩的组剩余量量化的纠正适配器</h2><a id="user-content-carvq具有用于llm嵌入压缩的组剩余量量化的纠正适配器" class="anchor" aria-label="Permalink: CARVQ：具有用于LLM嵌入压缩的组剩余量量化的纠正适配器" href="#carvq具有用于llm嵌入压缩的组剩余量量化的纠正适配器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12721v1宣布类型：新摘要：大型语言模型（LLM）通常依赖于大量参数进行令牌嵌入，从而导致大量存储需求和内存占用。特别是，部署在边缘设备上的LLM是受内存限制的，通过压缩嵌入层来减少内存占用不仅可以释放内存带宽，还可以加速推理。为了解决这个问题，我们引入了CARVQ，这是一种与组残留量量化相结合的训练后新颖纠正适配器。CARVQ依赖于线性和非线性映射的组合，并模仿原始模型嵌入以压缩到大约1.6位，而不需要专门的硬件来支持低位存储。我们在预训练的LLaMA-3.2-1B、LLaMA-3.2-3B、LLaMA-3.2 - 3B-Direct、LLaMA-3.1-8B、Qwen 2.5 - 7 B、Qwen 2.5-Math-7 B和Phi-4等上测试我们的方法，评估常见的生成性、区分性、数学和推理任务。我们表明，在大多数情况下，与纯量量化相比，CARVQ可以实现较低的平均每参数比特宽度，同时保持合理的复杂性和准确性。我们的贡献包括一种新颖的压缩技术，该技术与最先进的Transformer量化方法兼容，并且可以无缝集成到任何支持4位内存的硬件中，以减少模型在内存受限设备中的内存占用。这项工作展示了在边缘设备上高效部署LLM的关键一步。</p>
<div class="markdown-heading"><h2 class="heading-element">FedMMKT：在多模式联邦学习中联合增强服务器文本到图像模型和客户机任务模型</h2><a id="user-content-fedmmkt在多模式联邦学习中联合增强服务器文本到图像模型和客户机任务模型" class="anchor" aria-label="Permalink: FedMMKT：在多模式联邦学习中联合增强服务器文本到图像模型和客户机任务模型" href="#fedmmkt在多模式联邦学习中联合增强服务器文本到图像模型和客户机任务模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12254v1宣布类型：新摘要：文本到图像（T2 I）模型已在广泛的应用中展示了其多功能性。然而，由于隐私问题，T2 I模型对专业任务的适应通常会受到任务特定数据的可用性的限制。另一方面，利用现代移动系统和物联网基础设施的丰富多模式数据的力量提供了绝佳的机会。本文介绍了联邦多模式知识转移（FedMMKT），这是一种新颖的框架，可以使用去中心化多模式数据共同增强服务器T2 I模型和客户端特定任务模型，而不会损害数据隐私。</p>
<div class="markdown-heading"><h2 class="heading-element">VLM微调的学习动力学</h2><a id="user-content-vlm微调的学习动力学" class="anchor" aria-label="Permalink: VLM微调的学习动力学" href="#vlm微调的学习动力学"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2510.11978v1 Announce Type: new  Abstract: Preference-based finetuning of vision--language models (VLMs) is brittle: trivially wrong negatives inject uninformative gradients that destabilize training. We recast alignment as \textbf{learning-dynamics--aware optimization} and introduce \textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that explicitly models and exploits the training trajectory. \textbf{Stage 1} performs supervised finetuning with \textbf{gentle negatives}: \textbf{low-weight smoothed supervision} that regularizes the base policy and curbs overconfidence without explicit penalties. \textbf{Stage 2} applies a DPO objective in which the \textbf{negative term is scaled by a cooling weight} computed from the model's \textbf{average token log-probability} on each negative, suppressing uninformative gradients from easy or off-distribution samples while preserving signal from hard negatives. In practice, we emphasize \textbf{on-policy negatives} and allow \textbf{mixed negatives} by blending a controllable fraction of dataset negatives to maintain contrast freshness. Throughout, we instrument training with $\Delta!\log p$ probes on positives and negatives as first-class signals for early stopping, curriculum design, and failure diagnosis. Across diverse VLM tasks, CW-DPO yields \textbf{more stable optimization}, \textbf{better calibration}, and \textbf{higher pairwise win-rates} than SFT-only and vanilla DPO, while \textbf{converging in fewer steps}. Ablations isolate the \textbf{cooling-weight mechanism} as the primary driver of these gains and show complementary benefits from mixing on-policy and dataset negatives. Taken together, our results show that \textbf{smoothing learning dynamics before cooling preferences} is a simple, general principle for robust VLM alignment.</p>
<div class="markdown-heading"><h2 class="heading-element">LIBERO-Plus：视觉-语言-动作模型的深入鲁棒性分析</h2><a id="user-content-libero-plus视觉-语言-动作模型的深入鲁棒性分析" class="anchor" aria-label="Permalink: LIBERO-Plus：视觉-语言-动作模型的深入鲁棒性分析" href="#libero-plus视觉-语言-动作模型的深入鲁棒性分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.13626v1宣布类型：新摘要：视觉语言动作（VLA）模型在机器人操作基准上报告了令人印象深刻的成功率，但这些结果可能掩盖了鲁棒性方面的根本弱点。我们通过引入七个维度的受控扰动来进行系统性漏洞分析：对象布局、摄像机视角、机器人初始状态、语言指令、光线条件、背景纹理和传感器噪音。我们全面分析了多种最先进的模型，揭示了明显能力下的一致脆弱性。我们的分析暴露了关键弱点：模型对相机视角和机器人初始状态等扰动因素表现出极端敏感性，在适度扰动下，性能从95%下降到30%以下。令人惊讶的是，模型在很大程度上对语言变化不敏感，进一步的实验表明模型往往完全忽略语言指令。我们的研究结果挑战了高基准分数等同于真正能力的假设，并强调了评估实践的必要性，以评估现实变化下的可靠性。</p>
<div class="markdown-heading"><h2 class="heading-element">MosaicDiff：反映训练前动态的扩散模型加速的免训练结构修剪</h2><a id="user-content-mosaicdiff反映训练前动态的扩散模型加速的免训练结构修剪" class="anchor" aria-label="Permalink: MosaicDiff：反映训练前动态的扩散模型加速的免训练结构修剪" href="#mosaicdiff反映训练前动态的扩散模型加速的免训练结构修剪"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11962v1宣布类型：新摘要：扩散模型以其生成能力而闻名，但它们的训练前过程表现出不同的学习速度阶段，而这些阶段在社区之前的训练后加速工作中被完全忽视了。在这项研究中，我们引入了一个名为MosaicDiff的新型框架，该框架通过子索感知的结构修剪将扩散预训练动态与训练后采样加速相结合。我们的方法利用了这样的观察，即扩散预训练的中间快速学习阶段需要更保守的修剪来保留关键模型特征，而早期和后期的缓慢学习阶段则受益于更积极的修剪策略。这种自适应修剪机制是第一个明确反映扩散预训练固有学习速度变化的机制，从而协调模型的内部训练动态与其加速采样过程。DiT和SDXL上的大量实验表明，我们的方法在不影响输出质量的情况下实现了采样的显着加速，大幅优于之前的最先进方法，还为更高效和更稳健的免训练扩散加速提供了新的观点。</p>
<div class="markdown-heading"><h2 class="heading-element">MoRA：基于LLM的多模式分子助手的实时分子感知低等级适应框架</h2><a id="user-content-mora基于llm的多模式分子助手的实时分子感知低等级适应框架" class="anchor" aria-label="Permalink: MoRA：基于LLM的多模式分子助手的实时分子感知低等级适应框架" href="#mora基于llm的多模式分子助手的实时分子感知低等级适应框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12245v1宣布类型：新摘要：有效地将分子图结构与大型语言模型（LLM）集成是药物发现的一个关键挑战。大多数现有的多模式对齐方法通常通过微调LLM或同时添加静态适配器来处理这些结构。然而，这些方法有两个主要局限性：（1）它优化了所有分子输入的共享参数空间，限制了模型捕获特定实例结构特征的能力;（2）微调分子任务的LLM可能会导致灾难性的遗忘，破坏其一般推理能力。在本文中，我们不是静态的面向任务的适应，而是为每个分子实时提出了一种特定于实例的参数空间对齐方法。为此，我们引入了分子感知的低等级自适应（MoRA），它为每个输入分子图生成一组独特的低等级自适应权重。然后将这些权重动态注入到冻结的LLM中，使模型能够根据每个分子输入的结构调整其推理，同时保留LLM的核心知识。大量实验表明，在化学反应预测和分子字幕等关键分子任务中，MoRA的特定实例动态适应优于静态适应基线，包括反应预测精确匹配度相对提高14.1%，量子性质预测误差降低22%。该代码可在<a href="https://github.com/jk-sounds/MoRA%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jk-sounds/MoRA上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">HiLoRA：用于免培训领域通用的自适应分层LoRA路由</h2><a id="user-content-hilora用于免培训领域通用的自适应分层lora路由" class="anchor" aria-label="Permalink: HiLoRA：用于免培训领域通用的自适应分层LoRA路由" href="#hilora用于免培训领域通用的自适应分层lora路由"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.12266v1宣布类型：新摘要：低等级自适应（LoRA）因其模块化设计和HuggingFace等平台上的广泛可用性，已成为一种广泛使用的将大型语言模型（LLM）适应新领域的技术。这种可用性激励了人们重新使用现有LoRA进行域概括的努力。   然而，现有的方法往往依赖于明确的任务标签或额外的训练，这是不切实际的部署。此外，它们通常会激活固定数量的整个LoRA模块，导致参数冗余或不足，从而降低性能。   在本文中，我们提出了\texttt{HiLoRA}，一个无训练的框架，在LoRA池上执行自适应分层路由。利用LoRA的结构特性，我们定义了秩一分量（ROC），其中每个秩参数被视为一个独立的单元。对于给定的输入序列，\textttt {HiLoRA}首先自适应地选择LoRA的子集，并基于序列级别的高斯似然性确定其ROC分配。在代币级别，它通过仅激活信息最丰富的ROC来进一步细化路由。   我们进一步提供了理论保证，即\textttt {HiLoRA}以高概率选择最相关的LoRA。   大量实验表明，\textttt {HiLoRA}在域概括方面实现了重大改进，准确性提高比最先进的基线高达{\small $55%$}，同时保持相当的推理吞吐量。</p>
<div class="markdown-heading"><h2 class="heading-element">InternVLA-M1：通用机器人政策的空间引导视觉-语言-动作框架</h2><a id="user-content-internvla-m1通用机器人政策的空间引导视觉-语言-动作框架" class="anchor" aria-label="Permalink: InternVLA-M1：通用机器人政策的空间引导视觉-语言-动作框架" href="#internvla-m1通用机器人政策的空间引导视觉-语言-动作框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.13778v1宣布类型：新摘要：我们引入InternVLA-M1，这是一个用于空间接地和机器人控制的统一框架，可将跟踪机器人推向可扩展的通用智能。其核心理念是空间引导的视觉-语言-动作训练，其中空间基础充当指令和机器人动作之间的关键联系。InternVLA-M1采用两阶段流水线：（i）在超过230万个空间推理数据上进行空间基础预训练，通过将指令与视觉、视觉不可知位置对齐来确定“在哪里行动”，以及（ii）空间引导动作训练后，通过即插即用空间提示生成感知动作来决定“如何行动”。这种空间引导训练方案产生了一致的收益：InternVLA-M1在SimplerEnv Google Robot上的表现比其没有空间引导的变体高出14.6%，在WidowX上的表现比其变体高出17%，在LIBERO Franka上的表现比其没有空间引导的变体高出4.3%，同时在框、点和轨迹预测方面表现出更强的空间推理能力。为了进一步扩展指令遵循，我们构建了一个模拟引擎来收集244 K个可概括的拾取和放置场景，从而在200个任务和3 K+对象中实现了6.2%的平均改进。在现实世界的集群拾取和放置中，InternVLA-M1提高了7.3%，通过合成联合训练，在未见对象和新颖配置上实现了+20.6%。此外，在长期推理密集型场景中，它超过现有作品10%以上。这些结果凸显了空间引导训练是可扩展和弹性通才机器人的统一原则。代码和型号可在<a href="https://github.com/InternRobotics/InternVLA-M1%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/InternRobotics/InternVLA-M1上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">评估用于多模式讽刺检测的开源视觉语言模型</h2><a id="user-content-评估用于多模式讽刺检测的开源视觉语言模型" class="anchor" aria-label="Permalink: 评估用于多模式讽刺检测的开源视觉语言模型" href="#评估用于多模式讽刺检测的开源视觉语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.11852v1宣布类型：新摘要：开源视觉语言模型（VLM）的最新进展为理解讽刺等复杂和主观的多模式现象提供了新的机会。在这项工作中，我们评估了七种最先进的VLMS -BLIP 2、INSTRUITBLIP、OpenFlamingo、LLaVA、PaliGemma、Gemma 3和Qwen-BL-使用零次、一次和几次提示检测多模式讽刺的能力。此外，我们还评估了模型对讽刺实例进行解释的能力。我们在三个基准讽刺数据集（Muse、MMSD2.0和SarcNet）上评估了VLM的能力。我们的主要目标有两个：（1）量化每个模型在检测讽刺图像-字幕对方面的表现，以及（2）评估它们生成人性化解释的能力，这些解释强调驱动讽刺的视觉-文本不一致。我们的结果表明，虽然当前的模型在二元讽刺检测方面取得了一定的成功，但如果没有针对特定任务的微调，它们仍然无法生成高质量的解释。</p>
<div class="markdown-heading"><h2 class="heading-element">社交模拟中LLM授权代理的具有愿望驱动目标优化的情感认知建模框架</h2><a id="user-content-社交模拟中llm授权代理的具有愿望驱动目标优化的情感认知建模框架" class="anchor" aria-label="Permalink: 社交模拟中LLM授权代理的具有愿望驱动目标优化的情感认知建模框架" href="#社交模拟中llm授权代理的具有愿望驱动目标优化的情感认知建模框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.13195v1宣布类型：新摘要：大型语言模型（LLM）的出现使代理能够在社会模拟中代表虚拟人，促进复杂社会系统内的多样化互动。然而，现有的基于LLM的代理在情感认知方面表现出严重的局限性：它们未能模拟连接虚拟和现实世界服务所必需的有限理性;它们缺乏将情感嵌入到代理决策架构中的经验验证的集成机制。本文构建了一个情感认知框架，结合愿望产生和目标管理，旨在实现基于LLM的代理和人类之间的情感对齐，建模基于LLM的代理的完整决策过程，包括状态进化，愿望产生，目标优化，决策产生，和行动执行。本研究实现了我们专有的多智能体交互环境中提出的框架。实验结果表明，我们的框架下管理的代理不仅表现出与他们的情绪状态相一致的行为，而且在对其他代理类型的比较评估中，表现出优越的生态有效性，并产生更接近人类行为模式的决策结果。</p>
<div class="markdown-heading"><h2 class="heading-element">VLA-0：构建零修改的最先进的VLA</h2><a id="user-content-vla-0构建零修改的最先进的vla" class="anchor" aria-label="Permalink: VLA-0：构建零修改的最先进的VLA" href="#vla-0构建零修改的最先进的vla"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.13054v1宣布类型：新摘要：视觉-语言-动作模型（VLA）对于实现通才机器人操纵具有巨大的前景。然而，构建它们的最佳方法仍然是一个悬而未决的问题。当前的方法通常会增加复杂性，例如用动作标记修改视觉语言模型（VLM）的现有词汇表或引入特殊的动作头。奇怪的是，直接将动作表示为文本的最简单策略在很大程度上仍然没有被探索。这项工作引入VLA-0来研究这个想法。我们发现VLA-0不仅有效，而且功能惊人。通过正确的设计，VLA-0的性能优于更复杂的型号。在LIBERO（评估VLA的流行基准）上，VLA-0的性能优于在相同机器人数据上训练的所有现有方法，包括$\pi_0.5 $-KI、OpenVLA-OFT和SmolVLA。此外，如果没有大规模机器人特定的训练，它的性能优于在大规模机器人数据上训练的方法，例如$\pi_0.5 $-KI、$\pi_0 $、GR 00 T-N1和MolmoAct。这些发现也转化为现实世界，其中VLA-0的表现优于SmolVLA，SmolVLA是一种在大规模真实数据上预训练的VLA模型。本文总结了我们意想不到的发现，并阐述了实现这种简单而强大的VLA设计的高性能所需的具体技术。此处提供了视觉结果、代码和训练模型：<a href="https://vla0.github.io/%E3%80%82" rel="nofollow">https://vla0.github.io/。</a></p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>