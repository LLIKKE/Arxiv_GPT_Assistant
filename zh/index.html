<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">利用空间细胞模型在开放场地迷宫中通过神经形态机器人模拟大鼠的联想学习</h2><a id="user-content-利用空间细胞模型在开放场地迷宫中通过神经形态机器人模拟大鼠的联想学习" class="anchor" aria-label="Permalink: 利用空间细胞模型在开放场地迷宫中通过神经形态机器人模拟大鼠的联想学习" href="#利用空间细胞模型在开放场地迷宫中通过神经形态机器人模拟大鼠的联想学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18460v1 公告类型：新研究<br>
摘要：数据驱动的人工智能（AI）方法通过大量训练数据在各种认知任务中展现出卓越能力。然而，其对大规模数据集和神经网络的依赖也带来了高功耗和适应性有限等挑战，尤其在行星探索等SWaP（尺寸、重量和功耗）受限的应用中更为突出。为解决这些问题，我们提出通过模拟动物联想学习机制来增强智能机器人的自主能力。联想学习使动物能够通过记忆并发事件适应环境。通过复现这一机制，神经形态机器人可以在动态环境中自主导航，并通过交互学习优化性能。本文探索在开放场地迷宫环境中，利用位置细胞与网格细胞等空间细胞的生物学洞察，通过神经形态机器人模拟啮齿类动物的联想学习。通过整合这些模型，我们旨在实现实时场景中空间任务的在线联想学习，弥合生物空间认知与机器人技术之间的鸿沟，推动自主系统的发展。</p>
<div class="markdown-heading"><h2 class="heading-element">HAEPO：历史聚合探索策略优化</h2><a id="user-content-haepo历史聚合探索策略优化" class="anchor" aria-label="Permalink: HAEPO：历史聚合探索策略优化" href="#haepo历史聚合探索策略优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18884v1 公告类型：新成果<br>
摘要：在现代学习体系中，探索机制至关重要——从采用小型神经策略的强化学习环境到大型语言模型（LLM）均如此。现有研究（如DPO）利用全序列对数似然来捕捉模型决策的完整轨迹，而GRPO等方法则将逐令牌比率聚合为轨迹级更新。然而，这两种方法往往在长周期任务中限制探索能力。我们提出历史聚合探索策略优化（HAEPO），通过历史感知的探索性损失函数解决上述缺陷。HAEPO将每条轨迹压缩为其对数概率之和（累积对数似然），并应用Plackett-Luce软最大值函数对轨迹进行归一化加权（权重与回报值成正比），从而激励更广泛的探索。我们引入熵正则化以稳定激进更新防止早熟收敛，同时采用相对于上一轮冻结策略（参考策略）的软KL惩罚项。实证表明，HAEPO具有快速收敛、全面探索、与真实奖励紧密对齐的特性，在多样任务中展现出优于或持平PPO/GRPO/DPO的稳健学习性能。该框架通过显式利用全轨迹历史数据，在探索与稳定性间取得平衡，提供了稳定且可解释的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">历史韵脚：运用韵脚强化学习加速大型语言模型的训练</h2><a id="user-content-历史韵脚运用韵脚强化学习加速大型语言模型的训练" class="anchor" aria-label="Permalink: 历史韵脚：运用韵脚强化学习加速大型语言模型的训练" href="#历史韵脚运用韵脚强化学习加速大型语言模型的训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18588v1 公告类型：新研究<br>
摘要：随着大语言模型（LLM）的快速发展，强化学习（RL）已成为提升LLM推理能力的关键方法。与传统预训练方式不同，RL包含多个阶段： rollout（执行）、奖励和训练，需要多种类型的工作节点协同合作。然而，当前RL系统仍面临严重的GPU利用率不足问题，主要原因有二：（1）由于测试时扩展的需求，rollout阶段在整体RL过程中占据主导地位；（2）同一批次内rollout长度的不均衡导致GPU出现空闲气泡。虽然现有解决方案（如异步执行和截断处理）能部分缓解问题，但可能为了效率牺牲训练准确性。</p>
<p>我们的核心发现源于一个曾被忽视的现象：相邻训练周期间的rollout响应表现出高度相似性。基于此，我们提出RhymeRL——一个专为加速RL训练设计的LLM系统，其具备两大创新：首先，为优化rollout生成，我们开发了HistoSpec推测解码推理引擎，利用历史rollout令牌序列的相似性获取精确草案；其次，针对rollout气泡问题，我们引入HistoPipe双层调度策略，通过历史rollout分布的相似性实现rollout工作节点间的负载均衡。我们在真实生产环境中对RhymeRL进行评估，证明其可扩展性支持从数十到数千个GPU的部署。实验结果表明，RhymeRL在保持准确性且不改变RL范式的前提下，相比现有方法实现了2.6倍的性能提升。</p>
<div class="markdown-heading"><h2 class="heading-element">视觉安全增强型VPC：面向自主机器人手术的不确定性下带可见性约束的谨慎预测控制</h2><a id="user-content-视觉安全增强型vpc面向自主机器人手术的不确定性下带可见性约束的谨慎预测控制" class="anchor" aria-label="Permalink: 视觉安全增强型VPC：面向自主机器人手术的不确定性下带可见性约束的谨慎预测控制" href="#视觉安全增强型vpc面向自主机器人手术的不确定性下带可见性约束的谨慎预测控制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译时采用技术文献常用处理方式：</p>
<ol>
<li>保留专业缩写VPC保持术语一致性</li>
<li>"Visibility Constraints"译为"可见性约束"符合控制领域术语规范</li>
<li>"Cautious Predictive Control"译为"谨慎预测控制"准确传达算法特性</li>
<li>通过"面向..."的句式保持技术文档的严谨性</li>
<li>使用"不确定性下"替代"在不确定条件下"更符合中文技术表达习惯）</li>
</ol>
<p>arXiv:2508.18937v1 公告类型：新成果<br>
摘要：机器人辅助微创手术（MIS）中腹腔镜的自主控制因其提升手术安全性的潜力而备受关注。尽管基于像素级的图像视觉伺服（IBVS）控制取得进展，但持续可视性要求以及参数化误差、测量噪声、载荷不确定性等复杂干扰的存在，可能影响外科医生的视觉体验并危及手术安全。针对这些局限，本文提出VisionSafe增强型视觉预测控制（VPC）——一种鲁棒且能自适应不确定性的自主腹腔镜控制框架，可在不确定性条件下保障视野（FoV）安全。首先采用高斯过程回归（GPR）对操作不确定性（包括残余模型不确定性、随机不确定性和外部干扰）进行混合（确定性+随机性）量化。基于不确定性量化，提出具有概率保障的新型安全轨迹优化框架：通过不确定性传播推导出自适应安全控制屏障函数（CBF）条件，同时基于概率近似构建机会约束。这种不确定性感知机制可实现自适应控制力分配，在保持鲁棒性的同时最小化不必要的镜头运动。通过对比仿真和在商业手术机器人平台（微创医疗机器人图迈）上进行的多靶点淋巴结序贯清扫实验验证表明：相较于基线方法，该框架在保持近乎完美的目标可视性（&gt;99.9%）的同时，显著降低了跟踪误差...（译文截断处保留原文未完成句式）</p>
<div class="markdown-heading"><h2 class="heading-element">基于最小完整语义单元的多语言模型动态协作</h2><a id="user-content-基于最小完整语义单元的多语言模型动态协作" class="anchor" aria-label="Permalink: 基于最小完整语义单元的多语言模型动态协作" href="#基于最小完整语义单元的多语言模型动态协作"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18763v1 公告类型：新论文<br>
摘要：本文研究通过令牌级多模型协作增强语言模型的推理能力。我们提出从多个模型提供的下一令牌分布中选择最优令牌进行自回归推理。与"模型越多效果越好"的传统认知不同，我们引入基于分布距离的动态选择策略（DDS）来优化多模型协作过程。针对多模型协作中词汇表不对齐的关键挑战，我们提出最小完整语义单元（MCSU）的概念，这一简洁设计能使多个语言模型在语义空间中实现自然对齐。多个基准测试的实验结果证明了我们方法的优越性。代码将在<a href="https://github.com/Fanye12/DDS%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Fanye12/DDS开源。</a></p>
<div class="markdown-heading"><h2 class="heading-element">顺便提一下：一种用于多模态模型整合的非参数方差稳定化框架</h2><a id="user-content-顺便提一下一种用于多模态模型整合的非参数方差稳定化框架" class="anchor" aria-label="Permalink: 顺便提一下：一种用于多模态模型整合的非参数方差稳定化框架" href="#顺便提一下一种用于多模态模型整合的非参数方差稳定化框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18551v1 公告类型：新研究<br>
摘要：混合专家（MoE）模型通过实现跨模态的模块化 specialization，在多模态学习中日益强大。然而当额外模态引入的噪声超过互补信息时，其有效性仍不明确。现有方法（如部分信息分解）难以扩展到两个以上模态，且缺乏实例级控制所需的精度。我们提出超双模态加权（BTW）——一种结合实例级KL散度与模态级互信息的双层非参数加权框架，可在训练过程中动态调整模态重要性。该方法无需额外参数，适用于任意数量模态。具体而言，BTW通过测量每个单模态预测与当前多模态预测之间的散度来计算样本级KL权重，并通过估计单模态与多模态输出的全局对齐度来计算模态级互信息权重。在情感回归和临床分类上的大量实验表明，本方法显著提升了回归性能和多类分类准确率。</p>
<div class="markdown-heading"><h2 class="heading-element">FFT-MoE：基于异构边缘环境下大规模稀疏专家混合模型的高效联邦微调基础框架</h2><a id="user-content-fft-moe基于异构边缘环境下大规模稀疏专家混合模型的高效联邦微调基础框架" class="anchor" aria-label="Permalink: FFT-MoE：基于异构边缘环境下大规模稀疏专家混合模型的高效联邦微调基础框架" href="#fft-moe基于异构边缘环境下大规模稀疏专家混合模型的高效联邦微调基础框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：在保持技术术语准确性的前提下，采用符合中文科技文献习惯的四字格结构。"Foundation Models"译为"基础框架"以契合上下文，"Large-scale Sparse MoE"完整保留专业缩写并添加"专家混合模型"的明确释义，"Heterogeneous Edge"译为"异构边缘环境"体现边缘计算特性，整体句式采用中文常见的破折号标题形式实现技术概念的有效传递。）</p>
<p>arXiv:2508.18663v1 公告类型：新研究<br>
摘要：随着基础模型（FMs）推动人工智能向通用人工智能（AGI）迈进，在隐私和资源受限环境下对其进行微调变得尤为关键——尤其是当高质量训练数据分布于边缘设备时。联邦学习（FL）通过联邦微调（FFT）提供了创新解决方案，使得多方能够在不共享原始数据的情况下协同优化模型。现有方法采用参数高效微调（PEFT）技术（如低秩自适应LoRA）以降低计算开销，但基于LoRA的FFT在异构FL环境中面临两大局限：不同客户端间因LoRA配置差异导致的结构不兼容性，以及对非独立同分布数据分布的适应能力不足，从而影响收敛性与泛化性能。为此，我们提出FFT MoE框架，采用稀疏专家混合（MoE）适配器替代LoRA。每个客户端训练轻量级门控网络，选择性激活个性化专家子集，在适配本地资源预算的同时保持聚合兼容性。针对设备与数据异构导致的专家负载失衡问题，我们进一步引入异构感知辅助损失函数，动态调节路由分布以确保专家多样性与均衡使用。在独立同分布与非独立同分布场景下的大规模实验表明，FFT MoE在泛化性能和训练效率上持续优于现有最先进的FFT基线方法。</p>
<div class="markdown-heading"><h2 class="heading-element">MemoryVLA：用于机器人操作的视觉-语言-动作模型中的感知认知记忆</h2><a id="user-content-memoryvla用于机器人操作的视觉-语言-动作模型中的感知认知记忆" class="anchor" aria-label="Permalink: MemoryVLA：用于机器人操作的视觉-语言-动作模型中的感知认知记忆" href="#memoryvla用于机器人操作的视觉-语言-动作模型中的感知认知记忆"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.19236v1 公告类型：新研究<br>
摘要：时序上下文对于机器人操作至关重要，因为这类任务本质上是非马尔可夫过程，但主流视觉语言动作模型（VLA）通常忽略这一点，难以处理长时程、具有时序依赖的任务。认知科学表明，人类依赖工作记忆缓冲短暂的表征以实现即时控制，而海马体系统则保存过往经历的精确情景细节和语义要点以形成长期记忆。受此启发，我们提出MemoryVLA——一种面向长时程机器人操作的"认知-记忆-动作"框架。预训练的VLM将观测编码为构成工作记忆的感知与认知令牌，而感知-认知记忆库则存储从中整合的低层级细节和高层级语义。工作记忆从记忆库检索决策相关条目，将其与当前令牌自适应融合，并通过合并冗余信息更新记忆库。基于这些令牌，记忆条件扩散动作专家生成具有时序感知的动作序列。我们在3种机器人上完成150+项仿真与真实世界任务测试：在SimplerEnv-Bridge、Fractal和LIBERO-5测试集中分别达到71.9%、72.7%和96.5%的成功率，全面超越先进基线CogACT与pi-0，其中Bridge任务显著提升14.6个百分点；在涵盖通用技能与长时程时序依赖的12项真实任务中达成84.0%成功率，长时程任务较先进基线提升26个百分点。项目页面：<a href="https://shihao1895.github.io/MemoryVLA" rel="nofollow">https://shihao1895.github.io/MemoryVLA</a></p>
<p>（注：根据学术规范保留专业术语原称如VLA/VLM，对"verbatim episodic details"等认知科学概念采用"精确情景细节"的译法，保持技术指标数字精度，并通过"长时程"统一翻译long-horizon保持上下文一致性。）</p>
<div class="markdown-heading"><h2 class="heading-element">反射增强型元优化：融合TextGrad风格提示优化与记忆驱动的自我进化</h2><a id="user-content-反射增强型元优化融合textgrad风格提示优化与记忆驱动的自我进化" class="anchor" aria-label="Permalink: 反射增强型元优化：融合TextGrad风格提示优化与记忆驱动的自我进化" href="#反射增强型元优化融合textgrad风格提示优化与记忆驱动的自我进化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：译文采用技术文献常用翻译策略，保留核心术语"Meta-Optimization"译为"元优化"，"TextGrad"作为专有名词保留直译。"Reflection-Enhanced"译为"反射增强型"体现其自我反馈特性，"Memory-Driven Self-Evolution"采用"记忆驱动的自我进化"准确传达系统通过历史记忆实现迭代进化的核心机制，连字符"-"转换为中文更符合阅读习惯的冒号分隔结构。）</p>
<p>arXiv:2508.18749v1 公告类型：新研究<br>
摘要：以TextGrad为代表的提示优化技术最新进展，能够通过类梯度方式自动优化文本提示，从而提升大语言模型（LLMs）在特定下游任务中的表现。然而现有方法通常采用无状态设计，各轮优化相互独立，缺乏历史优化经验的保存与利用机制，且易出现过拟合现象，导致生成的提示更新方案在跨任务场景中泛化能力不足。</p>
<p>为解决这些局限性，我们提出反射增强元优化框架（REMO），该创新架构整合了：（1）采用"错误笔记"结构化设计的内存增强型反射检索生成模块（RAG）；（2）通过LLM驱动的元控制器实现的自适应优化器——该控制器能综合 epoch 级别的反思洞见，迭代改进系统级提示策略。该框架不仅支持类似TextGrad的局部细粒度提示调优，更能实现跨轮次优化知识的系统化积累与复用，从而支撑持续性能提升。</p>
<p>我们基于Qwen3-32B标准推理模式（未采用显式思维链提示）实例化REMO框架，并在GSM8K数学推理基准上评估其效能。实验结果表明：相较于TextGrad基线，REMO在付出更高计算开销的同时，实现了更稳定、更鲁棒的泛化性能。我们详细阐述了算法设计，对优化动态进行了定性与定量分析，并通过系统化的消融实验揭示了各组件的贡献度。</p>
<div class="markdown-heading"><h2 class="heading-element">C-Flat++：迈向更高效与强大的持续学习框架</h2><a id="user-content-c-flat迈向更高效与强大的持续学习框架" class="anchor" aria-label="Permalink: C-Flat++：迈向更高效与强大的持续学习框架" href="#c-flat迈向更高效与强大的持续学习框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18860v1 公告类型：新成果<br>
摘要：在持续学习（CL）中，平衡对新任务的敏感性与保持历史知识的稳定性至关重要。近年来，锐度感知最小化方法在迁移学习中成效显著，并被引入持续学习领域以提升记忆保持和学习效率。然而，在某些场景下仅依赖零阶锐度可能使优化过程倾向于选择尖锐极小值而非平坦极小值，导致解决方案鲁棒性不足且可能次优。本文提出<strong>持续平坦化方法（C-Flat）</strong>，通过促进形成更平坦的损失景观来适配持续学习需求。该方法具备即插即用特性，仅需对代码流程进行最小改动即可快速集成。此外，我们构建了将C-Flat整合至所有主流持续学习范式的通用框架，并与损失极小化优化器及基于平坦极小值的持续学习方法进行全面对比。实验结果表明，C-Flat在多种设定下均能持续提升性能。我们进一步提出C-Flat++框架，通过选择性平坦化驱动机制显著降低C-Flat所需的更新成本，在保持高效性的同时确保有效性。跨多个持续学习方法、数据集和场景的大规模实验验证了所提方法的效能与效率。代码已开源：<a href="https://github.com/WanNaa/C-Flat%E3%80%82">https://github.com/WanNaa/C-Flat。</a></p>
<div class="markdown-heading"><h2 class="heading-element">DualSparse-MoE：通过专家划分与重构协调张量/神经元级稀疏性</h2><a id="user-content-dualsparse-moe通过专家划分与重构协调张量神经元级稀疏性" class="anchor" aria-label="Permalink: DualSparse-MoE：通过专家划分与重构协调张量/神经元级稀疏性" href="#dualsparse-moe通过专家划分与重构协调张量神经元级稀疏性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：在保持技术术语准确性的前提下，采用"张量/神经元级"对应原文层级概念，"专家划分与重构"准确传达expert partition and reconstruction的操作内涵，冒号后的中文表述既符合学术翻译规范，又通过"协调"一词精准体现coordinating的动态协作关系）</p>
<p>arXiv:2508.18376v1 公告类型：新成果<br>
摘要：混合专家模型（MoE）通过减少单令牌计算量同时实现模型扩展，已成为构建大语言模型（LLM）的主流架构。其核心思想是将大型前馈网络（FFN）在张量层级细粒度地划分为多个子前馈网络（即专家），且每个输入仅激活稀疏的子集。虽然这种稀疏性提升了效率，但由于其巨大的计算规模与不可预测的激活模式，MoE仍面临重大挑战。</p>
<p>为实现高效MoE部署，我们发现预训练MoE模块中存在的张量级与神经元级双重稀疏性是影响精度与效率的关键因素。不同于先前研究通过在预训练阶段设计更细粒度专家来增加张量级稀疏性，我们引入训练后专家划分技术，无需重新训练即可诱导此类稀疏性。这种方法既保持了模型变换的数学一致性，又提升了后续微调与推理的效率和精度。基于此，我们提出DualSparse-MoE推理系统，通过动态张量级计算丢弃与静态神经元级重构相结合，以极小的精度损失实现显著效率提升。</p>
<p>实验结果表明：在三种主流MoE模型上，采用我们的方法实施约25%的计算丢弃率时，平均精度仅下降0.08%-0.28%，且几乎所有程度的计算丢弃均能带来成比例的计算加速。此外，将负载均衡感知机制融入专家并行策略后，MoE模块速度提升达1.41倍，而平均精度损失仅0.5%。</p>
<div class="markdown-heading"><h2 class="heading-element">预测即将出现的词序可提升语言建模效果</h2><a id="user-content-预测即将出现的词序可提升语言建模效果" class="anchor" aria-label="Permalink: 预测即将出现的词序可提升语言建模效果" href="#预测即将出现的词序可提升语言建模效果"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.19228v1 公告类型：新成果<br>
摘要：多令牌预测（MTP）曾被提出作为语言模型训练中下一令牌预测（NTP）的辅助目标，但改进效果不稳定，在标准自然语言处理基准测试中表现欠佳。我们认为MTP的精确未来令牌预测作为辅助损失函数过于困难。为此，我们提出令牌顺序预测（TOP），该方法通过排序学习损失训练模型根据邻近性对后续令牌进行排序。与MTP需要多个Transformer层相比，TOP仅需增加单个解嵌入层。我们使用NTP、MTP和TOP目标分别对3.4亿、18亿和70亿参数的模型进行预训练。在八个标准自然语言处理基准测试中，TOP在不同规模模型上均整体优于NTP和MTP。代码已开源：<a href="https://github.com/zaydzuhri/token-order-prediction">https://github.com/zaydzuhri/token-order-prediction</a></p>
<p>（注：根据学术规范保留专业术语英文缩写，同时确保技术细节的准确传达：</p>
<ol>
<li>"unembedding layer"译为"解嵌入层"以保持技术一致性</li>
<li>"learning-to-rank loss"采用通用译法"排序学习损失"</li>
<li>模型参数量级保留国际通用数字单位表述）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">线性成本互信息估计与性能类似于HSIC的独立性检验</h2><a id="user-content-线性成本互信息估计与性能类似于hsic的独立性检验" class="anchor" aria-label="Permalink: 线性成本互信息估计与性能类似于HSIC的独立性检验" href="#线性成本互信息估计与性能类似于hsic的独立性检验"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18338v1 公告类型：新研究<br>
摘要：评估两个数据样本间的统计依赖性是数据科学/机器学习的基础问题，而HSIC（希尔伯特-施密特信息准则）被视为当前最先进的方法。然而对于规模为n的数据样本，该方法需要计算n×n矩阵的乘积，目前其计算复杂度约为O(n^{2.37})，导致在大规模数据样本中难以实际应用。我们提出HCR（分层相关性重建）作为其线性计算成本的实用替代方案——该方案在测试中具有更高的依赖敏感性，还能通过特征（即混合矩，从相关性和同方差性开始）描述依赖性来构建真实的联合分布模型，并允许将互信息近似为两数据样本间此类非平凡混合矩的平方和。每个依赖描述特征的计算仅需O(n)线性时间。需要测试的特征数量随维度d变化：考虑两两依赖时需要O(d²)特征量，若需进一步检测更细微的三元依赖则需O(d³)特征量，依此类推。</p>
<div class="markdown-heading"><h2 class="heading-element">约束因素的重要性：多模态表示在减少混合整数线性规划中的应用</h2><a id="user-content-约束因素的重要性多模态表示在减少混合整数线性规划中的应用" class="anchor" aria-label="Permalink: 约束因素的重要性：多模态表示在减少混合整数线性规划中的应用" href="#约束因素的重要性多模态表示在减少混合整数线性规划中的应用"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18742v1 公告类型：新成果<br>
摘要：模型简化旨在通过学习原始混合整数线性规划（MILP）的更简洁模型，从而大幅加速大规模MILP问题的求解。现有模型简化方法多基于变量约简，即预测部分变量的解值。从对偶视角看，将部分不等式约束转化为等式约束同样能降低MILP复杂度，但这一思路长期被忽视。为此，本文提出一种创新的基于约束的MILP模型简化方法。该方法面临两大挑战：1）如何识别关键不等式约束，使其在保持可行性的同时加速求解；2）如何高效预测这些关键约束。为识别关键约束，我们首先将最优解处的紧约束标记为潜在关键约束，并设计启发式规则筛选关键紧约束子集。为学习关键紧约束，我们提出多模态表征技术，融合实例层级和抽象层级的MILP formulation信息。实验结果表明：相较最先进方法，本方法将解的质量提升超50%，计算时间降低17.47%。</p>
<div class="markdown-heading"><h2 class="heading-element">通过重要性驱动的专家调度实现边缘设备上的混合专家模型</h2><a id="user-content-通过重要性驱动的专家调度实现边缘设备上的混合专家模型" class="anchor" aria-label="Permalink: 通过重要性驱动的专家调度实现边缘设备上的混合专家模型" href="#通过重要性驱动的专家调度实现边缘设备上的混合专家模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18983v1 公告类型：新成果<br>
摘要：混合专家（MoE）架构通过仅激活每个查询所需的专家子集，已成为扩展大语言模型的关键技术。然而，在消费级边缘硬件上部署MoE受限于设备内存容量，这使得动态专家卸载技术至关重要。与先前仅将卸载视为调度问题的研究不同，我们利用专家重要性指导决策——用GPU内存中已缓存的功能相似专家替代低重要性激活专家，从而保持模型精度。这一设计显著降低了内存使用和数据传输量，同时基本消除了PCIe通信开销。此外，我们引入了最大化GPU缓存专家复用率的调度策略，进一步提升效率。大量实验表明，该方法在保持近乎无损精度的同时，解码延迟降低48%，专家缓存命中率超过60%。</p>
<div class="markdown-heading"><h2 class="heading-element">ZTFed-MAS2S：一种基于零信任联邦学习的风电数据填补框架，具备可验证隐私保护与信任感知聚合机制</h2><a id="user-content-ztfed-mas2s一种基于零信任联邦学习的风电数据填补框架具备可验证隐私保护与信任感知聚合机制" class="anchor" aria-label="Permalink: ZTFed-MAS2S：一种基于零信任联邦学习的风电数据填补框架，具备可验证隐私保护与信任感知聚合机制" href="#ztfed-mas2s一种基于零信任联邦学习的风电数据填补框架具备可验证隐私保护与信任感知聚合机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译在保持专业术语准确性的同时，采用符合中文技术文献表述习惯的四六字结构，将"Zero-Trust"译为"零信任"，"Federated Learning"译为"联邦学习"，"Verifiable Privacy"译为"可验证隐私保护"，"Trust-Aware Aggregation"译为"信任感知聚合"，并通过冒号与副标题形式呈现框架全称与特性说明，确保技术含义的完整传递。）</p>
<p>arXiv:2508.18318v1 公告类型：新研究<br>
摘要：风电数据常因传感器故障与边缘站点传输不稳定而存在缺失值。联邦学习虽能实现隐私保护下的数据协作（无需共享原始数据），但在参数交换过程中仍面临异常更新和隐私泄露的风险。这些挑战在开放的工业环境中尤为突出，需采用零信任机制——即不默认信任任何参与方。为此，本研究提出ZTFed-MAS2S框架，该零信任联邦学习框架整合了基于多头注意力的序列到序列填补模型。ZTFed通过可验证差分隐私与非交互式零知识证明相结合，配备保密性与完整性验证机制，确保可验证的隐私保护与安全的模型参数传输。采用动态信任感知聚合机制，基于相似性图谱传播信任值以增强鲁棒性，并通过稀疏化与量化压缩降低通信开销。MAS2S模型能够捕捉风电数据的长期依赖关系以实现精准填补。在真实风场数据集上的大量实验表明，ZTFed-MAS2S在联邦学习性能与缺失数据填补方面均具有优越性，为能源领域实际应用提供了安全高效的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">基于最近邻方法的人工智能发展新途径</h2><a id="user-content-基于最近邻方法的人工智能发展新途径" class="anchor" aria-label="Permalink: 基于最近邻方法的人工智能发展新途径" href="#基于最近邻方法的人工智能发展新途径"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18953v1 公告类型：新成果<br>
摘要：现代神经网络技术（包括大语言模型）虽在各类人工智能应用中获得显著成功，但仍面临一系列根本性局限。这些问题包括幻觉效应、训练与推理的高计算复杂度、昂贵的微调成本以及灾难性遗忘现象。这些限制严重阻碍了神经网络在医疗、工业流程管理和科学研究等关键领域的应用。本文提出一种基于层次聚类结构的最近邻替代方法。采用k近邻算法可显著减少或完全消除幻觉效应，同时简化模型扩展与微调过程，无需重新训练整个网络。为克服k近邻方法的高计算负载，论文提出采用基于科霍宁自组织映射的树状数据结构，从而极大加速最近邻搜索。在手写数字识别和简单字幕翻译任务上的测试验证了该方法的有效性：在精度仅轻微下降的前提下，最近邻搜索时间较穷举搜索方法缩短数百倍。该方法具有透明性和可解释性，与人类认知机制高度契合，在需要高可靠性与可解释结果的任务中展现出广泛应用潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">矩阵：多智能体仿真框架——面向安全交互与情境化临床对话评估</h2><a id="user-content-矩阵多智能体仿真框架面向安全交互与情境化临床对话评估" class="anchor" aria-label="Permalink: 矩阵：多智能体仿真框架——面向安全交互与情境化临床对话评估" href="#矩阵多智能体仿真框架面向安全交互与情境化临床对话评估"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.19163v1 公告类型：新成果<br>
摘要：尽管大语言模型（LLM）在临床对话系统中的使用日益增多，但现有评估主要关注任务完成度或流畅性，对安全关键系统至关重要的行为与风险管理需求缺乏深入洞察。本文提出MATRIX（面向安全交互与情境化临床对话评估的多智能体仿真框架），这是一个结构化、可扩展的框架，专用于临床对话代理的安全导向评估。</p>
<p>MATRIX整合三大核心组件：（1）通过结构化安全工程方法构建的临床场景安全对齐分类体系，涵盖预期系统行为与失效模式；（2）BehvJudge——基于LLM的评估器，用于检测安全相关对话故障，其检测结果经临床专家标注验证；（3）PatBot——能够生成多样化场景条件响应的模拟患者代理，其真实性与行为保真度通过人因工程专业知识及患者偏好研究进行评估。</p>
<p>通过三项实验，我们证明MATRIX可实现系统化、可扩展的安全评估。搭载Gemini 2.5-Pro的BehvJudge在240组对话盲测中达到专家级风险检测水平（F1值0.96，灵敏度0.999），表现超越临床医生。我们同时开展了首批基于LLM的患者模拟真实性分析，表明PatBot在定量与定性评估中能可靠模拟真实患者行为。利用MATRIX框架，我们在涵盖14类风险场景和10个临床领域的2,100组模拟对话中，成功对五种LLM代理进行了基准测试。</p>
<p>MATRIX是首个将结构化安全工程与可扩展、可验证的对话式AI评估相统一的框架，支持符合监管要求的安全审计。我们公开全部评估工具、提示词、结构化场景及数据集。</p>
<div class="markdown-heading"><h2 class="heading-element">pyFAST：一个基于PyTorch的模块化框架，用于处理多源稀疏数据的时间序列建模</h2><a id="user-content-pyfast一个基于pytorch的模块化框架用于处理多源稀疏数据的时间序列建模" class="anchor" aria-label="Permalink: pyFAST：一个基于PyTorch的模块化框架，用于处理多源稀疏数据的时间序列建模" href="#pyfast一个基于pytorch的模块化框架用于处理多源稀疏数据的时间序列建模"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18891v1 公告类型：新成果<br>
摘要：现代时间序列分析需要灵活、高效且可扩展的框架。然而，现有许多Python库在模块化以及对不规则、多源或稀疏数据的原生支持方面存在局限。我们推出pyFAST——一个面向研究的PyTorch框架，其显式地将数据处理与模型计算解耦，实现了更清晰的关注点分离并促进快速实验。该框架的数据引擎专为复杂场景设计，支持多源数据加载、蛋白质序列处理、高效的序列级与片段级填充、动态标准化，以及基于掩码的插补与预测建模。pyFAST集成了受大语言模型启发的架构，可实现稀疏数据源的无对齐融合，并提供原生稀疏度量指标、专用损失函数和灵活的外生数据融合机制。训练工具包含基于批流的聚合评估和设备协同机制，以最大化计算效率。框架采用模块化架构提供经典与深度学习模型（线性模型、CNN、RNN、Transformer、GNN）的完整套件，鼓励扩展开发。项目以MIT许可证发布于GitHub，为推进时间序列研究和应用提供了精简而强大的平台。</p>
<div class="markdown-heading"><h2 class="heading-element">STRATA-TS：基于检索引导推理的城市时间序列预测选择性知识迁移方法</h2><a id="user-content-strata-ts基于检索引导推理的城市时间序列预测选择性知识迁移方法" class="anchor" aria-label="Permalink: STRATA-TS：基于检索引导推理的城市时间序列预测选择性知识迁移方法" href="#strata-ts基于检索引导推理的城市时间序列预测选择性知识迁移方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译保留了技术术语的准确性，同时采用"选择性知识迁移"对应"Selective Knowledge Transfer"，"检索引导推理"对应"Retrieval-Guided Reasoning"，并通过添加"方法"二字使技术方案名称更符合中文表达习惯。）</p>
<p>arXiv:2508.18635v1 公告类型：新研究<br>
摘要：城市预测模型常面临严重的数据不平衡问题：仅有少数城市拥有密集的长周期记录，而多数城市仅存在短期或不完整的历史数据。直接从数据丰富城市向数据稀缺城市迁移的方法并不可靠，因为仅有有限部分的源模式能真正惠及目标域，而无差别迁移可能引入噪声并导致负迁移。我们提出STRATA-TS（基于目标感知检索的选择性时间序列迁移框架），该框架结合领域自适应检索与具备推理能力的大模型，以改进稀缺数据环境下的预测性能。STRATA-TS采用基于片段的时间编码器，识别与目标查询在语义和动态特征上对齐的源序列子集。这些检索到的样本随后被注入检索引导的推理阶段，由大语言模型对目标输入和检索到的支持信息进行结构化推断。为实现高效部署，我们通过监督微调将推理过程蒸馏至紧凑的开源模型中。在新加坡、诺丁汉和格拉斯哥的三个停车可用性数据集上进行大量实验表明，STRATA-TS始终优于强预测和迁移基线方法，同时提供可解释的知识迁移路径。</p>
<div class="markdown-heading"><h2 class="heading-element">APT-LLM：利用任意精度张量核心计算实现大语言模型加速</h2><a id="user-content-apt-llm利用任意精度张量核心计算实现大语言模型加速" class="anchor" aria-label="Permalink: APT-LLM：利用任意精度张量核心计算实现大语言模型加速" href="#apt-llm利用任意精度张量核心计算实现大语言模型加速"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：LLM在此语境中指Large Language Model，采用"大语言模型"这一通用译法；Tensor Core保持技术术语"张量核心"的译法；Arbitrary-Precision译为"任意精度"符合计算机科学术语规范）</p>
<p>arXiv:2508.19087v1 公告类型：新成果<br>
摘要：大语言模型（LLM）虽已彻底变革人工智能应用，但其巨大的计算需求严重制约了部署效率与实时性能。量化方法虽能降低计算成本，但在GPU上实现任意精度的超低位量化LLM仍面临挑战，主要源于GPU张量核心支持有限、内存管理效率低下以及内核优化灵活性不足。为此，我们提出面向任意精度LLM的全面加速方案APT-LLM。首先，我们提出新型双极整型数据格式（bipolar-INT），既可实现与有符号整型的高效无损转换，又更利于并行计算。同时开发了基于比特级矩阵解构与重组的矩阵乘法（MatMul）方法，在实现灵活精度的同时优化GPU张量核心利用率。此外，我们设计了以数据恢复为核心的内存管理系统，通过策略性使用高速共享内存显著提升内核执行速度并降低内存访问延迟。最后提出内核映射方法，能针对不同矩阵尺寸动态选择最优可配置超参数，确保在不同LLM架构和精度设置下均获得最佳性能。实验表明：在LLM推理中，APT-LLM相较于FP16基线最高实现3.99倍加速，在RTX 3090上较NVIDIA CUTLASS INT4加速方案提升2.16倍；在RTX 4090和H800平台上，相对FP16基准最高取得2.44倍加速，较CUTLASS整数基线实现1.65倍性能提升。</p>
<div class="markdown-heading"><h2 class="heading-element">UltraMemV2：内存网络扩展至1200亿参数，实现卓越长上下文学习能力</h2><a id="user-content-ultramemv2内存网络扩展至1200亿参数实现卓越长上下文学习能力" class="anchor" aria-label="Permalink: UltraMemV2：内存网络扩展至1200亿参数，实现卓越长上下文学习能力" href="#ultramemv2内存网络扩展至1200亿参数实现卓越长上下文学习能力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18756v1 公告类型：新成果<br>
摘要：虽然混合专家（MoE）模型通过仅激活参数子集实现了显著效率，但其推理过程中存在高内存访问成本的问题。内存层架构以极低的内存访问量提供了诱人替代方案，但先前如UltraMem等尝试仅达到2专家MoE模型性能，远未达到当前最先进的8专家配置水平。我们提出UltraMemV2——一种重新设计的内存层架构，成功弥合了这一性能差距。我们的方法引入五项关键改进：将内存层集成至每个Transformer模块、通过单线性投影简化值扩展、采用基于PEER的前馈网络值处理、实施原则性参数初始化，以及重新平衡内存与FFN的计算比例。经广泛评估，我们证明UltraMemV2在相同计算量和参数条件下达到与8专家MoE模型相当的性能，同时显著降低内存访问。值得注意的是，UltraMemV2在内存密集型任务上表现卓越：长上下文记忆任务提升1.6个百分点，多轮记忆任务提升6.2个百分点，上下文学习任务提升7.9个百分点。我们通过总参数量达1200亿、激活参数量达25亿的模型进行大规模验证，证实激活密度对性能的影响大于稀疏参数总量。本研究使内存层架构达到与最先进MoE模型相当的性能，为高效稀疏计算提供了极具竞争力的替代方案。</p>
<div class="markdown-heading"><h2 class="heading-element">推理任务中专家混合语言模型的最优稀疏性</h2><a id="user-content-推理任务中专家混合语言模型的最优稀疏性" class="anchor" aria-label="Permalink: 推理任务中专家混合语言模型的最优稀疏性" href="#推理任务中专家混合语言模型的最优稀疏性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18672v1  公告类型：新成果<br>
摘要：经验缩放定律一直推动着大语言模型（LLM）的发展，但每当模型架构或数据管道发生变化时，其系数就会发生偏移。混合专家（MoE）模型作为当前先进系统的标准配置，引入了现有密集模型前沿研究未涉及的新型稀疏维度。我们探究了MoE稀疏性如何影响两种不同的能力机制：记忆与推理。通过训练一系列MoE Transformer模型族，在保持计算预算不变的情况下系统性地调整总参数量、激活参数量和top-$k$路由机制。针对每个模型，我们记录了预训练损失、下游任务损失和任务准确率，从而将训练-测试泛化间隙与损失-准确率间隙分离。记忆类基准测试结果随总参数增加而单调提升，与训练损失变化趋势一致；相比之下，推理性能会出现饱和甚至衰退——尽管总参数和训练损失持续改善。当激活参数恒定时，单独调整top-$k$影响甚微，而学习率、初始化等经典超参数对泛化间隙的调节作用与稀疏性同向。无论是训练后的强化学习（GRPO）还是额外测试时计算资源，都无法挽救过度稀疏模型的推理缺陷。我们的模型检查点、代码及实验日志已开源：<a href="https://github.com/rioyokotalab/optimal-sparsity%E3%80%82">https://github.com/rioyokotalab/optimal-sparsity。</a></p>
<div class="markdown-heading"><h2 class="heading-element">通过经验驱动的终身学习构建自我进化智能体：框架与基准</h2><a id="user-content-通过经验驱动的终身学习构建自我进化智能体框架与基准" class="anchor" aria-label="Permalink: 通过经验驱动的终身学习构建自我进化智能体：框架与基准" href="#通过经验驱动的终身学习构建自我进化智能体框架与基准"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.19005v1 公告类型：新成果<br>
摘要：随着人工智能向通用智能迈进，研究重点正从针对静态任务优化的系统转向创建能够持续学习的开放式智能体。本文提出经验驱动的终身学习框架（ELL），通过构建能够与现实世界互动实现持续进化的自演进智能体。该框架基于四个核心原则：（1）经验探索：智能体通过与动态环境持续自主的交互进行学习，处理相互关联的任务并生成丰富的经验轨迹；（2）长期记忆：智能体将历史知识（包括个人经历、领域专长和常识推理）结构化存储于持久记忆系统中；（3）技能学习：智能体通过从经验中抽象出可复用的模式来自主提升能力，将这些模式提炼为可主动优化并适用于新任务的技能；（4）知识内化：智能体将显性离散的经验转化为隐性直觉的"第二天性"能力。</p>
<p>我们同时推出ELL基准数据集StuLife，模拟学生从入学到学术与个人发展的完整大学历程，包含三个核心阶段和十个精细子场景。该数据集围绕三大范式转变设计：从被动到主动、从情境到记忆、从模仿到学习。在这个动态环境中，智能体必须获取并提炼实践技能，维持持久记忆，并根据演进的状态变量做出决策。StuLife为评估终身学习能力（包括记忆保持、技能迁移和自主行为）提供综合平台。除了在StuLife基准上评估现有最先进大语言模型外，我们还探讨了情境工程在推进通用人工智能发展中的作用。</p>
<div class="markdown-heading"><h2 class="heading-element">利用机器人系统实现维护自动化的高效任务与路径规划</h2><a id="user-content-利用机器人系统实现维护自动化的高效任务与路径规划" class="anchor" aria-label="Permalink: 利用机器人系统实现维护自动化的高效任务与路径规划" href="#利用机器人系统实现维护自动化的高效任务与路径规划"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.18400v1 公告类型：新成果<br>
摘要：智能自动化解决方案的研发是未来工厂发展的突破点。其中，利用自主机器人系统实现维护领域的任务自动化是一项前景广阔且极具挑战性的使命。为此，机器人系统需具备自主规划不同操作任务及对应路径的能力。核心要求在于开发计算复杂度低的算法，并具备应对环境不确定性的能力。本研究提出一种特别适用于维护自动化问题的解决方案：通过概率滤波器将CAD离线数据与RGBD视觉系统的在线数据融合，以补偿离线数据的不确定性。在任务规划方面，阐述了一种基于符号描述的方法，该方法建立在新型抽样计算拆解空间的算法基础上。路径规划则采用全球顶尖算法，配合可调整探索步长的优化方法以缩短规划时间。各项方法均通过实验验证并展开讨论。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>