<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">第三步：规模大且经济实惠——面向高性价比解码的模型系统协同设计</h2><a id="user-content-第三步规模大且经济实惠面向高性价比解码的模型系统协同设计" class="anchor" aria-label="Permalink: 第三步：规模大且经济实惠——面向高性价比解码的模型系统协同设计" href="#第三步规模大且经济实惠面向高性价比解码的模型系统协同设计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.19427v1 公告类型：新研究<br>
摘要：大语言模型（LLM）在解码过程中面临硬件效率低下的问题，尤其在长上下文推理任务中。本文提出Step-3模型，这是一个通过硬件感知的模型-系统协同设计优化的3210亿参数视觉语言模型（VLM），旨在最小化解码成本。Step-3在两大核心维度实现创新：（1）新型多矩阵分解注意力机制（MFA），在保持高注意力表达能力的同时，显著减少键值缓存（KV Cache）大小和计算量；（2）注意力-前馈网络解耦系统（AFD），将注意力层与前馈网络层（FFN）分离为专用子系统进行分布式推理。该协同设计实现了前所未有的成本效益：相比DeepSeek-V3和Qwen3 MoE 235B等模型，Step-3在理论解码成本上显著降低，且上下文越长优势越明显。Step-3在每令牌激活380亿参数（超过DeepSeek-V3和Qwen3 MoE 235B）的情况下仍保持低成本，证明硬件对齐的注意力计算强度、混合专家（MoE）稀疏性和AFD系统对成本效益至关重要。我们在DeepSeek-V3的优势场景中与其进行直接对比。基于Hopper GPU的实现显示，在50毫秒每令牌服务等级协议（4K上下文、FP8精度、无多令牌并行）下，单GPU解码吞吐量高达4,039令牌/秒，优于同等条件下DeepSeek-V3的2,324令牌/秒，为LLM解码树立了新的帕累托前沿。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"KV cache"译为"键值缓存"（行业通用译法）</li>
<li>"TPOT SLA"译为"每令牌服务等级协议"（结合Time-Per-Token和SLA的意译）</li>
<li>"Pareto frontier"保留经济学经典译法"帕累托前沿"</li>
<li>模型参数规模"321B"译为"3210亿"符合中文计数习惯）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">知识嫁接：一种在资源受限环境中优化AI模型部署的机制</h2><a id="user-content-知识嫁接一种在资源受限环境中优化ai模型部署的机制" class="anchor" aria-label="Permalink: 知识嫁接：一种在资源受限环境中优化AI模型部署的机制" href="#知识嫁接一种在资源受限环境中优化ai模型部署的机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.19261v1 公告类型：新研究<br>
摘要：随着人工智能（AI）应用的日益普及，模型规模不断扩大、复杂度持续攀升，海量参数需要强大的计算力支撑——而这正是许多现实应用场景所缺乏的。本文提出"知识嫁接"这一创新机制，通过将大型供体模型中的精选特征（接穗）移植到小型砧木模型上，实现了资源受限环境下的AI模型优化。该方法使模型体积缩减88.54%（从64.39 MB降至7.38 MB），同时提升了模型的泛化能力。新培育的砧木模型验证准确率达89.97%（供体模型为87.47%），保持更低的验证损失（0.2976 vs 0.5068），在未见测试数据上表现尤为出色，准确率高达90.45%。这一突破性进展成功解决了模型规模与性能的经典权衡问题，使得增强性能的AI框架能够部署在资源受限设备上。虽然我们以农业杂草检测场景进行验证，但该方法可扩展至各类边缘计算场景——正如园艺嫁接技术能在恶劣农业环境中实现高效栽培那样，这种机制有望在软硬件支持有限的领域加速AI技术的落地应用。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语处理："knowledge grafting"译为"知识嫁接"，保留园艺学隐喻；"scion/rootstock"对应"接穗/砧木"的植物学术语</li>
<li>技术概念转化：将"edge computing scenarios"意译为"边缘计算场景"而非字面直译</li>
<li>数据呈现：精确保持所有百分比和数值的原始格式</li>
<li>长句拆分：将原文复合句重组为符合中文表达习惯的短句结构</li>
<li>修辞对应："mirroring..."的隐喻通过"正如...那样"的句式自然呈现</li>
<li>学术风格：使用"泛化能力""权衡问题"等符合计算机领域论文摘要的规范表述）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>