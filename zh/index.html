<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">VIFO：通过跨模式融合实现视觉特征赋予的多元时间序列预测</h2><a id="user-content-vifo通过跨模式融合实现视觉特征赋予的多元时间序列预测" class="anchor" aria-label="Permalink: VIFO：通过跨模式融合实现视觉特征赋予的多元时间序列预测" href="#vifo通过跨模式融合实现视觉特征赋予的多元时间序列预测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03244v1宣布类型：新摘要：大型时间序列基础模型通常采用与渠道无关的架构来处理不同的数据维度，但这种设计忽略了关键的跨渠道依赖性。与此同时，现有的多模式方法尚未充分利用大视觉模型（LCC）的能力来解释时空数据。此外，在利用不同模式的信息提取优势来提高时间序列预测性能方面，仍然存在巨大的未开发潜力。为了解决这些差距，我们提出了VIFO，这是一种跨模式预测模型。VIFO独特地将多元时间序列渲染为图像，使预训练的LCC能够提取复杂的跨渠道模式，而这些模式对于渠道无关的模型来说是不可见的。然后，这些视觉特征与时间序列形态的表示对齐并融合。通过冻结LCC并仅训练其7.45%的参数，VIFO在多个基准测试上实现了有竞争力的性能，为捕获跨变量关系提供了高效且有效的解决方案</p>
<div class="markdown-heading"><h2 class="heading-element">大型系统的小型语言模型：体系结构、能力和部署权衡的调查</h2><a id="user-content-大型系统的小型语言模型体系结构能力和部署权衡的调查" class="anchor" aria-label="Permalink: 大型系统的小型语言模型：体系结构、能力和部署权衡的调查" href="#大型系统的小型语言模型体系结构能力和部署权衡的调查"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03847v1宣布类型：新摘要：小型语言模型（SLC; 1- 12 B参数，有时高达20 B）对于目标是模式和API约束的准确性而不是开放式生成的代理工作负载来说已经足够了，并且通常更好。我们综合了开放和专有的STM（Phi-4-Mini、Qwen-2.5- 7 B、Gemma-2- 9 B、Llama-3.2-1B/3B、Ministral-3B/8B、Apple设备上3B、DeepSeek-R1-Distill）的最新证据，并将其连接到现代评估（BFCL v3/v4、StableTools Bench）和服务栈（vLLM、SGLang、TensorRT-LLM）与引导解码库（XGrammar、Outlines）配对。我们通过不确定性感知路由和验证器级联形式化了SLS默认、LLM后备系统，并提出了反映实际生产目标的工程指标：每成功任务的成本（CPS）、模式有效性、可执行调用率、p50/p95延迟和每次请求的能量。引导解码、严格的SON模式输出和验证者优先的工具执行缩小了与大型模型的大部分能力差距，并经常让SLC在工具使用、函数调用和RAG方面与LLM相匹配或超越LLM，代币成本低10 - 100倍，延迟和能源大幅改善。我们为优先考虑CRM的代理栈提供设计模式：模式优先提示、类型安全功能注册表、使用验证者汇总进行置信度评分以及通过LoRA/QLoRA进行轻量级调整。我们还划定了后备仍然有价值的限制（开放领域推理和一些长期规划）。其结果是一个实用的蓝图，可以构建快速、廉价且可靠的代理，这些代理默认为SLS，同时通过有针对性的LLM协助保留裕度。   关键词：小语言模型、代理、函数调用、结构化输出、SON模式、引导解码、LoRA/QLoRA、路由、能源效率、边缘推断</p>
<div class="markdown-heading"><h2 class="heading-element">OneFlow：具有编辑流的并发混合模式和交织生成</h2><a id="user-content-oneflow具有编辑流的并发混合模式和交织生成" class="anchor" aria-label="Permalink: OneFlow：具有编辑流的并发混合模式和交织生成" href="#oneflow具有编辑流的并发混合模式和交织生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03506v1宣布类型：新摘要：我们介绍了OneFlow，这是第一个非自回归多模式模型，可以实现变长和并发混合模式生成。与在文本和图像生成之间强制执行严格因果顺序的自回归模型不同，OneFlow将针对离散文本标记的基于插入的编辑流与针对图像潜伏的流匹配相结合。OneFlow通过分层采样支持并发文本图像合成，分层采样将内容优先于语法。通过模型大小从1B到8B的受控实验，我们证明OneFlow在生成和理解任务方面都优于自回归基线，同时使用的训练FLOP减少了多达50%。OneFlow超越了自回归和基于扩散的方法，同时释放了并发生成、迭代细化和类自然推理生成的新功能。</p>
<div class="markdown-heading"><h2 class="heading-element">弥合多模式基础模型与世界模型之间的差距</h2><a id="user-content-弥合多模式基础模型与世界模型之间的差距" class="anchor" aria-label="Permalink: 弥合多模式基础模型与世界模型之间的差距" href="#弥合多模式基础模型与世界模型之间的差距"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03727v1宣布类型：新摘要：人类通过多种感官模式的整合来理解世界，使他们能够感知、推理和想象动态物理过程。受这种能力的启发，多模式基础模型（MFM）已成为多模式理解和生成的强大工具。然而，今天的MFM无法成为有效的世界模式。他们缺乏执行反事实推理、模拟动态、理解时空信息、控制生成的视觉结果以及执行多方面推理等基本能力。我们研究如何弥合多模式基础模型和世界模型之间的差距。我们首先通过区分任务来提高MFM的推理能力，并为MFM配备结构化推理技能，例如因果推理、反事实思维和时空推理，使它们能够超越表面相关性并理解视觉和文本数据中更深层次的关系。接下来，我们探索图像和视频模式中多模式基础模型的生成能力，引入结构化和可控生成的新框架。我们的方法结合了场景图、多峰条件处理和多峰对齐策略来指导生成过程，确保与高级语义和细粒度用户意图的一致性。我们进一步将这些技术扩展到可控制的4D生成，从而实现随着时间和空间的交互式、可编辑和可变形的对象合成。</p>
<div class="markdown-heading"><h2 class="heading-element">LMM-Incentive：Web 3.0中针对用户生成内容的大型基于多模式模型的激励设计</h2><a id="user-content-lmm-incentiveweb-30中针对用户生成内容的大型基于多模式模型的激励设计" class="anchor" aria-label="Permalink: LMM-Incentive：Web 3.0中针对用户生成内容的大型基于多模式模型的激励设计" href="#lmm-incentiveweb-30中针对用户生成内容的大型基于多模式模型的激励设计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04765v1宣布类型：新摘要：Web 3.0代表了下一代互联网，它被广泛认为是一个专注于价值表达和数据所有权的去中心化生态系统。通过利用区块链和人工智能技术，Web 3.0为用户提供了前所未有的机会来创建、拥有和货币化其内容，从而将用户生成内容（UGC）提升到了一个全新的水平。然而，一些自私的用户可能会利用内容策展机制的局限性，以较少的努力生成低质量的内容，从而在信息不对称的情况下获得平台奖励。此类行为可能会损害Web 3.0的性能。为此，我们提出了\textit{LMM-Incentive}，这是Web 3.0中UGC的一种新型基于大型多模式模型（LMM）的激励机制。具体来说，我们提出了一个基于LMM的契约理论模型来激励用户生成高质量的UGC，从而缓解信息不对称带来的反向选择问题。为了减轻合同选择后潜在的道德风险，我们利用LMM代理来评估UGC质量（这是合同的主要组成部分），利用及时的工程技术来提高LMM代理的评估性能。认识到传统的合同设计方法无法有效适应Web 3.0的动态环境，我们开发了一种改进的基于专家混合（MoE）的近端策略优化（PPO）算法，用于最优合同设计。仿真结果表明，所提出的基于MoE的PPO算法在合同设计的背景下，具有代表性的基准的优越性。最后，我们在以太坊智能合约框架中部署了设计的合约，进一步验证了所提出方案的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">ChelBench：了解扩散LLM中并行解码的权衡</h2><a id="user-content-chelbench了解扩散llm中并行解码的权衡" class="anchor" aria-label="Permalink: ChelBench：了解扩散LLM中并行解码的权衡" href="#chelbench了解扩散llm中并行解码的权衡"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04767v1宣布类型：新摘要：虽然大多数自回归LLM被限制于一一解码，但扩散LLM（dLLM）因其通过并行解码大幅加速推理的潜力而引起了越来越多的兴趣。尽管有这样的承诺，dLLM中的条件独立性假设导致并行解码忽略令牌依赖性，当这些依赖性很强时，不可避免地会降低生成质量。然而，现有的作品在很大程度上忽视了这些固有的挑战，以及对标准基准的评估（例如，数学和编码）不足以捕捉并行解码造成的质量下降。为了解决这一差距，我们首先提供了并行解码的信息论分析。然后，我们从数据分布和解码策略的角度对分析上易于处理的合成列表操作进行案例研究，提供量化见解，强调并行解码的基本局限性。在这些见解的基础上，我们提出了专门为dLLM设计的第一个基准测试，它具有对人类和自回归LLM来说微不足道的现实任务，但对于并行解码下的dLLM来说却极具挑战性。我们使用ObjectBench系统地分析了dLLM和自回归LLM，发现：（i）并行解码下的dLLM在现实世界中可能会遭受严重的质量下降，（ii）当前的并行解码策略难以根据任务难度调整其并行度，因此无法在不影响质量的情况下实现有意义的加速。我们的研究结果强调了迫切需要创新的解码方法，可以克服目前的速度质量权衡。我们发布了我们的基准测试，以帮助加速真正高效的DLLM的开发。</p>
<div class="markdown-heading"><h2 class="heading-element">了解时间序列的变形金刚：等级结构、等级流和可压缩性</h2><a id="user-content-了解时间序列的变形金刚等级结构等级流和可压缩性" class="anchor" aria-label="Permalink: 了解时间序列的变形金刚：等级结构、等级流和可压缩性" href="#了解时间序列的变形金刚等级结构等级流和可压缩性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03358v1宣布类型：新摘要：变形器广泛应用于数据模式，但从文本模型中提取的原则通常不完美地转移到训练到其他模式的模型。本文从等级结构的角度分析《变形金刚》。我们的重点是时间序列设置，其中数据的结构属性与文本或视觉的结构属性显着不同。我们表明，与文本或视觉不同，时间序列嵌入表现出急剧衰减的奇异值谱：小补丁大小和光滑连续映射将数据集中到低阶子空间中。由此，我们证明了相关的$Q/K/V$投影允许准确的低阶逼近，并且注意力层变得与嵌入谱的衰减成比例可压缩。我们引入了行列流的概念，这是一种跨深度非线性混合膨胀行列的现象，解释了为什么早期层最容易压缩以及为什么行列随着深度而增长。在这些理论和实证结果的指导下，我们利用这些见解压缩了一个大型时间序列基础模型Chronos，在不损失精度的情况下，实现了推理时间减少65美元，内存减少81美元.我们的研究结果提供了原则性的指导分配宽度，深度和头在时间序列的基础模型，并利用其固有的压缩性。</p>
<div class="markdown-heading"><h2 class="heading-element">PolyKAN：可证明且最小KAN压缩的多边形分析框架</h2><a id="user-content-polykan可证明且最小kan压缩的多边形分析框架" class="anchor" aria-label="Permalink: PolyKAN：可证明且最小KAN压缩的多边形分析框架" href="#polykan可证明且最小kan压缩的多边形分析框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04205v1宣布类型：新摘要：Kolmogorov-Arnold网络（KAN）已成为传统多层感知器（MLPs）的一种有前途的替代方案，提供增强的可解释性和强大的数学基础。然而，它们的参数效率仍然是实际部署的重大挑战。本文介绍了PolyKAN，这是KAN压缩的一种新颖理论框架，它为模型大小减小和逼近误差提供了形式保证。通过利用KAN固有的分段多项结构，我们将压缩问题表述为最佳多边形区域合并问题之一。我们建立了KAN的严格多边形特征，开发了完整的$$-等效压缩理论，并设计了一种最佳动态规划算法，以保证在指定误差范围下最小的压缩。我们的理论分析表明，PolyKAN在保持严格的错误控制的同时实现了可证明的最小压缩，并且所有网络参数都具有多项时间复杂性。该框架为具有数学保证的KAN压缩提供了第一个正式基础，为可解释神经架构的高效部署开辟了新的方向。</p>
<div class="markdown-heading"><h2 class="heading-element">KVComm：通过选择性KN共享实现高效的LLM通信</h2><a id="user-content-kvcomm通过选择性kn共享实现高效的llm通信" class="anchor" aria-label="Permalink: KVComm：通过选择性KN共享实现高效的LLM通信" href="#kvcomm通过选择性kn共享实现高效的llm通信"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03346v1宣布类型：新摘要：大型语言模型（LLM）越来越多地部署在多代理系统中，其中有效的模型间通信至关重要。现有的通信协议要么依赖于自然语言，导致高推理成本和信息丢失，要么依赖于隐藏状态，导致信息集中偏差和低效率。为了解决这些限制，我们提出了KVComm，这是一种新颖的通信框架，通过选择性共享KV对来实现LLM之间的高效通信。KVComm利用了KV对中编码的丰富信息，同时避免隐藏状态的陷阱。我们引入了一种基于注意力重要性分数和高斯先验的KV分层选择策略，以识别信息最丰富的KV对进行通信。跨不同任务和模型对的广泛实验表明，KVComm实现了与上限方法相当的性能，上限方法将输入直接合并到一个模型，而无需任何通信，同时传输少至30%的层的KV对。我们的研究强调了KV对作为LLM间通信的有效媒介的潜力，为可扩展和高效的多智能体系统铺平了道路。</p>
<div class="markdown-heading"><h2 class="heading-element">扩散Transformer中总是存在罕见的文本语义</h2><a id="user-content-扩散transformer中总是存在罕见的文本语义" class="anchor" aria-label="Permalink: 扩散Transformer中总是存在罕见的文本语义" href="#扩散transformer中总是存在罕见的文本语义"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03886v1宣布类型：新摘要：多模式扩散变形器（MM-DiTS）从基于流和扩散的变形器开始，重塑了文本到视觉的生成，以卓越的视觉保真度而赢得赞誉。随着这些模型的进步，用户不断地用富有想象力或罕见的提示突破边界，而高级模型在生成这些提示时仍然犹豫不决，因为它们的概念往往太稀缺，无法在预训练期间留下强烈的印记。在本文中，我们提出了一种简单而有效的干预，在没有额外的训练步骤、数据、去噪时间优化或依赖外部模块（例如，大型语言模型）。特别是，MM-DiT固有的联合注意机制会在整个Transformer块中顺序更新文本嵌入和图像嵌入。我们发现，通过在联合注意块之前通过方差放大在数学上扩展文本标记嵌入周围的表示盆地，MM-DiT的输出中明显出现了罕见的语义。此外，我们的结果有效地概括了文本到视觉任务，包括文本到图像、文本到视频和文本驱动图像编辑。我们的工作邀请生成模型揭示用户想要的语义，一旦隐藏，即可浮出水面。</p>
<div class="markdown-heading"><h2 class="heading-element">无数据知识提炼的条件伪监督对比</h2><a id="user-content-无数据知识提炼的条件伪监督对比" class="anchor" aria-label="Permalink: 无数据知识提炼的条件伪监督对比" href="#无数据知识提炼的条件伪监督对比"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03375v1宣布类型：新摘要：无数据知识蒸馏（DFKD）是解决模型压缩和传输限制同时保留隐私保护的有效方法，近年来引起了广泛关注。目前，大多数现有方法都利用生成器来合成图像以支持蒸馏。尽管目前的方法取得了巨大成功，但仍有许多问题需要探索。首先，监督学习在深度学习中的出色表现促使我们探索DFKD上的伪监督范式。其次，目前的合成方法无法区分不同类别样本的分布，从而产生模糊的样本，可能导致教师的错误评价。此外，当前的方法无法优化类别多样性样本，这将阻碍学生模型从不同样本中学习并进一步实现更好的性能。在本文中，为了解决上述局限性，我们提出了一种新颖的学习范式，即无数据知识蒸馏的条件伪监督对比~（CPSC-DFKD）。CPSC-DFKD的主要创新是：（1）引入条件生成对抗网络来合成特定类别的多样化图像以进行伪监督学习，（2）改进生成器的模块以区分不同类别的分布，（3）提出基于教师和学生观点的伪监督对比学习以增强多样性。对三个常用数据集的综合实验验证了CPSC-DFKD带来的学生和生成器的性能提升。该代码可在<a href="https://github.com/RoryShao/CPSC-DFKD.git%E4%B8%8A%E8%8E%B7%E5%8F%96">https://github.com/RoryShao/CPSC-DFKD.git上获取</a></p>
<div class="markdown-heading"><h2 class="heading-element">人工智能认知考试：多模式评估从识别到推理的演变概览</h2><a id="user-content-人工智能认知考试多模式评估从识别到推理的演变概览" class="anchor" aria-label="Permalink: 人工智能认知考试：多模式评估从识别到推理的演变概览" href="#人工智能认知考试多模式评估从识别到推理的演变概览"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04141v1宣布类型：新摘要：这篇调查论文记录了多模式人工智能（AI）评估的演变，将其框架为日益复杂的“认知检查”的进展。“我们认为，该领域正在经历范式转变，从测试模型看到“什么”的简单识别任务转向探索“为什么”和“如何”理解的复杂推理基准。这种演变是由旧基准的饱和驱动的，其中高性能往往掩盖了根本性的弱点。我们绘制了从ImageNet时代的基础“知识测试”到GQA和视觉常识推理（DVR）等“应用逻辑和理解”考试的旅程，这些考试专门用于诊断系统性缺陷，例如捷径学习和组合概括失败。然后，我们调查“专家级集成”基准的当前前沿（例如，MMBunch、SEED-Bench、MMMU）专为当今强大的多模式大型语言模型（MLLM）设计，这些模型越来越多地评估推理过程本身。最后，我们探索了评估抽象、创造性和社交智力的未知领域。我们得出的结论是，人工智能评估的叙述不仅仅是数据集的历史，而是一个设计更好检查的持续、对抗的过程，这反过来又重新定义了我们创建真正智能系统的目标。</p>
<div class="markdown-heading"><h2 class="heading-element">扩展神经元，而不是参数</h2><a id="user-content-扩展神经元而不是参数" class="anchor" aria-label="Permalink: 扩展神经元，而不是参数" href="#扩展神经元而不是参数"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04500v1宣布类型：新摘要：这项工作展示了增加网络中神经元数量而不增加非零参数数量如何提高性能。我们表明，这种增加与共享相同神经元的多个特征之间干扰的减少相对应。为了减少固定非零参数计数下的此类纠缠，我们引入了固定参数扩展（FPE）：用多个孩子替换神经元，并在它们之间分开划分父母的权重，以便每个孩子继承一个非重叠的连接子集。对于符号任务，特别是布尔代码问题，条款对齐的FPE系统性地减少了多义性指标并产生更高的任务准确性。值得注意的是，神经元权重的随机分割接近这些收益，这表明减少的碰撞而不是精确的分配才是主要驱动因素。与叠加假设一致，FPE的好处随着干扰的增加而增加：当多义性负载很高时，准确性改进最大。将这些见解转移到真实模型（CLIP嵌入和更深层次的多层网络上的分类器），我们发现扩大网络的同时保持恒定的非零参数计数可以持续提高准确性。这些结果确定了一种基于可解释性的机制，可以利用宽度来对抗叠加，从而在不增加非零参数数量的情况下提高性能。这个方向与现代加速器非常匹配，现代加速器的主要瓶颈是非零参数的内存移动，而不是原始计算。</p>
<div class="markdown-heading"><h2 class="heading-element">低精度Transformer培训为何失败：闪光注意力分析</h2><a id="user-content-低精度transformer培训为何失败闪光注意力分析" class="anchor" aria-label="Permalink: 低精度Transformer培训为何失败：闪光注意力分析" href="#低精度transformer培训为何失败闪光注意力分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04212v1宣布类型：新摘要：对计算效率的追求推动了低精度格式的采用来训练Transformer模型。然而，这种进步往往受到臭名昭著的训练不稳定性的阻碍。本文为一个长期存在且未解决的失败案例提供了第一个机械解释，即在低精度设置下进行闪光训练会导致灾难性的损失爆炸。我们的深入分析表明，失败不是随机因素，而是由两种相互交织的现象引起的：注意力机制中类似的低等级表示的出现以及低精度算术固有的偏四舍五入误差的复合效应。我们展示了这些因素如何造成错误积累的恶性循环，从而破坏权重更新，最终使训练动态脱轨。为了验证我们的发现，我们对闪光注意力进行了最小限度的修改，以减轻四舍五入误差中的偏差。这个简单的改变稳定了培训过程，证实了我们的分析，并为这个持续存在的问题提供了实用的解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">通过高级调整策略优化微调以实现低等级适应</h2><a id="user-content-通过高级调整策略优化微调以实现低等级适应" class="anchor" aria-label="Permalink: 通过高级调整策略优化微调以实现低等级适应" href="#通过高级调整策略优化微调以实现低等级适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03731v1宣布类型：新摘要：参数高效的微调方法的快速发展显着提高了适应大型语言模型的效率。其中，LoRA因其有效性和参数效率的强平衡而受到广泛欢迎。然而，LoRA依赖于初始化两个积为零的低阶矩阵，这限制了其有效激活和利用原始模型权重的能力，从而为最佳性能创造了潜在的瓶颈。为了解决这一限制，我们提出了\textBF{ðLoRA}，这是一种新颖的初始化策略，可以初始化低阶矩阵以接近原始模型权重。实验结果表明，在一系列模型和任务中，SYS LoRA比LoRA实现了更好的性能。此外，我们还引入了两个变体，即SYS LoRA-$\Alpha$和SYS LoRA-$\Beta$，两者都利用不同的初始化方法来进一步增强性能。</p>
<div class="markdown-heading"><h2 class="heading-element">HyperVLA：基于超网络的视觉-语言-动作模型的高效推理</h2><a id="user-content-hypervla基于超网络的视觉-语言-动作模型的高效推理" class="anchor" aria-label="Permalink: HyperVLA：基于超网络的视觉-语言-动作模型的高效推理" href="#hypervla基于超网络的视觉-语言-动作模型的高效推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04898v1宣布类型：新摘要：视觉-语言-动作（VLA）模型建立在具有强大概括能力的语言和视觉基础模型之上，并在大规模机器人数据上进行训练，最近成为学习通才机器人政策的一种有前途的方法。然而，现有VGA的一个主要缺点是其极高的推断成本。在本文中，我们提出HyperVLA来解决这个问题。与现有的在训练和推理期间激活整个模型的单块VGA不同，HyperVLA使用一种新型的基于超网络（HN）的架构，该架构在推理期间仅激活小的特定于任务的策略，同时仍然保留了适应训练期间多样化多任务行为所需的高模型容量。成功训练基于HN的VLA并非易事，因此HyperVLA包含几个关键的算法设计功能可以提高其性能，包括正确利用现有视觉基础模型的先验知识、HN规范化和动作生成策略。与单片VGA相比，HyperVLA在零激发概括和少激发自适应方面都实现了类似甚至更高的成功率，同时显着降低了推理成本。与OpenVLA（最先进的VLA模型）相比，HyperVLA将测试时激活的参数数量减少了90美元，并将推理速度加快了120美元。代码可在<a href="https://github.com/MasterXiong/HyperVLA%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96">https://github.com/MasterXiong/HyperVLA上公开获取</a></p>
<div class="markdown-heading"><h2 class="heading-element">利用动态主题感知路由改进多模式大脑编码模型</h2><a id="user-content-利用动态主题感知路由改进多模式大脑编码模型" class="anchor" aria-label="Permalink: 利用动态主题感知路由改进多模式大脑编码模型" href="#利用动态主题感知路由改进多模式大脑编码模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04670v1宣布类型：新摘要：自然主义fMRI编码必须处理多模式输入、转变的融合风格和明显的受试者间变异性。我们引入了AFIRE（多模式fMRI响应编码的不可知框架），这是一种不可知的界面，可以同步化来自不同编码器的时间对齐的融合后令牌，还引入了MIND，这是一种即插即用的专家混合解码器，具有主题感知的动态门控。AFIRE经过全脑预测的端到端训练，将解码器与上游融合分开，而MIND在个性化专家使用之前将依赖代币的Top-K稀疏路由与主题结合起来，而不会牺牲通用性。跨多个多模式主干和主题的实验显示出相对于强基线的一致改进、增强的跨主题概括以及与内容类型相关的可解释专家模式。该框架为新编码器和数据集提供了一个简单的连接点，为自然神经成像研究提供稳健的、即插即补的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">Quant-dLLM：用于扩散大语言模型的训练后极低比特量化</h2><a id="user-content-quant-dllm用于扩散大语言模型的训练后极低比特量化" class="anchor" aria-label="Permalink: Quant-dLLM：用于扩散大语言模型的训练后极低比特量化" href="#quant-dllm用于扩散大语言模型的训练后极低比特量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03274v1宣布类型：新摘要：扩散大语言模型（dLLM）提供双向上下文和灵活的掩蔽去噪生成，正在成为自回归（AR）LLM的引人注目的替代方案。然而，与AR LLM一样，它们的型号尺寸不断增大，从而推动了部署时的重量压缩。尽管训练后量化（PTQ）对于AR LLM有效，但将其直接传输到2位的dLLM会导致性能不理想。为了应对这些挑战，我们提出了Quant-dLLM，这是一种针对dLLM量身定制的超低位PTQ框架。由于dLLM中的掩蔽去噪激活与标准PTQ方法假设的完全可见信号不同，因此我们引入掩蔽校准模拟（MC）来将校准与时间步相关的掩蔽保持一致，从而产生更可靠的校准。此外，我们提出了一种数据感知任意阶量化器（DAQ），它通过优化算法学习超低比特权重表示。它在我们模拟的校准数据的指导下执行迭代逼近。此外，在严格的2位预算下，我们引入了自适应带宽混合精度（ABMP），这是一种基于灵敏度的精度分配方案，可以在通道组之间自适应地分配比特宽度。当限制为2位精度时，Quant-dLLM始终比dLLM上最先进的（SOTA）AR传输PTQ方法实现更高的准确性。代码和型号可在<a href="https://github.com/ZTA2785/Quant-dLLM%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZTA2785/Quant-dLLM上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">UniPruning：统一可扩展稀疏LLM的局部度量和全局反馈</h2><a id="user-content-unipruning统一可扩展稀疏llm的局部度量和全局反馈" class="anchor" aria-label="Permalink: UniPruning：统一可扩展稀疏LLM的局部度量和全局反馈" href="#unipruning统一可扩展稀疏llm的局部度量和全局反馈"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03291v1宣布类型：新摘要：大型语言模型（LLM）在不同任务中实现了强劲的性能，但面临着高昂的计算和内存成本。修剪通过在保持建筑灵活性的同时引入稀疏性，提供了一条有希望的途径。然而，现有方法很难平衡效率和稳健性：局部指标方法逐层修剪，但通常在高稀疏性下崩溃，而全局反馈方法以昂贵的权重更新或限制性半结构化格式为代价来强制一致性。我们提出了UniPruning，这是一个统一的训练后修剪框架，它将局部显着性指标的速度与全局协调的稳定性相结合，通过基于镜像下降的优化实现，所有这些都无需更新模型权重。UniPruning利用快速分层评分和轻量级全局控制器来分配单个稀疏预算，支持在一个框架内进行非结构化和半结构化N：M修剪。经过简短的校准后，它可以一次生成任意稀疏级别的修剪模板，并无缝地适应硬件感知的约束。对多个预训练的LLM系列和标准基准的广泛实验表明，UniPruning始终提供有竞争力或卓越的困惑度和零射击准确性。消融研究进一步强调了镜像下降和局部显着锚定的重要性。总体而言，UniPruning为精简大规模LLM提供了一种高效、原则性和可扩展的解决方案。我们的代码可访问：<a href="https://github.com/RainbowQTT/UniPruning%E3%80%82">https://github.com/RainbowQTT/UniPruning。</a></p>
<div class="markdown-heading"><h2 class="heading-element">CAFL-L：针对设备上语言模型的拉格朗日二元优化的约束感知联邦学习</h2><a id="user-content-cafl-l针对设备上语言模型的拉格朗日二元优化的约束感知联邦学习" class="anchor" aria-label="Permalink: CAFL-L：针对设备上语言模型的拉格朗日二元优化的约束感知联邦学习" href="#cafl-l针对设备上语言模型的拉格朗日二元优化的约束感知联邦学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03298v1宣布类型：新摘要：我们引入了具有拉格朗日二元优化的约束感知联邦学习（CAFL-L），这是FedVID的原则性扩展，它显式地结合了设备级资源约束，包括能源、通信、内存和热预算。CAFL-L采用拉格朗日双重优化来动态调整训练超参数（冻结深度、局部步骤、批量大小和通信压缩），同时通过梯度累积的代币预算保留来保持训练稳定性。字符级语言模型的实验表明，与标准FedVID相比，CAFL-L实现了更好的约束满足度（内存使用减少20%，通信减少95%），同时保持有竞争力的验证性能，使其适合在资源受限的边缘设备上部署。</p>
<div class="markdown-heading"><h2 class="heading-element">COSMO-RL：通过关节安全性和稳定性实现可信赖的LMRM</h2><a id="user-content-cosmo-rl通过关节安全性和稳定性实现可信赖的lmrm" class="anchor" aria-label="Permalink: COSMO-RL：通过关节安全性和稳定性实现可信赖的LMRM" href="#cosmo-rl通过关节安全性和稳定性实现可信赖的lmrm"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04196v1宣布类型：新摘要：大型多模式推理模型（LMRM）正在进入实际应用程序，它们必须既有用又安全。安全性在多模式环境中尤其具有挑战性：图像和文本可以结合起来绕过护栏，而单一目标训练可能会导致政策漂移，从而导致对良性输入的过度拒绝或对风险输入的不安全合规。我们提出了COSMO-RL，这是一个混合强化学习框架，可以在多模式、多任务和多目标信号下训练面向推理的LMRM，并发布了最终模型COSMO-R1。我们的方法旨在让安全性和能力在一条稳定的管道中共同增长，而不是在对齐期间竞争。在实验中，COSMO-R1在排队的同时提高了安全性，并且经常改善了多模式推理和指令遵循，对多模式越狱表现出更强的鲁棒性，并减少了不必要的拒绝。该框架还在主干之间转移，并取得一致的收益。烧蚀支持设计选择，指出了在LMRM中同时提高安全性和通用能力的简单途径。</p>
<div class="markdown-heading"><h2 class="heading-element">MLLMEraser：通过激活转向实现多模态大型语言模型的测试时遗忘</h2><a id="user-content-mllmeraser通过激活转向实现多模态大型语言模型的测试时遗忘" class="anchor" aria-label="Permalink: MLLMEraser：通过激活转向实现多模态大型语言模型的测试时遗忘" href="#mllmeraser通过激活转向实现多模态大型语言模型的测试时遗忘"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04217v1宣布类型：新摘要：多模式大型语言模型（MLLM）在视觉语言任务中表现出了非凡的能力，但它们的大规模部署引发了人们对记忆的私人数据、过时知识和有害内容的紧迫担忧。MLLM的现有取消学习方法通常会适应基于训练的策略，例如梯度上升或偏好优化，但这些方法计算成本高、不可逆转，并且经常扭曲保留的知识。在这项工作中，我们提出了MLLMEraser，这是一个用于测试时取消学习的输入感知、免训练框架。我们的方法利用激活引导来实现动态知识擦除，而无需更新参数。具体来说，我们通过将对抗干扰的知识回忆图像-文本对与知识擦除对应物进行比较，从而捕获文本和视觉差异，来构建一个多模式擦除方向。为了防止不必要的干扰，我们进一步设计了一种输入感知的引导机制，该机制自适应地确定何时以及如何应用擦除方向，从而保留保留知识的实用性，同时强制忘记指定内容。LLaVA-1.5和Qwen-2.5-DL上的实验表明，MLLMEraser始终优于最先进的MLLM去学习基线，以更低的计算成本和最小的效用退化实现了更强的遗忘性能。</p>
<div class="markdown-heading"><h2 class="heading-element">ChartAgent：复杂图表问题回答中用于视觉基础推理的多模式代理</h2><a id="user-content-chartagent复杂图表问题回答中用于视觉基础推理的多模式代理" class="anchor" aria-label="Permalink: ChartAgent：复杂图表问题回答中用于视觉基础推理的多模式代理" href="#chartagent复杂图表问题回答中用于视觉基础推理的多模式代理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04514v1宣布类型：新摘要：最近的多模式LLM在基于图表的视觉问答方面表现出了希望，但它们在未注释的图表上的性能急剧下降，这些图表需要精确的视觉解释而不是依赖于文本快捷方式。为了解决这个问题，我们引入了ChartAgent，这是一种新颖的代理框架，可以直接在图表的空间域中显式地执行视觉推理。与文本思维链推理不同，ChartAgent迭代地将查询分解为视觉子任务，并通过绘制注释、裁剪区域（例如，分割饼形切片、隔离条）和定位轴，使用特定于图表的视觉工具库来完成每个子任务。这个迭代推理过程密切反映了人类图表理解的认知策略。ChartAgent在ChartBench和ChartX基准上实现了最先进的准确性，总体绝对收益高达16.07%，在未注释、数字密集型查询上超过了17.31%。此外，我们的分析表明，ChartAgent（a）在不同的图表类型中有效，（b）在不同的视觉和推理复杂性水平上获得最高分数，（c）充当即插即用框架，可以提高不同底层LLM的性能。我们的工作是最早使用工具增强的多模式代理演示图表理解的视觉基础推理的工作之一。</p>
<div class="markdown-heading"><h2 class="heading-element">PT $#2 $-LLM：大型语言模型的训练后模块化</h2><a id="user-content-pt-2--llm大型语言模型的训练后模块化" class="anchor" aria-label="Permalink: PT $#2 $-LLM：大型语言模型的训练后模块化" href="#pt-2--llm大型语言模型的训练后模块化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03267v1宣布类型：新摘要：大型语言模型（LLM）在各种任务中表现出令人印象深刻的能力，但其大内存和计算需求阻碍了部署。三进制化作为一种有前途的压缩技术而受到关注，可以大幅减少大小并提高计算效率。然而，由于无训练参数优化的挑战以及异常值和分散权重带来的量化困难，其在训练后量化（PTQ）设置中的潜力仍然没有得到充分开发。为了解决这些问题，我们提出了PT $' 2 $-LLM，这是一个为LLM量身定制的训练后三值化框架。其核心是一个不对称三进制量化器，配备了两阶段细化管道：（1）迭代三进制匹配（ITF），它在最佳三进制网格构造和灵活的舍入之间交替，以最大限度地减少量化误差，以及（2）激活感知网格对齐（AGA），它进一步细化三进制网格以更好地匹配全精度输出。此外，我们还提出了一种即插即用的基于结构相似性的重新排序（SR）策略，该策略利用列间结构相似性来简化量化并减轻异常值效应，进一步提高整体性能。大量实验表明，PT $' 2 $-LLM与最先进的（SOTA）2位PTQ方法相比具有竞争力的性能，内存成本更低，同时还加速预填充和解码以实现端到端加速。代码和型号可在<a href="https://github.com/XIANGLONGYAN/PT2-LLM%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/XIANGLONGYAN/PT2-LLM上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">Edge-FIT：针对隐私保护的智能家居环境对量化LLM进行联合指令调整</h2><a id="user-content-edge-fit针对隐私保护的智能家居环境对量化llm进行联合指令调整" class="anchor" aria-label="Permalink: Edge-FIT：针对隐私保护的智能家居环境对量化LLM进行联合指令调整" href="#edge-fit针对隐私保护的智能家居环境对量化llm进行联合指令调整"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03284v1宣布类型：新摘要：本文提出了Edge-FIT（边缘联合指令调优），这是一个用于大型语言模型（LLM）联合指令调优（FIT）的可扩展框架。传统的联邦学习（TFL）方法（例如FedVID）在面对LLM的巨大参数大小时会失败[3]、[6]。我们的Edge-FIT框架将联邦学习与4位量化低等级自适应（QLORA）相结合，减轻了通信和计算负担的核心问题。我们通过过滤物联网域的通用Databricks Dolly 15 k数据集来证明这一点。实验结果显示，经过边缘调整的Lama 2（7 B）的F1评分为0.89。我们还使用3.8B Phi-3-mini模型展示了一种可行的权衡，验证Edge-FIT作为在家庭计算网关上进行去中心化LLM部署的可扩展框架。</p>
<div class="markdown-heading"><h2 class="heading-element">SDQ-LLM：用于任何大小的1位LLM的Σ-Δ量化</h2><a id="user-content-sdq-llm用于任何大小的1位llm的σ-δ量化" class="anchor" aria-label="Permalink: SDQ-LLM：用于任何大小的1位LLM的Σ-Δ量化" href="#sdq-llm用于任何大小的1位llm的σ-δ量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03275v1宣布类型：新摘要：大型语言模型（LLM）面临着巨大的计算和内存挑战，使得极低比特量化对其高效部署至关重要。在这项工作中，我们引入了SDQ-LLM：适用于任何大小的1位LLM的Sigma-Delta量化，这是一种新颖的框架，可以实现LLM的极低位量化，同时保留其语言推理能力。SDQ-LLM的一个独特特征是过采样比（OSR）的连续可调性，通过选择分数OSR（例如2.5倍）以实现模型大小和准确性之间的最佳权衡，从而动态适应内存或VRAM约束。SDQ-LLM使用上采样与Sigma-Delta Quantizer相结合来二值化或三值化LLM权重，将高精度参数编码为1位或1.58位表示，用加法取代线性层内的相乘运算。这种方法显着提高了极低比特量化下的推理效率。为了进一步减少量化精度的损失，我们在量化之前结合了基于Hadamard的权重平滑，提高了权重表示的稳定性和鲁棒性。此外，为了充分利用OSR的连续性并减少精度损失，认识到量化灵敏度和权重方差之间的相关性，我们提出了一种细粒度、分层和线性OSR分配策略MultiOSR。该策略根据权重方差和参数规模在层之间和每个层内分布OSR。最后，对OPT和LLaMA模型系列的大量实验表明，即使在高度激进的低OSR设置下，SDQ-LLM也能实现更高效和高精度的性能。我们的代码可在<a href="https://github.com/Dreamlittlecat/LLM-Quant-Factory%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Dreamlittlecat/LLM-Quant-Factory上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">回旋镖蒸馏实现零镜头模型尺寸插值</h2><a id="user-content-回旋镖蒸馏实现零镜头模型尺寸插值" class="anchor" aria-label="Permalink: 回旋镖蒸馏实现零镜头模型尺寸插值" href="#回旋镖蒸馏实现零镜头模型尺寸插值"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05064v1宣布类型：新摘要：大型语言模型（LLM）通常在不同的内存和计算限制下部署。现有的方法通过独立训练每个尺寸来构建模型族，这成本高得令人望而却步，并且仅提供粗粒度的尺寸选项。在这项工作中，我们发现了一种新颖的现象，我们称之为回旋镖蒸馏：从一个大型基础模型（教师）开始，首先蒸馏到一个小学生，然后通过将教师层块重新纳入学生中来逐步重建中等规模的模型，无需任何额外的培训。该过程会产生许多中间大小的零镜头插值模型，其性能在学生和教师之间平稳扩展，通常匹配或超过相同大小的预训练或提炼模型。我们进一步分析了这种类型的插值何时成功，表明通过修剪和提炼实现教师和学生之间的一致至关重要。因此，回旋镖蒸馏提供了一种简单有效的方法来生成细粒度的模型族，从而大幅降低训练成本，同时实现跨部署环境的灵活适应。代码和型号可在<a href="https://github.com/dcml-lab/boomerang-distillation%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/dcml-lab/boomerang-distillation上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">迈向多模式主动学习：利用有限的配对数据进行高效学习</h2><a id="user-content-迈向多模式主动学习利用有限的配对数据进行高效学习" class="anchor" aria-label="Permalink: 迈向多模式主动学习：利用有限的配对数据进行高效学习" href="#迈向多模式主动学习利用有限的配对数据进行高效学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03247v1宣布类型：新摘要：主动学习（AL）是一种原则性策略，旨在降低数据需求深度学习中的注释成本。然而，现有的AL算法几乎只关注单模式数据，而忽视了多模式学习中的大量注释负担。我们引入了第一个具有未对齐数据的多模式主动学习框架，其中学习者必须积极获取跨模式对齐，而不是预对齐对上的标签。这种设置抓住了CLIP和SigLIP等现代多峰管道中的实际瓶颈，其中单峰特征很容易获得，但高质量对齐成本高昂。我们开发了一种新的算法，结合了不确定性和多样性原则的模态感知设计，实现线性时间采集，并无缝地应用于基于池和基于流的设置。在基准数据集上进行的大量实验表明，我们的方法在保持性能的同时不断降低多模态注释成本;例如，在ColorSwap数据集上，它将注释要求降低了40美元，而不会损失准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">用于引导Web代理偏好的跨模式内容优化</h2><a id="user-content-用于引导web代理偏好的跨模式内容优化" class="anchor" aria-label="Permalink: 用于引导Web代理偏好的跨模式内容优化" href="#用于引导web代理偏好的跨模式内容优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03612v1宣布类型：新摘要：基于视觉语言模型（VLM）的网络代理通过将多模式感知与偏好推理相结合，越来越多地支持内容推荐或产品排名等高风险选择任务。最近的研究表明，这些代理很容易受到攻击者的攻击，攻击者可以通过使用对抗性弹出窗口、图像扰动或内容调整的偏好操纵来偏差选择结果。然而，现有的工作要么假设强白盒访问，单模式扰动有限，要么使用不切实际的设置。在本文中，我们首次证明，在现实的攻击者能力下，视觉和文本渠道的联合利用会产生显着更强大的偏好操纵。我们引入了跨模式偏好引导（CPS），它联合优化对物品的视觉和自然语言描述的不可感知的修改，利用CLIP可转移的图像扰动和RLHF引起的语言偏差来引导代理决策。与假设梯度访问或控制网页或代理内存的先前研究相反，我们采用了现实的黑匣子威胁设置：非特权对手只能编辑自己列表的图像和文本元数据，而不深入了解代理的模型内部。我们在电影选择和电子商务任务方面评估由最先进的专有和开源VLM（包括GPT-4.1、Qwen-2.5BL和Pixtral-Large）提供支持的代理的CPS。我们的结果表明，CPS比领先的基线方法明显更有效。例如，我们的结果表明，CPS在所有型号中始终优于基线，同时保持了70%的检测率，证明了有效性和隐蔽性。这些发现凸显了对强大防御的迫切需要，因为代理系统在社会中发挥着越来越重要的作用。</p>
<div class="markdown-heading"><h2 class="heading-element">思考然后嵌入：生成上下文改进多模式嵌入</h2><a id="user-content-思考然后嵌入生成上下文改进多模式嵌入" class="anchor" aria-label="Permalink: 思考然后嵌入：生成上下文改进多模式嵌入" href="#思考然后嵌入生成上下文改进多模式嵌入"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05014v1宣布类型：新摘要：人们对通用多模式嵌入（UME）的兴趣越来越大，其中需要模型来生成特定于任务的表示。虽然最近的研究表明多模式大型语言模型（MLLM）在此类任务中表现良好，但它们仅将MLLM视为编码器，忽视了它们的生成能力。然而，随着指令变得更加复杂并且需要组合推理，这样的编码范式变得不那么有效。受到思想链推理已被证明有效性的启发，我们为UME提出了一个通用的思想嵌入（TTE）框架，由推理器和嵌入器组成。推理器MLLM首先生成解释复杂查询的推理痕迹，然后是嵌入器，生成基于原始查询和中间推理的表示。这个明确的推理步骤可以更细致地理解复杂的多模式指令。我们的贡献是三倍的。首先，通过利用强大的MLLM推理机，我们在MMEB-V2基准上实现了最先进的性能，超越了在大量内部数据集上训练的专有模型。其次，为了减少对大型MLLM推理机的依赖，我们使用高质量的以嵌入为中心的推理轨迹微调了较小的MLLM推理机，实现了开源模型中的最佳性能，比最近提出的模型获得了7%的绝对收益。第三，我们研究将推理器和嵌入器集成到统一模型中的策略，以在不牺牲性能的情况下提高效率。</p>
<div class="markdown-heading"><h2 class="heading-element">具有解耦动量优化的分布式低通信训练</h2><a id="user-content-具有解耦动量优化的分布式低通信训练" class="anchor" aria-label="Permalink: 具有解耦动量优化的分布式低通信训练" href="#具有解耦动量优化的分布式低通信训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03371v1宣布类型：新摘要：大型模型的训练需要大量的计算资源，这些资源通常仅在具有高带宽互连的数据中心中可用。但是，减少对节点之间高带宽互连的依赖，可以使用分布式计算资源作为集中式数据中心培训的替代方案。基于分布式模型训练的最新进展，我们提出了一种方法，通过将分布式模型副本之间的不频繁同步与梯度动量压缩相结合，进一步减少通信。特别是，我们将优化器动量视为信号，并通过离散Cosine变换（Discover）将Nesterov动量分解为高频和低频分量。每$H$步，只有高频分量才会在模型副本之间同步。从经验上看，与基线DiLoCo相比，我们的方法实现了高达16美元的通信减少，并且它在跨架构中进行了推广，包括基于转换器的语言模型和图像卷积神经网络。总体而言，这项工作提高了在具有低带宽互连的分布式节点上训练大型模型的可行性。</p>
<div class="markdown-heading"><h2 class="heading-element">合并与引导：统一模型合并和引导解码以实现可控多目标生成</h2><a id="user-content-合并与引导统一模型合并和引导解码以实现可控多目标生成" class="anchor" aria-label="Permalink: 合并与引导：统一模型合并和引导解码以实现可控多目标生成" href="#合并与引导统一模型合并和引导解码以实现可控多目标生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03782v1宣布类型：新摘要：在测试时适应多样化的用户需求是可控多目标生成的关键挑战。现有方法还不够：基于合并的方法在参数层面提供间接的、次优的控制，通常忽视多个目标的影响。虽然基于解码的指导更加直接，但它通常需要汇总来自多个专家模型的日志，从而产生大量的空间负担并严重依赖单个模型的容量。为了解决这些问题，我们引入了Merge-And-GuidE（MAGE），这是一个两阶段框架，利用模型合并进行引导解码。我们首先确定引导模型和基本模型之间的关键兼容性问题。在第一阶段，MAGE通过动态构建更稳健的基础模型来解决这个问题，合并一系列考虑多个目标的主干模型。在第二阶段，我们将显式和隐式价值模型合并到统一的指导代理中，然后引导第一阶段基本模型的解码。我们的分析从经验上验证了价值模型中的线性模式连通性（LMC），探索了模型合并和预测集成之间的关系，并展示了我们的方法提供的增强的可控性。大量实验表明，我们的方法优于现有方法，实现了卓越的可控性、帕累托最优性能和增强的适应性。</p>
<div class="markdown-heading"><h2 class="heading-element">Gemini Robotics 1.5：利用高级推理、思维和运动转移推动通才机器人的前沿</h2><a id="user-content-gemini-robotics-15利用高级推理思维和运动转移推动通才机器人的前沿" class="anchor" aria-label="Permalink: Gemini Robotics 1.5：利用高级推理、思维和运动转移推动通才机器人的前沿" href="#gemini-robotics-15利用高级推理思维和运动转移推动通才机器人的前沿"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03342v1宣布类型：新摘要：通用机器人需要对物理世界有深入的了解、先进的推理以及通用而灵巧的控制。本报告介绍了最新一代Gemini Robotics模型系列：Gemini Robotics 1.5（多实施例视觉-语言-动作（VLA）模型）和Gemini Robotics-ER 1.5（最先进的启发推理（ER）模型）。我们正在汇集三大创新。首先，Gemini Robotics 1.5具有新颖的架构和运动传递（MT）机制，使其能够从异类、多实施例机器人数据中学习，并使VLA更加通用。其次，Gemini Robotics 1.5将动作与自然语言的多层内部推理过程交织在一起。这使机器人能够“先思考后行动”，显着提高了其分解和执行复杂、多步骤任务的能力，也使机器人的行为更容易被用户解释。第三，Gemini Robotics-ER 1.5为具体推理建立了新的最先进技术，即对于机器人至关重要的推理能力，例如视觉和空间理解、任务规划和进度估计。这一系列模型共同带领我们迈向物理代理时代--使机器人能够感知、思考和行动，以便解决复杂的多步骤任务。</p>
<div class="markdown-heading"><h2 class="heading-element">比眼睛更符合？发现训练视觉语言驱动模型中的推理与规划脱节</h2><a id="user-content-比眼睛更符合发现训练视觉语言驱动模型中的推理与规划脱节" class="anchor" aria-label="Permalink: 比眼睛更符合？发现训练视觉语言驱动模型中的推理与规划脱节" href="#比眼睛更符合发现训练视觉语言驱动模型中的推理与规划脱节"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04532v1宣布类型：新摘要：视觉语言模型（VLM）驱动代理通过首先产生自然语言推理，然后预测轨迹规划来承诺可解释的端到端自主性。然而，规划是否由这一推理因果驱动仍然是一个关键但未经证实的假设。为了调查这一点，我们构建了DriveMind，这是一个大规模驾驶视觉问题解答（VQA）数据库，具有计划对齐的思想链（CoT），从nuPlan自动生成。我们的数据生成过程将传感器和注释转换为结构化输入，最重要的是，将先验信息与需要推理的信号分开，从而实现干净的信息消融。使用DriveMind，我们通过监督微调（SFT）和组相对政策优化（GRPO）培训代表性VLM代理，并使用nuPlan的指标对他们进行评估。不幸的是，我们的结果表明推理与规划中存在一致的因果脱节：删除自我/导航先验会导致规划分数大幅下降，而删除CoT只会产生较小的变化。注意力分析进一步表明，规划主要关注先验而不是CoT。基于这一证据，我们提出了推理-规划脱钩假说，假设训练产生的推理是辅助副产品，而不是因果中介者。为了实现高效诊断，我们还引入了一种新颖的免训练探测器，该探测器通过评估其对微小输入扰动的规划稳健性来衡量代理对先验的依赖。总而言之，我们为社区提供了新的数据集和诊断工具，以评估未来模型的因果保真度。</p>
<div class="markdown-heading"><h2 class="heading-element">视觉编码器的训练后量化需要添加寄存器</h2><a id="user-content-视觉编码器的训练后量化需要添加寄存器" class="anchor" aria-label="Permalink: 视觉编码器的训练后量化需要添加寄存器" href="#视觉编码器的训练后量化需要添加寄存器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04547v1宣布类型：新摘要：基于转换器的视觉编码器（例如CLIP）是多模式智能的核心，为从自主网络代理到机器人控制的应用程序提供动力。由于这些应用通常需要实时处理大量视觉数据，因此降低视觉编码器的推理成本至关重要。训练后量化提供了一种实用的路径，但由于锯齿规模的激活（即，即使在8位精度下也仍然具有挑战性离群值）。在这项工作中，我们提出了$\textit{Regache}$，这是一种免训练算法，用于减轻视觉编码器中的异常值，从而实现量化，准确度下降要小得多。拟议的Regache向目标视觉编码器引入了容易出现异常值但在语义上毫无意义的前置标记，这可以防止其他标记出现异常值。值得注意的是，我们观察到视觉编码器中的异常值的表现与语言模型中的异常值不同，从而激发了两项技术创新：中间层前置和令牌删除。实验表明，我们的方法始终提高了文本监督和自我监督视觉编码器量化模型的准确性。</p>
<div class="markdown-heading"><h2 class="heading-element">药物文本对齐的瘦桥：针对特定目标药物检索的轻量级对比学习</h2><a id="user-content-药物文本对齐的瘦桥针对特定目标药物检索的轻量级对比学习" class="anchor" aria-label="Permalink: 药物文本对齐的瘦桥：针对特定目标药物检索的轻量级对比学习" href="#药物文本对齐的瘦桥针对特定目标药物检索的轻量级对比学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03309v1宣布类型：新摘要：多模式基础模型为药物发现和生物医学应用带来了希望，但大多数现有方法依赖于大量预训练或大规模多模式库。我们研究薄对比桥、冻结的单模式编码器上的轻量级投影头是否可以在不训练完整的多模式模型的情况下对齐化学和文本表示。使用ChMBE的配对机制，我们通过用对比目标训练的双线性投影将ECFP 4分子指纹与生物医学句子嵌入对齐。为了更好地处理具有相同治疗目标的药物，我们结合了硬负加权和边际损失。基于支架的分裂下的评估需要对不相交的化学核心进行概括，这表明我们的方法实现了非平凡的跨模式对齐，并且与冻结基线相比，在目标区分度内大幅提高。这些结果表明，细桥为大规模多模式预训练提供了一种高效的计算替代方案，从而实现了精确医学中的支架感知药物文本对齐和目标特定检索。</p>
<div class="markdown-heading"><h2 class="heading-element">小型嵌入模型的压缩级联</h2><a id="user-content-小型嵌入模型的压缩级联" class="anchor" aria-label="Permalink: 小型嵌入模型的压缩级联" href="#小型嵌入模型的压缩级联"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04626v1宣布类型：新摘要：嵌入模型是密集检索、语义搜索和推荐系统的核心，但其规模通常使其在浏览器或边缘设备等资源受限的环境中部署不切实际。虽然较小的嵌入模型提供了实际优势，但与较大的嵌入模型相比，它们的表现通常不佳。为了弥合这一差距，我们证明，将多个小模型的原始嵌入载体连接起来可以优于标准检索基准上的单个更大基线。为了克服由此产生的朴素级联的高维度，我们引入了一个用Matryoshka表示学习（MRL）损失训练的轻量级统一解码器。该解码器将多维联合表示映射到低维空间，在无需微调基本模型的情况下保留大部分原始性能。我们还表明，虽然连接更多的基本模型会产生越来越小的收益，但解码器表示在压缩和量化下的鲁棒性会得到改善。我们的实验表明，在MTEB检索任务的一个子集上，当管道应用于四个小型嵌入模型的级联时，我们的Contat-encod-encod-tem管道以48倍的压缩因子恢复了原始性能的89%。</p>
<div class="markdown-heading"><h2 class="heading-element">解密多模式对比学习中的情态差距：从收敛表示到配对对齐</h2><a id="user-content-解密多模式对比学习中的情态差距从收敛表示到配对对齐" class="anchor" aria-label="Permalink: 解密多模式对比学习中的情态差距：从收敛表示到配对对齐" href="#解密多模式对比学习中的情态差距从收敛表示到配对对齐"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03268v1宣布类型：新摘要：多模式对比学习（MCL）旨在将来自不同模式的数据嵌入共享嵌入空间中。然而，经验证据表明，来自不同模式的表示占据了嵌入空间的完全独立的区域，这种现象被称为模式间隙。此外，关于模式差距的大小如何影响下游性能的实验结果不一致。这些观察提出了两个关键问题：（1）是什么导致了模式差距？(2)它如何影响下游任务？为了解决这些问题，本文引入了第一个用于分析MCL的收敛最佳表示和优化训练时的模式对齐的理论框架。具体来说，我们证明了在没有任何约束或在锥约束下，模式间隙收敛到零。在子空间约束下（即，由于维度塌陷，两个模式的表示落入两个不同的超平面中），模式间隙收敛到两个超平面之间的最小角度。此结果将{维度塌陷}确定为模式差距的根本根源。此外，我们的定理证明，在子空间约束下，成对样本无法完全对齐。模式差距通过影响样本对之间的对齐来影响下游性能。我们证明，在这种情况下，两种模式之间的完美对齐仍然可以通过两种方式实现：超平面旋转和共享空间投影。</p>
<div class="markdown-heading"><h2 class="heading-element">通过混合属性和修剪框架发现Transformer电路</h2><a id="user-content-通过混合属性和修剪框架发现transformer电路" class="anchor" aria-label="Permalink: 通过混合属性和修剪框架发现Transformer电路" href="#通过混合属性和修剪框架发现transformer电路"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03282v1宣布类型：新摘要：解释语言模型通常涉及电路分析，旨在识别完成特定任务的稀疏子网络或电路。现有的电路发现算法面临着一个基本的权衡：属性修补速度快，但不忠实于完整模型，而边缘修剪是忠实的，但计算昂贵。这项研究提出了一种混合归因和修剪（HAP）框架，该框架使用归因修补来识别高潜力子图，然后应用边缘修剪来从中提取忠实的电路。我们表明HAP比基线算法快46%，而不会牺牲电路忠实性。此外，我们还提供了一个关于间接对象识别任务的案例研究，表明我们的方法保留了归因修补方法在高度稀疏时修剪的协作电路组件（例如S抑制头）。我们的结果表明，HAP可能是提高机械解释性研究对更大模型的可扩展性的有效方法。我们的代码可在<a href="https://anonymous.4open.science/r/HAP-circuit-discovery%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82" rel="nofollow">https://anonymous.4open.science/r/HAP-circuit-discovery上获取。</a></p>
<div class="markdown-heading"><h2 class="heading-element">ContextNav：迈向显式多模式上下文内学习</h2><a id="user-content-contextnav迈向显式多模式上下文内学习" class="anchor" aria-label="Permalink: ContextNav：迈向显式多模式上下文内学习" href="#contextnav迈向显式多模式上下文内学习"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04560v1宣布类型：新摘要：最近的进展表明，多模式大型语言模型（MLLM）表现出强大的多模式上下文学习（ICL）能力，使它们能够从一些上下文示例中适应新颖的视觉语言任务。然而，现有的ICL方法在协调不同任务和嘈杂的上下文示例的可扩展性与鲁棒性方面面临着挑战：手动选择示例会产生干净的上下文，但需要劳动密集型且特定于任务，而基于相似性的检索可以提高可扩展性，但可能会引入不相关或结构上不一致的样本，从而降低ICL的性能。为了解决这些限制，我们提出了ContextNav，这是第一个将自动检索的可扩展性与类人策展的质量和适应性集成在一起的代理框架，为多模式ICL实现噪音稳健且动态优化的情境化。ContextNav在基于图形的编排驱动的闭环工作流程中统一了上下文管理和抗噪上下文化。具体来说，它构建了一个资源感知的多模式嵌入管道，维护可检索的载体数据库，并应用代理检索和结构对齐来构建抗噪上下文。操作语法图（OGG）进一步支持自适应工作流程规划和优化，使代理能够根据下游ICL反馈完善其运营策略。实验结果表明，ContextNav在各种数据集中实现了最先进的性能，强调了代理工作流程在多模式ICL中推进可扩展和稳健的上下文化方面的前景。</p>
<div class="markdown-heading"><h2 class="heading-element">StructPrune：具有$\mathCal{O}（\SQRT{N}）$图形处理器的结构化全局修剪渐进性</h2><a id="user-content-structprune具有mathcalosqrtn图形处理器的结构化全局修剪渐进性" class="anchor" aria-label="Permalink: StructPrune：具有$\mathCal{O}（\SQRT{N}）$图形处理器的结构化全局修剪渐进性" href="#structprune具有mathcalosqrtn图形处理器的结构化全局修剪渐进性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03246v1宣布类型：新摘要：修剪对于扩展大型语言模型（LLM）至关重要。全局修剪可实现强大的性能，但需要$\mathCal{O}（N）$内存，这对于十亿参数模型来说是不可行的。通过独立修剪层，本地修剪将图形处理器内存使用量减少到单层，但它忽略了层间依赖性，并且通常导致高稀疏性机制中的性能次优。与非结构化修剪不同，结构化修剪会产生规则的稀疏模式，这些模式与图形处理器内核和库优化很好地一致，使其硬件效率更高。然而，结构化修剪通常依赖于全局修剪，因为结构化模式在局部优化下更容易出现严重的性能下降。为了共同实现结构化修剪和局部修剪的内存效率，我们提出了一种分而治之的策略，将全局修剪问题分解为不同模块之间的协调子问题，每个子问题都适合有限的图形处理器内存。基于这个想法，我们设计了\textBF{ðPRUNE}，这是一个基于ADMM的框架，它将结构化稀疏性集成到修剪过程中，将本地修剪的内存效率与结构化方法的硬件兼容性相结合。我们为结构化修剪面具推导出一个封闭形式的分析解决方案，为逐层稀疏性分配提供了显式规则，并进一步开发了一个基于能量的渐进框架，产生了一个软最大形式分配方案，该方案简化了优化，同时适应不同层的重要性。实验表明，SEARCH PRUNE满足了全局结构化修剪的复杂性，同时将内存成本从$\mathCal{O}（N）$减少到$\mathCal{O}（\SQRT{N}）$，实现了数十亿参数规模的实际部署。</p>
<div class="markdown-heading"><h2 class="heading-element">ContextVLA：具有摊销多框架上下文的视觉-语言-动作模型</h2><a id="user-content-contextvla具有摊销多框架上下文的视觉-语言-动作模型" class="anchor" aria-label="Permalink: ContextVLA：具有摊销多框架上下文的视觉-语言-动作模型" href="#contextvla具有摊销多框架上下文的视觉-语言-动作模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04246v1宣布类型：新摘要：利用时间上下文对于部分可观察的机器人任务的成功至关重要。然而，先前在行为克隆方面的工作表明，使用多帧观察时的性能提高不一致。在本文中，我们引入了ContextVLA，这是一个政策模型，通过有效利用多帧观察来稳健地提高机器人任务性能。我们的方法的动机是视觉-语言-动作模型（VLA）的关键观察，即基于视觉语言模型（VLM）构建的策略模型可以更有效地利用多帧观察来生成动作。这表明VLM固有的时间理解能力使它们能够从多帧观察中提取更有意义的上下文。然而，视频输入的高维度带来了大量的计算负担，使得VLA训练和推理效率低下。为了解决这个问题，ContextVLA将过去的观察结果压缩到单个上下文令牌中，允许策略有效地利用时间上下文来生成动作。我们的实验表明，ContextVLA始终优于单帧VLA，并实现了完整多帧训练的好处，但训练和推理时间减少。</p>
<div class="markdown-heading"><h2 class="heading-element">GroK：通过知识引导教学的接地MLLM从定量生物标志物到定性诊断</h2><a id="user-content-grok通过知识引导教学的接地mllm从定量生物标志物到定性诊断" class="anchor" aria-label="Permalink: GroK：通过知识引导教学的接地MLLM从定量生物标志物到定性诊断" href="#grok通过知识引导教学的接地mllm从定量生物标志物到定性诊断"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04281v1宣布类型：新摘要：多模式大型语言模型（MLLM）有望集成不同的数据模式，但当前的医学适应措施（例如LLaVA-Med）往往无法充分利用彩色视网膜摄影（CFP）和光学相干断层扫描（Optical Conservation Tomography）之间的协同作用，并且提供有限的量化生物标志物的解释性。我们引入GroK，这是一种基础多模式大型语言模型，可联合处理CFP、光学断层扫描和文本，以提供眼部和全身性疾病的临床级诊断。GroK由三个核心模块组成：知识引导的指令生成、CLIP风格的OCT-生物标志物对齐和监督的指令微调，它们共同建立了定量到定性的诊断思维链，在生成详细的病变注释时反映了真实的临床推理。为了评估我们的方法，我们引入了接地眼科理解基准，该基准涵盖六种疾病类别和三项任务：宏观层面的诊断分类、报告生成质量以及对生成的思想链的细粒度临床评估。实验表明，仅对7 B参数Qwen 2主干进行LoRA（低等级自适应）微调，CROK在报告质量和细粒度临床指标方面都优于可比的7 B和32 B基线，甚至超过OpenAI o3。代码和数据在CROK存储库中公开可用。</p>
<div class="markdown-heading"><h2 class="heading-element">用于低延迟多智能体推理的楼梯流媒体</h2><a id="user-content-用于低延迟多智能体推理的楼梯流媒体" class="anchor" aria-label="Permalink: 用于低延迟多智能体推理的楼梯流媒体" href="#用于低延迟多智能体推理的楼梯流媒体"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05059v1宣布类型：新摘要：大型语言模型（LLM）的最新进展为利用多个LLM的集体专业知识开辟了新的方向。这些方法（例如Mixture-of-Agents）通常采用额外的推理步骤来生成中间输出，然后用于生成最终响应。虽然多代理推理可以提高响应质量，但它会显着增加到第一个令牌的时间（TTFT），对延迟敏感的应用程序构成挑战并损害用户体验。为了解决这个问题，我们提出了用于低延迟多代理推理的阶梯流。我们不再等待之前步骤的完整中间输出，而是在收到这些步骤的部分输出后立即开始生成最终响应。实验结果表明，楼梯流在保持响应质量的同时将TTFT降低高达93%。</p>
<div class="markdown-heading"><h2 class="heading-element">资源受限移动设备上微调LLM的内存高效反向传播</h2><a id="user-content-资源受限移动设备上微调llm的内存高效反向传播" class="anchor" aria-label="Permalink: 资源受限移动设备上微调LLM的内存高效反向传播" href="#资源受限移动设备上微调llm的内存高效反向传播"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.03425v1宣布类型：新摘要：使用反向传播\textemdash对大型语言模型（LLM）进行微调，即使是针对LoRA\textemdash等参数子集，也可能比推理更消耗内存，并且通常被认为对于资源受限的移动设备来说是不切实际的。替代方法，例如零阶优化（Zero），可以大大减少内存占用，但代价是模型收敛速度明显减慢（比反向传播多10 $\乘$到100 $\乘$）。我们提出了一种在移动设备上实现反向传播（MeBP）的内存高效实现，该实现在内存使用和计算时间之间提供更好的权衡，同时比Zero基线更快地收敛并实现更好的性能。我们在iPhone 15 Pro Max上验证了MeBP的有效性，并表明可以使用不到1 GB的内存对从0.5B到4 B参数的各种LLM进行微调。我们在<a href="https://github.com/apple/ml-mebp%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86MeBP%E5%AE%9E%E6%96%BD%E7%9A%84%E7%A4%BA%E4%BE%8B%E3%80%82">https://github.com/apple/ml-mebp上发布了MeBP实施的示例。</a></p>
<div class="markdown-heading"><h2 class="heading-element">DoRAN：通过噪音注入和辅助网络稳定权重分解的低等级自适应</h2><a id="user-content-doran通过噪音注入和辅助网络稳定权重分解的低等级自适应" class="anchor" aria-label="Permalink: DoRAN：通过噪音注入和辅助网络稳定权重分解的低等级自适应" href="#doran通过噪音注入和辅助网络稳定权重分解的低等级自适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.04331v1宣布类型：新摘要：参数高效微调（PEFT）方法已成为适应大规模模型的标准范式。在这些技术中，权重分解低等级自适应（DoRA）已被证明可以通过将预训练的权重显式分解为幅度和方向分量来提高香草低等级自适应（LoRA）方法的学习能力和训练稳定性。在这项工作中，我们提出了DoRAN，这是DoRA的一种新变体，旨在进一步稳定训练并提高DoRA的样本效率。我们的方法包括两个关键阶段：（i）将噪音注入DoRA权重分解的分母中，它充当自适应正规化器来减轻不稳定性;（ii）用动态生成静态低阶矩阵的辅助网络替换静态低阶矩阵，实现跨层的参数耦合，并在理论和实践中产生更好的样本效率。针对视觉和语言基准的全面实验表明，DoRAN始终优于LoRA、DoRA和其他PEFT基线。这些结果强调了通过基于噪音的正规化实现稳定与基于网络的参数生成相结合的有效性，为基础模型的稳健和高效的微调提供了一个有希望的方向。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>