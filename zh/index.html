<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">vAttention：已验证稀疏注意力</h2><a id="user-content-vattention已验证稀疏注意力" class="anchor" aria-label="Permalink: vAttention：已验证稀疏注意力" href="#vattention已验证稀疏注意力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05688v1宣布类型：新摘要：用于减少解码延迟的最先进的稀疏注意力方法分为两大类：大约top-$k$（及其扩展top-$p$）和最近引入的基于采样的估计。然而，这些方法在逼近完全注意力的能力方面从根本上受到限制：它们未能在头部和查询载体之间提供一致的逼近，而且最重要的是，缺乏对逼近质量的保证，限制了它们的实际部署。我们观察到top-$k$和随机抽样是互补的：当注意力分数由少数代币主导时，top-$k$表现良好，而当注意力分数相对均匀时，随机抽样提供了更好的估计。基于这一见解并利用抽样的统计保证，我们引入了vAttention，这是第一个实用的稀疏注意机制，具有用户指定的$（\，\delta）$保证逼近准确性（因此，经过验证）。这些保证使vAttention朝着实际、可靠的大规模部署稀疏注意力迈出了令人信服的一步。通过统一top-k和采样，vAttention的性能优于两者，提供了卓越的质量效率权衡。我们的实验表明，vAttention显着提高了稀疏注意力的质量（例如，$\sim$4.5个百分点（RULER-HARD上的Llama-3.1-8B-Inst和Deepseek-R1-Distill-Llama-8B），并有效地弥合了充分关注和稀疏关注之间的差距（例如，在整个数据集中，它以高达20倍的稀疏度匹配完整模型质量）。我们还证明，它可以部署在推理场景中，以实现快速解码，而不损害模型质量（例如，vAttention在AIME 2024上以10倍的稀疏度实现了完整模型质量，最多可达32 K个代币代）。代码在<a href="https://github.com/xAlg-ai/sparse-attention-hub%E4%B8%8A%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/xAlg-ai/sparse-attention-hub上开放源代码。</a></p>
<div class="markdown-heading"><h2 class="heading-element">训练动态影响训练后量化稳健性</h2><a id="user-content-训练动态影响训练后量化稳健性" class="anchor" aria-label="Permalink: 训练动态影响训练后量化稳健性" href="#训练动态影响训练后量化稳健性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06213v1宣布类型：新摘要：虽然训练后量化被广泛采用来高效部署大型语言模型，但量化稳健性的基础机制仍不清楚。我们对开源语言模型训练轨迹（高达32 B参数和15 T训练令牌）的量化退化进行了全面分析，以准确评估训练动态和量化性能之间的关系。我们的关键发现是，大规模训练运行中的量化误差是由学习率和其他训练超参数之间复杂的相互作用驱动的。具体来说，一旦学习率下降，验证损失和量化误差就会出现分歧，这在很大程度上与训练数据规模无关。为了研究对训练动态的干预并识别可以有利地调节量化稳健性的特定配置，我们在多达100 B个令牌的受控实验中训练我们自己的模型。我们的结果挑战了增加数据集规模本质上会损害量化有效性的假设，相反，这表明战略训练超参数干预可以大规模提高量化质量。</p>
<div class="markdown-heading"><h2 class="heading-element">起草、验证和改进：迈向培训意识的推测解码</h2><a id="user-content-起草验证和改进迈向培训意识的推测解码" class="anchor" aria-label="Permalink: 起草、验证和改进：迈向培训意识的推测解码" href="#起草验证和改进迈向培训意识的推测解码"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05421v1宣布类型：新摘要：自回归（AR）解码是大型语言模型的主要延迟瓶颈。推测解码（SD）通过让起草者提出验证者接受或拒绝的多令牌块来加速AR。然而，许多SD系统需要大量的离线培训或额外的组件。这些选择增加了数据/计算成本，并可能在分布漂移下导致起草者变得脆弱。我们引入了\RST {Draft，Verify，&amp; Improvement（DVI）}，这是一个具有训练意识的自我推测框架，将推理与持续在线学习相结合。我们将LLM划分为起草者和验证者，在生成过程中，验证者接受/拒绝决策被转换为监督信号并用于更新起草者头部。一个简单的{KL$\rightarrow$RL}计划通过在线蒸馏引导校准，然后添加奖励屏蔽交叉熵和政策上的政策梯度项，以保留无损的单模型部署。在Spec-Bench上，DVI实现了2.16美元的停顿时间加速，与EAGLE-2等SoTA方法相当，而用于训练的数据减少了几个数量级，并且消融表明DVI优于仅限KL的在线蒸馏。DVI证明{training-aware}自我推测可以以最小的训练费用提供最先进的无损加速。</p>
<div class="markdown-heading"><h2 class="heading-element">精确的因果注意力，减少10%的操作</h2><a id="user-content-精确的因果注意力减少10的操作" class="anchor" aria-label="Permalink: 精确的因果注意力，减少10%的操作" href="#精确的因果注意力减少10的操作"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05175v1宣布类型：新摘要：我们提出了快速因果注意力（FCA），这是一种只需减少10%的操作即可计算精确的因果注意力的算法。FCA加速了一类特殊的矩阵相乘，其中一个操作数或输出矩阵是上三角或下三角的。这包括因果注意力向前和向后传递的所有操作，例如掩蔽产品$\mathrm{Mass}（QK^{T}）$。对于图形处理器上的这些矩阵相乘，FCA比默认的PyTorch实现和Triton编译的内核达到了明显的加速。FCA建立在通过机器学习和组合搜索发现的代数等式之上。</p>
<div class="markdown-heading"><h2 class="heading-element">PER：通过基础蒸馏和动态路由进行机器人学习的Vision Expert Transformer</h2><a id="user-content-per通过基础蒸馏和动态路由进行机器人学习的vision-expert-transformer" class="anchor" aria-label="Permalink: PER：通过基础蒸馏和动态路由进行机器人学习的Vision Expert Transformer" href="#per通过基础蒸馏和动态路由进行机器人学习的vision-expert-transformer"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05213v1宣布类型：新摘要：预训练的视觉基础模型（VFM）通过丰富的视觉表示来推进机器人学习，但单个VFM通常仅在特定领域表现出色，从而限制了任务的通用性。将多个VFM提炼成统一的政策表示可以减轻这一限制，但通常会产生不灵活的特定任务功能选择，并且需要昂贵的全面重新培训才能融入机器人领域知识。我们提出VAR，一种用于机器人学习的Vision Expert Transformer。在预训练期间，VER将多个VFM提取到视觉专家库中。然后，它只微调轻量级路由网络（少于0.4%的参数），以从预先训练的库中动态选择与任务相关的专家来执行下游机器人任务。我们进一步引入带课程Top-K Annealing的Patchwise专家路由，以提高动态专家选择的灵活性和精确性。此外，VIA支持参数高效的微调，以实现可扩展的专家利用和自适应机器人领域知识集成。在17个不同的机器人任务和多个政策负责人中，PER实现了最先进的性能。我们发现VAR减少了任务无关区域中的大规范异常值（例如，背景）并专注于任务关键型区域。可视化和代码可在<a href="https://yixiaowang7.github.io/ver_page/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82" rel="nofollow">https://yixiaowang7.github.io/ver_page/上找到。</a></p>
<div class="markdown-heading"><h2 class="heading-element">AMAQ：自适应混合位激活量化，用于协作参数高效微调</h2><a id="user-content-amaq自适应混合位激活量化用于协作参数高效微调" class="anchor" aria-label="Permalink: AMAQ：自适应混合位激活量化，用于协作参数高效微调" href="#amaq自适应混合位激活量化用于协作参数高效微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05468v1宣布类型：新摘要：大型语言模型（LLM）正在迅速扩展，为协作服务器客户端分布式培训带来了重大挑战，特别是在通信效率和计算管理费用方面。为了应对这些挑战，我们实施了参数高效的拆分学习，可以有效平衡低资源设备上的协作培训的效率和性能。   为了减少协作训练中的通信负担，我们引入了自适应混合位激活量化（AMAQ），这是一种将激活和梯度从高精度（6至8位）逐步压缩到低精度（3至4位）的策略。AMAQ通过使用位正规化基于特征和分层重要性在通道之间有效分配位预算来实现这一目标。   在相同的位预算下，AMAQ优于固定精度方法，为LLaMA 3 8B和Qwen 2.5 7B等模型提供约2.5%的生成准确性和约1.3%的分类准确性。此外，它还显着增强了训练稳定性，减少了训练期间超低位表示崩溃。   实验表明，AMAQ有效集成到实际的多机协同训练设置中，提供卓越的推理准确性，而训练期间的比特自适应通信负担也很小。这种权衡使AMAQ成为实用有效的协作培训解决方案，且通信成本最低。</p>
<div class="markdown-heading"><h2 class="heading-element">多模式对齐基础模型的表示潜力：调查</h2><a id="user-content-多模式对齐基础模型的表示潜力调查" class="anchor" aria-label="Permalink: 多模式对齐基础模型的表示潜力：调查" href="#多模式对齐基础模型的表示潜力调查"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05184v1宣布类型：新摘要：基础模型通过对不同数据的大规模预训练来学习高度可转移的表示。越来越多的研究表明，这些表示在架构和模式之间表现出显着程度的相似性。在这项调查中，我们研究了基础模型的表示潜力，将其定义为其习得的表示在单一模式中捕获特定任务信息的潜在能力，同时还为跨模式的对齐和统一提供可转移的基础。我们首先回顾具有代表性的基金会模型和使一致性可衡量的关键指标。然后，我们综合了来自视觉、语言、言语、多模式和神经科学研究的代表潜力的经验证据。证据表明，基础模型经常在其表示空间中表现出结构性和语义性，将它们定位为跨模式转移和对齐的有力候选者。我们进一步分析培养代表潜力的关键因素，讨论悬而未决的问题并强调潜在的挑战。</p>
<div class="markdown-heading"><h2 class="heading-element">PatternKV：扁平KV表示扩展量化余量</h2><a id="user-content-patternkv扁平kv表示扩展量化余量" class="anchor" aria-label="Permalink: PatternKV：扁平KV表示扩展量化余量" href="#patternkv扁平kv表示扩展量化余量"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05176v1宣布类型：新摘要：自回归LLM中的KV缓存消除了冗余的重新计算，但已成为推理期间的主要内存和带宽瓶颈，特别是在长上下文和测试时间扩展的情况下。KV量化是降低缓存成本的关键杠杆，但由于原生KV分布缺乏平坦性，因此保持了较宽的量化范围，准确性急剧下降。之前的工作重点是隔离异常值，这限制了它们的误差，但未能拉平总体分布，从而导致性能在低位设置下脆弱。在这项工作中，我们表明K缓存保持了一个稳定的结构，该结构随着上下文逐渐演变，而V缓存则携带潜在的语义预设。基于这些见解，我们提出了PatternKV，这是一种模式对齐的残余量化方案。它在线挖掘代表性模式载体，将每个KV载体与其最近的模式对齐，并仅量化剩余。这种对KV分布的重塑使量化目标量化并缩小其范围，从而提高了低位KV量化的保真度。在多个主干上的长上下文和测试时扩展设置中，PatternKW提供一致的2位增长，相对于FP 16平均4位下降0.08%，将测试时扩展准确性平均提高10%，并将吞吐量提高1.4倍，同时支持1.25倍更大的批次。</p>
<div class="markdown-heading"><h2 class="heading-element">ThomomTS：用于时间序列和语言分析的多模式可观察性数据集</h2><a id="user-content-thomomts用于时间序列和语言分析的多模式可观察性数据集" class="anchor" aria-label="Permalink: ThomomTS：用于时间序列和语言分析的多模式可观察性数据集" href="#thomomts用于时间序列和语言分析的多模式可观察性数据集"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06063v1宣布类型：新摘要：现代企业在监控复杂系统时会生成大量的时间序列指标流，称为可观察性数据。与来自天气等领域的传统时间序列不同，可观测性数据是零膨胀的、高度随机的，并且呈现出最小的时间结构。尽管可观察性数据集很重要，但由于专有限制，在公共基准中的代表性不足。现有的数据集通常被匿名化和规范化，删除规模信息并限制其用于预测以外的任务，例如异常检测、根本原因分析和多模式推理。为了解决这一差距，我们引入了ThomomTS，这是一个源自5G电信网络的大规模可观测性数据集。ThomomTS具有具有显式规模信息的异类、去匿名协变量，并支持一系列下游任务，包括异常检测、根本原因分析和需要多模式推理的问答基准。对最先进的时间序列、语言和推理模型进行基准测试表明，现有方法难以应对可观测性数据的突然性、有噪音和高方差动态。我们的实验还强调了保留协变量绝对规模的重要性，强调了对基础时间序列模型的必要性，这些模型天生利用规模信息来实现实际的可观察性应用。</p>
<div class="markdown-heading"><h2 class="heading-element">MetaVLA：统一Meta联合训练，实现高效的预定适应</h2><a id="user-content-metavla统一meta联合训练实现高效的预定适应" class="anchor" aria-label="Permalink: MetaVLA：统一Meta联合训练，实现高效的预定适应" href="#metavla统一meta联合训练实现高效的预定适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05580v1宣布类型：新摘要：视觉-语言-动作（VLA）模型在体现推理方面表现出了希望，但与真正的通才主义者相去甚远-它们通常需要针对特定任务的微调，并且对未见任务的概括性较差。我们提出了MetaVLA，这是一个统一的、主干不可知的训练后框架，用于高效且可扩展的对齐。MetaVLA引入了上下文感知Meta Co-Training，将不同的目标任务整合到一个微调阶段中，同时利用结构上不同的辅助任务来改进领域内概括。与天真的多任务SFT不同，MetaVLA集成了一种轻量级的元学习机制（源自Attentive Neural Process），可以在最小的架构更改或推理费用的情况下快速适应不同的上下文。在LIBERO基准测试中，具有六个辅助任务的MetaVLA在长期任务上的性能比OpenVLA高出8.0%，将训练步骤从240 K减少到75 K，并将图形处理器时间缩短约76%。这些结果表明，可扩展、低资源的后训练是可行的，为通用的具体化代理铺平了道路。代码将可用。</p>
<div class="markdown-heading"><h2 class="heading-element">D2 E：扩展桌面数据的视觉行动预训练，以传输到预定的人工智能</h2><a id="user-content-d2-e扩展桌面数据的视觉行动预训练以传输到预定的人工智能" class="anchor" aria-label="Permalink: D2 E：扩展桌面数据的视觉行动预训练，以传输到预定的人工智能" href="#d2-e扩展桌面数据的视觉行动预训练以传输到预定的人工智能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05684v1宣布类型：新摘要：大型语言模型利用互联网规模的文本数据，但具体化的人工智能仍然受到物理轨迹收集高昂成本的限制。桌面环境（尤其是游戏）提供了一个令人信服的替代方案：它们大规模提供丰富的感觉运动交互，同时保持对体现学习至关重要的结构化观察-动作耦合。我们提出了D2 E（桌面到虚拟人工智能），这是一个框架，演示桌面交互可以作为机器人体现人工智能任务的有效预训练基础。与之前保持特定于领域的工作不同（例如，VPT for Minecraft）或保留数据专有（例如，SIM），D2 E建立了从可扩展桌面数据收集到具体域中验证传输的完整管道。我们的框架由三个组件组成：（1）OWA工具包，将各种桌面交互统一为具有152倍压缩的标准化格式，（2）Generalist-IDM，通过基于时间戳的事件预测，在未见过的游戏中实现强大的零射击概括，从而实现互联网规模的伪标签，以及（3）VAPT，将桌面预训练的表示转移到物理操纵和导航。使用1.3K+小时的数据（259小时的人类演示和1 K+小时的伪标记游戏玩法），我们在LIBERO操作上实现了总计96.6%的成功率，在CANVAS导航基准上实现了83.3%的成功率。这验证了数字交互中的感觉运动基元表现出足够的不变性，可以有意义地转移到物理具体任务，从而将桌面预训练建立为机器人的实用范式。我们将公开我们的所有工作，包括OWA工具包、人类收集和伪标记的数据集以及VAPT训练模型，可在<a href="https://worv-ai.github.io/d2e/%E4%B8%8A%E8%8E%B7%E5%8F%96" rel="nofollow">https://worv-ai.github.io/d2e/上获取</a></p>
<div class="markdown-heading"><h2 class="heading-element">缩小规模并受到损害？：评估模型压缩的可靠性</h2><a id="user-content-缩小规模并受到损害评估模型压缩的可靠性" class="anchor" aria-label="Permalink: 缩小规模并受到损害？：评估模型压缩的可靠性" href="#缩小规模并受到损害评估模型压缩的可靠性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06125v1宣布类型：新摘要：在现实世界的应用程序中，计算约束通常需要通过模型压缩将大型模型转换为更小、更高效的版本。虽然这些技术的目标是在不牺牲性能的情况下减少大小和计算成本，但它们的评估传统上专注于大小和准确性之间的权衡，而忽视了模型忠实性方面。这种有限的观点对于医疗保健、金融和刑事司法等高风险领域来说是不够的，这些领域的压缩模型必须保持忠实于原始模型的行为。本文提出了一种新的方法来评估压缩模型中的忠实度，超越了标准指标。我们引入并演示了一组忠实度指标，这些指标捕捉模型行为在压缩后的变化。我们的贡献包括引入使用模型协议评估原始模型和压缩模型之间预测一致性的技术，并应用卡方检验来检测整个数据集和人口分组中预测模式的统计学显着变化，从而揭示汇总公平指标可能掩盖的变化。我们通过对在三个不同且具有社会意义的数据集上训练的人工神经网络（ANN）应用量化和修剪来展示我们的方法。我们的研究结果表明，高准确性并不能保证忠实性，我们的统计测试可以检测到标准指标（例如准确性和均衡赔率）所忽视的微妙但显着的变化。拟议的指标提供了一种实用且更直接的方法，可确保通过压缩获得的效率收益不会损害值得信赖的人工智能所必需的公平性或忠实性。</p>
<div class="markdown-heading"><h2 class="heading-element">DP-Adam-AC：使用Adam优化和自适应剪辑对可本地化语言模型进行隐私保护微调</h2><a id="user-content-dp-adam-ac使用adam优化和自适应剪辑对可本地化语言模型进行隐私保护微调" class="anchor" aria-label="Permalink: DP-Adam-AC：使用Adam优化和自适应剪辑对可本地化语言模型进行隐私保护微调" href="#dp-adam-ac使用adam优化和自适应剪辑对可本地化语言模型进行隐私保护微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05288v1宣布类型：新摘要：ChatGPT等大型语言模型（LLM）已发展成为强大且无处不在的工具。对小型数据集的微调使LLM能够有效地获得特定任务的专业技能。尽管LLM在一般和特定任务用例中都提供了很大的实用性，但它们受到两个安全相关问题的限制。首先，传统的LLM硬件要求使它们无法在消费级设备上本地运行。通常需要与LLM提供商的服务器建立远程网络连接，这使得系统容易受到网络攻击。其次，针对敏感任务微调LLM可能涉及敏感数据。非私有微调算法产生的模型容易受到训练数据复制攻击。我们的工作通过增强差异私密优化算法并将其应用于微调可本地化语言模型来解决这些安全问题。我们对标准DP-Adam优化器引入了可适应的梯度剪裁以及其他工程增强，以创建DP-Adam-AC。我们使用优化器来微调两种可本地化LLM设计的示例，即小型语言模型（Qwen 2.5 -0.5B）和1.58位量化（Bitnet-b1.58-2B）。我们通过对两个合成数据集的实验证明了损失方面的有希望的改善。</p>
<div class="markdown-heading"><h2 class="heading-element">门口的野蛮人：人工智能如何颠覆系统研究</h2><a id="user-content-门口的野蛮人人工智能如何颠覆系统研究" class="anchor" aria-label="Permalink: 门口的野蛮人：人工智能如何颠覆系统研究" href="#门口的野蛮人人工智能如何颠覆系统研究"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.06189v1宣布类型：新摘要：人工智能（AI）正在通过自动发现新解决方案来改变我们所知的研究过程。给定一项任务，典型的人工智能驱动方法是（i）生成一组不同的解决方案，然后（ii）验证这些解决方案并选择解决问题的解决方案。至关重要的是，这种方法假设存在可靠的验证器，即，它可以准确地确定解决方案是否解决了给定问题。我们认为，系统研究长期以来专注于设计和评估新的面向性能的算法，特别适合人工智能驱动的解决方案发现。这是因为系统性能问题自然需要可靠的验证器：解决方案通常在真实的系统或模拟器中实现，验证简化为针对预定义的工作负载运行这些软件工件并测量性能。我们将这种方法称为人工智能驱动的系统研究（ADRS），它迭代地生成、评估和完善解决方案。使用penEvolve（一个现有的开源ADRS实例），我们提供了跨不同领域的案例研究，包括多区域云调度的负载平衡、专家混合推理、基于LLM的SQL查询和事务调度。在多种情况下，ADRS发现了优于最先进人类设计的算法（例如，实现高达5.0倍的运行时间改进或50%的成本降低）。我们为现有框架提炼出指导算法进化的最佳实践，从即时设计到评估器构建。然后，我们讨论了对系统界更广泛的影响：随着人工智能在算法设计中发挥核心作用，我们认为人类研究人员将越来越关注问题制定和战略指导。我们的结果凸显了人工智能时代的颠覆性潜力和调整系统研究实践的迫切需要。</p>
<div class="markdown-heading"><h2 class="heading-element">超越整体奖励：MLLM一致的混合和多方面奖励优化</h2><a id="user-content-超越整体奖励mllm一致的混合和多方面奖励优化" class="anchor" aria-label="Permalink: 超越整体奖励：MLLM一致的混合和多方面奖励优化" href="#超越整体奖励mllm一致的混合和多方面奖励优化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05283v1宣布类型：新摘要：将多模式大型语言模型（MLLM）与人类偏好保持一致通常依赖于单信号、基于模型的奖励方法。这种单一奖励通常缺乏跨特定领域任务的信心校准，无法捕捉人类偏好的各个方面，并且需要大量的数据注释和奖励模型训练。在这项工作中，我们提出了一个混合奖励建模框架，它集成了互补的奖励范例：（i）基于模型的奖励，其中学习的奖励模型从合成和人类反馈中预测标量或矢量分数，以及（ii）基于规则的奖励，其中特定领域的奖励提供了明确的正确性信号。除了准确性之外，我们还引入了多方面的奖励来加强对指令的遵守，并引入了广义长度惩罚奖励来稳定训练和提高性能。该框架提供了一种灵活有效的方法，通过强化学习策略优化来调整MLLM。我们的实验表明，在不同的多模态基准应用混合和多方面的奖励建模时，一致的改进。我们在3B系列中表现最好的模型在一般和数学推理任务中实现了约9.5%的总体平均改进。该模型特别关注数学基准，实现了~ 16%的显着平均改进，凸显了其在数学推理和解决问题方面的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">关注混合注意力：解决转换方法中的问题</h2><a id="user-content-关注混合注意力解决转换方法中的问题" class="anchor" aria-label="Permalink: 关注混合注意力：解决转换方法中的问题" href="#关注混合注意力解决转换方法中的问题"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05901v1宣布类型：新摘要：尽管性能出色，变形金刚的二次计算复杂性限制了其可扩展性。虽然线性注意力将其降低为线性复杂性，但在大多数情况下，从头开始预训练此类模型仍然昂贵得令人望而却步。最近的训练后线性化方法将预先训练的变形金刚有效地转换为线性模型，通常使用将线性注意力与滑动窗口softmax相结合的混合方法。我们发现了一个关键缺陷：现有的混合方法无意中绕过了线性分量，几乎完全依赖SWA。学生级诊断揭示了这种以前未检测到的行为源于常识基准上被忽视的评估实践。我们提出了三种解决方案来确保均衡的组件使用：（i）纯线性转换与滑动窗口softmax的推理时混合;（ii）HedgeCATs，将注意力权重转移与有针对性的LoRA微调相结合;和（iii）预定滑动窗口Dropout（SSD），它在训练期间随机抑制softmax分支，以防止组件崩溃。我们的方法在保持计算效率的同时恢复大多数基本模型性能并确保真正的线性注意力采用，恢复混合转换中性能属性的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">KVLinC：具有Hadamard旋转和线性纠正的KV缓存量化</h2><a id="user-content-kvlinc具有hadamard旋转和线性纠正的kv缓存量化" class="anchor" aria-label="Permalink: KVLinC：具有Hadamard旋转和线性纠正的KV缓存量化" href="#kvlinc具有hadamard旋转和线性纠正的kv缓存量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05373v1宣布类型：新摘要：量化关键字-值（KV）缓存是提高大型语言模型（LLM）推理效率的一种有前途的策略。然而，激进量化到非常低的精确度（例如，2位）在存储的密钥和值张量中引入了重大错误，这些错误通过点积注意力机制传播并最终降低生成质量。为了解决这个问题，我们提出了KVLinC，这是一个用于减轻极低精确度下KV缓存量化引入的注意力错误的框架。KVLinC结合了Hadamard旋转（可以减少值的量化误差）与轻量级线性纠正适配器（可以显式补偿量化密钥引入的误差）。在对LLaMA、Qwen 2.5和Qwen 3型号系列的广泛评估中，KVLinC始终符合或超越强基线，同时实现更高的KV缓存压缩。此外，我们实现了一个自定义注意力内核，与Flash注意力基线相比，该内核的推理速度可提高2.55倍，从而实现高效的长上下文LLM推理。</p>
<div class="markdown-heading"><h2 class="heading-element">ARMOR：通过自适应矩阵分解的高性能半结构化修剪</h2><a id="user-content-armor通过自适应矩阵分解的高性能半结构化修剪" class="anchor" aria-label="Permalink: ARMOR：通过自适应矩阵分解的高性能半结构化修剪" href="#armor通过自适应矩阵分解的高性能半结构化修剪"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05528v1宣布类型：新摘要：大型语言模型（LLM）因其巨大的计算和内存需求而面临巨大的部署挑战。虽然半结构化修剪，尤其是2：4稀疏性，提供了实现实际硬件加速的途径，但现有方法通常会导致性能大幅下降。为了弥合这一差距，我们引入了ARMOR：（带矩阵因子化的自适应表示），这是一种新型的一次性训练后修剪算法。ARMOR不是直接修剪权重，而是将每个权重矩阵分解为一个2：4稀疏核心，由两个低开销的块对角矩阵包裹。与传统的2：4修剪技术相比，这些包装器充当高效的转换前和转换后错误纠正器，提供更大的灵活性来保持模型质量。稀疏核心和块对角线包装器是通过块坐标下降算法来选择的，该算法最大限度地减少分层代理损失。我们从理论上证明了这种优化保证收敛到代理损失小于或等于最先进的修剪算法的解决方案。对美洲驼的实验（Touvron等人，2023年;杜比等人，2024年）和Qwen（Yang等人，2025）模型系列表明，ARMOR在广泛的下游任务和困惑度评估中始终且显着优于最先进的2：4修剪方法。ARMOR实现了这种卓越的性能，同时保留了2：4修剪的推理加速和大幅减少内存使用，从而在模型压缩和任务准确性之间建立了更有效的权衡</p>
<div class="markdown-heading"><h2 class="heading-element">主动语义感知</h2><a id="user-content-主动语义感知" class="anchor" aria-label="Permalink: 主动语义感知" href="#主动语义感知"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv：2510.05430v1宣布类型：新摘要：我们开发了一种主动语义感知方法，指的是使用场景的语义来执行探索等任务。我们构建了一个紧凑、分层的多层场景图，可以在各种抽象级别上表示大型、复杂的室内环境，例如，与房间、对象、墙壁、窗户等对应的节点以及其几何形状的细粒度细节。我们开发了一个基于大型语言模型（LLM）的程序，以对未观察区域的合理场景图进行采样，这些区域与场景的部分观察一致。这些样本用于计算潜在航路点的信息收益，以进行复杂的空间推理，例如，客厅的两扇门可以通向厨房或卧室。我们在模拟的复杂、真实的3D室内环境中评估这种方法。我们使用定性和定量实验表明，我们的方法可以比基线方法更快、更准确地确定环境的语义。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>