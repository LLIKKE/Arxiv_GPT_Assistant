<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">本地化利用大型语言模型：实证结果及对人工智能个人电脑的启示</h2><a id="user-content-本地化利用大型语言模型实证结果及对人工智能个人电脑的启示" class="anchor" aria-label="Permalink: 本地化利用大型语言模型：实证结果及对人工智能个人电脑的启示" href="#本地化利用大型语言模型实证结果及对人工智能个人电脑的启示"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.15030v1 公告类型：新研究<br>
摘要：随着模型技术的进步与硬件性能的提升，大型语言模型（LLM）在边缘设备上的部署日益增多，这带来了显著的隐私优势。然而，受限于模型容量的缩减与必要的压缩技术，这些设备端LLM天然面临性能瓶颈。为此，我们提出了一套系统化评估方法论——涵盖模型能力、开发效率与系统资源三大维度——用于评估设备端LLM。通过对0.5B至14B参数规模的模型在商用笔记本上实施七种训练后量化（PTQ）方法的全面测试，我们获得以下关键发现：1）系统级指标与有效权重比特数（BPW）呈近似线性关系；2）存在约3.5有效BPW的实用阈值，低比特量化的大模型始终优于高比特精度的小模型；3）低BPW量化仅造成轻微精度损失，却能显著节省内存；4）CPU功耗由底层实现细节决定，计算密集型操作比内存密集型操作耗能更高。这些发现为资源受限的边缘设备高效部署与优化配置LLM提供了重要洞见与实践指南。代码库已开源：<a href="https://github.com/simmonssong/LLMOnDevice%E3%80%82">https://github.com/simmonssong/LLMOnDevice。</a></p>
<p>（注：根据学术文献翻译规范，专业术语如"post-training quantization (PTQ)"采用通用译法"训练后量化"；技术指标"effective bits-per-weight (BPW)"保留英文缩写但首次出现时标注全称"有效权重比特数"；数学符号"$\sim$3.5"转换为中文常用波浪线"约3.5"；GitHub链接等专有名词保留原格式）</p>
<div class="markdown-heading"><h2 class="heading-element">《极地稀疏性：利用可扩展上下文稀疏性实现高吞吐量批量LLM推理》</h2><a id="user-content-极地稀疏性利用可扩展上下文稀疏性实现高吞吐量批量llm推理" class="anchor" aria-label="Permalink: 《极地稀疏性：利用可扩展上下文稀疏性实现高吞吐量批量LLM推理》" href="#极地稀疏性利用可扩展上下文稀疏性实现高吞吐量批量llm推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.14884v1 公告类型：新研究<br>
摘要：加速大语言模型（LLM）推理对于需要高吞吐量和低延迟的实际部署至关重要。上下文稀疏性（即每个令牌动态激活仅一小部分模型参数）虽展现出潜力，但由于活跃神经元的并集会迅速逼近密集计算，难以扩展至大批量场景。我们提出极性稀疏性，揭示了一个关键现象：随着批量规模和序列长度的增加，稀疏性的重要性从MLP层转移至注意力层。虽然MLP层在批处理下计算效率提升，但其稀疏性逐渐消失；而注意力层的计算成本随规模增长急剧上升，其头部稀疏性却保持稳定且与批量无关。我们开发了硬件高效的稀疏感知GPU内核，用于选择性MLP和注意力计算，在OPT、LLaMA-2和3等模型上实现了最高达2.2倍的端到端加速，且在不同批量规模和序列长度下均保持精度无损。据我们所知，这是首个证明上下文稀疏性可有效扩展至大批量规模的研究，通过极小改动即可实现显著推理加速，使极性稀疏性适用于大规模高吞吐LLM部署系统。代码已开源：<a href="https://github.com/susavlsh10/Polar-Sparsity">https://github.com/susavlsh10/Polar-Sparsity</a></p>
<p>（注：根据学术文献翻译规范，技术术语如"MLP"（多层感知机）、"GPU kernels"（GPU内核）等保留英文缩写；数学符号"(2.2\times)"转换为中文排版习惯的"2.2倍"；长句按中文表达习惯拆分为短句，如将原文条件状语从句重组为分号连接的并列结构；项目名称"Polar Sparsity"采用直译"极性稀疏性"以保持概念一致性）</p>
<div class="markdown-heading"><h2 class="heading-element">《Quaff：基于离群值空间稳定性假设的量化参数高效微调》</h2><a id="user-content-quaff基于离群值空间稳定性假设的量化参数高效微调" class="anchor" aria-label="Permalink: 《Quaff：基于离群值空间稳定性假设的量化参数高效微调》" href="#quaff基于离群值空间稳定性假设的量化参数高效微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译时进行了以下处理：</p>
<ol>
<li>"Quaff" 保留原名不译，作为技术术语标识</li>
<li>"Quantized Parameter-Efficient Fine-Tuning" 译为"量化参数高效微调"，准确传达"量化+高效参数微调"双重技术特征</li>
<li>"Outlier Spatial Stability Hypothesis" 译为"离群值空间稳定性假设"，其中：
<ul>
<li>"Outlier" 采用统计学通用译法"离群值"</li>
<li>"Spatial Stability" 译为"空间稳定性"以保持原术语在机器学习中的特定含义</li>
</ul>
</li>
<li>整体采用"基于...假设"的学术论文标题惯用结构，符合中文技术文献命名规范）</li>
</ol>
<p>arXiv:2505.14742v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）在各领域取得了令人振奋的成就，但其在资源受限的个人设备上的部署仍受限于任务特定微调所需的巨大计算与内存开销。虽然量化技术提供了效率提升的路径，但现有方法难以平衡性能与开销——要么导致高昂的计算/内存成本，要么无法解决激活值异常值这一量化微调中的关键瓶颈。针对这些挑战，我们提出<strong>异常值空间稳定性假说（OSSH）</strong>：在微调过程中，某些激活异常通道会保持稳定的空间位置。基于此，我们开发了<strong>Quaff</strong>框架，这是一种面向大语言模型的量化参数高效微调方案，通过定向动量缩放优化低精度激活表示。Quaff利用轻量级操作，仅对空间位置不变的通道进行动态异常值抑制，无需全精度权重存储和全局重缩放，同时降低量化误差。在十个基准测试上的广泛实验验证了OSSH假说，并证明Quaff的有效性。具体而言，在GPQA推理基准测试中，Phi-3模型上Quaff相比全精度微调实现了1.73倍的延迟降低和30%的内存节省，同时准确率提升0.6%，在效率、性能与可部署性的三重权衡中取得突破。通过支持消费级GPU（如RTX 2080 Super）的微调且不牺牲模型性能，Quaff推动了个性化大语言模型的普惠化部署。代码已开源：<a href="https://github.com/Little0o0/Quaff.git">https://github.com/Little0o0/Quaff.git</a></p>
<p>（注：根据技术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"outlier"统一译为"异常值"而非"离群值"以符合机器学习领域惯例</li>
<li>"momentum scaling"译为"动量缩放"保留物理含义</li>
<li>"democratize"意译为"普惠化"以契合技术普惠概念</li>
<li>长复合句按中文习惯拆分为短句，如将原文"while reducing..."处理为独立分句</li>
<li>技术指标保留原始数字格式（如1.73x）符合学术文本惯例）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>