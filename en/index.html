<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 08/30/2025</h1><a id="user-content-personalized-daily-arxiv-papers-08302025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 08/30/2025" href="#personalized-daily-arxiv-papers-08302025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 32</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Safety Alignment Should Be Made More Than Just A Few Attention Heads</a>
<strong>Authors:</strong> Chao Huang, Zefeng Zhang, Juewei Yue, Quangang Li, Chuang Zhang, Tingwen Liu</p>
</li>
<li>
<p><a href="#link1">Learning Primitive Embodied World Models: Towards Scalable Robotic Learning</a>
<strong>Authors:</strong> Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu</p>
</li>
<li>
<p><a href="#link2">Photonic restricted Boltzmann machine for content generation tasks</a>
<strong>Authors:</strong> Li Luo, Yisheng Fang, Wanyi Zhang, Zhichao Ruan</p>
</li>
<li>
<p><a href="#link3">CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference</a>
<strong>Authors:</strong> Guanyu Xu, Zhiwei Hao, Li Shen, Yong Luo, Fuhui Sun, Xiaoyan Wang, Han Hu, Yonggang Wen</p>
</li>
<li>
<p><a href="#link4">Turning Tabular Foundation Models into Graph Foundation Models</a>
<strong>Authors:</strong> Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova</p>
</li>
<li>
<p><a href="#link5">Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments</a>
<strong>Authors:</strong> Liding Zhang, Zeqi Li, Kuanqi Cai, Qian Huang, Zhenshan Bing, Alois Knoll</p>
</li>
<li>
<p><a href="#link6">Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception</a>
<strong>Authors:</strong> Yang Li, Quan Yuan, Guiyang Luo, Xiaoyuan Fu, Rui Pan, Yujia Yang, Congzhang Shao, Yuewen Liu, Jinglin Li</p>
</li>
<li>
<p><a href="#link7">SemSR: Semantics aware robust Session-based Recommendations</a>
<strong>Authors:</strong> Jyoti Narwariya, Priyanka Gupta, Muskan Gupta, Jyotsana Khatri, Lovekesh Vig</p>
</li>
<li>
<p><a href="#link8">Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning</a>
<strong>Authors:</strong> Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Jonathan Kua, Imran Razzak, Dung Nguyen, Saeid Nahavandi</p>
</li>
<li>
<p><a href="#link9">Beacon: Post-Training Quantization with Integrated Grid Selection</a>
<strong>Authors:</strong> Shihao Zhang, Rayan Saab</p>
</li>
<li>
<p><a href="#link10">SoK: Large Language Model Copyright Auditing via Fingerprinting</a>
<strong>Authors:</strong> Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin</p>
</li>
<li>
<p><a href="#link11">FORGE: Foundational Optimization Representations from Graph Embeddings</a>
<strong>Authors:</strong> Zohair Shafi, Serdar Kadioglu</p>
</li>
<li>
<p><a href="#link12">FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control</a>
<strong>Authors:</strong> Tan Jing, Shiting Chen, Yangfan Li, Weisheng Xu, Renjing Xu</p>
</li>
<li>
<p><a href="#link13">To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software</a>
<strong>Authors:</strong> Lo√Øc Stratil, Felix Fent, Esteban Rivera, Markus Lienkamp</p>
</li>
<li>
<p><a href="#link14">Survey of Specialized Large Language Model</a>
<strong>Authors:</strong> Chenghan Yang, Ruiyu Zhao, Yang Liu, Ling Jiang</p>
</li>
<li>
<p><a href="#link15">Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge</a>
<strong>Authors:</strong> Maha Shatta, Konstantinos Balaskas, Paula Carolina Lozano Duarte, Georgios Panagopoulos, Mehdi B. Tahoori, Georgios Zervakis</p>
</li>
<li>
<p><a href="#link16">CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification</a>
<strong>Authors:</strong> Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie</p>
</li>
<li>
<p><a href="#link17">CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems</a>
<strong>Authors:</strong> Jiaxi Huang, Yan Huang, Yixian Zhao, Wenchao Meng, Jinming Xu</p>
</li>
<li>
<p><a href="#link18">Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</a>
<strong>Authors:</strong> Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu</p>
</li>
<li>
<p><a href="#link19">RelAItionship Building: Analyzing Recruitment Strategies for Participatory AI</a>
<strong>Authors:</strong> Eugene Kim, Vaibhav Balloli, Berelian Karimian, Elizabeth Bondi-Kelly, Benjamin Fish</p>
</li>
<li>
<p><a href="#link20">CrystalICL: Enabling In-Context Learning for Crystal Generation</a>
<strong>Authors:</strong> Ruobing Wang, Qiaoyu Tan, Yili Wang, Ying Wang, Xin Wang</p>
</li>
<li>
<p><a href="#link21">WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization</a>
<strong>Authors:</strong> Eduardo Davalos, Yike Zhang, Namrata Srivastava, Yashvitha Thatigotla, Jorge A. Salas, Sara McFadden, Sun-Joo Cho, Amanda Goodwin, Ashwin TS, Gautam Biswas</p>
</li>
<li>
<p><a href="#link22">Interact-Custom: Customized Human Object Interaction Image Generation</a>
<strong>Authors:</strong> Zhu Xu, Zhaowen Wang, Yuxin Peng, Yang Liu</p>
</li>
<li>
<p><a href="#link23">Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought</a>
<strong>Authors:</strong> Lingzhe Zhang, Tong Jia, Kangjin Wang, Weijie Hong, Chiming Duan, Minghua He, Ying Li</p>
</li>
<li>
<p><a href="#link24">The Anatomy of a Personal Health Agent</a>
<strong>Authors:</strong> A. Ali Heydari, Ken Gu, Vidya Srinivas, Hong Yu, Zhihan Zhang, Yuwei Zhang, Akshay Paruchuri, Qian He, Hamid Palangi, Nova Hammerquist, Ahmed A. Metwally, Brent Winslow, Yubin Kim, Kumar Ayush, Yuzhe Yang, Girish Narayanswamy, Maxwell A. Xu, Jake Garrison, Amy Aremnto Lee, Jenny Vafeiadou, Ben Graef, Isaac R. Galatzer-Levy, Erik Schenck, Andrew Barakat, Javier Perez, Jacqueline Shreibati, John Hernandez, Anthony Z. Faranesh, Javier L. Prieto, Connor Heneghan, Yun Liu, Jiening Zhan, Mark Malhotra, Shwetak Patel, Tim Althoff, Xin Liu, Daniel McDuff, Xuhai "Orson" Xu</p>
</li>
<li>
<p><a href="#link25">Fast 3D Diffusion for Scalable Granular Media Synthesis</a>
<strong>Authors:</strong> Muhammad Moeeze Hassan, R√©gis Cottereau, Filippo Gatti, Patryk Dec</p>
</li>
<li>
<p><a href="#link26">A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation</a>
<strong>Authors:</strong> Kyungho Kim, Sunwoo Kim, Geon Lee, Kijung Shin</p>
</li>
<li>
<p><a href="#link27">Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention</a>
<strong>Authors:</strong> Zhongpan Tang</p>
</li>
<li>
<p><a href="#link28">DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search</a>
<strong>Authors:</strong> Zhibang Yang, Xinke Jiang, Rihong Qiu, Ruiqing Li, Yihang Zhang, Yue Fang, Yongxin Xu, Hongxin Ding, Xu Chu, Junfeng Zhao, Yasha Wang</p>
</li>
<li>
<p><a href="#link29">Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering</a>
<strong>Authors:</strong> Elisha Dayag, Nhat Thanh Van Tran, Jack Xin</p>
</li>
<li>
<p><a href="#link30">Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks</a>
<strong>Authors:</strong> Chen Shang, Jiadong Yu, Dinh Thai Hoang</p>
</li>
<li>
<p><a href="#link31">Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models</a>
<strong>Authors:</strong> Yuhang Liu, Tao Li, Zhehao Huang, Zuopeng Yang, Xiaolin Huang</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2508.19697v1" rel="nofollow">Safety Alignment Should Be Made More Than Just A Few Attention Heads</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-safety-alignment-should-be-made-more-than-just-a-few-attention-heads-" class="anchor" aria-label="Permalink: 0. Safety Alignment Should Be Made More Than Just A Few Attention Heads" href="#0-safety-alignment-should-be-made-more-than-just-a-few-attention-heads-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19697v1
<strong>Authors:</strong> Chao Huang, Zefeng Zhang, Juewei Yue, Quangang Li, Chuang Zhang, Tingwen Liu</p>
<p><strong>Abstract:</strong> Current safety alignment for large language models(LLMs) continues to present vulnerabilities, given that adversarial prompting can effectively bypass their safety measures.Our investigation shows that these safety mechanisms predominantly depend on a limited subset of attention heads: removing or ablating these heads can severely compromise model safety. To identify and evaluate these safety-critical components, we introduce RDSHA, a targeted ablation method that leverages the model's refusal direction to pinpoint attention heads mostly responsible for safety behaviors. Further analysis shows that existing jailbreak attacks exploit this concentration by selectively bypassing or manipulating these critical attention heads. To address this issue, we propose AHD, a novel training strategy designed to promote the distributed encoding of safety-related behaviors across numerous attention heads. Experimental results demonstrate that AHD successfully distributes safety-related capabilities across more attention heads. Moreover, evaluations under several mainstream jailbreak attacks show that models trained with AHD exhibit considerably stronger safety robustness, while maintaining overall functional utility.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2508.20840v1" rel="nofollow">Learning Primitive Embodied World Models: Towards Scalable Robotic Learning</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-learning-primitive-embodied-world-models-towards-scalable-robotic-learning-" class="anchor" aria-label="Permalink: 1. Learning Primitive Embodied World Models: Towards Scalable Robotic Learning" href="#1-learning-primitive-embodied-world-models-towards-scalable-robotic-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20840v1
<strong>Authors:</strong> Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu</p>
<p><strong>Abstract:</strong> While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2508.20472v1" rel="nofollow">Photonic restricted Boltzmann machine for content generation tasks</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-photonic-restricted-boltzmann-machine-for-content-generation-tasks-" class="anchor" aria-label="Permalink: 2. Photonic restricted Boltzmann machine for content generation tasks" href="#2-photonic-restricted-boltzmann-machine-for-content-generation-tasks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20472v1
<strong>Authors:</strong> Li Luo, Yisheng Fang, Wanyi Zhang, Zhichao Ruan</p>
<p><strong>Abstract:</strong> The restricted Boltzmann machine (RBM) is a neural network based on the Ising model, well known for its ability to learn probability distributions and stochastically generate new content. However, the high computational cost of Gibbs sampling in content generation tasks imposes significant bottlenecks on electronic implementations. Here, we propose a photonic restricted Boltzmann machine (PRBM) that leverages photonic computing to accelerate Gibbs sampling, enabling efficient content generation. By introducing an efficient encoding method, the PRBM eliminates the need for computationally intensive matrix decomposition and reduces the computational complexity of Gibbs sampling from $O(N)$ to $O(1)$. Moreover, its non-Von Neumann photonic computing architecture circumvents the memory storage of interaction matrices, providing substantial advantages for large-scale RBMs. We experimentally validate the photonic-accelerated Gibbs sampling by simulating a two-dimensional Ising model, where the observed phase transition temperature closely matches the theoretical predictions. Beyond physics-inspired tasks, the PRBM demonstrates robust capabilities in generating and restoring diverse content, including images and temporal sequences, even in the presence of noise and aberrations. The scalability and reduced training cost of the PRBM framework underscore its potential as a promising pathway for advancing photonic computing in generative artificial intelligence.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2508.20375v1" rel="nofollow">CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-coformer-collaborating-with-heterogeneous-edge-devices-for-scalable-transformer-inference-" class="anchor" aria-label="Permalink: 3. CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference" href="#3-coformer-collaborating-with-heterogeneous-edge-devices-for-scalable-transformer-inference-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20375v1
<strong>Authors:</strong> Guanyu Xu, Zhiwei Hao, Li Shen, Yong Luo, Fuhui Sun, Xiaoyan Wang, Han Hu, Yonggang Wen</p>
<p><strong>Abstract:</strong> The impressive performance of transformer models has sparked the deployment of intelligent applications on resource-constrained edge devices. However, ensuring high-quality service for real-time edge systems is a significant challenge due to the considerable computational demands and resource requirements of these models. Existing strategies typically either offload transformer computations to other devices or directly deploy compressed models on individual edge devices. These strategies, however, result in either considerable communication overhead or suboptimal trade-offs between accuracy and efficiency. To tackle these challenges, we propose a collaborative inference system for general transformer models, termed CoFormer. The central idea behind CoFormer is to exploit the divisibility and integrability of transformer. An off-the-shelf large transformer can be decomposed into multiple smaller models for distributed inference, and their intermediate results are aggregated to generate the final output. We formulate an optimization problem to minimize both inference latency and accuracy degradation under heterogeneous hardware constraints. DeBo algorithm is proposed to first solve the optimization problem to derive the decomposition policy, and then progressively calibrate decomposed models to restore performance. We demonstrate the capability to support a wide range of transformer models on heterogeneous edge devices, achieving up to 3.1$\times$ inference speedup with large transformer models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6 billion parameters on edge devices, reducing memory requirements by 76.3%. CoFormer can also reduce energy consumption by approximately 40% while maintaining satisfactory inference performance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2508.20906v1" rel="nofollow">Turning Tabular Foundation Models into Graph Foundation Models</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-turning-tabular-foundation-models-into-graph-foundation-models-" class="anchor" aria-label="Permalink: 4. Turning Tabular Foundation Models into Graph Foundation Models" href="#4-turning-tabular-foundation-models-into-graph-foundation-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20906v1
<strong>Authors:</strong> Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova</p>
<p><strong>Abstract:</strong> While foundation models have revolutionized such fields as natural language processing and computer vision, their application and potential within graph machine learning remain largely unexplored. One of the key challenges in designing graph foundation models (GFMs) is handling diverse node features that can vary across different graph datasets. Although many works on GFMs have been focused exclusively on text-attributed graphs, the problem of handling arbitrary features of other types in GFMs has not been fully addressed. However, this problem is not unique to the graph domain, as it also arises in the field of machine learning for tabular data. In this work, motivated by the recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a simple graph foundation model that employs TabPFNv2 as a backbone. Specifically, G2T-FM augments the original node features with neighborhood feature aggregation, adds structural embeddings, and then applies TabPFNv2 to the constructed node representations. Even in a fully in-context regime, our model achieves strong results, significantly outperforming publicly available GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the potential of the proposed approach. More broadly, our paper reveals a previously overlooked direction of utilizing tabular foundation models for graph machine learning tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2508.20899v1" rel="nofollow">Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-language-enhanced-mobile-manipulation-for-efficient-object-search-in-indoor-environments-" class="anchor" aria-label="Permalink: 5. Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments" href="#5-language-enhanced-mobile-manipulation-for-efficient-object-search-in-indoor-environments-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20899v1
<strong>Authors:</strong> Liding Zhang, Zeqi Li, Kuanqi Cai, Qian Huang, Zhenshan Bing, Alois Knoll</p>
<p><strong>Abstract:</strong> Enabling robots to efficiently search for and identify objects in complex, unstructured environments is critical for diverse applications ranging from household assistance to industrial automation. However, traditional scene representations typically capture only static semantics and lack interpretable contextual reasoning, limiting their ability to guide object search in completely unfamiliar settings. To address this challenge, we propose a language-enhanced hierarchical navigation framework that tightly integrates semantic perception and spatial reasoning. Our method, Goal-Oriented Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large language models (LLMs) to infer scene semantics and guide the search process through a multi-level decision hierarchy. Reliability in reasoning is achieved through the use of structured prompts and logical constraints applied at each stage of the hierarchy. For the specific challenges of mobile manipulation, we introduce a heuristic-based motion planner that combines polar angle sorting with distance prioritization to efficiently generate exploration paths. Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our framework, showing that GODHS can locate target objects with higher search efficiency compared to conventional, non-semantic search strategies. Website and Video are available at: <a href="https://drapandiger.github.io/GODHS" rel="nofollow">https://drapandiger.github.io/GODHS</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2508.19638v1" rel="nofollow">Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-beyond-bev-optimizing-point-level-tokens-for-collaborative-perception-" class="anchor" aria-label="Permalink: 6. Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception" href="#6-beyond-bev-optimizing-point-level-tokens-for-collaborative-perception-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19638v1
<strong>Authors:</strong> Yang Li, Quan Yuan, Guiyang Luo, Xiaoyuan Fu, Rui Pan, Yujia Yang, Congzhang Shao, Yuewen Liu, Jinglin Li</p>
<p><strong>Abstract:</strong> Collaborative perception allows agents to enhance their perceptual capabilities by exchanging intermediate features. Existing methods typically organize these intermediate features as 2D bird's-eye-view (BEV) representations, which discard critical fine-grained 3D structural cues essential for accurate object recognition and localization. To this end, we first introduce point-level tokens as intermediate representations for collaborative perception. However, point-cloud data are inherently unordered, massive, and position-sensitive, making it challenging to produce compact and aligned point-level token sequences that preserve detailed structural information. Therefore, we present CoPLOT, a novel Collaborative perception framework that utilizes Point-Level Optimized Tokens. It incorporates a point-native processing pipeline, including token reordering, sequence modeling, and multi-agent spatial alignment. A semantic-aware token reordering module generates adaptive 1D reorderings by leveraging scene-level and token-level semantic information. A frequency-enhanced state space model captures long-range sequence dependencies across both spatial and spectral domains, improving the differentiation between foreground tokens and background clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop process, combining global agent-level correction with local token-level refinement to mitigate localization noise. Extensive experiments on both simulated and real-world datasets show that CoPLOT outperforms state-of-the-art models, with even lower communication and computation overhead. Code will be available at <a href="https://github.com/CheeryLeeyy/CoPLOT">https://github.com/CheeryLeeyy/CoPLOT</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2508.20587v1" rel="nofollow">SemSR: Semantics aware robust Session-based Recommendations</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-semsr-semantics-aware-robust-session-based-recommendations-" class="anchor" aria-label="Permalink: 7. SemSR: Semantics aware robust Session-based Recommendations" href="#7-semsr-semantics-aware-robust-session-based-recommendations-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20587v1
<strong>Authors:</strong> Jyoti Narwariya, Priyanka Gupta, Muskan Gupta, Jyotsana Khatri, Lovekesh Vig</p>
<p><strong>Abstract:</strong> Session-based recommendation (SR) models aim to recommend items to anonymous users based on their behavior during the current session. While various SR models in the literature utilize item sequences to predict the next item, they often fail to leverage semantic information from item titles or descriptions impeding session intent identification and interpretability. Recent research has explored Large Language Models (LLMs) as promising approaches to enhance session-based recommendations, with both prompt-based and fine-tuning based methods being widely investigated. However, prompt-based methods struggle to identify optimal prompts that elicit correct reasoning and lack task-specific feedback at test time, resulting in sub-optimal recommendations. Fine-tuning methods incorporate domain-specific knowledge but incur significant computational costs for implementation and maintenance. In this paper, we present multiple approaches to utilize LLMs for session-based recommendation: (i) in-context LLMs as recommendation agents, (ii) LLM-generated representations for semantic initialization of deep learning SR models, and (iii) integration of LLMs with data-driven SR models. Through comprehensive experiments on two real-world publicly available datasets, we demonstrate that LLM-based methods excel at coarse-level retrieval (high recall values), while traditional data-driven techniques perform well at fine-grained ranking (high Mean Reciprocal Rank values). Furthermore, the integration of LLMs with data-driven SR models significantly out performs both standalone LLM approaches and data-driven deep learning models, as well as baseline SR models, in terms of both Recall and MRR metrics.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2508.20688v1" rel="nofollow">Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-task-allocation-for-autonomous-machines-using-computational-intelligence-and-deep-reinforcement-learning-" class="anchor" aria-label="Permalink: 8. Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning" href="#8-task-allocation-for-autonomous-machines-using-computational-intelligence-and-deep-reinforcement-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20688v1
<strong>Authors:</strong> Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Jonathan Kua, Imran Razzak, Dung Nguyen, Saeid Nahavandi</p>
<p><strong>Abstract:</strong> Enabling multiple autonomous machines to perform reliably requires the development of efficient cooperative control algorithms. This paper presents a survey of algorithms that have been developed for controlling and coordinating autonomous machines in complex environments. We especially focus on task allocation methods using computational intelligence (CI) and deep reinforcement learning (RL). The advantages and disadvantages of the surveyed methods are analysed thoroughly. We also propose and discuss in detail various future research directions that shed light on how to improve existing algorithms or create new methods to enhance the employability and performance of autonomous machines in real-world applications. The findings indicate that CI and deep RL methods provide viable approaches to addressing complex task allocation problems in dynamic and uncertain environments. The recent development of deep RL has greatly contributed to the literature on controlling and coordinating autonomous machines, and it has become a growing trend in this area. It is envisaged that this paper will provide researchers and engineers with a comprehensive overview of progress in machine learning research related to autonomous machines. It also highlights underexplored areas, identifies emerging methodologies, and suggests new avenues for exploration in future research within this domain.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2508.20293v1" rel="nofollow">Beacon: Post-Training Quantization with Integrated Grid Selection</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-beacon-post-training-quantization-with-integrated-grid-selection-" class="anchor" aria-label="Permalink: 9. Beacon: Post-Training Quantization with Integrated Grid Selection" href="#9-beacon-post-training-quantization-with-integrated-grid-selection-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20293v1
<strong>Authors:</strong> Shihao Zhang, Rayan Saab</p>
<p><strong>Abstract:</strong> Quantization is a widely used compression technique for reducing the memory and computation costs of large pre-trained models. A key challenge in per-channel post-training quantization (PTQ) is selecting appropriate scaling factors to replace weight values with values from a scaled quantization grid. Existing methods typically fix the scale at the outset via heuristic tuning or grid search. In this note, we propose Beacon, a simple and effective algorithm that eliminates the need for such manual tuning. Beacon performs per-channel PTQ directly using a fixed non-scaled alphabet and automatically determines the optimal scaling factors by exploiting the geometry of symmetric scalar quantization. It supports both symmetric and asymmetric quantization with minimal modifications and does not rely on back-propagation or large calibration sets. Despite its simplicity and tuning-free nature, Beacon achieves competitive performance compared to state-of-the-art methods, making it a practical solution for efficient model deployment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2508.19843v1" rel="nofollow">SoK: Large Language Model Copyright Auditing via Fingerprinting</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-sok-large-language-model-copyright-auditing-via-fingerprinting-" class="anchor" aria-label="Permalink: 10. SoK: Large Language Model Copyright Auditing via Fingerprinting" href="#10-sok-large-language-model-copyright-auditing-via-fingerprinting-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19843v1
<strong>Authors:</strong> Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin</p>
<p><strong>Abstract:</strong> The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from LLMs to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of LLM fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at <a href="https://github.com/shaoshuo-ss/LeaFBench">https://github.com/shaoshuo-ss/LeaFBench</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2508.20330v1" rel="nofollow">FORGE: Foundational Optimization Representations from Graph Embeddings</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-forge-foundational-optimization-representations-from-graph-embeddings-" class="anchor" aria-label="Permalink: 11. FORGE: Foundational Optimization Representations from Graph Embeddings" href="#11-forge-foundational-optimization-representations-from-graph-embeddings-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20330v1
<strong>Authors:</strong> Zohair Shafi, Serdar Kadioglu</p>
<p><strong>Abstract:</strong> Combinatorial optimization problems are ubiquitous in science and engineering, yet learning-based approaches to accelerate their solution often require solving a large number of hard-to-solve optimization instances to collect training data, incurring significant computational overhead. Existing methods require training dedicated models for each problem distribution for each downstream task, severely limiting their scalability and generalization. In this work, we introduce Forge, a method of pre-training a vector-quantized graph autoencoder on a large and diverse collection of mixed-integer programming (MIP) instances in an unsupervised fashion without dependency on their solution. The vector quantization process creates discrete code assignments that act as a vocabulary to represent optimization instances. We evaluate our approach under both supervised and unsupervised settings. For the unsupervised setting, we demonstrate that Forge embeddings effectively differentiate and cluster unseen instances. For the supervised setting, we fine-tune Forge embeddings and show that a single model predicts both the variables for warm-starts and integrality gaps for cut-generation across multiple problem type distributions. Both predictions help improve performance of a state-of-the-art, commercial optimization solver. Finally, we release our code and pre-trained Forge weights to encourage further research and practical use of instance-level MIP embeddings at <a href="https://github.com/skadio/forge/">https://github.com/skadio/forge/</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2508.19926v1" rel="nofollow">FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-farm-frame-accelerated-augmentation-and-residual-mixture-of-experts-for-physics-based-high-dynamic-humanoid-control-" class="anchor" aria-label="Permalink: 12. FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control" href="#12-farm-frame-accelerated-augmentation-and-residual-mixture-of-experts-for-physics-based-high-dynamic-humanoid-control-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19926v1
<strong>Authors:</strong> Tan Jing, Shiting Chen, Yangfan Li, Weisheng Xu, Renjing Xu</p>
<p><strong>Abstract:</strong> Unified physics-based humanoid controllers are pivotal for robotics and character animation, yet models that excel on gentle, everyday motions still stumble on explosive actions, hampering real-world deployment. We bridge this gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts), an end-to-end framework composed of frame-accelerated augmentation, a robust base controller, and a residual mixture-of-experts (MoE). Frame-accelerated augmentation exposes the model to high-velocity pose changes by widening inter-frame gaps. The base controller reliably tracks everyday low-dynamic motions, while the residual MoE adaptively allocates additional network capacity to handle challenging high-dynamic actions, significantly enhancing tracking accuracy. In the absence of a public benchmark, we curate the High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8% and lowers global mean per-joint position error by 14.6% relative to the baseline, while preserving near-perfect accuracy on low-dynamic motions. These results establish FARM as a new baseline for high-dynamic humanoid control and introduce the first open benchmark dedicated to this challenge. The code and dataset will be released at <a href="https://github.com/Colin-Jing/FARM">https://github.com/Colin-Jing/FARM</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2508.20892v1" rel="nofollow">To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-to-new-beginnings-a-survey-of-unified-perception-in-autonomous-vehicle-software-" class="anchor" aria-label="Permalink: 13. To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software" href="#13-to-new-beginnings-a-survey-of-unified-perception-in-autonomous-vehicle-software-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20892v1
<strong>Authors:</strong> Lo√Øc Stratil, Felix Fent, Esteban Rivera, Markus Lienkamp</p>
<p><strong>Abstract:</strong> Autonomous vehicle perception typically relies on modular pipelines that decompose the task into detection, tracking, and prediction. While interpretable, these pipelines suffer from error accumulation and limited inter-task synergy. Unified perception has emerged as a promising paradigm that integrates these sub-tasks within a shared architecture, potentially improving robustness, contextual reasoning, and efficiency while retaining interpretable outputs. In this survey, we provide a comprehensive overview of unified perception, introducing a holistic and systemic taxonomy that categorizes methods along task integration, tracking formulation, and representation flow. We define three paradigms -Early, Late, and Full Unified Perception- and systematically review existing methods, their architectures, training strategies, datasets used, and open-source availability, while highlighting future research directions. This work establishes the first comprehensive framework for understanding and advancing unified perception, consolidates fragmented efforts, and guides future research toward more robust, generalizable, and interpretable perception.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2508.19667v1" rel="nofollow">Survey of Specialized Large Language Model</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-survey-of-specialized-large-language-model-" class="anchor" aria-label="Permalink: 14. Survey of Specialized Large Language Model" href="#14-survey-of-specialized-large-language-model-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19667v1
<strong>Authors:</strong> Chenghan Yang, Ruiyu Zhao, Yang Liu, Ling Jiang</p>
<p><strong>Abstract:</strong> The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2508.19637v1" rel="nofollow">Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-invited-paper-feature-to-classifier-co-design-for-mixed-signal-smart-flexible-wearables-for-healthcare-at-the-extreme-edge-" class="anchor" aria-label="Permalink: 15. Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge" href="#15-invited-paper-feature-to-classifier-co-design-for-mixed-signal-smart-flexible-wearables-for-healthcare-at-the-extreme-edge-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19637v1
<strong>Authors:</strong> Maha Shatta, Konstantinos Balaskas, Paula Carolina Lozano Duarte, Georgios Panagopoulos, Mehdi B. Tahoori, Georgios Zervakis</p>
<p><strong>Abstract:</strong> Flexible Electronics (FE) offer a promising alternative to rigid silicon-based hardware for wearable healthcare devices, enabling lightweight, conformable, and low-cost systems. However, their limited integration density and large feature sizes impose strict area and power constraints, making ML-based healthcare systems-integrating analog frontend, feature extraction and classifier-particularly challenging. Existing FE solutions often neglect potential system-wide solutions and focus on the classifier, overlooking the substantial hardware cost of feature extraction and Analog-to-Digital Converters (ADCs)-both major contributors to area and power consumption. In this work, we present a holistic mixed-signal feature-to-classifier co-design framework for flexible smart wearable systems. To the best of our knowledge, we design the first analog feature extractors in FE, significantly reducing feature extraction cost. We further propose an hardware-aware NAS-inspired feature selection strategy within ML training, enabling efficient, application-specific designs. Our evaluation on healthcare benchmarks shows our approach delivers highly accurate, ultra-area-efficient flexible systems-ideal for disposable, low-power wearable monitoring.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2508.21046v1" rel="nofollow">CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-cogvla-cognition-aligned-vision-language-action-model-via-instruction-driven-routing--sparsification-" class="anchor" aria-label="Permalink: 16. CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification" href="#16-cogvla-cognition-aligned-vision-language-action-model-via-instruction-driven-routing--sparsification-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.21046v1
<strong>Authors:</strong> Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie</p>
<p><strong>Abstract:</strong> Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at <a href="https://github.com/JiuTian-VL/CogVLA">https://github.com/JiuTian-VL/CogVLA</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2508.20898v1" rel="nofollow">CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-cocol-a-communication-efficient-decentralized-collaborative-method-for-multi-robot-systems-" class="anchor" aria-label="Permalink: 17. CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems" href="#17-cocol-a-communication-efficient-decentralized-collaborative-method-for-multi-robot-systems-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20898v1
<strong>Authors:</strong> Jiaxi Huang, Yan Huang, Yixian Zhao, Wenchao Meng, Jinming Xu</p>
<p><strong>Abstract:</strong> Collaborative learning enhances the performance and adaptability of multi-robot systems in complex tasks but faces significant challenges due to high communication overhead and data heterogeneity inherent in multi-robot tasks. To this end, we propose CoCoL, a Communication efficient decentralized Collaborative Learning method tailored for multi-robot systems with heterogeneous local datasets. Leveraging a mirror descent framework, CoCoL achieves remarkable communication efficiency with approximate Newton-type updates by capturing the similarity between objective functions of robots, and reduces computational costs through inexact sub-problem solutions. Furthermore, the integration of a gradient tracking scheme ensures its robustness against data heterogeneity. Experimental results on three representative multi robot collaborative learning tasks show the superiority of the proposed CoCoL in significantly reducing both the number of communication rounds and total bandwidth consumption while maintaining state-of-the-art accuracy. These benefits are particularly evident in challenging scenarios involving non-IID (non-independent and identically distributed) data distribution, streaming data, and time-varying network topologies.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2508.19884v2" rel="nofollow">Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-parameter-free-structural-diversity-message-passing-for-graph-neural-networks-" class="anchor" aria-label="Permalink: 18. Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks" href="#18-parameter-free-structural-diversity-message-passing-for-graph-neural-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19884v2
<strong>Authors:</strong> Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu</p>
<p><strong>Abstract:</strong> Graph Neural Networks (GNNs) have shown remarkable performance in structured data modeling tasks such as node classification. However, mainstream approaches generally rely on a large number of trainable parameters and fixed aggregation rules, making it difficult to adapt to graph data with strong structural heterogeneity and complex feature distributions. This often leads to over-smoothing of node representations and semantic degradation. To address these issues, this paper proposes a parameter-free graph neural network framework based on structural diversity, namely SDGNN (Structural-Diversity Graph Neural Network). The framework is inspired by structural diversity theory and designs a unified structural-diversity message passing mechanism that simultaneously captures the heterogeneity of neighborhood structures and the stability of feature semantics, without introducing additional trainable parameters. Unlike traditional parameterized methods, SDGNN does not rely on complex model training, but instead leverages complementary modeling from both structure-driven and feature-driven perspectives, thereby effectively improving adaptability across datasets and scenarios. Experimental results show that on eight public benchmark datasets and an interdisciplinary PubMed citation network, SDGNN consistently outperforms mainstream GNNs under challenging conditions such as low supervision, class imbalance, and cross-domain transfer. This work provides a new theoretical perspective and general approach for the design of parameter-free graph neural networks, and further validates the importance of structural diversity as a core signal in graph representation learning. To facilitate reproducibility and further research, the full implementation of SDGNN has been released at: <a href="https://github.com/mingyue15694/SGDNN/tree/main">https://github.com/mingyue15694/SGDNN/tree/main</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2508.20176v1" rel="nofollow">RelAItionship Building: Analyzing Recruitment Strategies for Participatory AI</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-relaitionship-building-analyzing-recruitment-strategies-for-participatory-ai-" class="anchor" aria-label="Permalink: 19. RelAItionship Building: Analyzing Recruitment Strategies for Participatory AI" href="#19-relaitionship-building-analyzing-recruitment-strategies-for-participatory-ai-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20176v1
<strong>Authors:</strong> Eugene Kim, Vaibhav Balloli, Berelian Karimian, Elizabeth Bondi-Kelly, Benjamin Fish</p>
<p><strong>Abstract:</strong> Participatory AI, in which impacted community members and other stakeholders are involved in the design and development of AI systems, holds promise as a way to ensure AI is developed to meet their needs and reflect their values. However, the process of identifying, reaching out, and engaging with all relevant stakeholder groups, which we refer to as recruitment methodology, is still a practical challenge in AI projects striving to adopt participatory practices. In this paper, we investigate the challenges that researchers face when designing and executing recruitment methodology for Participatory AI projects, and the implications of current recruitment practice for Participatory AI. First, we describe the recruitment methodologies used in AI projects using a corpus of 37 projects to capture the diversity of practices in the field and perform an initial analysis on the documentation of recruitment practices, as well as specific strategies that researchers use to meet goals of equity and empowerment. To complement this analysis, we interview five AI researchers to learn about the outcomes of recruitment methodologies. We find that these outcomes are shaped by structural conditions of their work, researchers' own goals and expectations, and the relationships built from the recruitment methodology and subsequent collaboration. Based on these analyses, we provide recommendations for designing and executing relationship-forward recruitment methods, as well as reflexive recruitment documentation practices for Participatory AI researchers.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2508.20143v1" rel="nofollow">CrystalICL: Enabling In-Context Learning for Crystal Generation</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-crystalicl-enabling-in-context-learning-for-crystal-generation-" class="anchor" aria-label="Permalink: 20. CrystalICL: Enabling In-Context Learning for Crystal Generation" href="#20-crystalicl-enabling-in-context-learning-for-crystal-generation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20143v1
<strong>Authors:</strong> Ruobing Wang, Qiaoyu Tan, Yili Wang, Ying Wang, Xin Wang</p>
<p><strong>Abstract:</strong> Designing crystal materials with desired physicochemical properties remains a fundamental challenge in materials science. While large language models (LLMs) have demonstrated strong in-context learning (ICL) capabilities, existing LLM-based crystal generation approaches are limited to zero-shot scenarios and are unable to benefit from few-shot scenarios. In contrast, human experts typically design new materials by modifying relevant known structures which aligns closely with the few-shot ICL paradigm. Motivated by this, we propose CrystalICL, a novel model designed for few-shot crystal generation. Specifically, we introduce a space-group based crystal tokenization method, which effectively reduces the complexity of modeling crystal symmetry in LLMs. We further introduce a condition-structure aware hybrid instruction tuning framework and a multi-task instruction tuning strategy, enabling the model to better exploit ICL by capturing structure-property relationships from limited data. Extensive experiments on four crystal generation benchmarks demonstrate the superiority of CrystalICL over the leading baseline methods on conditional and unconditional generation tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2508.19544v1" rel="nofollow">WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-webeyetrack-scalable-eye-tracking-for-the-browser-via-on-device-few-shot-personalization-" class="anchor" aria-label="Permalink: 21. WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization" href="#21-webeyetrack-scalable-eye-tracking-for-the-browser-via-on-device-few-shot-personalization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19544v1
<strong>Authors:</strong> Eduardo Davalos, Yike Zhang, Namrata Srivastava, Yashvitha Thatigotla, Jorge A. Salas, Sara McFadden, Sun-Joo Cho, Amanda Goodwin, Ashwin TS, Gautam Biswas</p>
<p><strong>Abstract:</strong> With advancements in AI, new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement. To tackle these issues, we introduce We bEyeTrack, a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k &lt; 9). WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14. Our open-source code is available at <a href="https://github.com/RedForestAi/WebEyeTrack">https://github.com/RedForestAi/WebEyeTrack</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2508.19575v2" rel="nofollow">Interact-Custom: Customized Human Object Interaction Image Generation</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-interact-custom-customized-human-object-interaction-image-generation-" class="anchor" aria-label="Permalink: 22. Interact-Custom: Customized Human Object Interaction Image Generation" href="#22-interact-custom-customized-human-object-interaction-image-generation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19575v2
<strong>Authors:</strong> Zhu Xu, Zhaowen Wang, Yuxin Peng, Yang Liu</p>
<p><strong>Abstract:</strong> Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild application. Existing approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target entities. To enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between them. Two primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction semantics. To tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive poses. Then we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities features. Furthermore, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2508.20370v1" rel="nofollow">Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-adaptive-root-cause-localization-for-microservice-systems-with-multi-agent-recursion-of-thought-" class="anchor" aria-label="Permalink: 23. Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought" href="#23-adaptive-root-cause-localization-for-microservice-systems-with-multi-agent-recursion-of-thought-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20370v1
<strong>Authors:</strong> Lingzhe Zhang, Tong Jia, Kangjin Wang, Weijie Hong, Chiming Duan, Minghua He, Ying Li</p>
<p><strong>Abstract:</strong> As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are facing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While traces and metrics have proven to be effective data sources for this task, existing methods either heavily rely on pre-defined schemas, which struggle to adapt to evolving operational contexts, or lack interpretability in their reasoning process, thereby leaving Site Reliability Engineers (SREs) confused. In this paper, we conduct a comprehensive study on how SREs localize the root cause of failures, drawing insights from multiple professional SREs across different organizations. Our investigation reveals that human root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent, an adaptive root cause localization method for microservice systems that leverages a multi-agent recursion-of-thought framework. RCLAgent employs a novel recursion-of-thought strategy to guide the LLM's reasoning process, effectively integrating data from multiple agents and tool-assisted analysis to accurately pinpoint the root cause. Experimental evaluations on various public datasets demonstrate that RCLAgent achieves superior performance by localizing the root cause using only a single request-outperforming state-of-the-art methods that depend on aggregating multiple requests. These results underscore the effectiveness of RCLAgent in enhancing the efficiency and precision of root cause localization in complex microservice environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2508.20148v1" rel="nofollow">The Anatomy of a Personal Health Agent</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-the-anatomy-of-a-personal-health-agent-" class="anchor" aria-label="Permalink: 24. The Anatomy of a Personal Health Agent" href="#24-the-anatomy-of-a-personal-health-agent-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20148v1
<strong>Authors:</strong> A. Ali Heydari, Ken Gu, Vidya Srinivas, Hong Yu, Zhihan Zhang, Yuwei Zhang, Akshay Paruchuri, Qian He, Hamid Palangi, Nova Hammerquist, Ahmed A. Metwally, Brent Winslow, Yubin Kim, Kumar Ayush, Yuzhe Yang, Girish Narayanswamy, Maxwell A. Xu, Jake Garrison, Amy Aremnto Lee, Jenny Vafeiadou, Ben Graef, Isaac R. Galatzer-Levy, Erik Schenck, Andrew Barakat, Javier Perez, Jacqueline Shreibati, John Hernandez, Anthony Z. Faranesh, Javier L. Prieto, Connor Heneghan, Yun Liu, Jiening Zhan, Mark Malhotra, Shwetak Patel, Tim Althoff, Xin Liu, Daniel McDuff, Xuhai "Orson" Xu</p>
<p><strong>Abstract:</strong> Health is a fundamental pillar of human wellness, and the rapid advancements in large language models (LLMs) have driven the development of a new generation of health agents. However, the application of health agents to fulfill the diverse needs of individuals in daily non-clinical settings is underexplored. In this work, we aim to build a comprehensive personal health agent that is able to reason about multimodal data from everyday consumer wellness devices and common personal health records, and provide personalized health recommendations. To understand end-users' needs when interacting with such an assistant, we conducted an in-depth analysis of web search and health forum queries, alongside qualitative insights from users and health experts gathered through a user-centered design process. Based on these findings, we identified three major categories of consumer health needs, each of which is supported by a specialist sub-agent: (1) a data science agent that analyzes personal time-series wearable and health record data, (2) a health domain expert agent that integrates users' health and contextual data to generate accurate, personalized insights, and (3) a health coach agent that synthesizes data insights, guiding users using a specified psychological strategy and tracking users' progress. Furthermore, we propose and develop the Personal Health Agent (PHA), a multi-agent framework that enables dynamic, personalized interactions to address individual health needs. To evaluate each sub-agent and the multi-agent system, we conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">25. <a href="https://arxiv.org/abs/2508.19752v1" rel="nofollow">Fast 3D Diffusion for Scalable Granular Media Synthesis</a> <a id="user-content-link25"></a>
</h2><a id="user-content-25-fast-3d-diffusion-for-scalable-granular-media-synthesis-" class="anchor" aria-label="Permalink: 25. Fast 3D Diffusion for Scalable Granular Media Synthesis" href="#25-fast-3d-diffusion-for-scalable-granular-media-synthesis-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19752v1
<strong>Authors:</strong> Muhammad Moeeze Hassan, R√©gis Cottereau, Filippo Gatti, Patryk Dec</p>
<p><strong>Abstract:</strong> Simulating granular media, using Discrete Element Method is a computationally intensive task. This is especially true during initialization phase, which dominates total simulation time because of large displacements involved and associated kinetic energy. We overcome this bottleneck with a novel generative pipeline based on 3D diffusion models that directly synthesizes arbitrarily large granular assemblies in their final and physically realistic configurations. The approach frames the problem as a 3D generative modeling task, consisting of a two-stage pipeline. First a diffusion model is trained to generate independent 3D voxel grids representing granular media. Second, a 3D inpainting model, adapted from 2D inpainting techniques using masked inputs, stitches these grids together seamlessly, enabling synthesis of large samples with physically realistic structure. The inpainting model explores several masking strategies for the inputs to the underlying UNets by training the network to infer missing portions of voxel grids from a concatenation of noised tensors, masks, and masked tensors as input channels. The model also adapts a 2D repainting technique of re-injecting noise scheduler output with ground truth to provide a strong guidance to the 3D model. This along with weighted losses ensures long-term coherence over generation of masked regions. Both models are trained on the same binarized 3D occupancy grids extracted from small-scale DEM simulations, achieving linear scaling of computational time with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track synthesis equivalent to a 3-hour DEM simulation, was completed under 20 seconds. The generated voxel grids can also be post-processed to extract grain geometries for DEM-compatibility as well, enabling physically coherent, real-time, scalable granular media synthesis for industrial applications.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">26. <a href="https://arxiv.org/abs/2508.19507v2" rel="nofollow">A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation</a> <a id="user-content-link26"></a>
</h2><a id="user-content-26-a-self-supervised-mixture-of-experts-framework-for-multi-behavior-recommendation-" class="anchor" aria-label="Permalink: 26. A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation" href="#26-a-self-supervised-mixture-of-experts-framework-for-multi-behavior-recommendation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19507v2
<strong>Authors:</strong> Kyungho Kim, Sunwoo Kim, Geon Lee, Kijung Shin</p>
<p><strong>Abstract:</strong> In e-commerce, where users face a vast array of possible item choices, recommender systems are vital for helping them discover suitable items they might otherwise overlook. While many recommender systems primarily rely on a user's purchase history, recent multi-behavior recommender systems incorporate various auxiliary user behaviors, such as item clicks and cart additions, to enhance recommendations. Despite their overall performance gains, their effectiveness varies considerably between visited items (i.e., those a user has interacted with through auxiliary behaviors) and unvisited items (i.e., those with which the user has had no such interactions). Specifically, our analysis reveals that (1) existing multi-behavior recommender systems exhibit a significant gap in recommendation quality between the two item types (visited and unvisited items) and (2) achieving strong performance on both types with a single model architecture remains challenging. To tackle these issues, we propose a novel multi-behavior recommender system, MEMBER. It employs a mixture-of-experts framework, with experts designed to recommend the two item types, respectively. Each expert is trained using a self-supervised method specialized for its design goal. In our comprehensive experiments, we show the effectiveness of MEMBER across both item types, achieving up to 65.46% performance gain over the best competitor in terms of Hit Ratio@20.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">27. <a href="https://arxiv.org/abs/2508.20407v1" rel="nofollow">Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention</a> <a id="user-content-link27"></a>
</h2><a id="user-content-27-rethinking-transformer-connectivity-tlinformer-a-path-to-exact-full-context-aware-linear-attention-" class="anchor" aria-label="Permalink: 27. Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention" href="#27-rethinking-transformer-connectivity-tlinformer-a-path-to-exact-full-context-aware-linear-attention-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20407v1
<strong>Authors:</strong> Zhongpan Tang</p>
<p><strong>Abstract:</strong> The Transformer architecture has become a cornerstone of modern artificial intelligence, but its core self-attention mechanism suffers from a complexity bottleneck that scales quadratically with sequence length, severely limiting its application in long-sequence tasks. To address this challenge, existing linear attention methods typically sacrifice model performance by relying on data-agnostic kernel approximations or restrictive context selection. This paper returns to the first principles of connectionism, starting from the topological structure of information flow, to introduce a novel linear attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection patterns, TLinFormer achieves strict linear complexity while computing exact attention scores and ensuring information flow remains aware of the full historical context. This design aims to bridge the performance gap prevalent between existing efficient attention methods and standard attention. Through a series of experiments, we systematically evaluate the performance of TLinFormer against a standard Transformer baseline on long-sequence inference tasks. The results demonstrate that TLinFormer exhibits overwhelming advantages in key metrics such as \textbf{inference latency}, \textbf{KV cache efficiency}, \textbf{memory footprint}, and \textbf{overall speedup}.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">28. <a href="https://arxiv.org/abs/2508.20353v1" rel="nofollow">DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search</a> <a id="user-content-link28"></a>
</h2><a id="user-content-28-dfams-dynamic-flow-guided-federated-alignment-based-multi-prototype-search-" class="anchor" aria-label="Permalink: 28. DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search" href="#28-dfams-dynamic-flow-guided-federated-alignment-based-multi-prototype-search-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20353v1
<strong>Authors:</strong> Zhibang Yang, Xinke Jiang, Rihong Qiu, Ruiqing Li, Yihang Zhang, Yue Fang, Yongxin Xu, Hongxin Ding, Xu Chu, Junfeng Zhao, Yasha Wang</p>
<p><strong>Abstract:</strong> Federated Retrieval (FR) routes queries across multiple external knowledge sources, to mitigate hallucinations of LLMs, when necessary external knowledge is distributed. However, existing methods struggle to retrieve high-quality and relevant documents for ambiguous queries, especially in cross-domain scenarios, which significantly limits their effectiveness in supporting downstream generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS, a novel framework that leverages DIF to identify latent query intents and construct semantically aligned knowledge partitions for accurate retrieval across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by leveraging gradient signals from a few annotated queries and employing Shapley value-based attribution to trace neuron activation paths associated with intent recognition and subdomain boundary detection. Then, DFAMS leverages DIF to train an alignment module via multi-prototype contrastive learning, enabling fine-grained intra-source modeling and inter-source semantic alignment across knowledge bases. Experimental results across five benchmarks show that DFAMS outperforms advanced FR methods by up to 14.37% in knowledge classification accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy, demonstrating its effectiveness in complex FR scenarios.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">29. <a href="https://arxiv.org/abs/2508.20206v1" rel="nofollow">Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering</a> <a id="user-content-link29"></a>
</h2><a id="user-content-29-filter-then-attend-improving-attention-based-time-series-forecasting-with-spectral-filtering-" class="anchor" aria-label="Permalink: 29. Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering" href="#29-filter-then-attend-improving-attention-based-time-series-forecasting-with-spectral-filtering-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.20206v1
<strong>Authors:</strong> Elisha Dayag, Nhat Thanh Van Tran, Jack Xin</p>
<p><strong>Abstract:</strong> Transformer-based models are at the forefront in long time-series forecasting (LTSF). While in many cases, these models are able to achieve state of the art results, they suffer from a bias toward low-frequencies in the data and high computational and memory requirements. Recent work has established that learnable frequency filters can be an integral part of a deep forecasting model by enhancing the model's spectral utilization. These works choose to use a multilayer perceptron to process their filtered signals and thus do not solve the issues found with transformer-based models. In this paper, we establish that adding a filter to the beginning of transformer-based models enhances their performance in long time-series forecasting. We add learnable filters, which only add an additional $\approx 1000$ parameters to several transformer-based models and observe in multiple instances 5-10 % relative improvement in forecasting performance. Additionally, we find that with filters added, we are able to decrease the embedding dimension of our models, resulting in transformer-based architectures that are both smaller and more effective than their non-filtering base models. We also conduct synthetic experiments to analyze how the filters enable Transformer-based models to better utilize the full spectrum for forecasting.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">30. <a href="https://arxiv.org/abs/2508.19566v1" rel="nofollow">Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks</a> <a id="user-content-link30"></a>
</h2><a id="user-content-30-energy-efficient-learning-based-beamforming-for-isac-enabled-v2x-networks-" class="anchor" aria-label="Permalink: 30. Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks" href="#30-energy-efficient-learning-based-beamforming-for-isac-enabled-v2x-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19566v1
<strong>Authors:</strong> Chen Shang, Jiadong Yu, Dinh Thai Hoang</p>
<p><strong>Abstract:</strong> This work proposes an energy-efficient, learning-based beamforming scheme for integrated sensing and communication (ISAC)-enabled V2X networks. Specifically, we first model the dynamic and uncertain nature of V2X environments as a Markov Decision Process. This formulation allows the roadside unit to generate beamforming decisions based solely on current sensing information, thereby eliminating the need for frequent pilot transmissions and extensive channel state information acquisition. We then develop a deep reinforcement learning (DRL) algorithm to jointly optimize beamforming and power allocation, ensuring both communication throughput and sensing accuracy in highly dynamic scenario. To address the high energy demands of conventional learning-based schemes, we embed spiking neural networks (SNNs) into the DRL framework. Leveraging their event-driven and sparsely activated architecture, SNNs significantly enhance energy efficiency while maintaining robust performance. Simulation results confirm that the proposed method achieves substantial energy savings and superior communication performance, demonstrating its potential to support green and sustainable connectivity in future V2X systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">31. <a href="https://arxiv.org/abs/2508.19564v1" rel="nofollow">Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models</a> <a id="user-content-link31"></a>
</h2><a id="user-content-31-bi-lora-efficient-sharpness-aware-minimization-for-fine-tuning-large-scale-models-" class="anchor" aria-label="Permalink: 31. Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models" href="#31-bi-lora-efficient-sharpness-aware-minimization-for-fine-tuning-large-scale-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2508.19564v1
<strong>Authors:</strong> Yuhang Liu, Tao Li, Zhehao Huang, Zuopeng Yang, Xiaolin Huang</p>
<p><strong>Abstract:</strong> Fine-tuning large-scale pre-trained models with limited data presents significant challenges for generalization. While Sharpness-Aware Minimization (SAM) has proven effective in improving generalization by seeking flat minima, its substantial extra memory and computation overhead make it impractical for large models. Integrating SAM with parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) is a promising direction. However, we find that directly applying SAM to LoRA parameters limits the sharpness optimization to a restricted subspace, hindering its effectiveness. To address this limitation, we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an auxiliary LoRA module to model SAM's adversarial weight perturbations. It decouples SAM's weight perturbations from LoRA optimization: the primary LoRA module adapts to specific tasks via standard gradient descent, while the auxiliary module captures the sharpness of the loss landscape through gradient ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness for achieving flatter minima while remaining memory-efficient. Another important benefit is that the dual design allows for simultaneous optimization and perturbation, eliminating SAM's doubled training costs. Extensive experiments across diverse tasks and architectures demonstrate Bi-LoRA's efficiency and effectiveness in enhancing generalization.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, inference acceleration techniques, and related algorithms such as quantization and pruning. He is particularly interested in research that explores new methods for reducing large language model size and improving inference speed, as well as innovative approaches to optimizing large language model. Furthermore, it incorporates Mixture-of-Experts (MoE) architectures to significantly decrease deployment overhead and accelerate inference speed, enabling more efficient and scalable model serving in resource-constrained environments.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>