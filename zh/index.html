<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">MiLo：采用低秩补偿器混合策略的高效量化混合专家模型推理</h2><a id="user-content-milo采用低秩补偿器混合策略的高效量化混合专家模型推理" class="anchor" aria-label="Permalink: MiLo：采用低秩补偿器混合策略的高效量化混合专家模型推理" href="#milo采用低秩补偿器混合策略的高效量化混合专家模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.02658v1 公告类型：新研究<br>
摘要：量化是高效部署参数规模庞大的混合专家（MoE）模型的关键方法。然而，现有最先进的MoE模型在极端量化（如低于4比特）时存在显著的精度损失。为此，我们提出MiLo——一种创新方法，通过为高度量化的MoE模型配备低秩补偿器混合组件来缓解此问题。这些补偿器仅消耗少量额外内存，却能显著恢复极端量化导致的精度损失。MiLo还发现：由于MoE模型具有稠密-稀疏混合架构，其权重表现出独特特性，因此采用自适应秩选择策略与迭代优化来弥补精度差距。该方法无需依赖校准数据，可泛化至不同MoE模型和数据集，避免对校准集的过拟合。为避免极端量化（如3比特）的硬件效率问题，MiLo开发了与Tensor Core兼容的3比特计算内核，实测可在3比特量化MoE模型上实现延迟加速。评估表明，MiLo在各种任务的SoTA MoE模型上均优于现有方法。</p>
<p>（注：根据学术文献翻译规范，关键术语处理如下：</p>
<ol>
<li>"Mixture-of-Experts (MoE)" 保留英文缩写并首次出现时标注全称"混合专家"</li>
<li>"SoTA" 译为"最先进的"（State-of-the-Art）</li>
<li>"Tensor Core" 作为英伟达专用硬件术语保留英文原名</li>
<li>被动语态转换为中文主动表述（如"are quantized"→"采用量化"）</li>
<li>长难句拆分重组（如原文最后两句合并为符合中文阅读习惯的递进句式））</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">GPTQv2：面向非对称校准的高效免微调量化技术</h2><a id="user-content-gptqv2面向非对称校准的高效免微调量化技术" class="anchor" aria-label="Permalink: GPTQv2：面向非对称校准的高效免微调量化技术" href="#gptqv2面向非对称校准的高效免微调量化技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.02692v1 公告类型：新成果<br>
摘要：我们推出GPTQv2，一种无需微调的新型量化方法，专为压缩大规模Transformer架构设计。与先前逐层独立校准的GPTQ方法不同，我们始终确保量化层的输出与全精度模型中的精确输出相匹配，这种方案被称为非对称校准。该方案能有效减少先前层级积累的量化误差。我们通过最优脑压缩理论分析该问题，推导出闭式解。新解法显式最小化量化误差与非对称累积误差，并运用通道并行化、神经元分解、矩阵融合的Cholesky重构等技术实现方案计算的并行化。最终GPTQv2仅比GPTQ多20行代码即可实现，显著提升了低比特量化下的性能。值得注意的是，在单块GPU上，我们成功量化了4050亿参数的语言Transformer及计算机视觉领域排名第一的EVA-02视觉Transformer（后者保持ImageNet预训练90%的准确率）。代码已开源于github.com/Intelligent-Computing-Lab-Yale/GPTQv2。</p>
<p>（注：根据学术规范，"405B"译为"4050亿"符合中文计量习惯；"EVA-02 the rank first vision transformer"采用增译法补充"计算机视觉领域"背景；"Imagenet accuracy"保留专业术语首字母大写并增译"预训练"以明确上下文；技术术语如"channel parallelization"等采用计算机领域通用译法）</p>
<div class="markdown-heading"><h2 class="heading-element">当推理遇上压缩：在复杂推理任务上评估压缩后的大型推理模型性能</h2><a id="user-content-当推理遇上压缩在复杂推理任务上评估压缩后的大型推理模型性能" class="anchor" aria-label="Permalink: 当推理遇上压缩：在复杂推理任务上评估压缩后的大型推理模型性能" href="#当推理遇上压缩在复杂推理任务上评估压缩后的大型推理模型性能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.02010v1 公告类型：新研究<br>
摘要：近期开源的大型推理模型（LRMs）在复杂推理任务中展现出强劲性能，但其庞大的参数量使得个人用户难以承担高昂的计算成本。大型语言模型（LLM）的压缩技术为降低计算资源开销提供了有效解决方案。然而，目前缺乏针对压缩后LLM在复杂推理任务（尤其是LRMs）性能的系统性研究。多数量化与剪枝研究聚焦于保留语言建模能力，而现有蒸馏工作未能基于推理难度或压缩对知识与推理能力的影响对学生模型进行全面基准测试。本文采用量化、蒸馏和剪枝方法，在涵盖数学推理至多跳推理的四类数据集（AIME 2024、FOLIO、BIG-Bench Hard时序序列和MuSiQue）上对压缩版DeepSeek-R1模型进行基准测试。我们测试了采用动态量化的2.51位、1.73位和1.58位R1模型，并基于LLaMA或Qwen框架的蒸馏R1模型，通过SparseGPT获得不同稀疏度版本。通过研究压缩LRMs的性能与行为，我们报告了其性能得分与测试时计算量（每道题的token消耗量）。值得注意的是，在MuSiQue数据集上的实验表明，参数量对LRMs知识记忆能力的影响远大于对其推理能力的影响，这一发现可为压缩技术选择提供指导。通过对测试时计算量的实证分析，我们发现无论是原始R1还是其压缩变体，在多个基准测试中较短模型输出通常能获得更好性能，这凸显了构建更简洁推理链的必要性。</p>
<p>（注：根据学术文献翻译规范，技术术语保持统一："benchmark"译为"基准测试"，"quantization/pruning/distillation"分别译为"量化/剪枝/蒸馏"，"multihop reasoning"译为"多跳推理"。动态调整句式结构以符合中文表达习惯，如将英文长句拆分为中文短句群，被动语态转换为主动表述。）</p>
<div class="markdown-heading"><h2 class="heading-element">LLMPi：为树莓派优化大语言模型以实现高吞吐量</h2><a id="user-content-llmpi为树莓派优化大语言模型以实现高吞吐量" class="anchor" aria-label="Permalink: LLMPi：为树莓派优化大语言模型以实现高吞吐量" href="#llmpi为树莓派优化大语言模型以实现高吞吐量"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.02118v1 公告类型：新论文<br>
摘要：在树莓派等资源受限的边缘设备上部署大语言模型（LLMs）面临着计算效率、功耗和响应延迟等多重挑战。本文探索基于量化的优化技术，旨在实现低功耗嵌入式系统中大语言模型的高吞吐量、高能效运行。我们采用支持多比特位宽的k-quantization后训练量化（PTQ）方法，实现高效的2比特、4比特、6比特和8比特权重量化。同时针对BitNet模型，我们运用基于量化感知训练（QAT）的三元量化技术，使模型在保持精度的同时更好地适应低位宽表示。</p>
<p>研究结果表明，量化后的大语言模型能够为边缘设备上的实时对话式AI提供可行方案，为移动和嵌入式应用中的低功耗高效能AI部署开辟道路。本研究表明，激进的量化策略能在保持推理质量的同时显著降低能耗，使得大语言模型在资源受限环境中具备实际应用价值。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>