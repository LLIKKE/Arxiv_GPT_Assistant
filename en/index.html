<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/14/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10142025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/14/2025" href="#personalized-daily-arxiv-papers-10142025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 22</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Revisiting Model Interpolation for Efficient Reasoning</a>
<strong>Authors:</strong> Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Ngai Wong</p>
</li>
<li>
<p><a href="#link1">Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion</a>
<strong>Authors:</strong> Jiabao Shi, Minfeng Qi, Lefeng Zhang, Di Wang, Yingjie Zhao, Ziying Li, Yalong Xing, Ningran Li</p>
</li>
<li>
<p><a href="#link2">Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control</a>
<strong>Authors:</strong> Haolang Lu, Bolun Chu, WeiYe Fu, Guoshun Nan, Junning Liu, Minghui Pan, Qiankun Li, Yi Yu, Hua Wang, Kun Wang</p>
</li>
<li>
<p><a href="#link3">RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model</a>
<strong>Authors:</strong> Mingtong Dai, Lingbo Liu, Yongjie Bai, Yang Liu, Zhouxia Wang, Rui SU, Chunjie Chen, Liang Lin, Xinyu Wu</p>
</li>
<li>
<p><a href="#link4">Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning</a>
<strong>Authors:</strong> Junyuan Liu, Quan Qin, Guangsheng Dong, Xinglei Wang, Jiazhuang Feng, Zichao Zeng, Tao Cheng</p>
</li>
<li>
<p><a href="#link5">LOTION: Smoothing the Optimization Landscape for Quantized Training</a>
<strong>Authors:</strong> Mujin Kwun, Depen Morwani, Chloe Huangyuan Su, Stephanie Gil, Nikhil Anand, Sham Kakade</p>
</li>
<li>
<p><a href="#link6">SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation</a>
<strong>Authors:</strong> Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou</p>
</li>
<li>
<p><a href="#link7">Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting</a>
<strong>Authors:</strong> Suming Qiu, Jing Li, Zhicheng Zhou, Junjie Huang, Linyuan Qiu, Zhijie Sun</p>
</li>
<li>
<p><a href="#link8">DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay</a>
<strong>Authors:</strong> Yunxiang Mo, Tianshi Zheng, Qing Zong, Jiayu Liu, Baixuan Xu, Yauwai Yim, Chunkit Chan, Jiaxin Bai, Yangqiu Song</p>
</li>
<li>
<p><a href="#link9">OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs</a>
<strong>Authors:</strong> Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, Jiaheng Liu</p>
</li>
<li>
<p><a href="#link10">X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model</a>
<strong>Authors:</strong> Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, Ya-Qin Zhang, Jiangmiao Pang, Jingjing Liu, Tai Wang, Xianyuan Zhan</p>
</li>
<li>
<p><a href="#link11">Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs</a>
<strong>Authors:</strong> Yumin Choi, Dongki Kim, Jinheon Baek, Sung Ju Hwang</p>
</li>
<li>
<p><a href="#link12">Automated Evolutionary Optimization for Resource-Efficient Neural Network Training</a>
<strong>Authors:</strong> Ilia Revin, Leon Strelkov, Vadim A. Potemkin, Ivan Kireev, Andrey Savchenko</p>
</li>
<li>
<p><a href="#link13">Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization</a>
<strong>Authors:</strong> Le-Trung Nguyen, Enzo Tartaglione, Van-Tam Nguyen</p>
</li>
<li>
<p><a href="#link14">SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions</a>
<strong>Authors:</strong> Ziyi Wang, Nan Jiang, Guang Lin, Qifan Song</p>
</li>
<li>
<p><a href="#link15">Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training</a>
<strong>Authors:</strong> T. Ed Li, Junyu Ren</p>
</li>
<li>
<p><a href="#link16">Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph</a>
<strong>Authors:</strong> Wentao Wang, Heqing Zou, Tianze Luo, Rui Huang, Yutian Zhao, Zhuochen Wang, Hansheng Zhang, Chengwei Qin, Yan Wang, Lin Zhao, Huaijian Zhang</p>
</li>
<li>
<p><a href="#link17">FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms</a>
<strong>Authors:</strong> Atul Shree, Harshith Jupuru</p>
</li>
<li>
<p><a href="#link18">Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers</a>
<strong>Authors:</strong> Rui Bu, Haofeng Zhong, Wenzheng Chen, Yangyan Li</p>
</li>
<li>
<p><a href="#link19">ManiAgent: An Agentic Framework for General Robotic Manipulation</a>
<strong>Authors:</strong> Yi Yang, Kefan Gu, Yuqing Wen, Hebei Li, Yucheng Zhao, Tiancai Wang, Xudong Liu</p>
</li>
<li>
<p><a href="#link20">AdaPM: a Partial Momentum Algorithm for LLM Training</a>
<strong>Authors:</strong> Yimu Zhang, Yuanshi Liu, Cong Fang</p>
</li>
<li>
<p><a href="#link21">CompassNav: Steering From Path Imitation To Decision Understanding In Navigation</a>
<strong>Authors:</strong> LinFeng Li, Jian Zhao, Yuan Xie, Xin Tan, Xuelong Li</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.10977" rel="nofollow">Revisiting Model Interpolation for Efficient Reasoning</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-revisiting-model-interpolation-for-efficient-reasoning-" class="anchor" aria-label="Permalink: 0. Revisiting Model Interpolation for Efficient Reasoning" href="#0-revisiting-model-interpolation-for-efficient-reasoning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10977
<strong>Authors:</strong> Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, Ngai Wong</p>
<p><strong>Abstract:</strong> arXiv:2510.10977v1 Announce Type: new  Abstract: Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \href{<a href="https://github.com/wutaiqiang/MI%7D%7BGithub%7D">https://github.com/wutaiqiang/MI}{Github}</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.10633" rel="nofollow">Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-collaborative-text-to-image-generation-via-multi-agent-reinforcement-learning-and-semantic-fusion-" class="anchor" aria-label="Permalink: 1. Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion" href="#1-collaborative-text-to-image-generation-via-multi-agent-reinforcement-learning-and-semantic-fusion-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10633
<strong>Authors:</strong> Jiabao Shi, Minfeng Qi, Lefeng Zhang, Di Wang, Yingjie Zhao, Ziying Li, Yalong Xing, Ningran Li</p>
<p><strong>Abstract:</strong> arXiv:2510.10633v1 Announce Type: new  Abstract: Multimodal text-to-image generation remains constrained by the difficulty of maintaining semantic alignment and professional-level detail across diverse visual domains. We propose a multi-agent reinforcement learning framework that coordinates domain-specialized agents (e.g., focused on architecture, portraiture, and landscape imagery) within two coupled subsystems: a text enhancement module and an image generation module, each augmented with multimodal integration components. Agents are trained using Proximal Policy Optimization (PPO) under a composite reward function that balances semantic similarity, linguistic visual quality, and content diversity. Cross-modal alignment is enforced through contrastive learning, bidirectional attention, and iterative feedback between text and image. Across six experimental settings, our system significantly enriches generated content (word count increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion methods, Transformer-based strategies achieve the highest composite score (0.521), despite occasional stability issues. Multimodal ensembles yield moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent challenges of cross-modal semantic grounding. These findings underscore the promise of collaborative, specialization-driven architectures for advancing reliable multimodal generative systems.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.10285" rel="nofollow">Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-mitigating-hallucination-in-multimodal-reasoning-via-functional-attention-control-" class="anchor" aria-label="Permalink: 2. Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control" href="#2-mitigating-hallucination-in-multimodal-reasoning-via-functional-attention-control-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10285
<strong>Authors:</strong> Haolang Lu, Bolun Chu, WeiYe Fu, Guoshun Nan, Junning Liu, Minghui Pan, Qiankun Li, Yi Yu, Hua Wang, Kun Wang</p>
<p><strong>Abstract:</strong> arXiv:2510.10285v1 Announce Type: new  Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing vision-language reasoning and are emerging as a foundation for cross-modal intelligence. Hallucination remains a persistent failure mode, manifesting itself as erroneous reasoning chains and misinterpretation of visual content. In this study, we observe that attention heads exhibit a staged division: shallow heads predominantly serve perception, while deeper heads shift toward symbolic reasoning, revealing two major causes of hallucination, namely perceptual bias and reasoning drift. To address these issues, we propose a lightweight and interpretable two-step plugin, Functional Head Identification and Class-conditioned Rescaling, which locates perception- and reasoning-oriented heads and regulates their contributions without retraining. Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six benchmarks across three domains, and four baselines show that our plugin achieves an average improvement of 5% and up to 15%, with only &lt;1% additional computation and 9% of baseline latency. Our approach is completely model-agnostic and significantly enhances both the reliability and interpretability of the off-the-shelf MLRMs, thereby enabling their safe deployment in high-stakes applications. Our code is available at <a href="https://anonymous.4open.science/r/Functional-Attention-Control" rel="nofollow">https://anonymous.4open.science/r/Functional-Attention-Control</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.10975" rel="nofollow">RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-rover-robot-reward-model-as-test-time-verifier-for-vision-language-action-model-" class="anchor" aria-label="Permalink: 3. RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model" href="#3-rover-robot-reward-model-as-test-time-verifier-for-vision-language-action-model-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10975
<strong>Authors:</strong> Mingtong Dai, Lingbo Liu, Yongjie Bai, Yang Liu, Zhouxia Wang, Rui SU, Chunjie Chen, Liang Lin, Xinyu Wu</p>
<p><strong>Abstract:</strong> arXiv:2510.10975v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have become a prominent paradigm for embodied intelligence, yet further performance improvements typically rely on scaling up training data and model size -- an approach that is prohibitively expensive for robotics and fundamentally limited by data collection costs.We address this limitation with $\mathbf{RoVer}$, an embodied test-time scaling framework that uses a $\mathbf{Ro}$bot Process Reward Model (PRM) as a Test-Time $\mathbf{Ver}$ifier to enhance the capabilities of existing VLA models without modifying their architectures or weights. Specifically, RoVer (i) assigns scalar-based process rewards to evaluate the reliability of candidate actions, and (ii) predicts an action-space direction for candidate expansion/refinement. During inference, RoVer generates multiple candidate actions concurrently from the base policy, expands them along PRM-predicted directions, and then scores all candidates with PRM to select the optimal action for execution. Notably, by caching shared perception features, it can amortize perception cost and evaluate more candidates under the same test-time computational budget. Essentially, our approach effectively transforms available computing resources into better action decision-making, realizing the benefits of test-time scaling without extra training overhead. Our contributions are threefold: (1) a general, plug-and-play test-time scaling framework for VLAs; (2) a PRM that jointly provides scalar process rewards and an action-space direction to guide exploration; and (3) an efficient direction-guided sampling strategy that leverages a shared perception cache to enable scalable candidate generation and selection during inference.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.09894" rel="nofollow">Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-beyond-alphaearth-toward-human-centered-spatial-representation-via-poi-guided-contrastive-learning-" class="anchor" aria-label="Permalink: 4. Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning" href="#4-beyond-alphaearth-toward-human-centered-spatial-representation-via-poi-guided-contrastive-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09894
<strong>Authors:</strong> Junyuan Liu, Quan Qin, Guangsheng Dong, Xinglei Wang, Jiazhuang Feng, Zichao Zeng, Tao Cheng</p>
<p><strong>Abstract:</strong> arXiv:2510.09894v1 Announce Type: new  Abstract: General-purpose spatial representations are essential for building transferable geospatial foundation models (GFMs). Among them, the AlphaEarth Foundation (AE) represents a major step toward a global, unified representation of the Earth's surface, learning 10-meter embeddings from multi-source Earth Observation (EO) data that capture rich physical and environmental patterns across diverse landscapes. However, such EO-driven representations remain limited in capturing the functional and socioeconomic dimensions of cities, as they primarily encode physical and spectral patterns rather than human activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched Representation Learning), a lightweight framework that adapts AlphaEarth to human-centered urban analysis through multimodal alignment guided by Points of Interest (POIs). AETHER aligns AE embeddings with textual representations of POIs, enriching physically grounded EO features with semantic cues about urban functions and socioeconomic contexts. In Greater London, AETHER achieves consistent gains over the AE baseline, with a 7.2% relative improvement in land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler divergence for socioeconomic mapping. Built upon pretrained AE, AETHER leverages a lightweight multimodal alignment to enrich it with human-centered semantics while remaining computationally efficient and scalable for urban applications. By coupling EO with human-centered semantics, it advances geospatial foundation models toward general-purpose urban representations that integrate both physical form and functional meaning.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.08757" rel="nofollow">LOTION: Smoothing the Optimization Landscape for Quantized Training</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-lotion-smoothing-the-optimization-landscape-for-quantized-training-" class="anchor" aria-label="Permalink: 5. LOTION: Smoothing the Optimization Landscape for Quantized Training" href="#5-lotion-smoothing-the-optimization-landscape-for-quantized-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08757
<strong>Authors:</strong> Mujin Kwun, Depen Morwani, Chloe Huangyuan Su, Stephanie Gil, Nikhil Anand, Sham Kakade</p>
<p><strong>Abstract:</strong> arXiv:2510.08757v1 Announce Type: new  Abstract: Optimizing neural networks for quantized objectives is fundamentally challenging because the quantizer is piece-wise constant, yielding zero gradients everywhere except at quantization thresholds where the derivative is undefined. Most existing methods deal with this issue by relaxing gradient computations with techniques like Straight Through Estimators (STE) and do not provide any guarantees of convergence. In this work, taking inspiration from Nesterov smoothing, we approximate the quantized loss surface with a continuous loss surface. In particular, we introduce LOTION, \textbf{L}ow-precision \textbf{O}ptimization via s\textbf{T}ochastic-no\textbf{I}se sm\textbf{O}othi\textbf{N}g, a principled smoothing framework that replaces the raw quantized loss with its expectation under unbiased randomized-rounding noise. In this framework, standard optimizers are guaranteed to converge to a local minimum of the loss surface. Moreover, when using noise derived from stochastic rounding, we show that the global minima of the original quantized loss are preserved. We empirically demonstrate that this method outperforms standard QAT on synthetic testbeds and on 150M- and 300M- parameter language models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.10069" rel="nofollow">SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-synclipmae-contrastive-masked-pretraining-for-audio-visual-talking-face-representation-" class="anchor" aria-label="Permalink: 6. SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation" href="#6-synclipmae-contrastive-masked-pretraining-for-audio-visual-talking-face-representation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10069
<strong>Authors:</strong> Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou</p>
<p><strong>Abstract:</strong> arXiv:2510.10069v1 Announce Type: new  Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head/face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.09152" rel="nofollow">Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-logits-replay--moclip-stabilized-low-cost-post-training-with-minimal-forgetting-" class="anchor" aria-label="Permalink: 7. Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting" href="#7-logits-replay--moclip-stabilized-low-cost-post-training-with-minimal-forgetting-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09152
<strong>Authors:</strong> Suming Qiu, Jing Li, Zhicheng Zhou, Junjie Huang, Linyuan Qiu, Zhijie Sun</p>
<p><strong>Abstract:</strong> arXiv:2510.09152v1 Announce Type: new  Abstract: Large language models (LLMs) often face a trade-off in post-training: improvements on specialized domains frequently come at the expense of general capabilities. Existing solutions attempt to mitigate this tension via regularization, selective parameter updates, or data-centric replay, but each imposes significant costs in computation, data access, or adaptability. Recent work has shown that training signals can be compressed to subsets of logits without severe accuracy loss, suggesting a path toward efficient adaptation. However, naive truncation destabilizes optimization and exacerbates forgetting.   We introduce Logits Replay + MoClip, a two-stage framework that compresses supervision in the logit space and stabilizes optimization at the update level. In Stage 0, we record dynamic Top-K token subsets that cover a probability threshold, always including the gold label. In Stage 1, we replay these compact subsets to compute exact renormalized losses, avoiding full softmax computation and implicitly regularizing. To ensure stability, we design MoClip, an optimizer that caps gradient-momentum rotation and applies an arctan2-based rescaling of updates. Empirically, our method improves domain performance on Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over 40%. Together, these contributions offer a scalable, architecture-agnostic path for domain adaptation of LLMs without sacrificing generalization.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.10117" rel="nofollow">DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-dixitworld-evaluating-multimodal-abductive-reasoning-in-vision-language-models-with-multi-agent-dixit-gameplay-" class="anchor" aria-label="Permalink: 8. DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay" href="#8-dixitworld-evaluating-multimodal-abductive-reasoning-in-vision-language-models-with-multi-agent-dixit-gameplay-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10117
<strong>Authors:</strong> Yunxiang Mo, Tianshi Zheng, Qing Zong, Jiayu Liu, Baixuan Xu, Yauwai Yim, Chunkit Chan, Jiaxin Bai, Yangqiu Song</p>
<p><strong>Abstract:</strong> arXiv:2510.10117v1 Announce Type: new  Abstract: Multimodal abductive reasoning--the generation and selection of explanatory hypotheses from partial observations--is a cornerstone of intelligence. Current evaluations of this ability in vision-language models (VLMs) are largely confined to static, single-agent tasks. Inspired by Dixit, we introduce DixitWorld, a comprehensive evaluation suite designed to deconstruct this challenge. DIXITWORLD features two core components: DixitArena, a dynamic, multi-agent environment that evaluates both hypothesis generation (a "storyteller" crafting cryptic clues) and hypothesis selection ("listeners" choosing the target image from decoys) under imperfect information; and DixitBench, a static QA benchmark that isolates the listener's task for efficient, controlled evaluation. Results from DixitArena reveal distinct, role-dependent behaviors: smaller open-source models often excel as creative storytellers, producing imaginative yet less discriminative clues, whereas larger proprietary models demonstrate superior overall performance, particularly as listeners. Performance on DixitBench strongly correlates with listener results in DixitArena, validating it as a reliable proxy for hypothesis selection. Our findings reveal a key trade-off between generative creativity and discriminative understanding in multimodal abductive reasoning, a central challenge for developing more balanced and capable vision-language agents.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.10689" rel="nofollow">OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-omnivideobench-towards-audio-visual-understanding-evaluation-for-omni-mllms-" class="anchor" aria-label="Permalink: 9. OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs" href="#9-omnivideobench-towards-audio-visual-understanding-evaluation-for-omni-mllms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10689
<strong>Authors:</strong> Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, Jiaheng Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.10689v1 Announce Type: new  Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.10274" rel="nofollow">X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-x-vla-soft-prompted-transformer-as-scalable-cross-embodiment-vision-language-action-model-" class="anchor" aria-label="Permalink: 10. X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model" href="#10-x-vla-soft-prompted-transformer-as-scalable-cross-embodiment-vision-language-action-model-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10274
<strong>Authors:</strong> Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, Ya-Qin Zhang, Jiangmiao Pang, Jingjing Liu, Tai Wang, Xianyuan Zhan</p>
<p><strong>Abstract:</strong> arXiv:2510.10274v1 Announce Type: new  Abstract: Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: <a href="https://thu-air-dream.github.io/X-VLA/" rel="nofollow">https://thu-air-dream.github.io/X-VLA/</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.09201" rel="nofollow">Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-multimodal-prompt-optimization-why-not-leverage-multiple-modalities-for-mllms-" class="anchor" aria-label="Permalink: 11. Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs" href="#11-multimodal-prompt-optimization-why-not-leverage-multiple-modalities-for-mllms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09201
<strong>Authors:</strong> Yumin Choi, Dongki Kim, Jinheon Baek, Sung Ju Hwang</p>
<p><strong>Abstract:</strong> arXiv:2510.09201v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.09566" rel="nofollow">Automated Evolutionary Optimization for Resource-Efficient Neural Network Training</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-automated-evolutionary-optimization-for-resource-efficient-neural-network-training-" class="anchor" aria-label="Permalink: 12. Automated Evolutionary Optimization for Resource-Efficient Neural Network Training" href="#12-automated-evolutionary-optimization-for-resource-efficient-neural-network-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09566
<strong>Authors:</strong> Ilia Revin, Leon Strelkov, Vadim A. Potemkin, Ivan Kireev, Andrey Savchenko</p>
<p><strong>Abstract:</strong> arXiv:2510.09566v1 Announce Type: new  Abstract: There are many critical challenges in optimizing neural network models, including distributed computing, compression techniques, and efficient training, regardless of their application to specific tasks. Solving such problems is crucial because the need for scalable and resource-efficient models is increasing. To address these challenges, we have developed a new automated machine learning (AutoML) framework, Parameter Efficient Training with Robust Automation (PETRA). It applies evolutionary optimization to model architecture and training strategy. PETRA includes pruning, quantization, and loss regularization. Experimental studies on real-world data with financial event sequences, as well as image and time-series -- benchmarks, demonstrate PETRA's ability to improve neural model performance and scalability -- namely, a significant decrease in model size (up to 75%) and latency (up to 33%), and an increase in throughput (by 13%) without noticeable degradation in the target metric.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.09160" rel="nofollow">Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-efficient-resource-constrained-training-of-vision-transformers-via-subspace-optimization-" class="anchor" aria-label="Permalink: 13. Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization" href="#13-efficient-resource-constrained-training-of-vision-transformers-via-subspace-optimization-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09160
<strong>Authors:</strong> Le-Trung Nguyen, Enzo Tartaglione, Van-Tam Nguyen</p>
<p><strong>Abstract:</strong> arXiv:2510.09160v1 Announce Type: new  Abstract: As AI increasingly shapes daily life, energy consumption and data privacy have become pressing concerns. On-device learning trains models directly on edge devices, cutting energy consumption and safeguarding data privacy. However, the expanding scale of modern neural networks creates a major obstacle for on-device training. Although prior work has concentrated on compact convolutional architectures, we instead apply subspace-based training to transformer models. Motivated by the idea that a model's essential information lies in a fixed subspace, we introduce Weight-Activation Subspace Iteration (WASI), a method that mitigates the memory bottleneck of backpropagation and boosts inference efficiency in transformer models by restricting training to this subspace. Our results demonstrate that WASI maintains accuracy comparable to vanilla training while reducing memory usage by up to $62\times$ and computational cost (FLOPs) by up to $2\times$. On a Raspberry Pi 5, WASI achieves roughly $1.5\times$ faster training and inference than vanilla training.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.08999" rel="nofollow">SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-sqs-bayesian-dnn-compression-through-sparse-quantized-sub-distributions-" class="anchor" aria-label="Permalink: 14. SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions" href="#14-sqs-bayesian-dnn-compression-through-sparse-quantized-sub-distributions-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08999
<strong>Authors:</strong> Ziyi Wang, Nan Jiang, Guang Lin, Qifan Song</p>
<p><strong>Abstract:</strong> arXiv:2510.08999v1 Announce Type: new  Abstract: Compressing large-scale neural networks is essential for deploying models on resource-constrained devices. Most existing methods adopt weight pruning or low-bit quantization individually, often resulting in suboptimal compression rates to preserve acceptable performance drops. We introduce a unified framework for simultaneous pruning and low-bit quantization via Bayesian variational learning (SQS), which achieves higher compression rates than prior baselines while maintaining comparable performance. The key idea is to employ a spike-and-slab prior to inducing sparsity and model quantized weights using Gaussian Mixture Models (GMMs) to enable low-bit precision. In theory, we provide the consistent result of our proposed variational approach to a sparse and quantized deep neural network. Extensive experiments on compressing ResNet, BERT-base, Llama3, and Qwen2.5 models show that our method achieves higher compression rates than a line of existing methods with comparable performance drops.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2510.08855" rel="nofollow">Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-time-aware-feature-selection-adaptive-temporal-masking-for-stable-sparse-autoencoder-training-" class="anchor" aria-label="Permalink: 15. Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training" href="#15-time-aware-feature-selection-adaptive-temporal-masking-for-stable-sparse-autoencoder-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08855
<strong>Authors:</strong> T. Ed Li, Junyu Ren</p>
<p><strong>Abstract:</strong> arXiv:2510.08855v1 Announce Type: new  Abstract: Understanding the internal representations of large language models is crucial for ensuring their reliability and safety, with sparse autoencoders (SAEs) emerging as a promising interpretability approach. However, current SAE training methods face feature absorption, where features (or neurons) are absorbed into each other to minimize $L_1$ penalty, making it difficult to consistently identify and analyze model behaviors. We introduce Adaptive Temporal Masking (ATM), a novel training approach that dynamically adjusts feature selection by tracking activation magnitudes, frequencies, and reconstruction contributions to compute importance scores that evolve over time. ATM applies a probabilistic masking mechanism based on statistical thresholding of these importance scores, creating a more natural feature selection process. Through extensive experiments on the Gemma-2-2b model, we demonstrate that ATM achieves substantially lower absorption scores compared to existing methods like TopK and JumpReLU SAEs, while maintaining excellent reconstruction quality. These results establish ATM as a principled solution for learning stable, interpretable features in neural networks, providing a foundation for more reliable model analysis.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2510.10976" rel="nofollow">Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-video-str-reinforcing-mllms-in-video-spatio-temporal-reasoning-with-relation-graph-" class="anchor" aria-label="Permalink: 16. Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph" href="#16-video-str-reinforcing-mllms-in-video-spatio-temporal-reasoning-with-relation-graph-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10976
<strong>Authors:</strong> Wentao Wang, Heqing Zou, Tianze Luo, Rui Huang, Yutian Zhao, Zhuochen Wang, Hansheng Zhang, Chengwei Qin, Yan Wang, Lin Zhao, Huaijian Zhang</p>
<p><strong>Abstract:</strong> arXiv:2510.10976v1 Announce Type: new  Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated strong semantic understanding capabilities, but struggles to perform precise spatio-temporal understanding. Existing spatio-temporal methods primarily focus on the video itself, while overlooking the physical information within the video, such as multi-object layouts and motion. Such limitations restrict the use of MLLMs in downstream applications that demand high precision, including embodied intelligence and VR. To address this issue, we present Video-STR, a novel graph-based reinforcement method for precise Video Spatio-Temporal Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism using graph-based Group Relative Policy Optimization (GRPO) method to guide the model in inferring the underlying spatio-temporal topology of scenarios during the thinking process. To resolve the lack of spatio-temporal training data, we construct the STV-205k dataset with 205k question-answering pairs, covering dynamic multi-object scenes in both indoor and outdoor environments, to support the model training. Experiments show that Video-STR achieves state-of-the-art results on various benchmarks, outperforming the base model by 13% on STI-Bench, and demonstrating the effectiveness of our approach and dataset. Code, model, and data will be released.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2510.09085" rel="nofollow">FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-fltop-ctc-frame-level-token-pruning-via-relative-threshold-for-efficient-and-memory-saving-decoding-on-diverse-platforms-" class="anchor" aria-label="Permalink: 17. FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms" href="#17-fltop-ctc-frame-level-token-pruning-via-relative-threshold-for-efficient-and-memory-saving-decoding-on-diverse-platforms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09085
<strong>Authors:</strong> Atul Shree, Harshith Jupuru</p>
<p><strong>Abstract:</strong> arXiv:2510.09085v1 Announce Type: new  Abstract: CTC-based ASR systems face computational and memory bottlenecks in resource-limited environments. Traditional CTC decoders, requiring up to 90% of processing time in systems (e.g., wav2vec2-large on L4 GPUs), face inefficiencies due to exhaustive token-level operations. This paper introduces Frame Level Token Pruning for Connectionist Temporal Classification (FLToP CTC), a novel decoding algorithm that employs frame-level token pruning guided by a relative threshold probability. By dynamically eliminating low-probability tokens per frame, FLToP CTC reduces compute and memory demands while maintaining negligible WER degradation. On LibriSpeech, FLToP CTC achieves a 10.5x runtime speedup and 2.78x memory reduction versus standard CTC decoders. Its simplicity enables seamless integration into CTC decoders across platforms (CPUs, GPUs, etc.). FLToP CTC addresses CTC bottlenecks, offering scalability for resource-limited environments and realtime applications, enhancing speech recognition accessibility and efficiency.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2510.09017" rel="nofollow">Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-value-state-gated-attention-for-mitigating-extreme-token-phenomena-in-transformers-" class="anchor" aria-label="Permalink: 18. Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers" href="#18-value-state-gated-attention-for-mitigating-extreme-token-phenomena-in-transformers-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09017
<strong>Authors:</strong> Rui Bu, Haofeng Zhong, Wenzheng Chen, Yangyan Li</p>
<p><strong>Abstract:</strong> arXiv:2510.09017v1 Announce Type: new  Abstract: Large models based on the Transformer architecture are susceptible to extreme-token phenomena, such as attention sinks and value-state drains. These issues, which degrade model performance, quantization fidelity, and interpretability, arise from a problematic mutual reinforcement mechanism where the model learns an inefficient 'no-op' behavior by focusing attention on tokens with near-zero value states. In this paper, we propose Value-State Gated Attention (VGA), a simple, dedicated, and stable architectural mechanism for performing 'no-op' attention efficiently by directly breaking this cycle. VGA introduces a learnable, data-dependent gate, computed directly from the value vectors (V), to modulate the output. Through a theoretical analysis of the underlying gradients, we show that gating the value-state with a function of itself is more effective at decoupling value and attention score updates than prior methods that gate on input embeddings. This creates a direct regulatory pathway that allows the model to suppress a token's contribution based on its emergent value representation. Our experiments demonstrate that VGA significantly mitigates the formation of attention sinks and stabilizes value-state norms, leading to improved performance, robust quantization fidelity, and enhanced model interpretability.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2510.11660" rel="nofollow">ManiAgent: An Agentic Framework for General Robotic Manipulation</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-maniagent-an-agentic-framework-for-general-robotic-manipulation-" class="anchor" aria-label="Permalink: 19. ManiAgent: An Agentic Framework for General Robotic Manipulation" href="#19-maniagent-an-agentic-framework-for-general-robotic-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.11660
<strong>Authors:</strong> Yi Yang, Kefan Gu, Yuqing Wen, Hebei Li, Yucheng Zhao, Tiancai Wang, Xudong Liu</p>
<p><strong>Abstract:</strong> arXiv:2510.11660v1 Announce Type: new  Abstract: While Vision-Language-Action (VLA) models have demonstrated impressive capabilities in robotic manipulation, their performance in complex reasoning and long-horizon task planning is limited by data scarcity and model capacity. To address this, we introduce ManiAgent, an agentic architecture for general manipulation tasks that achieves end-to-end output from task descriptions and environmental inputs to robotic manipulation actions. In this framework, multiple agents involve inter-agent communication to perform environmental perception, sub-task decomposition and action generation, enabling efficient handling of complex manipulation scenarios. Evaluations show ManiAgent achieves an 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world pick-and-place tasks, enabling efficient data collection that yields VLA models with performance comparable to those trained on human-annotated datasets.The project webpage is available at <a href="https://yi-yang929.github.io/ManiAgent/" rel="nofollow">https://yi-yang929.github.io/ManiAgent/</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2510.09103" rel="nofollow">AdaPM: a Partial Momentum Algorithm for LLM Training</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-adapm-a-partial-momentum-algorithm-for-llm-training-" class="anchor" aria-label="Permalink: 20. AdaPM: a Partial Momentum Algorithm for LLM Training" href="#20-adapm-a-partial-momentum-algorithm-for-llm-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09103
<strong>Authors:</strong> Yimu Zhang, Yuanshi Liu, Cong Fang</p>
<p><strong>Abstract:</strong> arXiv:2510.09103v1 Announce Type: new  Abstract: In the training of large language models, momentum is widely used and often demonstrated to achieve significant acceleration. However, storing momentum typically presents memory challenges. In this paper, we propose AdaPM, an adaptive training strategy that leverages partial momentum to implement a memory-efficient optimizer. To this end, AdaPM utilizes a non-uniform momentum design: for most blocks, full momentum is not necessary to preserve the performance of the optimization. In the momentum design of AdaPM, to mitigate the bias and performance loss caused by partial momentum, we enhance the partial momentum by a bias correction technique. Empirically, we verify that our approach reduces memory by over $90%$ in momentum while maintaining both efficiency and performance for pretraining various language models ranging from 60M to 1.5B, as well as for supervised fine-tuning and RLHF. AdaPM can further reduce memory by up to $95%$ in optimizer states by combining the memory-efficient technique on the second-order statistic, saving over $30%$ GPU hours for pretraining GPT-2 1.5B.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2510.10154" rel="nofollow">CompassNav: Steering From Path Imitation To Decision Understanding In Navigation</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-compassnav-steering-from-path-imitation-to-decision-understanding-in-navigation-" class="anchor" aria-label="Permalink: 21. CompassNav: Steering From Path Imitation To Decision Understanding In Navigation" href="#21-compassnav-steering-from-path-imitation-to-decision-understanding-in-navigation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.10154
<strong>Authors:</strong> LinFeng Li, Jian Zhao, Yuan Xie, Xin Tan, Xuelong Li</p>
<p><strong>Abstract:</strong> arXiv:2510.10154v1 Announce Type: new  Abstract: The dominant paradigm for training Large Vision-Language Models (LVLMs) in navigation relies on imitating expert trajectories. This approach reduces the complex navigation task to a sequence-to-sequence replication of a single correct path, fundamentally limiting the agent's ability to explore and generalize. In this work, we argue for and introduce a new paradigm: a shift from Path Imitation to Decision Understanding. The goal of this paradigm is to build agents that do not just follow, but truly understand how to navigate. We materialize this through two core contributions: first, we introduce Compass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning (RFT) subset provides a panoramic view of the decision landscape by annotating all feasible actions with A* geodesic distances. Second, we design a novel gap-aware hybrid reward function that dynamically adapts its feedback to decision certainty, shifting between decisive signals for optimal actions and nuanced scores to encourage exploration. Integrated into an SFT-then-RFT recipe, our CompassNav agent is trained not to memorize static routes, but to develop an internal ``compass'' that constantly intuits the direction to the goal by evaluating the relative quality of all possible moves. This approach enables our 7B agent to set a new state-of-the-art on Goal navigation benchmarks, outperforming even larger proprietary models, and achieve robust real-world goal navigation on a physical robot.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) and Mixture of Experts (MoE) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research introduces a novel multimodal large model that integrates text, language, and vision modalities. The goal is to advance the performance and generalization capabilities of the model by establishing a new approach for training and fusing these modalities effectively, rather than focusing on incremental optimizations.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>