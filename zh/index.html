<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">已激活的LoRA：专为内在特性优化的微调大语言模型</h2><a id="user-content-已激活的lora专为内在特性优化的微调大语言模型" class="anchor" aria-label="Permalink: 已激活的LoRA：专为内在特性优化的微调大语言模型" href="#已激活的lora专为内在特性优化的微调大语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.12397v1 公告类型：新研究<br>
摘要：低秩自适应（LoRA）已成为微调大型基础模型权重的高效框架，并成为数据驱动定制大语言模型（LLM）的首选方法。尽管LoRA能实现高度定制化的行为和功能，但在多轮对话场景中切换不同LoRA模块的效率极低——每次生成前都必须用目标LoRA权重重新计算整个对话历史的关键值（KV）缓存。为解决这一问题，我们提出激活式LoRA（aLoRA），通过改进框架使其仅对调用位置之后的序列令牌进行权重适配。这一关键改进使得aLoRA能直接继承基础模型对输入序列的KV缓存，从而在对话链中实现即时激活而无需缓存重计算。基于此，我们构建了"本征模块"——这些高度专业化的模型仅在输入链或对话的特定片段被触发，执行明确定义的操作，其余场景则默认使用基础模型。实验表明，aLoRA训练的本征模块在保持与标准LoRA相当精度的同时，能带来显著的推理效率提升。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了如下处理：</p>
<ol>
<li>"intrinsics"译为"本征模块"，既保留数学术语"本征"的核心概念，又通过"模块"体现其可插拔特性</li>
<li>"chain"在不同语境分别译为"对话链/输入链"，以区分对话流与输入序列</li>
<li>保留LoRA/KV缓存等技术缩写，首次出现时标注全称</li>
<li>采用"令牌"而非"词元"统一翻译"tokens"，符合NLP领域惯例）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">妈妈：面向长上下文语言模型的高效内存卸载式迷你序列推理</h2><a id="user-content-妈妈面向长上下文语言模型的高效内存卸载式迷你序列推理" class="anchor" aria-label="Permalink: 妈妈：面向长上下文语言模型的高效内存卸载式迷你序列推理" href="#妈妈面向长上下文语言模型的高效内存卸载式迷你序列推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.12526v1 公告类型：新研究<br>
摘要：长上下文语言模型展现出卓越性能，但由于推理过程中对GPU内存的高需求，其部署仍面临挑战。我们提出高效内存卸载迷你序列推理法（MOM），该方法将关键层划分为更小的"迷你序列"，并与KV缓存卸载技术无缝集成。在Llama、Qwen和Mistral等多个模型上的实验表明，MOM平均可降低超过50%的峰值内存占用。在单块A100 80GB GPU上，MOM将Meta-Llama-3.2-8B的最大上下文长度从15.5万词元扩展至45.5万词元，同时保持输出完全一致且不损失准确性。得益于极低的计算开销和高效的末层处理，MOM还能保持极具竞争力的吞吐量。相比传统分块预填充方法，MOM实现了35%的额外上下文长度扩展。更重要的是，我们的方法大幅降低了预填充阶段的内存消耗，彻底消除了这一长期存在的推理内存瓶颈。这一突破性进展从根本上改变了研究重点，将未来优化方向从预填充阶段转向提升解码阶段残余KV缓存效率。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了如下处理：</p>
<ol>
<li>"KV cache"保留专业缩写形式"KV缓存"</li>
<li>"token"统一译为"词元"（NLP领域推荐译法）</li>
<li>"throughput"译为"吞吐量"（计算机领域标准译法）</li>
<li>长难句采用分切重组策略，如将"redirecting future efforts..."独立译为转折句以符合中文表达习惯）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">用于LLM服务中任意低精度GPGPU计算的虚拟机</h2><a id="user-content-用于llm服务中任意低精度gpgpu计算的虚拟机" class="anchor" aria-label="Permalink: 用于LLM服务中任意低精度GPGPU计算的虚拟机" href="#用于llm服务中任意低精度gpgpu计算的虚拟机"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.12984v1 公告类型：新研究<br>
摘要：服务大型语言模型（LLMs）是AI驱动应用的核心需求，但其消耗的计算资源巨大，尤其在内存带宽和计算吞吐量方面。低精度计算已成为提升效率、降低资源消耗的关键技术。现有生成低精度计算核的方法仅支持位宽为2的幂次方的权重，且因高层GPU编程抽象导致性能欠佳。这些抽象限制了关键优化（如细粒度寄存器管理和高效内存访问模式），而这些优化对低精度计算至关重要。本文提出一种面向通用GPU计算的虚拟机（VM），支持任意位宽的低精度数据类型，同时保持GPU可编程性。该虚拟机具有线程块级编程模型、分层内存空间、创新的代数布局系统，并全面支持多样化低精度数据类型。虚拟机程序可编译为高度优化的GPU代码，实现自动向量化与指令选择。大量实验表明，我们的虚拟机不仅能高效支持全谱系低精度数据类型，在其支持的类型上更优于当前最先进的低精度计算核。相较于Triton、Ladder等现有编译器，以及QuantLLM、Marlin等手工优化核，我们的虚拟机分别实现了1.75倍、2.61倍、1.29倍和1.03倍的性能提升。</p>
<p>（注：根据学术文献翻译规范，对技术术语如"thread-block-level programming model"采用"线程块级编程模型"等标准译法；长句按中文习惯拆分为短句；"performance improvements"译为"性能提升"而非直译"改进"以符合技术语境；保持"arXiv"等专有名词原貌。）</p>
<div class="markdown-heading"><h2 class="heading-element">揭示大型语言模型中专家混合机制下的隐性协作</h2><a id="user-content-揭示大型语言模型中专家混合机制下的隐性协作" class="anchor" aria-label="Permalink: 揭示大型语言模型中专家混合机制下的隐性协作" href="#揭示大型语言模型中专家混合机制下的隐性协作"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2504.12359v1 公告类型：新研究<br>
摘要：基于专家混合架构的大语言模型（MoE LLMs）通过动态路由输入至专业子网络，展现出卓越的多任务适应能力。然而专家间的协作机制仍不明确，制约了模型的可解释性与优化空间。本研究聚焦两大核心问题：（1）专家协作模式的识别；（2）通过专家剪枝优化MoE LLMs。针对前者，我们提出层次化稀疏字典学习（HSDL）方法以揭示专家协作规律；对于后者，开发了贡献感知专家剪枝算法（CAEP）以高效剔除低贡献专家。大量实验表明：专家协作模式与特定输入类型强相关，且在不同任务中呈现语义一致性；剪枝实验显示本方法平均提升模型性能2.5%，优于现有方案。这些发现为提升MoE LLMs的效率和可解释性提供了新视角，不仅深化了对专家交互机制的理解，更为模型优化开辟了新路径。</p>
<p>（注：根据学术论文摘要的文体特征，翻译时着重处理了以下要点：</p>
<ol>
<li>专业术语统一性："Mixture-of-Experts"统一译为"专家混合架构"，"pruning"译为"剪枝"符合计算机领域惯例</li>
<li>长句拆分：将原文复合句分解为符合中文表达习惯的短句结构</li>
<li>被动语态转化："are still not well understood"转为主动句式"仍不明确"</li>
<li>数据强调：精确保留"2.5%"等关键量化结果</li>
<li>学术用语规范："exhibit semantic significance"译为"呈现语义一致性"保持学术严谨性）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>