<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">锚点注意力：基于条纹粒度的差异感知稀疏注意力</h2><a id="user-content-锚点注意力基于条纹粒度的差异感知稀疏注意力" class="anchor" aria-label="Permalink: 锚点注意力：基于条纹粒度的差异感知稀疏注意力" href="#锚点注意力基于条纹粒度的差异感知稀疏注意力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>具有超长上下文处理能力的大语言模型（LLMs）在预填充阶段面临严峻的计算挑战，这主要源于自注意力机制的二次方复杂度。现有方法通常采用动态模式匹配和块稀疏底层实现，但其依赖局部信息进行模式识别的特性难以捕捉全局上下文，且粗粒度的块处理会导致持续的内部稀疏性，最终影响准确性与效率。为突破这些局限，我们提出\textbf{锚点注意力（AnchorAttention）}——一种差异感知的动态稀疏注意力机制，能在更精细的条状粒度下高效定位关键注意力区域，同时自适应全局上下文信息，实现速度与精度的双重提升。该机制包含三大核心组件：（1）\textbf{基于模式的锚点计算}：利用输入数据间的共性特征快速计算一组近最大分数作为锚点；（2）\textbf{差异感知的条状稀疏识别}：通过与锚点的差异感知比较，快速获取显著区域在条状稀疏模式中的离散坐标；（3）\textbf{细粒度稀疏计算}：用离散键值位置并行加载取代传统的连续键值块加载方式，在保留硬件完整计算潜力的同时最大化稀疏率。凭借更精细的稀疏策略，\textbf{锚点注意力}在相同召回率下可实现更高稀疏率，显著缩短计算耗时。相比现有最优方法，在处理128k文本长度时实现1.44$\times$加速，同时保持更高召回率。</p>
<div class="markdown-heading"><h2 class="heading-element">MuLoCo：μ子（Muon）作为DiLoCo的高效内置优化器</h2><a id="user-content-mulocoμ子muon作为diloco的高效内置优化器" class="anchor" aria-label="Permalink: MuLoCo：μ子（Muon）作为DiLoCo的高效内置优化器" href="#mulocoμ子muon作为diloco的高效内置优化器"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>DiLoCo是一种在有限网络条件下训练大语言模型（LLM）的强大框架，其优势在于能显著提升数据中心环境中的并行计算能力与加速器利用率。尽管该框架大幅降低了通信频率，但其通信步骤仍需要对模型全部参数执行全局归约操作。现有研究虽探索了降低DiLoCo通信负荷的方法，但误差反馈累加器的作用及内部优化器对参数可压缩性的影响尚未得到充分研究。本文系统评估了Top-k稀疏化与量化等标准压缩方法在搭配两种本地优化器（AdamW与Muon）时对降低DiLoCo通信开销的有效性。通过预训练仅解码器架构的Transformer语言模型实验，我们发现：采用Muon作为DiLoCo内部优化器并配合误差反馈累加器时，可将通信参数差值激进压缩至2比特且几乎无性能损失。尤为关键的是，MuLoCo（采用Muon内部优化器的DiLoCo）在通信量减少8倍、内存复杂度保持不变的条件下，性能显著超越原始DiLoCo方案。</p>
<p>（译文特点说明：</p>
<ol>
<li>技术术语规范处理："all-reducing"译为"全局归约"，"decoder-only transformer"译为"仅解码器架构的Transformer"</li>
<li>复杂句式重构：将英文长句拆分为符合中文表达习惯的短句，如将"Despite...however..."转折关系转换为"尽管...但..."的句式</li>
<li>被动语态转化："remain under-explored"主动化为"尚未得到充分研究"</li>
<li>概念准确传达："error-feedback accumulator"译为专业术语"误差反馈累加器"，"aggressively compress"意译为"激进压缩"</li>
<li>技术缩写处理：首次出现LLM时补充完整名称"大语言模型"，MuLoCo保留英文缩写并添加括号说明</li>
<li>逻辑关系显化：通过"尤为关键的是"等连接词强化段落间的递进关系）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">LoLA：采用稀疏缓存的低秩线性注意力机制</h2><a id="user-content-lola采用稀疏缓存的低秩线性注意力机制" class="anchor" aria-label="Permalink: LoLA：采用稀疏缓存的低秩线性注意力机制" href="#lola采用稀疏缓存的低秩线性注意力机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明如下：</p>
<ol>
<li>"LoLA"作为专有技术名称保留不译</li>
<li>"Low-Rank Linear Attention"译为"低秩线性注意力"，其中：
<ul>
<li>"Low-Rank"是机器学习术语，标准译法为"低秩"</li>
<li>"Linear Attention"译为"线性注意力"，这是Transformer架构中的专业概念</li>
</ul>
</li>
<li>"With Sparse Caching"译为"采用稀疏缓存"，其中：
<ul>
<li>"Sparse"是计算机术语，译为"稀疏"</li>
<li>"Caching"译为"缓存"，指临时存储机制</li>
</ul>
</li>
<li>整体采用技术文献常用的名词短语结构，用"机制"二字补足中文表达习惯</li>
<li>冒号使用中文全角符号，保持中文排版规范）</li>
</ol>
<p>基于Transformer的大型语言模型在长序列推理时面临二次方复杂度问题。线性注意力方法虽为高效替代方案，却难以精准逼近softmax注意力的效果。通过在每线性注意力头中引入滑动窗口注意力，可在短上下文任务中弥补这一差距。然而，由于"记忆碰撞"现象，这类方法难以从长上下文中提取关键信息。本文提出LoLA：支持稀疏缓存的低秩线性注意力机制。LoLA通过独立存储可能干扰历史关联记忆的键值对，将过往键值对分配至三种记忆形态：(i) 局部滑动窗口中的近期键值对；(ii) 稀疏全局缓存中难以记忆的键值对；(iii) 线性注意力循环隐状态中的通用键值对。作为纯推理策略，LoLA在RULER基准的"大海捞针"任务中实现了8K上下文长度的密钥检索，将基础次二次模型的准确率从4K长度时的0.6%提升至97.4%，且缓存体积比Llama-3.1 8B小4.6倍。在10亿和80亿参数规模的次二次模型中，LoLA在零样本常识推理任务上表现优异。该方法极具轻量化特性：几乎所有实验均可通过单张消费级GPU复现。</p>
<div class="markdown-heading"><h2 class="heading-element">降噪旋转器：通过重要性集中提升大型语言模型的修剪鲁棒性</h2><a id="user-content-降噪旋转器通过重要性集中提升大型语言模型的修剪鲁棒性" class="anchor" aria-label="Permalink: 降噪旋转器：通过重要性集中提升大型语言模型的修剪鲁棒性" href="#降噪旋转器通过重要性集中提升大型语言模型的修剪鲁棒性"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>剪枝是一种广泛用于压缩大语言模型（LLM）的技术，通过移除不重要的权重来减小模型规模，但这种方法通常会导致显著的性能下降——尤其是在半结构化稀疏性约束下。现有剪枝方法主要聚焦于评估单个权重的重要性，这种思路限制了其保留模型关键能力的效果。本研究提出新视角：与其单纯筛选待剪枝的权重，不如先对参数重要性进行重新分配，使模型本身更适应剪枝操作。通过最小化归一化重要性分数的信息熵，我们的方法将重要性集中到更小的权重子集上，从而增强剪枝鲁棒性。我们通过DenoiseRotator实现这一理念：该方法对模型权重矩阵施加可学习的正交变换。我们的方案与模型架构无关，可无缝集成Magnitude、SparseGPT、Wanda等现有剪枝技术。在LLaMA3、Qwen2.5和Mistral模型上的评估表明，无论是50%非结构化稀疏还是2:4半结构化稀疏场景，DenoiseRotator均能持续改善困惑度和零样本准确率。以SparseGPT对LLaMA3-70B进行2:4半结构化剪枝为例，DenoiseRotator将困惑度与稠密模型的差距缩小了58%，使性能下降从8.1个点降至3.4个点。代码已开源：<a href="https://github.com/Axel-gu/DenoiseRotator%E3%80%82">https://github.com/Axel-gu/DenoiseRotator。</a></p>
<p>（注：根据技术文本翻译规范，对以下术语进行了统一处理：</p>
<ol>
<li>"pruning"译为"剪枝"而非"修剪"，符合计算机领域术语标准</li>
<li>"semi-structured sparsity"译为"半结构化稀疏性"，保留"结构化"这一关键特征</li>
<li>"perplexity"译为"困惑度"，采用自然语言处理领域通用译法</li>
<li>"zero-shot"保留英文术语特征译为"零样本"，未直译为"零射击"</li>
<li>长难句采用拆分策略，如将原文"By minimizing..."复合句拆分为两个中文短句，符合汉语表达习惯）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">面向多目标域自适应的合并友好型训练后量化</h2><a id="user-content-面向多目标域自适应的合并友好型训练后量化" class="anchor" aria-label="Permalink: 面向多目标域自适应的合并友好型训练后量化" href="#面向多目标域自适应的合并友好型训练后量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>模型融合已成为一种强大的技术，能够整合任务特定权重，在多目标领域自适应中实现卓越性能。然而，当应用于量化模型等实际场景时，新的挑战随之出现。在实际应用中，量化通常针对特定目标数据进行，但这一过程会限制关注领域并引入离散化效应，使得模型融合变得异常复杂。本研究通过误差屏障的视角，系统分析了量化对模型融合的影响。基于这些发现，我们提出了一种新颖的训练后量化方法——HDRQ（海森矩阵与距离正则化量化），该方法专为多目标领域自适应的模型融合需求而设计。我们的方案确保量化过程与源预训练模型的偏差最小化，同时通过平坦化损失曲面来促进平滑的模型融合。据我们所知，这是针对该挑战的首项研究，大量实验证实了其有效性。</p>
<p>（注：翻译过程中对以下专业术语进行了标准化处理：</p>
<ol>
<li>"error barriers"译为"误差屏障"（机器学习领域常用术语）</li>
<li>"Hessian"保留专业称谓"海森矩阵"而非直译</li>
<li>"flattening the loss surface"译为"平坦化损失曲面"以准确反映优化目标</li>
<li>将英语长句合理切分为符合中文表达习惯的短句，如将原文最后复合句拆分为三个分句）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">"少即是多：通过结构化剪裁解锁时序基础模型的专精化"</h2><a id="user-content-少即是多通过结构化剪裁解锁时序基础模型的专精化" class="anchor" aria-label="Permalink: &quot;少即是多：通过结构化剪裁解锁时序基础模型的专精化&quot;" href="#少即是多通过结构化剪裁解锁时序基础模型的专精化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>扩展定律推动了时间序列基础模型（TSFMs）的发展，这些模型通过预训练海量参数实现了卓越的零样本预测性能。但令人惊讶的是，即便经过微调，TSFMs仍无法持续超越那些基于完整下游数据训练的小型专业模型。核心问题在于：如何实现TSFMs面向目标预测任务的有效适配？通过对多种TSFM的实证研究，我们发现预训练模型往往存在固有的计算稀疏性与冗余性，这表明TSFMs已学会激活任务相关的网络子结构来适应不同预测任务。为保留这一宝贵的先验知识，我们提出结构化剪枝方法，通过将微调过程聚焦于更相关且紧凑的参数空间来实现正则化。在七种TSFM和六个基准测试上的大量实验表明，相比原始模型，对经过剪枝的轻量化TSFM进行微调能显著提升预测性能。这种"先剪枝后微调"范式常能使TSFMs达到最先进的性能水平，甚至超越强大的专业基线模型。</p>
<div class="markdown-heading"><h2 class="heading-element">模型保持自适应舍入</h2><a id="user-content-模型保持自适应舍入" class="anchor" aria-label="Permalink: 模型保持自适应舍入" href="#模型保持自适应舍入"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>训练后量化（PTQ）的核心目标是生成一个压缩模型，其输出分布尽可能接近原始模型。为实现这一目标，几乎所有大语言模型（LLM）PTQ算法都通过独立最小化即时激活误差来量化线性层。然而，这种局部优化目标忽略了后续层的影响，因此误差的降低未必能使模型更接近原始表现。本研究提出"另一种量化算法"（YAQA），这是一种自适应舍入算法，利用基于\textit{全模型}KL散度的各线性层Hessian矩阵的克罗内克分解近似。YAQA包含两个组件：可高效计算千亿参数LLM全层级Hessian矩阵的克罗内克分解草图，以及利用这些草图并具备理论保证的量化器无关舍入算法。在多种模型和量化器的测试中，YAQA将模型KL散度降低约30%，同时在下游任务中达到最先进性能。</p>
<div class="markdown-heading"><h2 class="heading-element">KVzip：支持上下文重建的查询无关键值缓存压缩技术</h2><a id="user-content-kvzip支持上下文重建的查询无关键值缓存压缩技术" class="anchor" aria-label="Permalink: KVzip：支持上下文重建的查询无关键值缓存压缩技术" href="#kvzip支持上下文重建的查询无关键值缓存压缩技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"Query-Agnostic" 译为"查询无关"，表示该技术不受具体查询内容影响</li>
<li>"KV Cache Compression" 采用计算机领域通用译法"键值缓存压缩"</li>
<li>副标题采用"支持...的..."句式，通过"上下文重建"准确传达"Context Reconstruction"的技术特性</li>
<li>整体保持技术术语的准确性，同时符合中文技术文献的命名习惯）</li>
</ol>
<p>基于Transformer架构的大语言模型（LLM）在推理过程中会将上下文缓存为键值对（KV）。随着上下文长度增加，KV缓存规模急剧膨胀，导致显著的内存开销与注意力延迟上升。本文提出KVzip——一种与查询无关的KV缓存淘汰方法，通过压缩KV缓存实现跨多样化查询的高效复用。该方法利用底层LLM量化KV对的重要性：通过重构缓存KV对还原原始上下文，据此淘汰低重要性键值对。大量实验表明，KVzip能将KV缓存体积压缩至1/3-1/4，FlashAttention解码延迟降低约50%，且在问答、检索、推理及代码理解任务中性能损失可忽略不计。评估涵盖LLaMA3.1-8B、Qwen2.5-14B和Gemma3-12B等多种模型，上下文长度最高达17万token。相较于现有查询感知型KV淘汰方案（在多查询场景下即使保留90%缓存仍会出现性能衰减），KVzip展现出显著优势。</p>
<p>（注：根据技术文献翻译规范，对以下要点进行了专业化处理：</p>
<ol>
<li>"3-4×"转换为分数形式"1/3-1/4"更符合中文表达习惯</li>
<li>"170K tokens"译为"17万token"遵循中文数字单位</li>
<li>"query-agnostic"译为"与查询无关"准确传达技术概念</li>
<li>长难句拆分重组，如将原文最后复合句拆分为对比结构</li>
<li>专业术语如FlashAttention、Transformer等保留英文原名）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>