<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/13/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10132025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/13/2025" href="#personalized-daily-arxiv-papers-10132025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 15</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">LOTION: Smoothing the Optimization Landscape for Quantized Training</a>
<strong>Authors:</strong> Mujin Kwun, Depen Morwani, Chloe Huangyuan Su, Stephanie Gil, Nikhil Anand, Sham Kakade</p>
</li>
<li>
<p><a href="#link1">Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers</a>
<strong>Authors:</strong> Rui Bu, Haofeng Zhong, Wenzheng Chen, Yangyan Li</p>
</li>
<li>
<p><a href="#link2">Localist LLMs -- A Mathematical Framework for Dynamic Locality Control</a>
<strong>Authors:</strong> Joachim Diederich</p>
</li>
<li>
<p><a href="#link3">FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation</a>
<strong>Authors:</strong> Samuel Hildebrand (Louisiana State University), Curtis Taylor (Oak Ridge National Lab), Sean Oesch (Oak Ridge National Lab), James M Ghawaly Jr (Louisiana State University), Amir Sadovnik (Oak Ridge National Lab), Ryan Shivers (Oak Ridge National Lab), Brandon Schreiber (Oak Ridge National Lab), Kevin Kurian (University of Florida)</p>
</li>
<li>
<p><a href="#link4">Cross-attention Secretly Performs Orthogonal Alignment in Recommendation Models</a>
<strong>Authors:</strong> Hyunin Lee, Yong Zhang, Hoang Vu Nguyen, Xiaoyi Liu, Namyong Park, Christopher Jung, Rong Jin, Yang Wang, Zhigang Wang, Somayeh Sojoudi, Xue Feng</p>
</li>
<li>
<p><a href="#link5">Automated Evolutionary Optimization for Resource-Efficient Neural Network Training</a>
<strong>Authors:</strong> Ilia Revin, Leon Strelkov, Vadim A. Potemkin, Ivan Kireev, Andrey Savchenko</p>
</li>
<li>
<p><a href="#link6">Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging</a>
<strong>Authors:</strong> Qixiang Yin, Huanjin Yao, Jianghao Chen, Jiaxing Huang, Zhicheng Zhao, Fei Su</p>
</li>
<li>
<p><a href="#link7">AdaPM: a Partial Momentum Algorithm for LLM Training</a>
<strong>Authors:</strong> Yimu Zhang, Yuanshi Liu, Cong Fang</p>
</li>
<li>
<p><a href="#link8">Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs</a>
<strong>Authors:</strong> Yumin Choi, Dongki Kim, Jinheon Baek, Sung Ju Hwang</p>
</li>
<li>
<p><a href="#link9">Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting</a>
<strong>Authors:</strong> Suming Qiu, Jing Li, Zhicheng Zhou, Junjie Huang, Linyuan Qiu, Zhijie Sun</p>
</li>
<li>
<p><a href="#link10">iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation</a>
<strong>Authors:</strong> Chuanrui Zhang, Zhengxian Wu, Guanxing Lu, Yansong Tang, Ziwei Wang</p>
</li>
<li>
<p><a href="#link11">FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms</a>
<strong>Authors:</strong> Atul Shree, Harshith Jupuru</p>
</li>
<li>
<p><a href="#link12">LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition</a>
<strong>Authors:</strong> Yushuo Zheng, Zicheng Zhang, Xiongkuo Min, Huiyu Duan, Guangtao Zhai</p>
</li>
<li>
<p><a href="#link13">CDE: Concept-Driven Exploration for Reinforcement Learning</a>
<strong>Authors:</strong> Le Mao, Andrew H. Liu, Renos Zabounidis, Zachary Kingston, Joseph Campbell</p>
</li>
<li>
<p><a href="#link14">SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions</a>
<strong>Authors:</strong> Ziyi Wang, Nan Jiang, Guang Lin, Qifan Song</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2510.08757" rel="nofollow">LOTION: Smoothing the Optimization Landscape for Quantized Training</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-lotion-smoothing-the-optimization-landscape-for-quantized-training-" class="anchor" aria-label="Permalink: 0. LOTION: Smoothing the Optimization Landscape for Quantized Training" href="#0-lotion-smoothing-the-optimization-landscape-for-quantized-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08757
<strong>Authors:</strong> Mujin Kwun, Depen Morwani, Chloe Huangyuan Su, Stephanie Gil, Nikhil Anand, Sham Kakade</p>
<p><strong>Abstract:</strong> arXiv:2510.08757v1 Announce Type: new  Abstract: Optimizing neural networks for quantized objectives is fundamentally challenging because the quantizer is piece-wise constant, yielding zero gradients everywhere except at quantization thresholds where the derivative is undefined. Most existing methods deal with this issue by relaxing gradient computations with techniques like Straight Through Estimators (STE) and do not provide any guarantees of convergence. In this work, taking inspiration from Nesterov smoothing, we approximate the quantized loss surface with a continuous loss surface. In particular, we introduce LOTION, \textbf{L}ow-precision \textbf{O}ptimization via s\textbf{T}ochastic-no\textbf{I}se sm\textbf{O}othi\textbf{N}g, a principled smoothing framework that replaces the raw quantized loss with its expectation under unbiased randomized-rounding noise. In this framework, standard optimizers are guaranteed to converge to a local minimum of the loss surface. Moreover, when using noise derived from stochastic rounding, we show that the global minima of the original quantized loss are preserved. We empirically demonstrate that this method outperforms standard QAT on synthetic testbeds and on 150M- and 300M- parameter language models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2510.09017" rel="nofollow">Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-value-state-gated-attention-for-mitigating-extreme-token-phenomena-in-transformers-" class="anchor" aria-label="Permalink: 1. Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers" href="#1-value-state-gated-attention-for-mitigating-extreme-token-phenomena-in-transformers-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09017
<strong>Authors:</strong> Rui Bu, Haofeng Zhong, Wenzheng Chen, Yangyan Li</p>
<p><strong>Abstract:</strong> arXiv:2510.09017v1 Announce Type: new  Abstract: Large models based on the Transformer architecture are susceptible to extreme-token phenomena, such as attention sinks and value-state drains. These issues, which degrade model performance, quantization fidelity, and interpretability, arise from a problematic mutual reinforcement mechanism where the model learns an inefficient 'no-op' behavior by focusing attention on tokens with near-zero value states. In this paper, we propose Value-State Gated Attention (VGA), a simple, dedicated, and stable architectural mechanism for performing 'no-op' attention efficiently by directly breaking this cycle. VGA introduces a learnable, data-dependent gate, computed directly from the value vectors (V), to modulate the output. Through a theoretical analysis of the underlying gradients, we show that gating the value-state with a function of itself is more effective at decoupling value and attention score updates than prior methods that gate on input embeddings. This creates a direct regulatory pathway that allows the model to suppress a token's contribution based on its emergent value representation. Our experiments demonstrate that VGA significantly mitigates the formation of attention sinks and stabilizes value-state norms, leading to improved performance, robust quantization fidelity, and enhanced model interpretability.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2510.09338" rel="nofollow">Localist LLMs -- A Mathematical Framework for Dynamic Locality Control</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-localist-llms----a-mathematical-framework-for-dynamic-locality-control-" class="anchor" aria-label="Permalink: 2. Localist LLMs -- A Mathematical Framework for Dynamic Locality Control" href="#2-localist-llms----a-mathematical-framework-for-dynamic-locality-control-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09338
<strong>Authors:</strong> Joachim Diederich</p>
<p><strong>Abstract:</strong> arXiv:2510.09338v1 Announce Type: new  Abstract: We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovation is a locality dial, a tunable parameter that dynamically controls the degree of localization during both training and inference without requiring model retraining. This is achieved through group sparsity penalties on attention mechanisms, information-theoretic anchor design, and dynamic rule injection. We provide rigorous mathematical proofs establishing explicit threshold conditions under which attention provably concentrates on semantically relevant blocks, with exponential bounds on attention entropy and pointer fidelity. Specifically, we prove that when group sparsity penalties exceed certain threshold values, the model's attention mechanisms concentrate on semantically relevant blocks, achieving low entropy and high fidelity with negligible error. This framework enables practitioners to continuously interpolate between interpretable and high-performance modes, supporting applications in regulated domains requiring both transparency and capability.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2510.08945" rel="nofollow">FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-fathoms-rag-a-framework-for-the-assessment-of-thinking-and-observation-in-multimodal-systems-that-use-retrieval-augmented-generation-" class="anchor" aria-label="Permalink: 3. FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation" href="#3-fathoms-rag-a-framework-for-the-assessment-of-thinking-and-observation-in-multimodal-systems-that-use-retrieval-augmented-generation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08945
<strong>Authors:</strong> Samuel Hildebrand (Louisiana State University), Curtis Taylor (Oak Ridge National Lab), Sean Oesch (Oak Ridge National Lab), James M Ghawaly Jr (Louisiana State University), Amir Sadovnik (Oak Ridge National Lab), Ryan Shivers (Oak Ridge National Lab), Brandon Schreiber (Oak Ridge National Lab), Kevin Kurian (University of Florida)</p>
<p><strong>Abstract:</strong> arXiv:2510.08945v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) has emerged as a promising paradigm for improving factual accuracy in large language models (LLMs). We introduce a benchmark designed to evaluate RAG pipelines as a whole, evaluating a pipeline's ability to ingest, retrieve, and reason about several modalities of information, differentiating it from existing benchmarks that focus on particular aspects such as retrieval. We present (1) a small, human-created dataset of 93 questions designed to evaluate a pipeline's ability to ingest textual data, tables, images, and data spread across these modalities in one or more documents; (2) a phrase-level recall metric for correctness; (3) a nearest-neighbor embedding classifier to identify potential pipeline hallucinations; (4) a comparative evaluation of 2 pipelines built with open-source retrieval mechanisms and 4 closed-source foundation models; and (5) a third-party human evaluation of the alignment of our correctness and hallucination metrics. We find that closed-source pipelines significantly outperform open-source pipelines in both correctness and hallucination metrics, with wider performance gaps in questions relying on multimodal and cross-document information. Human evaluation of our metrics showed average agreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5 Likert scale (5 indicating "strongly agree").</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2510.09435" rel="nofollow">Cross-attention Secretly Performs Orthogonal Alignment in Recommendation Models</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-cross-attention-secretly-performs-orthogonal-alignment-in-recommendation-models-" class="anchor" aria-label="Permalink: 4. Cross-attention Secretly Performs Orthogonal Alignment in Recommendation Models" href="#4-cross-attention-secretly-performs-orthogonal-alignment-in-recommendation-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09435
<strong>Authors:</strong> Hyunin Lee, Yong Zhang, Hoang Vu Nguyen, Xiaoyi Liu, Namyong Park, Christopher Jung, Rong Jin, Yang Wang, Zhigang Wang, Somayeh Sojoudi, Xue Feng</p>
<p><strong>Abstract:</strong> arXiv:2510.09435v1 Announce Type: new  Abstract: Cross-domain sequential recommendation (CDSR) aims to align heterogeneous user behavior sequences collected from different domains. While cross-attention is widely used to enhance alignment and improve recommendation performance, its underlying mechanism is not fully understood. Most researchers interpret cross-attention as residual alignment, where the output is generated by removing redundant and preserving non-redundant information from the query input by referencing another domain data which is input key and value. Beyond the prevailing view, we introduce Orthogonal Alignment, a phenomenon in which cross-attention discovers novel information that is not present in the query input, and further argue that those two contrasting alignment mechanisms can co-exist in recommendation models We find that when the query input and output of cross-attention are orthogonal, model performance improves over 300 experiments. Notably, Orthogonal Alignment emerges naturally, without any explicit orthogonality constraints. Our key insight is that Orthogonal Alignment emerges naturally because it improves scaling law. We show that baselines additionally incorporating cross-attention module outperform parameter-matched baselines, achieving a superior accuracy-per-model parameter. We hope these findings offer new directions for parameter-efficient scaling in multi-modal research.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2510.09566" rel="nofollow">Automated Evolutionary Optimization for Resource-Efficient Neural Network Training</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-automated-evolutionary-optimization-for-resource-efficient-neural-network-training-" class="anchor" aria-label="Permalink: 5. Automated Evolutionary Optimization for Resource-Efficient Neural Network Training" href="#5-automated-evolutionary-optimization-for-resource-efficient-neural-network-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09566
<strong>Authors:</strong> Ilia Revin, Leon Strelkov, Vadim A. Potemkin, Ivan Kireev, Andrey Savchenko</p>
<p><strong>Abstract:</strong> arXiv:2510.09566v1 Announce Type: new  Abstract: There are many critical challenges in optimizing neural network models, including distributed computing, compression techniques, and efficient training, regardless of their application to specific tasks. Solving such problems is crucial because the need for scalable and resource-efficient models is increasing. To address these challenges, we have developed a new automated machine learning (AutoML) framework, Parameter Efficient Training with Robust Automation (PETRA). It applies evolutionary optimization to model architecture and training strategy. PETRA includes pruning, quantization, and loss regularization. Experimental studies on real-world data with financial event sequences, as well as image and time-series -- benchmarks, demonstrate PETRA's ability to improve neural model performance and scalability -- namely, a significant decrease in model size (up to 75%) and latency (up to 33%), and an increase in throughput (by 13%) without noticeable degradation in the target metric.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2510.08987" rel="nofollow">Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-tiny-r1v-lightweight-multimodal-unified-reasoning-model-via-model-merging-" class="anchor" aria-label="Permalink: 6. Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging" href="#6-tiny-r1v-lightweight-multimodal-unified-reasoning-model-via-model-merging-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08987
<strong>Authors:</strong> Qixiang Yin, Huanjin Yao, Jianghao Chen, Jiaxing Huang, Zhicheng Zhao, Fei Su</p>
<p><strong>Abstract:</strong> arXiv:2510.08987v1 Announce Type: new  Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, they encounter numerous challenges in terms of reasoning efficiency, such as large model size, overthinking, and compromised accuracy in lightweight scenarios. However, research on the reasoning capabilities of lightweight MLLMs is quite lacking. To this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves faster inference and higher accuracy via a two-stage optimization, while unifying multimodal reasoning across multiple tasks and using fewer tokens. In the first stage, Tiny-R1V introduces Length-Informed Relative Policy Optimization (LIPO), a novel reinforcement learning method, to train each reasoning model. The LIPO is designed to dynamically adjusts advantages of responses within groups, that is, by prioritizing concise yet high-quality responses to encourage the generation of shorter and more accurate response. In the second stage, we propose Adaptive Model Merging (AMM), a training-free model merging method that merges multiple specialist models into a unified architecture. Specifically, AMM adaptively adjusts the weights of task vectors and robustly optimizes the merged vectors via a novel gradient projection regularization loss function, thus mitigating redundant conflicts between them. Extensive evaluations on ten widely-used reasoning benchmarks covering mathematics, structured data (charts, tables, documents), OCR, and general capabilities showcase the superior performance of Tiny-R1V, enabling lightweight models to excel in diverse multimodal reasoning tasks.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2510.09103" rel="nofollow">AdaPM: a Partial Momentum Algorithm for LLM Training</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-adapm-a-partial-momentum-algorithm-for-llm-training-" class="anchor" aria-label="Permalink: 7. AdaPM: a Partial Momentum Algorithm for LLM Training" href="#7-adapm-a-partial-momentum-algorithm-for-llm-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09103
<strong>Authors:</strong> Yimu Zhang, Yuanshi Liu, Cong Fang</p>
<p><strong>Abstract:</strong> arXiv:2510.09103v1 Announce Type: new  Abstract: In the training of large language models, momentum is widely used and often demonstrated to achieve significant acceleration. However, storing momentum typically presents memory challenges. In this paper, we propose AdaPM, an adaptive training strategy that leverages partial momentum to implement a memory-efficient optimizer. To this end, AdaPM utilizes a non-uniform momentum design: for most blocks, full momentum is not necessary to preserve the performance of the optimization. In the momentum design of AdaPM, to mitigate the bias and performance loss caused by partial momentum, we enhance the partial momentum by a bias correction technique. Empirically, we verify that our approach reduces memory by over $90%$ in momentum while maintaining both efficiency and performance for pretraining various language models ranging from 60M to 1.5B, as well as for supervised fine-tuning and RLHF. AdaPM can further reduce memory by up to $95%$ in optimizer states by combining the memory-efficient technique on the second-order statistic, saving over $30%$ GPU hours for pretraining GPT-2 1.5B.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2510.09201" rel="nofollow">Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-multimodal-prompt-optimization-why-not-leverage-multiple-modalities-for-mllms-" class="anchor" aria-label="Permalink: 8. Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs" href="#8-multimodal-prompt-optimization-why-not-leverage-multiple-modalities-for-mllms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09201
<strong>Authors:</strong> Yumin Choi, Dongki Kim, Jinheon Baek, Sung Ju Hwang</p>
<p><strong>Abstract:</strong> arXiv:2510.09201v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2510.09152" rel="nofollow">Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-logits-replay--moclip-stabilized-low-cost-post-training-with-minimal-forgetting-" class="anchor" aria-label="Permalink: 9. Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting" href="#9-logits-replay--moclip-stabilized-low-cost-post-training-with-minimal-forgetting-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09152
<strong>Authors:</strong> Suming Qiu, Jing Li, Zhicheng Zhou, Junjie Huang, Linyuan Qiu, Zhijie Sun</p>
<p><strong>Abstract:</strong> arXiv:2510.09152v1 Announce Type: new  Abstract: Large language models (LLMs) often face a trade-off in post-training: improvements on specialized domains frequently come at the expense of general capabilities. Existing solutions attempt to mitigate this tension via regularization, selective parameter updates, or data-centric replay, but each imposes significant costs in computation, data access, or adaptability. Recent work has shown that training signals can be compressed to subsets of logits without severe accuracy loss, suggesting a path toward efficient adaptation. However, naive truncation destabilizes optimization and exacerbates forgetting.   We introduce Logits Replay + MoClip, a two-stage framework that compresses supervision in the logit space and stabilizes optimization at the update level. In Stage 0, we record dynamic Top-K token subsets that cover a probability threshold, always including the gold label. In Stage 1, we replay these compact subsets to compute exact renormalized losses, avoiding full softmax computation and implicitly regularizing. To ensure stability, we design MoClip, an optimizer that caps gradient-momentum rotation and applies an arctan2-based rescaling of updates. Empirically, our method improves domain performance on Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over 40%. Together, these contributions offer a scalable, architecture-agnostic path for domain adaptation of LLMs without sacrificing generalization.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2510.09036" rel="nofollow">iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-imowm-taming-interactive-multi-modal-world-model-for-robotic-manipulation-" class="anchor" aria-label="Permalink: 10. iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation" href="#10-imowm-taming-interactive-multi-modal-world-model-for-robotic-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09036
<strong>Authors:</strong> Chuanrui Zhang, Zhengxian Wu, Guanxing Lu, Yansong Tang, Ziwei Wang</p>
<p><strong>Abstract:</strong> arXiv:2510.09036v1 Announce Type: new  Abstract: Learned world models hold significant potential for robotic manipulation, as they can serve as simulator for real-world interactions. While extensive progress has been made in 2D video-based world models, these approaches often lack geometric and spatial reasoning, which is essential for capturing the physical structure of the 3D world. To address this limitation, we introduce iMoWM, a novel interactive world model designed to generate color images, depth maps, and robot arm masks in an autoregressive manner conditioned on actions. To overcome the high computational cost associated with three-dimensional information, we propose MMTokenizer, which unifies multi-modal inputs into a compact token representation. This design enables iMoWM to leverage large-scale pretrained VideoGPT models while maintaining high efficiency and incorporating richer physical information. With its multi-modal representation, iMoWM not only improves the visual quality of future predictions but also serves as an effective simulator for model-based reinforcement learning (MBRL) and facilitates real-world imitation learning. Extensive experiments demonstrate the superiority of iMoWM across these tasks, showcasing the advantages of multi-modal world modeling for robotic manipulation. Homepage: <a href="https://xingyoujun.github.io/imowm/" rel="nofollow">https://xingyoujun.github.io/imowm/</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2510.09085" rel="nofollow">FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-fltop-ctc-frame-level-token-pruning-via-relative-threshold-for-efficient-and-memory-saving-decoding-on-diverse-platforms-" class="anchor" aria-label="Permalink: 11. FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms" href="#11-fltop-ctc-frame-level-token-pruning-via-relative-threshold-for-efficient-and-memory-saving-decoding-on-diverse-platforms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.09085
<strong>Authors:</strong> Atul Shree, Harshith Jupuru</p>
<p><strong>Abstract:</strong> arXiv:2510.09085v1 Announce Type: new  Abstract: CTC-based ASR systems face computational and memory bottlenecks in resource-limited environments. Traditional CTC decoders, requiring up to 90% of processing time in systems (e.g., wav2vec2-large on L4 GPUs), face inefficiencies due to exhaustive token-level operations. This paper introduces Frame Level Token Pruning for Connectionist Temporal Classification (FLToP CTC), a novel decoding algorithm that employs frame-level token pruning guided by a relative threshold probability. By dynamically eliminating low-probability tokens per frame, FLToP CTC reduces compute and memory demands while maintaining negligible WER degradation. On LibriSpeech, FLToP CTC achieves a 10.5x runtime speedup and 2.78x memory reduction versus standard CTC decoders. Its simplicity enables seamless integration into CTC decoders across platforms (CPUs, GPUs, etc.). FLToP CTC addresses CTC bottlenecks, offering scalability for resource-limited environments and realtime applications, enhancing speech recognition accessibility and efficiency.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2510.08928" rel="nofollow">LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-lm-fight-arena-benchmarking-large-multimodal-models-via-game-competition-" class="anchor" aria-label="Permalink: 12. LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition" href="#12-lm-fight-arena-benchmarking-large-multimodal-models-via-game-competition-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08928
<strong>Authors:</strong> Yushuo Zheng, Zicheng Zhang, Xiongkuo Min, Huiyu Duan, Guangtao Zhai</p>
<p><strong>Abstract:</strong> arXiv:2510.08928v1 Announce Type: new  Abstract: Existing benchmarks for large multimodal models (LMMs) often fail to capture their performance in real-time, adversarial environments. We introduce LM Fight Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by pitting them against each other in the classic fighting game Mortal Kombat II, a task requiring rapid visual understanding and tactical, sequential decision-making. In a controlled tournament, we test six leading open- and closed-source models, where each agent operates controlling the same character to ensure a fair comparison. The models are prompted to interpret game frames and state data to select their next actions. Unlike static evaluations, LM Fight Arena provides a fully automated, reproducible, and objective assessment of an LMM's strategic reasoning capabilities in a dynamic setting. This work introduces a challenging and engaging benchmark that bridges the gap between AI evaluation and interactive entertainment.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2510.08851" rel="nofollow">CDE: Concept-Driven Exploration for Reinforcement Learning</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-cde-concept-driven-exploration-for-reinforcement-learning-" class="anchor" aria-label="Permalink: 13. CDE: Concept-Driven Exploration for Reinforcement Learning" href="#13-cde-concept-driven-exploration-for-reinforcement-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08851
<strong>Authors:</strong> Le Mao, Andrew H. Liu, Renos Zabounidis, Zachary Kingston, Joseph Campbell</p>
<p><strong>Abstract:</strong> arXiv:2510.08851v1 Announce Type: new  Abstract: Intelligent exploration remains a critical challenge in reinforcement learning (RL), especially in visual control tasks. Unlike low-dimensional state-based RL, visual RL must extract task-relevant structure from raw pixels, making exploration inefficient. We propose Concept-Driven Exploration (CDE), which leverages a pre-trained vision-language model (VLM) to generate object-centric visual concepts from textual task descriptions as weak, potentially noisy supervisory signals. Rather than directly conditioning on these noisy signals, CDE trains a policy to reconstruct the concepts via an auxiliary objective, using reconstruction accuracy as an intrinsic reward to guide exploration toward task-relevant objects. Because the policy internalizes these concepts, VLM queries are only needed during training, reducing dependence on external models during deployment. Across five challenging simulated visual manipulation tasks, CDE achieves efficient, targeted exploration and remains robust to noisy VLM predictions. Finally, we demonstrate real-world transfer by deploying CDE on a Franka Research 3 arm, attaining an 80% success rate in a real-world manipulation task.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2510.08999" rel="nofollow">SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-sqs-bayesian-dnn-compression-through-sparse-quantized-sub-distributions-" class="anchor" aria-label="Permalink: 14. SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions" href="#14-sqs-bayesian-dnn-compression-through-sparse-quantized-sub-distributions-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2510.08999
<strong>Authors:</strong> Ziyi Wang, Nan Jiang, Guang Lin, Qifan Song</p>
<p><strong>Abstract:</strong> arXiv:2510.08999v1 Announce Type: new  Abstract: Compressing large-scale neural networks is essential for deploying models on resource-constrained devices. Most existing methods adopt weight pruning or low-bit quantization individually, often resulting in suboptimal compression rates to preserve acceptable performance drops. We introduce a unified framework for simultaneous pruning and low-bit quantization via Bayesian variational learning (SQS), which achieves higher compression rates than prior baselines while maintaining comparable performance. The key idea is to employ a spike-and-slab prior to inducing sparsity and model quantized weights using Gaussian Mixture Models (GMMs) to enable low-bit precision. In theory, we provide the consistent result of our proposed variational approach to a sparse and quantized deep neural network. Extensive experiments on compressing ResNet, BERT-base, Llama3, and Qwen2.5 models show that our method achieves higher compression rates than a line of existing methods with comparable performance drops.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) and Mixture of Experts (MoE) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research introduces a novel multimodal large model that integrates text, language, and vision modalities. The goal is to advance the performance and generalization capabilities of the model by establishing a new approach for training and fusing these modalities effectively, rather than focusing on incremental optimizations.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>