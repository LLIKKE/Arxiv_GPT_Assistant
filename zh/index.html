<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">探索大语言模型压缩的极限：问答任务上的知识蒸馏研究</h2><a id="user-content-探索大语言模型压缩的极限问答任务上的知识蒸馏研究" class="anchor" aria-label="Permalink: 探索大语言模型压缩的极限：问答任务上的知识蒸馏研究" href="#探索大语言模型压缩的极限问答任务上的知识蒸馏研究"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>大型语言模型（LLM）在一系列自然语言处理任务中展现出卓越性能，但其庞大的计算需求阻碍了在现实资源受限环境中的部署。本研究探讨了如何通过知识蒸馏（KD）技术压缩LLM规模，同时保持其在问答任务中的强劲表现。我们以Pythia和Qwen2.5系列模型为教师模型，在SQuAD和MLQA两个问答基准上评估了零样本和单样本提示条件下的学生模型表现。实验结果表明，学生模型在参数量减少高达57.1%的情况下，仍能保留教师模型90%以上的性能。此外，单样本提示策略相较零样本设置能为两个模型家族带来额外性能提升。这些发现揭示了模型效率与任务性能之间的权衡关系，证明知识蒸馏结合最小化提示策略，可以构建出适合资源受限应用的紧凑型高效问答系统。</p>
<div class="markdown-heading"><h2 class="heading-element">UnIT：面向MCU上MAC高效神经推理的可扩展非结构化推理时剪枝技术</h2><a id="user-content-unit面向mcu上mac高效神经推理的可扩展非结构化推理时剪枝技术" class="anchor" aria-label="Permalink: UnIT：面向MCU上MAC高效神经推理的可扩展非结构化推理时剪枝技术" href="#unit面向mcu上mac高效神经推理的可扩展非结构化推理时剪枝技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"UnIT" 作为专有技术名称保留不译</li>
<li>"Scalable" 译为"可扩展"以体现技术方案的扩展能力</li>
<li>"Unstructured Inference-Time Pruning" 专业术语译为"非结构化推理时剪枝"，准确表达在推理阶段进行的非结构化剪枝技术特性</li>
<li>"MAC-efficient" 译为"MAC高效"，MAC指Multiply-Accumulate运算，是神经网络计算的核心操作</li>
<li>"Neural Inference on MCUs" 译为"MCU上神经推理"，MCU(Microcontroller Unit)是微控制器通用译法</li>
<li>整体采用技术文献的简洁译风，通过"面向...的..."结构保持技术描述的准确性）</li>
</ol>
<p>现有剪枝方法通常在训练或编译阶段实施，且多依赖于结构化稀疏策略。虽然这类方法与低功耗微控制器（MCU）兼容，但在缺乏SIMD支持或并行计算能力的设备上，结构化剪枝未能充分发挥细粒度效率优化的潜力。为突破这些限制，我们提出UnIT（非结构化推理时剪枝）——一种轻量级方法，通过输入特定的激活模式动态识别并跳过推理过程中不必要的乘累加（MAC）运算。与结构化剪枝不同，UnIT充分利用非规则稀疏性，且无需重新训练或专用硬件支持。该方法将剪枝决策转化为轻量级比较运算，用阈值比较和近似除法替代乘法操作。UnIT还通过跨连接复用阈值计算、应用分层分组敏感剪枝策略进一步优化计算效率。我们针对常见嵌入式平台特性，提出三种快速硬件友好的除法近似算法。在MSP430微控制器上的实验表明：相比训练时剪枝模型，UnIT可实现11.02%至82.03%的MAC运算削减，推理速度提升27.30%至84.19%，能耗降低27.33%至84.38%，同时仅损失0.48-7%的精度。在领域偏移场景下，UnIT在显著减少MAC运算的同时，其精度可匹配甚至超越重新训练的模型。这些成果证实了非结构化推理时剪枝是一种切实可行的解决方案，能够在不重新训练的情况下实现深度神经网络在MCU上的高效部署。</p>
<div class="markdown-heading"><h2 class="heading-element">COALA：数值稳定且高效的上下文感知低秩近似框架</h2><a id="user-content-coala数值稳定且高效的上下文感知低秩近似框架" class="anchor" aria-label="Permalink: COALA：数值稳定且高效的上下文感知低秩近似框架" href="#coala数值稳定且高效的上下文感知低秩近似框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>最新研究表明，上下文感知的低秩近似技术已成为现代大规模神经网络压缩与微调的有效工具。该方法通过输入激活矩阵对范数进行加权，相较于未加权情况显著提升了性能指标。然而，现有神经网络方法因依赖涉及显式格拉姆矩阵计算及后续求逆的经典公式，存在数值不稳定性问题。我们证明这会导致近似质量下降或产生数值奇异矩阵。</p>
<p>为突破这些局限，我们提出了一种基于稳定分解的新型免求逆正则化框架，从根本上规避了现有技术的数值缺陷。我们的方法能应对以下挑战性场景：（1）校准矩阵超出GPU内存容量时；（2）输入激活矩阵接近奇异时；甚至（3）数据不足导致无法获得唯一近似解时。针对最后一种情况，我们证明了所提解会收敛至期望近似值，并推导出明确的误差边界。</p>
<p>（注：根据学术文本特点，翻译时进行了以下处理：</p>
<ol>
<li>专业术语如"Gram matrix"采用学界通用译法"格拉姆矩阵"</li>
<li>"numerically singular"译为"数值奇异"以区分数学严格定义的奇异矩阵</li>
<li>长难句按中文表达习惯拆分重组，如将三个挑战场景的英文同位语结构转换为中文分项列举</li>
<li>被动语态转换为主动表述，如"can be degraded"译为"会导致"</li>
<li>保持学术文本的客观严谨性，避免口语化表达）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">《榨取饱和海绵的潜力：面向大型语言模型的高效离策略强化微调》</h2><a id="user-content-榨取饱和海绵的潜力面向大型语言模型的高效离策略强化微调" class="anchor" aria-label="Permalink: 《榨取饱和海绵的潜力：面向大型语言模型的高效离策略强化微调》" href="#榨取饱和海绵的潜力面向大型语言模型的高效离策略强化微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>强化学习（RL）已展现出提升大语言模型（LLMs）推理能力的巨大潜力。现有强化微调（RFT）方法的核心局限在于其本质属于同策略强化学习——即未能充分利用历史学习过程中产生的数据。这不可避免地导致计算与时间成本的显著增加，对持续经济高效的模型扩展形成严峻瓶颈。为此，我们复兴了异策略强化学习范式，提出"混合策略近端策略梯度重生算法"（ReMix），这是一种通用框架，可使PPO、GRPO等同策略RFT方法有效利用异策略数据。ReMix包含三大核心组件：（1）采用高更新数据比（UTD）的混合策略近端策略梯度实现高效训练；（2）通过KL-凸策略约束平衡稳定性与灵活性的权衡；（3）策略重生机制实现从高效早期学习到稳定渐进提升的无缝过渡。实验环节，我们在PPO、GRPO及1.5B/7B基础模型上训练了系列ReMix模型。在五个数学推理基准（AIME'24、AMC'23、Minerva、OlympiadBench和MATH500）上，ReMix以0.079M响应采样/350训练步数实现1.5B模型52.10%的平均Pass@1准确率，7B模型更以0.007M/0.011M响应采样配合50/75训练步数达到63.27%/64.39%的精度。与15个前沿模型对比，ReMix在保持SOTA级性能的同时，将训练成本（采样数据量）降低30至450倍。此外，我们通过多维度分析揭示了深刻发现：包括因异策略差异的鞭梢效应导致的模型隐式偏好简短响应，以及严重异策略性下自反思行为的崩溃模式等现象。</p>
<div class="markdown-heading"><h2 class="heading-element">生成式AI时代的钓鱼攻击检测：量化大语言模型与传统模型之争</h2><a id="user-content-生成式ai时代的钓鱼攻击检测量化大语言模型与传统模型之争" class="anchor" aria-label="Permalink: 生成式AI时代的钓鱼攻击检测：量化大语言模型与传统模型之争" href="#生成式ai时代的钓鱼攻击检测量化大语言模型与传统模型之争"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>网络钓鱼攻击正变得日益复杂精密，这凸显出在检测系统开发中必须兼顾高准确率与计算效率的迫切需求。本文对传统机器学习（ML）、深度学习（DL）以及量化小参数大语言模型（LLM）在钓鱼检测中的表现进行了对比评估。通过在精选数据集上的实验，我们发现尽管当前LLM在原始准确率上逊于ML和DL方法，但其在识别基于上下文细微钓鱼线索方面展现出强大潜力。我们还探究了零样本提示与少样本提示策略的影响，揭示出经过LLM重构的电子邮件会显著降低基于ML和LLM的检测器性能。基准测试表明，像DeepSeek R1 Distill Qwen 14B（Q8_0）这样的模型仅需17GB显存即可实现超过80%的竞争性准确率，证实了其低成本部署的可行性。我们进一步评估了模型的对抗鲁棒性与性价比，并论证了轻量化LLM如何通过简洁可解释的推理支持实时决策。这些发现将优化后的LLM定位为钓鱼防御系统的潜力组件，为将可解释、高效的人工智能整合进现代网络安全框架指明了发展方向。</p>
<div class="markdown-heading"><h2 class="heading-element">跳过一层还是循环利用？预训练大语言模型的测试时深度自适应</h2><a id="user-content-跳过一层还是循环利用预训练大语言模型的测试时深度自适应" class="anchor" aria-label="Permalink: 跳过一层还是循环利用？预训练大语言模型的测试时深度自适应" href="#跳过一层还是循环利用预训练大语言模型的测试时深度自适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>预训练的神经网络能否在不进行任何微调的情况下，根据不同输入自适应调整其架构？对于简单任务是否需要动用所有层，而现有层数又是否足以应对复杂挑战？我们发现，预训练大语言模型（LLM）的各层可作为独立模块进行灵活重组，从而为每个测试样本构建出更优且可能更浅的定制化模型。具体而言，预训练模型的每一层都可以被跳过/剪枝，或像循环神经网络（RNN）那样重复使用，并与其他层以任意顺序堆叠，最终为每个样本生成独特的层链（Chain-of-Layers，CoLa）。这种组合式架构极大拓展了现有工作的边界——无论是循环预训练模块、层剪枝还是早期退出网络。我们开发了蒙特卡洛树搜索（MCTS）协议，从数学和常识推理基准数据中为每个样本探索最优CoLa配置。与固定深度的静态模型相比，CoLa支持捷径路径（快速思考）、单层循环（深度思考）及二者组合，为不同输入提供更灵活的动态架构。通过对MCTS优化的CoLa进行深入分析，我们获得两项关键发现：（1）在原始LLM预测正确的样本中，超过75%可通过更短的CoLa路径达成，揭示巨大的推理效率提升空间；（2）在原始预测错误的样本中，超过60%可通过特定CoLa获得正确结果，展现显著的性能改进潜力。这些发现凸显了固定架构LLM在推理时的局限性，为解锁测试时深度自适应的泛化能力开辟了新途径。</p>
<div class="markdown-heading"><h2 class="heading-element">多粒度时空令牌合并：实现视频大语言模型的无训练加速</h2><a id="user-content-多粒度时空令牌合并实现视频大语言模型的无训练加速" class="anchor" aria-label="Permalink: 多粒度时空令牌合并：实现视频大语言模型的无训练加速" href="#多粒度时空令牌合并实现视频大语言模型的无训练加速"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>视频大语言模型（LLMs）通过利用大量时空标记实现了强大的视频理解能力，但其计算量会随标记数量呈二次方增长。为解决这一问题，我们提出了一种无需训练的时空标记合并方法STTM。我们的核心洞见在于挖掘视频数据中局部空间和时间冗余性——这一特性在先前工作中被忽视。STTM首先通过四叉树结构的粗细搜索将每帧转换为多粒度空间标记，随后在时间维度上进行定向成对合并。这种分解式合并方法在六个视频问答基准测试中均优于现有标记缩减技术。值得注意的是，在50%标记预算下，STTM能以仅0.5%的准确率下降实现2倍加速；在30%预算下，仅2%的准确率损失即可获得3倍加速。此外，STTM具有查询无关性，可对同一视频的不同问题重复使用KV缓存。项目页面详见<a href="https://www.jshyun.me/projects/sttm%E3%80%82" rel="nofollow">https://www.jshyun.me/projects/sttm。</a></p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>