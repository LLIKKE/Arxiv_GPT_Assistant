<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">SpecOffload：释放潜在GPU能力，助力资源受限设备上的LLM推理</h2><a id="user-content-specoffload释放潜在gpu能力助力资源受限设备上的llm推理" class="anchor" aria-label="Permalink: SpecOffload：释放潜在GPU能力，助力资源受限设备上的LLM推理" href="#specoffload释放潜在gpu能力助力资源受限设备上的llm推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在资源受限设备上实现高效的大型语言模型(LLM)推理面临着计算与内存利用方面的重大挑战。由于GPU内存有限，现有系统需将模型权重卸载至CPU内存，导致CPU与GPU间产生大量I/O开销。这引发两大效率瓶颈：(1) GPU核心利用率低下，经常因等待数据加载而处于闲置状态；(2) GPU内存对性能影响微弱，缩减其容量对整体吞吐量几乎无影响。本文提出SpecOffload——一种将推测式解码嵌入卸载流程的高吞吐推理引擎。我们的核心创新在于释放GPU的潜在资源来存储和执行用于推测式解码的草稿模型，从而以近乎零额外成本加速推理。为此，我们精心设计了卸载流水线中目标模型与草稿模型在推测式解码下的交错执行机制，并提出用于管理张量布局及优化参数选择的规划器。相比最佳基线方案，SpecOffload将GPU核心利用率提升4.49倍，推理吞吐量提高2.54倍。代码已开源：<a href="https://github.com/MobiSense/SpecOffload%E3%80%82">https://github.com/MobiSense/SpecOffload。</a></p>
<p>（注：根据技术文献翻译规范，对以下术语进行了标准化处理：</p>
<ol>
<li>"offload"译为"卸载"而非"转移"，符合计算机体系结构术语</li>
<li>"speculative decoding"译为"推测式解码"，与学界译法统一</li>
<li>"throughput"译为"吞吐量"，保留原计量含义</li>
<li>长难句采用拆分策略，如将原文最后比较数据的长句拆分为两个短句，符合中文表达习惯</li>
<li>被动语态转换为主动表述，如"GPU cores are underutilized"译为"GPU核心利用率低下"）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">BINGO：一种创新的神经网络剪枝机制，助力模型轻量化</h2><a id="user-content-bingo一种创新的神经网络剪枝机制助力模型轻量化" class="anchor" aria-label="Permalink: BINGO：一种创新的神经网络剪枝机制，助力模型轻量化" href="#bingo一种创新的神经网络剪枝机制助力模型轻量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>过去十年间，机器学习的应用呈现指数级增长。模型复杂度远超历史水平，其规模膨胀至惊人程度，往往包含数百万个权重参数。但遗憾的是，大型模型成为行业标杆的同时，也意味着训练和运行成本常高达数百万美元。这种高昂成本不仅令企业不堪重负，更将非富裕群体阻挡在技术创新的门槛之外，最终迫使消费者为人工智能服务支付更高溢价。当前采用的模型剪枝方法（如迭代幅度剪枝）虽能保持较高精度，但需要消耗巨大算力和环境资源的迭代训练流程。为此，我们提出BINGO解决方案。该技术在训练过程中逐次分析神经网络的特定子集，精准评估每个权重参数对模型准确性的贡献程度。训练完成后，BINGO会为每个权重生成重要性评分，从而实现一次性剪枝冗余参数。相较于现有方法，BINGO在保证精度的同时大幅降低计算强度，为人工智能发展开辟了新路径——技术进步不再必然伴随模型膨胀。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>