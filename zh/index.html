<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">安全对齐不应仅依赖少数注意力头</h2><a id="user-content-安全对齐不应仅依赖少数注意力头" class="anchor" aria-label="Permalink: 安全对齐不应仅依赖少数注意力头" href="#安全对齐不应仅依赖少数注意力头"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>当前，大型语言模型（LLM）的安全对齐机制仍存在脆弱性——对抗性提示能有效绕过其安全防护。我们的研究表明，这些安全机制主要依赖于有限范围的注意力头：移除或消融这些注意力头会严重破坏模型安全性。为识别和评估这些安全关键组件，我们提出RDSHA方法，该靶向消融技术利用模型的拒绝响应方向，精确定位对安全行为起主导作用的注意力头。进一步分析表明，现有越狱攻击正是通过选择性绕过或操纵这些关键注意力头来利用此种集中性缺陷。针对此问题，我们提出创新训练策略AHD，旨在推动安全相关行为在更多注意力头中的分布式编码。实验结果表明，AHD成功将安全相关能力分散至更多注意力头。在多种主流越狱攻击测试中，采用AHD训练的模型在保持整体功能效用的同时，展现出显著增强的安全鲁棒性。</p>
<div class="markdown-heading"><h2 class="heading-element">学习原始具身世界模型：迈向可扩展的机器人学习之路</h2><a id="user-content-学习原始具身世界模型迈向可扩展的机器人学习之路" class="anchor" aria-label="Permalink: 学习原始具身世界模型：迈向可扩展的机器人学习之路" href="#学习原始具身世界模型迈向可扩展的机器人学习之路"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>虽然基于视频生成的具身世界模型日益受到关注，但其对大规模具身交互数据的依赖仍是关键瓶颈。具身数据的稀缺性、采集难度及高维特性从根本上限制了语言与动作之间的对齐粒度，加剧了长时序视频生成的挑战——阻碍生成模型在具身领域实现"GPT时刻"。我们注意到一个朴素现象：具身数据的多样性远超有限的原始动作空间。基于此洞见，我们提出一种新颖的世界建模范式——原始具身世界模型（PEWM）。通过将视频生成限制在固定短时序内，我们的方法能够：1）实现语言概念与机器人动作视觉表征的细粒度对齐；2）降低学习复杂度；3）提高具身数据收集效率；4）减少推理延迟。通过配备模块化视觉语言模型（VLM）规划器和起止热图引导机制（SGG），PEWM进一步实现灵活闭环控制，支持原始级策略在长周期复杂任务中的组合泛化。该框架利用视频模型的时空视觉先验与VLM的语义感知能力，弥合细粒度物理交互与高层推理之间的鸿沟，为构建可扩展、可解释、通用型的具身智能铺平道路。</p>
<div class="markdown-heading"><h2 class="heading-element">面向内容生成任务的光子受限玻尔兹曼机</h2><a id="user-content-面向内容生成任务的光子受限玻尔兹曼机" class="anchor" aria-label="Permalink: 面向内容生成任务的光子受限玻尔兹曼机" href="#面向内容生成任务的光子受限玻尔兹曼机"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>受限玻尔兹曼机（RBM）是一种基于伊辛模型的神经网络，以其学习概率分布和随机生成新内容的能力而闻名。然而，在内容生成任务中，吉布斯采样的高计算成本对电子实现构成了显著瓶颈。本文提出一种光子受限玻尔兹曼机（PRBM），利用光子计算加速吉布斯采样，实现高效内容生成。通过引入高效编码方法，PRBM无需进行计算密集的矩阵分解，将吉布斯采样的计算复杂度从$O(N)$降至$O(1)$。其非冯·诺依曼光子计算架构规避了交互矩阵的存储需求，为大规模RBM提供了显著优势。我们通过模拟二维伊辛模型实验验证了光子加速吉布斯采样，观测到的相变温度与理论预测高度吻合。除物理启发任务外，PRBM在生成和还原多样化内容（包括图像和时间序列）方面展现出强大能力，即使在存在噪声和畸变的情况下亦然。PRBM框架的可扩展性和降低的训练成本，凸显了其在推动生成式人工智能中光子计算发展的巨大潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">CoFormer：与异构边缘设备协作实现可扩展Transformer推理</h2><a id="user-content-coformer与异构边缘设备协作实现可扩展transformer推理" class="anchor" aria-label="Permalink: CoFormer：与异构边缘设备协作实现可扩展Transformer推理" href="#coformer与异构边缘设备协作实现可扩展transformer推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Transformer模型的卓越性能推动了智能应用在资源受限边缘设备上的部署。然而，由于这类模型巨大的计算需求和资源消耗，如何为实时边缘系统提供高质量服务成为重大挑战。现有方案通常将Transformer计算任务卸载至其他设备，或直接在边缘设备部署压缩模型，但这些策略要么产生显著通信开销，要么导致精度与效率间的次优权衡。为应对这些挑战，我们提出名为CoFormer的通用Transformer协同推理系统，其核心思想是利用Transformer的可分解与可集成特性：将现成的大型Transformer拆解为多个小型模型进行分布式推理，并通过聚合中间结果生成最终输出。我们构建了优化问题以在异构硬件约束下最小化推理延迟与精度损失，提出DeBo算法先求解最优分解策略，再通过渐进式校准恢复模型性能。实验证明该系统能支持多种Transformer模型在异构边缘设备上运行，对大型Transformer实现最高3.1倍的推理加速。特别地，CoFormer使参数量达16亿的GPT2-XL模型能在边缘设备高效推理，内存需求降低76.3%，在保持满意推理性能的同时还能降低约40%的能耗。</p>
<div class="markdown-heading"><h2 class="heading-element">将表格基础模型转化为图基础模型</h2><a id="user-content-将表格基础模型转化为图基础模型" class="anchor" aria-label="Permalink: 将表格基础模型转化为图基础模型" href="#将表格基础模型转化为图基础模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>尽管基础模型已在自然语言处理和计算机视觉等领域引发革命性变革，但其在图机器学习领域的应用与潜力仍待充分探索。设计图基础模型（GFMs）的核心挑战之一在于处理不同图数据集中可能存在的多样化节点特征。虽然现有许多GFM研究仅聚焦于文本属性图，但对于处理其他类型任意特征的难题尚未得到彻底解决。这一问题并非图领域独有，在表格数据的机器学习领域同样存在。受TabPFNv2等表格基础模型近期成功的启发，本研究提出G2T-FM——一个以TabPFNv2为骨干网络的简易图基础模型。具体而言，G2T-FM通过邻域特征聚合增强原始节点特征，添加结构嵌入，再将构建的节点表征输入TabPFNv2进行处理。即使在完全上下文学习设定下，我们的模型仍取得强劲性能：显著优于现有公开GFMs，并与经过充分调优的从头训练图神经网络（GNNs）表现相当。经过微调后，G2T-FM更超越了精心调优的GNN基线模型，彰显出所提方法的潜力。更广泛而言，本文揭示了利用表格基础模型处理图机器学习任务这一曾被忽视的研究方向。</p>
<div class="markdown-heading"><h2 class="heading-element">室内环境中基于语言增强的移动操作高效目标搜索系统</h2><a id="user-content-室内环境中基于语言增强的移动操作高效目标搜索系统" class="anchor" aria-label="Permalink: 室内环境中基于语言增强的移动操作高效目标搜索系统" href="#室内环境中基于语言增强的移动操作高效目标搜索系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：在保持专业术语准确性的基础上，采用符合中文科技文献习惯的四六字结构，将"Language-Enhanced"译为"语言增强"，"Mobile Manipulation"译为"移动操作"，"Efficient Object Search"处理为"高效目标搜索"，并通过增译"系统"二字使技术概念更完整。整体采用"定语+中心词"的科技文本典型结构，确保学术翻译的严谨性。）</p>
<p>实现机器人在复杂非结构化环境中高效搜索与识别物体的能力，对于从家庭辅助到工业自动化等多样化应用至关重要。然而，传统场景表征通常仅能捕捉静态语义，缺乏可解释的上下文推理能力，这限制了其在完全陌生环境中指导物体搜索的效能。为应对这一挑战，我们提出了一种语言增强的分层导航框架，将语义感知与空间推理紧密融合。我们的目标导向动态启发式分层搜索方法（GODHS）利用大语言模型（LLM）推断场景语义，并通过多级决策层次结构引导搜索过程。通过在层级每个阶段应用结构化提示与逻辑约束，确保了推理过程的可靠性。针对移动操作的特殊挑战，我们引入了基于启发式的运动规划器，结合极角排序与距离优先级算法，高效生成探索路径。在Isaac Sim中的综合评估验证了框架的可行性，表明相较于传统非语义搜索策略，GODHS能以更高搜索效率定位目标物体。项目网站与演示视频详见：<a href="https://drapandiger.github.io/GODHS" rel="nofollow">https://drapandiger.github.io/GODHS</a></p>
<div class="markdown-heading"><h2 class="heading-element">超越鸟瞰图：优化点级令牌以实现协同感知</h2><a id="user-content-超越鸟瞰图优化点级令牌以实现协同感知" class="anchor" aria-label="Permalink: 超越鸟瞰图：优化点级令牌以实现协同感知" href="#超越鸟瞰图优化点级令牌以实现协同感知"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>协作感知通过智能体间交换中间特征来增强其感知能力。现有方法通常将这些中间特征组织为二维鸟瞰图（BEV）表示，但这种方式会丢失对精确目标识别与定位至关重要的细粒度三维结构信息。为此，我们首次引入点级令牌作为协作感知的中间表示。然而点云数据本身具有无序性、海量性和位置敏感性，难以生成既紧凑又对齐且能保留细节结构信息的点级令牌序列。因此，我们提出CoPLOT——一种利用点级优化令牌的新型协作感知框架。该框架采用原生点云处理流程，包含令牌重排序、序列建模和多智能体空间对齐三大模块：语义感知的令牌重排序模块通过场景级和令牌级语义信息生成自适应的一维重排序方案；频率增强型状态空间模型捕获跨空间与频域的远程序列依赖关系，提升前景令牌与背景杂波的区分能力；邻域-本体的对齐模块采用全局智能体级校正与局部令牌级优化的闭环流程，有效抑制定位噪声。在仿真和真实数据集上的大量实验表明，CoPLOT以更低的通信与计算开销超越了现有最优模型。代码将在<a href="https://github.com/CheeryLeeyy/CoPLOT">https://github.com/CheeryLeeyy/CoPLOT</a> 开源。</p>
<p>（注：翻译过程中对技术术语进行了统一处理，如"agent"译为"智能体"，"token"译为"令牌"，"bird's-eye-view"保留专业缩写"BEV"并补充中文全称。长难句采用拆分重组策略，如将原文最后两个长句拆分为三个中文短句。专业表述如"state space model"译为"状态空间模型"，"foreground/background"译为"前景/背景"符合计算机视觉领域惯例。）</p>
<div class="markdown-heading"><h2 class="heading-element">SemSR：基于语义感知的鲁棒会话推荐系统</h2><a id="user-content-semsr基于语义感知的鲁棒会话推荐系统" class="anchor" aria-label="Permalink: SemSR：基于语义感知的鲁棒会话推荐系统" href="#semsr基于语义感知的鲁棒会话推荐系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此处"Session-based Recommendations"译为"会话推荐系统"，是推荐系统领域常用术语，指基于用户短期会话行为进行推荐的系统。"Semantics aware"译为"语义感知"，"robust"译为"鲁棒"，均为计算机领域标准译法。整体翻译在保持专业性的同时确保中文表达流畅。）</p>
<p>基于会话的推荐（SR）模型旨在根据匿名用户在当前会话期间的行为为其推荐项目。尽管现有多种SR模型利用项目序列预测下一项目，但它们往往未能充分利用项目标题或描述中的语义信息，这阻碍了会话意图识别与模型可解释性。近期研究探索了将大语言模型（LLM）作为增强会话推荐的可行路径，其中基于提示词和基于微调的方法得到广泛研究。然而基于提示词的方法难以找到能激发正确推理的最优提示，且在测试时缺乏任务特异性反馈，导致推荐效果欠佳；微调方法虽能融入领域知识，但需要高昂的计算成本进行实施和维护。本文提出多种利用LLM进行会话推荐的方案：（一）作为推荐代理的上下文LLM；（二）为深度学习SR模型提供语义初始化的LLM生成表征；（三）LLM与数据驱动SR模型的融合。通过在两个真实世界公开数据集上的综合实验，我们证明LLM方法擅长粗粒度检索（高召回率），而传统数据驱动技术精于细粒度排序（高平均倒数排名值）。更重要的是，LLM与数据驱动SR模型的融合方案在召回率和MRR指标上均显著优于独立LLM方法、数据驱动深度学习模型以及基线SR模型。</p>
<div class="markdown-heading"><h2 class="heading-element">利用计算智能与深度强化学习的自主机器任务分配</h2><a id="user-content-利用计算智能与深度强化学习的自主机器任务分配" class="anchor" aria-label="Permalink: 利用计算智能与深度强化学习的自主机器任务分配" href="#利用计算智能与深度强化学习的自主机器任务分配"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>实现多台自主机器的可靠运行，需要开发高效的协同控制算法。本文系统综述了为复杂环境中自主机器的控制与协调而开发的各类算法，特别聚焦于采用计算智能（CI）与深度强化学习（RL）的任务分配方法。我们深入分析了现有方法的优势与局限性，详细提出并探讨了未来多个研究方向，这些方向为改进现有算法或创建新方法以提升自主机器在实际应用中的适用性与性能提供了思路。研究结果表明，计算智能和深度强化学习方法为解决动态不确定环境中的复杂任务分配问题提供了有效途径。深度强化学习的最新进展极大丰富了自主机器控制协调领域的学术成果，并已成为该领域日益显著的发展趋势。本文旨在为研究人员和工程师提供关于自主机器相关机器学习研究进展的全面概览，同时指出尚未充分探索的研究领域，识别新兴方法论，并为该领域未来研究提出新的探索方向。</p>
<div class="markdown-heading"><h2 class="heading-element">信标：集成网格选择的训练后量化</h2><a id="user-content-信标集成网格选择的训练后量化" class="anchor" aria-label="Permalink: 信标：集成网格选择的训练后量化" href="#信标集成网格选择的训练后量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此处"Beacon"作为技术术语或产品名称，在中文技术文献中常保留英文不译或音译为"比肯"，但根据上下文判断此处应为量化技术的名称，故采用"信标"的意译以体现其指引性功能。"Post-Training Quantization"标准译法为"训练后量化"，"Integrated Grid Selection"译为"集成网格选择"符合计算机领域术语规范。）</p>
<p>量化是一种广泛应用的压缩技术，用于降低大型预训练模型的内存与计算成本。在逐通道训练后量化（PTQ）中，关键挑战在于选择合适的缩放因子，将权重值替换为缩放量化网格中的数值。现有方法通常通过启发式调优或网格搜索预先固定缩放比例。本文提出Beacon算法——一种简单有效的解决方案，无需人工调优即可实现量化。该方法直接使用固定非缩放字母表执行逐通道PTQ，并通过利用对称标量量化的几何特性自动确定最优缩放因子。仅需最小改动即可同时支持对称与非对称量化，且不依赖反向传播或大型校准数据集。尽管方法简单且无需调参，Beacon仍能达到与最先进方法相媲美的性能，为高效模型部署提供了实用解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">系统知识：通过指纹技术进行大语言模型版权审计</h2><a id="user-content-系统知识通过指纹技术进行大语言模型版权审计" class="anchor" aria-label="Permalink: 系统知识：通过指纹技术进行大语言模型版权审计" href="#系统知识通过指纹技术进行大语言模型版权审计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>训练大型语言模型（LLMs）所需的广泛能力与庞大资源使其成为宝贵知识产权，但这些模型仍面临版权侵害风险，例如未经授权的使用和模型窃取。LLM指纹识别作为一种非侵入式技术，通过提取并比对LLMs的独有特征来识别侵权行为，为版权审计提供了前景广阔的解决方案。然而，由于普遍存在的多样化模型修改手段以及缺乏标准化评估，该技术的可靠性仍存在不确定性。本文首次对LLM指纹识别进行全面研究，提出统一框架和形式化分类法，将现有方法划分为白盒与黑盒两大类型，为领域现状提供结构化概览。我们进一步推出LeaFBench——首个在真实部署场景下系统评估LLM指纹识别的基准测试平台。该平台基于主流基础模型构建，包含149个独特模型实例，整合了13种具有代表性的后开发技术，涵盖参数修改方法（如微调、量化）和参数无关机制（如系统提示、RAG）。在LeaFBench上进行的大规模实验揭示了现有方法的优势与局限，从而为该新兴领域勾勒出未来研究方向与关键开放性问题。代码已开源：<a href="https://github.com/shaoshuo-ss/LeaFBench%E3%80%82">https://github.com/shaoshuo-ss/LeaFBench。</a></p>
<div class="markdown-heading"><h2 class="heading-element">FORGE：基于图嵌入的基础优化表示法</h2><a id="user-content-forge基于图嵌入的基础优化表示法" class="anchor" aria-label="Permalink: FORGE：基于图嵌入的基础优化表示法" href="#forge基于图嵌入的基础优化表示法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>组合优化问题在科学与工程领域无处不在，但基于学习的加速求解方法通常需要解决大量难解优化实例以收集训练数据，导致巨大的计算开销。现有方法需为每个下游任务的特定问题分布训练独立模型，严重限制了方法的可扩展性与泛化能力。本研究提出Forge方法，通过在大规模混合整数规划（MIP）实例库上以无监督方式预训练矢量量化图自编码器，且不依赖问题解。矢量量化过程生成的离散代码分配可作为表示优化实例的词汇表。我们在监督与无监督两种设置下评估该方法：在无监督场景中，Forge嵌入能有效区分和聚类未见过的实例；在监督场景中，通过微调Forge嵌入，单一模型即可预测多类问题分布中的热启动变量和割平面生成的整性间隙——这两类预测均能提升业界领先商业优化求解器的性能。我们已在<a href="https://github.com/skadio/forge/">https://github.com/skadio/forge/</a> 开源代码与预训练权重，以促进实例级MIP嵌入的进一步研究与实践应用。</p>
<div class="markdown-heading"><h2 class="heading-element">FARM：基于物理的高动态人形控制中的帧加速增强与残差专家混合体</h2><a id="user-content-farm基于物理的高动态人形控制中的帧加速增强与残差专家混合体" class="anchor" aria-label="Permalink: FARM：基于物理的高动态人形控制中的帧加速增强与残差专家混合体" href="#farm基于物理的高动态人形控制中的帧加速增强与残差专家混合体"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>统一基于物理的人形控制器对机器人技术和角色动画至关重要，但擅长处理温和日常动作的模型在爆发性动作上仍存在困难，这阻碍了其实际应用。我们通过FARM（帧加速增强与残差专家混合）框架填补了这一空白——该端到端系统由帧加速增强、鲁棒的基础控制器和残差专家混合模块（MoE）构成。帧加速增强通过扩大帧间间隔使模型接触高速姿态变化，基础控制器可靠追踪日常低动态运动，而残差MoE自适应分配额外网络容量来处理高动态动作，显著提升了追踪精度。针对公开基准数据的缺失，我们策划了包含3593个物理合理片段的高动态人形运动（HDHM）数据集。在HDHM测试中，FARM相较基线方法将追踪失败率降低42.8%，全局关节平均位置误差减少14.6%，同时在低动态运动上保持近乎完美的精度。这些成果使FARM成为高动态人形控制的新基准，并首次为此挑战设立了开放测试标准。代码与数据集将在<a href="https://github.com/Colin-Jing/FARM">https://github.com/Colin-Jing/FARM</a> 发布。</p>
<div class="markdown-heading"><h2 class="heading-element">迈向新起点：自动驾驶软件中的统一感知技术综述</h2><a id="user-content-迈向新起点自动驾驶软件中的统一感知技术综述" class="anchor" aria-label="Permalink: 迈向新起点：自动驾驶软件中的统一感知技术综述" href="#迈向新起点自动驾驶软件中的统一感知技术综述"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>自动驾驶感知系统通常采用模块化流水线架构，将整体任务分解为检测、跟踪与预测三个独立环节。虽然这种架构具有可解释性优势，但存在误差累积和跨任务协同不足的固有缺陷。统一感知范式应运而生，通过将子任务整合到共享架构中，在保持输出可解释性的同时，有望提升系统鲁棒性、上下文推理能力和运行效率。本综述首次建立统一感知的整体性系统分类框架，从任务集成方式、跟踪建模形式和表征流模式三个维度对现有方法进行体系化归类。我们界定了早期统一、晚期统一和完全统一三大技术范式，系统梳理了各类方法的架构设计、训练策略、使用数据集及开源情况，并指明未来研究方向。此项研究构建了首个理解与推进统一感知技术的完整框架，整合了当前碎片化的研究实践，为发展更具鲁棒性、泛化性和可解释性的感知系统指明方向。</p>
<div class="markdown-heading"><h2 class="heading-element">专业大型语言模型综述</h2><a id="user-content-专业大型语言模型综述" class="anchor" aria-label="Permalink: 专业大型语言模型综述" href="#专业大型语言模型综述"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>专业大语言模型（LLM）的快速发展已从简单的领域适应转向精密的本土架构设计，标志着人工智能发展的范式转变。本综述系统梳理了医疗、金融、法律和技术领域在这一演进过程中的实践轨迹。除了专用大语言模型的广泛应用，技术突破亦层出不穷：超越微调的领域原生设计不断涌现，通过稀疏计算与量化技术提升参数效率的需求日益增强，多模态能力融合持续深化——这些创新均已应用于当代LLM智能体。我们的分析揭示了这些技术如何突破通用大语言模型在专业应用中的根本局限，使专用模型在领域特定基准测试中持续展现性能优势。本研究进一步强调了这些进展对电子商务领域的启示，为该领域填补关键空白提供了重要参考。</p>
<div class="markdown-heading"><h2 class="heading-element">特邀论文：面向极限边缘医疗保健的混合信号智能柔性可穿戴设备中特征与分类器的协同设计</h2><a id="user-content-特邀论文面向极限边缘医疗保健的混合信号智能柔性可穿戴设备中特征与分类器的协同设计" class="anchor" aria-label="Permalink: 特邀论文：面向极限边缘医疗保健的混合信号智能柔性可穿戴设备中特征与分类器的协同设计" href="#特邀论文面向极限边缘医疗保健的混合信号智能柔性可穿戴设备中特征与分类器的协同设计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>柔性电子技术（FE）为可穿戴医疗设备提供了相较于刚性硅基硬件的理想替代方案，其轻量化、可贴合及低成本特性优势显著。然而，该技术有限的集成密度和较大特征尺寸对系统面积与功耗造成严格限制，这使得整合模拟前端、特征提取与分类器的机器学习医疗系统面临特殊挑战。现有FE解决方案往往忽视系统级优化可能，过度聚焦分类器设计，却忽略了特征提取和模数转换器（ADC）带来的巨大硬件成本——这两者正是面积与功耗的主要来源。</p>
<p>本研究提出了一种面向柔性智能可穿戴系统的混合信号特征-分类器协同设计框架。我们率先在FE领域设计了模拟特征提取器，显著降低了特征提取成本。进一步提出机器学习训练中结合硬件感知的神经架构搜索（NAS）启发式特征选择策略，实现高效且定制化的应用设计。在医疗基准测试中，我们的方案展现出高精度与超低面积占用的优势，特别适用于一次性低功耗可穿戴监测场景。</p>
<div class="markdown-heading"><h2 class="heading-element">CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型</h2><a id="user-content-cogvla通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型" class="anchor" aria-label="Permalink: CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型" href="#cogvla通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>基于预训练视觉语言模型（VLA）构建的近期视觉-语言-动作模型需要进行大量后训练，导致计算开销巨大，限制了其扩展性和部署能力。我们提出CogVLA——一种认知对齐的视觉-语言-动作框架，通过指令驱动的路由与稀疏化技术同步提升效率与性能。该框架借鉴人类多模态协调机制，采用三阶段渐进式架构：1）基于编码器-FiLM的聚合路由（EFA-Routing）将指令信息注入视觉编码器，选择性聚合压缩双流视觉标记，形成指令感知的潜在表征；2）在此紧凑视觉编码基础上，基于LLM-FiLM的剪枝路由（LFP-Routing）通过剔除指令无关的视觉接地标记，将动作意图引入语言模型，实现标记级稀疏化；3）为确保压缩后的感知输入仍能支持精准连贯的动作生成，我们提出视觉-语言-动作耦合注意力机制（CAtten），融合因果视觉语言注意力与双向动作并行解码。在LIBERO基准测试和真实机器人任务上的大量实验表明，CogVLA以97.4%和70.0%的成功率实现最先进性能，相较OpenVLA训练成本降低2.5倍，推理延迟减少2.8倍。本项目已开源：<a href="https://github.com/JiuTian-VL/CogVLA%E3%80%82">https://github.com/JiuTian-VL/CogVLA。</a></p>
<div class="markdown-heading"><h2 class="heading-element">CoCoL：一种面向多机器人系统的高效通信去中心化协同方法</h2><a id="user-content-cocol一种面向多机器人系统的高效通信去中心化协同方法" class="anchor" aria-label="Permalink: CoCoL：一种面向多机器人系统的高效通信去中心化协同方法" href="#cocol一种面向多机器人系统的高效通信去中心化协同方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>协同学习能提升多机器人系统在复杂任务中的性能与适应能力，但由于多机器人任务固有的高通信开销和数据异构性，该方法面临重大挑战。为此，我们提出CoCoL——一种专为具有异构本地数据集的多机器人系统设计的通信高效去中心化协同学习方法。该方法基于镜像下降框架，通过捕捉机器人目标函数间的相似性实现近似牛顿型更新，显著提升通信效率，并采用非精确子问题求解降低计算成本。此外，梯度追踪机制的融入确保了算法对数据异构的鲁棒性。在三个典型多机器人协同学习任务上的实验结果表明，CoCoL在保持顶尖精度的同时，能显著减少通信轮数和总带宽消耗。这些优势在涉及非独立同分布数据、流数据以及时变网络拓扑的挑战性场景中尤为突出。</p>
<div class="markdown-heading"><h2 class="heading-element">无参数结构多样性消息传递在图神经网络中的应用</h2><a id="user-content-无参数结构多样性消息传递在图神经网络中的应用" class="anchor" aria-label="Permalink: 无参数结构多样性消息传递在图神经网络中的应用" href="#无参数结构多样性消息传递在图神经网络中的应用"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>图神经网络（GNN）在节点分类等结构化数据建模任务中展现出卓越性能，但主流方法通常依赖大量可训练参数和固定聚合规则，难以适应具有强结构异质性和复杂特征分布的图数据，这往往导致节点表示过度平滑与语义退化。针对这些问题，本文提出一种基于结构多样性的无参数图神经网络框架SDGNN（Structural-Diversity Graph Neural Network）。该框架受结构多样性理论启发，设计了统一的结构多样性消息传递机制，无需引入额外可训练参数即可同时捕捉邻域结构的异质性与特征语义的稳定性。与传统参数化方法不同，SDGNN不依赖复杂模型训练，而是通过结构驱动与特征驱动的互补建模，有效提升跨数据集和跨场景的适应能力。实验结果表明，在八个公共基准数据集和跨学科PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战性条件下持续优于主流GNN。这项工作为无参数图神经网络设计提供了新的理论视角和通用方法，进一步验证了结构多样性作为图表示学习核心信号的重要性。为促进可复现性和后续研究，SDGNN的完整实现已发布于：<a href="https://github.com/mingyue15694/SGDNN/tree/main">https://github.com/mingyue15694/SGDNN/tree/main</a></p>
<div class="markdown-heading"><h2 class="heading-element">关系构建：参与式人工智能招聘策略分析</h2><a id="user-content-关系构建参与式人工智能招聘策略分析" class="anchor" aria-label="Permalink: 关系构建：参与式人工智能招聘策略分析" href="#关系构建参与式人工智能招聘策略分析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>参与式人工智能（即让受影响的社区成员及其他利益相关方参与AI系统设计与开发）作为一种确保人工智能发展符合其需求并体现其价值观的路径展现出巨大潜力。然而在力求采用参与式实践的AI项目中，如何识别、接触并动员所有相关利益群体——我们称之为招募方法论——仍是实际运作中的难题。本文通过研究37个项目的实践样本，首先系统梳理了该领域招募方法的多样性，对招募实践的文档记录以及研究者为实现公平赋权目标采取的具体策略进行了初步分析。为深化研究，我们访谈了五位AI学者以探究招募方法的实际成效，发现这些成效受到项目结构性条件、研究者自身目标预期、以及通过招募建立起的合作关系等多重因素影响。基于分析，我们为参与式人工智能研究者提出了关系导向的招募方法设计与实施建议，以及具有反思性的招募文档记录实践方案。</p>
<div class="markdown-heading"><h2 class="heading-element">CrystalICL：赋能晶体生成的上下文学习技术</h2><a id="user-content-crystalicl赋能晶体生成的上下文学习技术" class="anchor" aria-label="Permalink: CrystalICL：赋能晶体生成的上下文学习技术" href="#crystalicl赋能晶体生成的上下文学习技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>设计具有理想理化性能的晶体材料始终是材料科学领域的核心挑战。尽管大语言模型（LLM）已展现出强大的上下文学习（ICL）能力，但现有基于LLM的晶体生成方法仅限于零样本场景，无法受益于少样本学习。相比之下，人类专家通常通过修改相关已知结构来设计新材料，这种模式与少样本ICL范式高度契合。受此启发，我们提出CrystalICL——专为少样本晶体生成设计的新型模型。具体而言，我们创新性地引入了基于空间群的晶体标记化方法，有效降低了LLM中晶体对称性建模的复杂度。进一步提出条件-结构感知的混合指令微调框架与多任务指令微调策略，使模型能够通过有限数据捕捉结构-性能关联，从而更好地利用ICL。在四个晶体生成基准上的大量实验表明，CrystalICL在条件生成和无条件生成任务上均优于主流基线方法。</p>
<div class="markdown-heading"><h2 class="heading-element">WEBEYETRACK：通过设备端少样本个性化实现浏览器规模化眼动追踪</h2><a id="user-content-webeyetrack通过设备端少样本个性化实现浏览器规模化眼动追踪" class="anchor" aria-label="Permalink: WEBEYETRACK：通过设备端少样本个性化实现浏览器规模化眼动追踪" href="#webeyetrack通过设备端少样本个性化实现浏览器规模化眼动追踪"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>随着人工智能技术的进步，新型视线追踪方法不断突破最先进（SOTA）基准，但其实际应用仍与商业眼动追踪方案存在差距。模型体积、推理时间和隐私保护等关键因素常被忽视，而基于普通摄像头的眼动追踪方法受头部移动等因素影响，精度始终不足。为此，我们推出WebEyeTrack框架——通过将轻量化SOTA视线估计模型直接集成至浏览器，结合基于模型的头部姿态估计技术，并采用仅需9个校准样本（k&lt;9）的设备端少样本学习方案。该系统可自适应新用户，在GazeCapture数据集上实现2.32厘米误差的SOTA性能，在iPhone 14上达到2.4毫秒的实时推理速度。开源代码已发布：<a href="https://github.com/RedForestAi/WebEyeTrack%E3%80%82">https://github.com/RedForestAi/WebEyeTrack。</a></p>
<div class="markdown-heading"><h2 class="heading-element">交互式定制：个性化人机交互图像生成</h2><a id="user-content-交互式定制个性化人机交互图像生成" class="anchor" aria-label="Permalink: 交互式定制：个性化人机交互图像生成" href="#交互式定制个性化人机交互图像生成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>组合式定制图像生成旨在生成内容中定制多个目标概念，因其广泛的应用前景而备受关注。现有方法主要关注目标实体的外观保持，却忽略了目标实体间细粒度的交互控制。为实现模型的交互控制能力，我们聚焦人物-物体交互场景，提出定制化人机交互图像生成任务（CHOI），该任务需同时实现目标人物/物体的身份特征保持与交互语义控制。CHOI任务面临两大挑战：(1)身份保持与交互控制的双重要求需要模型将人物/物体分解为自包含的身份特征和姿态导向的交互特征，而现有人机交互图像数据集无法为此类特征解耦学习提供理想样本；(2)不恰当的空间配置可能导致交互语义缺失。为此，我们首先构建大规模数据集，其中每个样本包含同一组人物-物体在不同交互姿态下的数据；继而设计两阶段模型Interact-Custom：首阶段通过生成描绘交互行为的前景掩模显式建模空间配置，次阶段在该掩模引导下生成保持身份特征的目标交互图像。此外，若用户提供背景图像及目标人物/物体的出现位置，Interact-Custom还可支持背景与位置的指定，提供高度可控的内容生成能力。基于我们为CHOI任务设计的评估指标进行的广泛实验验证了该方法的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">面向多智能体递归思考的微服务系统自适应根因定位</h2><a id="user-content-面向多智能体递归思考的微服务系统自适应根因定位" class="anchor" aria-label="Permalink: 面向多智能体递归思考的微服务系统自适应根因定位" href="#面向多智能体递归思考的微服务系统自适应根因定位"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>随着当代微服务系统日益普及且复杂化——通常包含数百甚至数千个细粒度、相互依赖的子系统——系统故障愈发频繁。确保系统可靠性需要精准的根因定位。虽然追踪数据和性能指标已被证明是有效的分析数据源，但现有方法要么过度依赖预定义模式（难以适应动态演化的运维环境），要么在推理过程中缺乏可解释性，导致站点可靠性工程师（SRE）难以理解。本文通过调研多个不同机构的专业SRE人员，系统性研究了SRE进行故障根因定位的方法。研究发现，人类根因分析具有三个关键特征：递归性、多维度扩展和跨模态推理。基于这些发现，我们提出RCLAgent——一种基于多智能体思维递归框架的自适应微服务系统根因定位方法。该方法采用创新的思维递归策略引导大语言模型的推理过程，有效整合多智能体数据与工具辅助分析，精准定位根本原因。在多个公开数据集上的实验表明，RCLAgent仅需单次请求即可实现根因定位，其性能优于依赖聚合多次请求的现有最优方法，这充分证明了RCLAgent在提升复杂微服务环境根因定位效率与精确性方面的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">个人健康助手的构成剖析</h2><a id="user-content-个人健康助手的构成剖析" class="anchor" aria-label="Permalink: 个人健康助手的构成剖析" href="#个人健康助手的构成剖析"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>健康作为人类福祉的基石，其与大型语言模型（LLM）的快速发展共同催生了新一代健康助手。然而，健康助手如何满足个人在日常非临床场景中的多样化需求仍待探索。本研究致力于构建一个综合性个人健康助手，能够解析来自日常消费级健康设备的多模态数据和常见个人健康记录，并提供个性化健康建议。为深入理解终端用户与此类助手交互时的需求，我们通过以用户为中心的设计流程，综合分析了网络搜索与健康论坛查询数据，并收集了用户和健康专家的定性见解。基于这些发现，我们识别出消费者健康需求的三大类别，每类均由专业子助手支持：（1）数据分析助手——负责处理个人时间序列的可穿戴设备与健康记录数据；（2）健康领域专家助手——整合用户健康数据与情境信息以生成精准的个性化洞察；（3）健康教练助手——综合数据洞察，运用特定心理策略指导用户并追踪进展。我们进一步提出并开发了"个人健康助手"（PHA）多智能体框架，通过动态个性化交互满足个体健康需求。为评估各子助手及多智能体系统，我们在10项基准任务中开展了自动化与人工评估，累计获得健康专家和终端用户超过7,000条标注数据和1,100小时投入。这项研究代表了迄今对健康助手最全面的评估，为实现人人可用的个人健康助手这一未来愿景奠定了坚实基础。</p>
<div class="markdown-heading"><h2 class="heading-element">快速三维扩散实现可扩展颗粒介质合成</h2><a id="user-content-快速三维扩散实现可扩展颗粒介质合成" class="anchor" aria-label="Permalink: 快速三维扩散实现可扩展颗粒介质合成" href="#快速三维扩散实现可扩展颗粒介质合成"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>使用离散元法模拟颗粒介质是一项计算密集型任务，初始化阶段尤其如此——由于涉及大位移和相应动能，该阶段往往占据总仿真时间的主导地位。我们通过基于3D扩散模型的新型生成管道突破了这一瓶颈，能够直接合成任意规模、具有最终形态且物理真实的颗粒集合体。该方法将问题构建为3D生成建模任务，采用两阶段流程：首先训练扩散模型生成独立的代表颗粒介质的3D体素网格；随后采用基于掩码输入从2D修复技术适配的3D修复模型，将这些网格无缝拼接，实现具有物理真实结构的大样本合成。修复模型通过训练网络从噪声张量、掩码和掩码张量组成的输入通道推断缺失的体素网格部分，探索了多种针对底层UNet的输入掩码策略。该模型还适配了2D重绘技术，将噪声调度器输出与真实数据重新注入，为3D模型提供强引导。结合加权损失函数，这一机制确保了掩码区域生成过程中的长期一致性。两个模型均通过从小规模DEM仿真提取的二值化3D占据网格进行训练，实现了计算时间随样本尺寸的线性缩放。量化数据显示，合成1.2米长有砟轨道（等效于3小时DEM仿真）仅需20秒以内。生成的体素网格还可后处理提取颗粒几何形态以保持DEM兼容性，为工业应用提供物理一致、实时可扩展的颗粒介质合成方案。</p>
<div class="markdown-heading"><h2 class="heading-element">自监督专家混合框架下的多行为推荐系统</h2><a id="user-content-自监督专家混合框架下的多行为推荐系统" class="anchor" aria-label="Permalink: 自监督专家混合框架下的多行为推荐系统" href="#自监督专家混合框架下的多行为推荐系统"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在电子商务领域，用户面临海量商品选择，推荐系统对于帮助他们发现可能被忽视的合适商品至关重要。虽然多数推荐系统主要依赖用户购买历史，但近年出现的多行为推荐系统通过整合点击商品、加入购物车等辅助行为来提升推荐效果。尽管整体性能有所提升，但这些系统对已交互商品（用户通过辅助行为接触过的商品）与未交互商品（用户从未接触的商品）的推荐效果存在显著差异。具体而言，我们的分析表明：（1）现有多行为推荐系统在这两类商品上存在明显的推荐质量差距；（2）使用单一模型架构同时实现两类商品的优质推荐仍具挑战性。为此，我们提出创新性多行为推荐系统MEMBER。该系统采用专家混合框架，分别设置专门针对两类商品推荐的专家模型，每个专家通过适配其设计目标的自监督方法进行训练。综合实验表明，MEMBER在两类商品推荐上均表现优异，在Hit Ratio@20指标上较最佳基线模型最高提升65.46%。</p>
<div class="markdown-heading"><h2 class="heading-element">重新思考Transformer连接性：TLinFormer，通向精确、全上下文感知线性注意力的路径</h2><a id="user-content-重新思考transformer连接性tlinformer通向精确全上下文感知线性注意力的路径" class="anchor" aria-label="Permalink: 重新思考Transformer连接性：TLinFormer，通向精确、全上下文感知线性注意力的路径" href="#重新思考transformer连接性tlinformer通向精确全上下文感知线性注意力的路径"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Transformer架构已成为现代人工智能的基石，但其核心自注意力机制存在随序列长度呈二次方增长的计算复杂度瓶颈，严重制约了在长序列任务中的应用。为应对这一挑战，现有线性注意力方法通常依赖数据无关的核近似或受限上下文选择，以牺牲模型性能为代价。本文回归连接主义的第一性原理，从信息流的拓扑结构出发，提出新型线性注意力架构——\textbf{TLinFormer}。通过重构神经元连接模式，TLinFormer在计算精确注意力分值、确保信息流完整感知历史上下文的同时，实现了严格的线性复杂度。该设计旨在弥合现有高效注意力方法与标准注意力之间的性能鸿沟。通过系列实验，我们在长序列推理任务中系统评估了TLinFormer相对于标准Transformer基线的性能。结果表明，TLinFormer在\textbf{推理延迟}、\textbf{KV缓存效率}、\textbf{内存占用}和\textbf{整体加速比}等关键指标上均展现出压倒性优势。</p>
<div class="markdown-heading"><h2 class="heading-element">DFAMS：基于动态流引导的联邦对齐多原型搜索</h2><a id="user-content-dfams基于动态流引导的联邦对齐多原型搜索" class="anchor" aria-label="Permalink: DFAMS：基于动态流引导的联邦对齐多原型搜索" href="#dfams基于动态流引导的联邦对齐多原型搜索"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>联邦检索（FR）通过跨多个外部知识源路由查询，在必要的外部知识分散分布时减轻大语言模型的幻觉问题。然而现有方法难以对模糊查询获取高质量相关文档，尤其在跨域场景下，这显著限制了其支持下游生成任务的效果。受动态信息流（DIF）启发，我们提出DFAMS框架——该创新方案利用DIF识别潜在查询意图，并构建语义对齐的知识分区以实现跨异构源的精准检索。具体而言，DFAMS通过少量标注查询的梯度信号探测大语言模型中的DIF，采用基于沙普利值的归因方法追踪与意图识别及子域边界检测相关的神经元激活路径。随后利用DIF通过多原型对比学习训练对齐模块，实现知识库内细粒度建模和跨库语义对齐。在五个基准测试上的实验表明，DFAMS在知识分类准确率上最高超越先进FR方法14.37%，检索召回率提升5.38%，下游问答准确率提高6.45%，证明了其在复杂联邦检索场景中的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">滤波后关注：通过谱滤波改进基于注意力的时间序列预测</h2><a id="user-content-滤波后关注通过谱滤波改进基于注意力的时间序列预测" class="anchor" aria-label="Permalink: 滤波后关注：通过谱滤波改进基于注意力的时间序列预测" href="#滤波后关注通过谱滤波改进基于注意力的时间序列预测"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>注：此翻译保持了专业术语的准确性，同时符合中文表达习惯。"Filter then Attend"采用意译处理为"滤波后关注"，既保留了原意又体现操作顺序；"Spectral Filtering"译为专业术语"谱滤波"；整体句式结构根据中文特点进行了优化调整。</p>
<p>基于Transformer的模型在长时间序列预测（LTSF）领域处于领先地位。尽管这些模型在许多情况下能够取得最先进的结果，但它们存在对数据低频分量的偏好以及高计算和内存需求的问题。近期研究表明，通过增强模型对频谱的利用能力，可学习频率滤波器可以成为深度预测模型的核心组成部分。这些研究选择使用多层感知机处理滤波后的信号，因此未能解决基于Transformer模型存在的问题。本文论证了在基于Transformer的模型前端添加滤波器可提升其长时序预测性能。我们引入的可学习滤波器仅增加约1000个参数，在多个实例中观察到预测性能相对提升5-10%。此外，我们发现添加滤波器后能够降低模型的嵌入维度，从而获得比未滤波基准模型更精简且更高效的Transformer架构。我们还通过合成实验分析滤波器如何帮助基于Transformer的模型更好地利用全频谱进行预测。</p>
<div class="markdown-heading"><h2 class="heading-element">面向ISAC赋能车联网的节能型学习波束成形技术</h2><a id="user-content-面向isac赋能车联网的节能型学习波束成形技术" class="anchor" aria-label="Permalink: 面向ISAC赋能车联网的节能型学习波束成形技术" href="#面向isac赋能车联网的节能型学习波束成形技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>本研究提出了一种面向支持集成传感与通信（ISAC）的V2X网络的高效能效、基于学习的波束成形方案。具体而言，我们首先将V2X环境的动态不确定性建模为马尔可夫决策过程，该建模使路侧单元仅需基于当前传感信息即可生成波束成形决策，从而避免频繁的导频传输与大规模信道状态信息获取。随后开发深度强化学习（DRL）算法联合优化波束成形与功率分配，在高度动态场景中同时保障通信吞吐量与感知精度。针对传统学习方案的高能耗问题，我们将脉冲神经网络（SNN）嵌入DRL框架，利用其事件驱动与稀疏激活特性，在保持强劲性能的同时显著提升能效。仿真结果表明，所提方案实现了显著的节能效果与卓越的通信性能，展现出支撑未来V2X系统绿色可持续连接的潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">Bi-LoRA：面向大规模模型微调的高效锐度感知最小化方法</h2><a id="user-content-bi-lora面向大规模模型微调的高效锐度感知最小化方法" class="anchor" aria-label="Permalink: Bi-LoRA：面向大规模模型微调的高效锐度感知最小化方法" href="#bi-lora面向大规模模型微调的高效锐度感知最小化方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：Bi-LoRA在此保留原文缩写形式，因其为特定技术名称。翻译采用技术文献常用表达方式，"Sharpness-Aware Minimization"译为"锐度感知最小化"，"Efficient"译为"高效"以准确传达算法特性，同时保持学术文本的简洁性和专业性。）</p>
<p>在有限数据下微调大规模预训练模型对泛化能力提出了重大挑战。虽然锐度感知最小化（SAM）通过寻找平坦最小值被证明能有效提升泛化性能，但其巨大的额外内存与计算开销使其难以应用于大型模型。将SAM与参数高效微调方法（如低秩自适应LoRA）相结合是一个前景广阔的方向。然而我们发现，直接将SAM应用于LoRA参数会限制锐度优化在低维子空间中的有效性。为突破这一局限，我们提出双向低秩自适应（Bi-LoRA），通过引入辅助LoRA模块来建模SAM的对抗性权重扰动。该设计将SAM的权重扰动与LoRA优化解耦：主LoRA模块通过标准梯度下降适应特定任务，而辅助模块通过梯度上升捕捉损失景观的锐度。这种双模块设计使Bi-LoRA能捕获更广泛的锐度以实现更平坦的最小值，同时保持内存高效性。另一重要优势是双模块设计允许同步进行优化和扰动，消除了SAM双倍训练成本的问题。在多任务和多架构的大量实验证明了Bi-LoRA在提升泛化能力方面的高效性和有效性。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>