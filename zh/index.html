<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/gpt_paper_assistant_ori</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/gpt_paper_assistant_ori/"><meta property="og:title" content="LLIKKE/gpt_paper_assistant_ori"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/gpt_paper_assistant_ori/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/gpt_paper_assistant_ori"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/gpt_paper_assistant_ori"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">SHARP：通过共享相邻层与恢复参数加速语言模型推理</h2><a id="user-content-sharp通过共享相邻层与恢复参数加速语言模型推理" class="anchor" aria-label="Permalink: SHARP：通过共享相邻层与恢复参数加速语言模型推理" href="#sharp通过共享相邻层与恢复参数加速语言模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.07832v1 Announce Type: new  Abstract: While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance. Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT). The SLW stage aligns the outputs of the shared layers using L_2 loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%. We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources.</p>
<div class="markdown-heading"><h2 class="heading-element">BalanceKV：基于差异理论的键值缓存压缩</h2><a id="user-content-balancekv基于差异理论的键值缓存压缩" class="anchor" aria-label="Permalink: BalanceKV：基于差异理论的键值缓存压缩" href="#balancekv基于差异理论的键值缓存压缩"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.07861v1 公告类型：新研究  摘要：大型语言模型（LLMs）已取得显著成就，但其高内存需求对生成长上下文标记提出了挑战。长上下文LLMs的内存复杂性主要源于需要在键值（KV）缓存中存储键值嵌入。我们提出了BalanceKV，一种基于几何采样过程的KV缓存压缩方法，该方法源自Banaszczyk的向量平衡理论，通过引入由键和值标记的几何形状所指导的依赖关系，提高了精度。BalanceKV在理论上和实证上都优于现有方法，提供了性能提升的证明和验证。</p>
<div class="markdown-heading"><h2 class="heading-element">LowRA：在2比特以下实现LLM的精确高效LoRA微调</h2><a id="user-content-lowra在2比特以下实现llm的精确高效lora微调" class="anchor" aria-label="Permalink: LowRA：在2比特以下实现LLM的精确高效LoRA微调" href="#lowra在2比特以下实现llm的精确高效lora微调"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.08141v1 公告类型：新研究 摘要：随着大型语言模型（LLMs）规模扩展至数千亿参数，对其进行微调的成本日益增加，即便是像LoRA这样的参数高效微调（PEFT）方法也依然资源密集。我们推出了LowRA，这是首个能够在每个参数低于2比特的情况下进行LoRA微调且性能损失最小的框架。LowRA优化了细粒度量化——包括映射、阈值选择和精度分配——同时利用高效的CUDA内核实现可扩展部署。在4个LLMs和4个数据集上的广泛评估表明，LowRA在2比特以上实现了卓越的性能-精度权衡，并在低至1.15比特时仍保持准确，内存使用量最多减少50%。我们的成果凸显了在资源受限环境下进行超低位LoRA微调的潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">无需训练的修剪后神经网络恢复</h2><a id="user-content-无需训练的修剪后神经网络恢复" class="anchor" aria-label="Permalink: 无需训练的修剪后神经网络恢复" href="#无需训练的修剪后神经网络恢复"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2502.08474v1 公告类型：新摘要 摘要：尽管网络剪枝技术已被广泛用于压缩深度神经网络，但其最终精度很大程度上依赖于一个计算成本高昂且需要原始数据的微调过程。然而，在现实场景中，这可能并不现实，因此最近有少数研究尝试在不进行昂贵再训练的情况下恢复剪枝后的网络。这些研究基于一个强有力的假设，即每个被剪枝的神经元都可以被另一个与之非常相似的神经元替代，但遗憾的是，这一假设在许多神经网络中并不成立，尤其是在某些层中神经元间的相似性极低。本文提出了一种更为严谨和鲁棒的方法，名为LBYL（Leave Before You Leave），以无需微调和数据的方式恢复剪枝网络。LBYL极大地放宽了上述假设，使得每个被剪枝的神经元尽可能多地向保留的神经元传递其信息片段，从而多个神经元共同实现对刚离开神经元原始输出的更稳健近似。我们的方法基于对如何构建原始网络与其近似之间重构误差的理论分析，这巧妙地引导出我们推导出的损失函数的闭式解。通过大量实验验证，与最近利用两个神经元间相似性的方法相比，LBYL在近似原始网络方面确实更为有效，因而能够使恢复后的网络达到更高的准确率。本工作的最初版本，包含主要技术和理论部分，已提交至NeurIPS 2021和ICML 2022。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/gpt_paper_assistant_ori" href="https://github.com/LLIKKE/gpt_paper_assistant_ori" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>