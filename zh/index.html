<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">COALA：一种数值稳定且高效的上下文感知低秩近似框架</h2><a id="user-content-coala一种数值稳定且高效的上下文感知低秩近似框架" class="anchor" aria-label="Permalink: COALA：一种数值稳定且高效的上下文感知低秩近似框架" href="#coala一种数值稳定且高效的上下文感知低秩近似框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>最新研究表明，上下文感知的低秩近似技术已成为现代大规模神经网络压缩与微调的有效工具。该方法通过输入激活矩阵对范数进行加权，相较于未加权方案能显著提升各项性能指标。然而，现有神经网络处理方法因依赖经典计算公式（涉及显式格拉姆矩阵计算及其后续求逆）而存在数值不稳定性问题。我们证明这会导致近似质量下降或产生数值奇异矩阵。</p>
<p>为突破这些局限，我们提出了一种基于稳定分解的新型免求逆正则化框架，从根本上克服了现有技术的数值缺陷。我们的方法能应对以下挑战性场景：(1) 校准矩阵超出GPU内存容量时，(2) 输入激活矩阵接近奇异时，甚至(3) 数据不足导致无法获得唯一近似解时。针对最后一种情况，我们证明了所提解法能收敛至期望近似，并推导出明确的误差边界。</p>
<p>（注：译文严格遵循学术论文的严谨表述风格，在保持专业术语准确性的同时，对英语长句进行了符合中文表达习惯的拆分与重组。关键概念如"context-aware"译为"上下文感知"、"low-rank approximation"译为"低秩近似"等均采用计算机领域标准译法。技术难点部分通过增补"显式""后续"等衔接词确保逻辑清晰，并采用"免求逆""数值奇异"等专业表述体现技术特色。）</p>
<div class="markdown-heading"><h2 class="heading-element">UnIT：面向MCU上高效MAC神经推理的可扩展非结构化推理时剪枝技术</h2><a id="user-content-unit面向mcu上高效mac神经推理的可扩展非结构化推理时剪枝技术" class="anchor" aria-label="Permalink: UnIT：面向MCU上高效MAC神经推理的可扩展非结构化推理时剪枝技术" href="#unit面向mcu上高效mac神经推理的可扩展非结构化推理时剪枝技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>现有剪枝方法通常应用于训练或编译阶段，且多依赖结构化稀疏策略。虽然这类方法能与低功耗微控制器（MCU）兼容，但在缺乏SIMD支持或并行计算能力的设备上，结构化剪枝未能充分发挥细粒度效率优化的潜力。为突破这些限制，我们提出UnIT（非结构化推理时剪枝）——一种轻量级方法，通过输入特定的激活模式动态识别并跳过推理过程中不必要的乘累加（MAC）运算。与结构化剪枝不同，UnIT充分利用非规则稀疏性，既不需要重新训练也无需硬件定制化改造。该方法将剪枝决策转化为轻量级比较运算，用阈值比较和近似除法取代乘法操作。UnIT还通过跨连接复用阈值计算、应用分层分组敏感剪枝策略进一步优化计算效率。我们针对常见嵌入式平台特性提出了三种快速硬件友好型除法近似方案。在MSP430微控制器上的实验表明：相比训练阶段剪枝模型，UnIT可实现11.02%-82.03%的MAC运算削减，推理速度提升27.30%-84.19%，能耗降低27.33%-84.38%，同时保持0.48-7%的精度损失。在领域偏移场景下，UnIT在显著减少MAC运算的同时，其精度表现与重新训练模型相当或更优。这些成果证实了非结构化推理时剪枝作为一种免重训练的高效部署方案，在MCU上实现深度神经网络实际应用的可行性与实用性。</p>
<div class="markdown-heading"><h2 class="heading-element">生成式AI时代的钓鱼攻击检测：量化大语言模型与传统模型之争</h2><a id="user-content-生成式ai时代的钓鱼攻击检测量化大语言模型与传统模型之争" class="anchor" aria-label="Permalink: 生成式AI时代的钓鱼攻击检测：量化大语言模型与传统模型之争" href="#生成式ai时代的钓鱼攻击检测量化大语言模型与传统模型之争"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>网络钓鱼攻击手段日益复杂多变，这凸显出构建兼具高准确性与计算效率的检测系统的迫切需求。本文对传统机器学习（ML）、深度学习（DL）以及量化小参数大语言模型（LLM）在钓鱼检测中的表现进行了对比评估。通过在精选数据集上的实验，我们发现：虽然当前LLM在原始准确率上逊于ML和DL方法，但其在识别基于上下文的隐蔽钓鱼线索方面展现出强大潜力。我们还探究了零样本和少样本提示策略的影响，发现经过LLM重写的邮件会显著降低基于ML和LLM的检测器性能。基准测试表明，诸如DeepSeek R1 Distill Qwen 14B（Q8_0）等模型仅需17GB显存即可实现超过80%的竞争性准确率，验证了其低成本部署的可行性。我们进一步评估了模型的对抗鲁棒性与性价比，并展示了轻量级LLM如何通过简洁可解释的推理过程支持实时决策。这些发现将优化后的LLM确立为钓鱼防御系统的潜力组件，为将可解释、高效的人工智能整合进现代网络安全框架提供了可行路径。</p>
<div class="markdown-heading"><h2 class="heading-element">多粒度时空令牌合并：实现视频大语言模型的无训练加速</h2><a id="user-content-多粒度时空令牌合并实现视频大语言模型的无训练加速" class="anchor" aria-label="Permalink: 多粒度时空令牌合并：实现视频大语言模型的无训练加速" href="#多粒度时空令牌合并实现视频大语言模型的无训练加速"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>视频大语言模型（LLMs）通过利用大量时空标记实现了强大的视频理解能力，但其计算量会随标记数量呈二次方增长。为解决这一问题，我们提出了一种无需训练的时空标记合并方法STTM。我们的核心洞见在于挖掘视频数据中局部空间与时间维度的冗余性——这一特性在先前研究中被忽视。STTM首先通过四叉树结构的粗细搜索将每帧转换为多粒度空间标记，随后在时间维度上进行定向成对合并。这种分解式合并方法在六个视频问答基准测试中均优于现有标记压缩技术。值得注意的是，在标记预算为50%时，STTM能以仅0.5%的准确率下降实现2倍加速；当预算降至30%时，仍能保持3倍加速且准确率仅下降2%。此外，STTM具有查询无关性，可对同一视频的不同问题重复使用键值缓存。项目页面详见<a href="https://www.jshyun.me/projects/sttm%E3%80%82" rel="nofollow">https://www.jshyun.me/projects/sttm。</a></p>
<p>（注：根据技术文本翻译规范，对以下要点进行了优化处理：</p>
<ol>
<li>"quadratic computational scaling"译为"二次方增长"符合数学术语习惯</li>
<li>"coarse-to-fine search over a quadtree structure"采用"粗细搜索"与"四叉树结构"的专业译法</li>
<li>"KV cache"保留技术缩写"键值缓存"并添加"（key-value）"的说明性翻译</li>
<li>百分数表达统一为"X%"</li>
<li>长难句拆分为符合中文表达习惯的短句结构</li>
<li>专业术语如"spatio-temporal tokens"统一译为"时空标记"保持全文一致性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">跳过一层还是循环利用？预训练大语言模型的测试时深度自适应</h2><a id="user-content-跳过一层还是循环利用预训练大语言模型的测试时深度自适应" class="anchor" aria-label="Permalink: 跳过一层还是循环利用？预训练大语言模型的测试时深度自适应" href="#跳过一层还是循环利用预训练大语言模型的测试时深度自适应"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>预训练的神经网络能否在不进行任何微调的情况下，根据不同输入自适应调整其架构？对于简单任务是否需要动用所有层，而现有层数又是否足以应对复杂挑战？我们发现，预训练大语言模型（LLM）的各层可作为独立模块进行灵活重组，从而为每个测试样本构建出更优甚至更浅的定制化模型。具体而言，预训练模型的每一层都可以被跳过/剪枝，或像循环神经网络（RNN）那样重复使用，并以任意顺序与其他层堆叠组合，最终为每个样本生成专属的层链（Chain-of-Layers，CoLa）。这种组合式架构极大拓展了现有工作的可能性边界——无论是循环式预训练模块、层剪枝还是早期退出网络。我们开发了蒙特卡洛树搜索（MCTS）协议，用于在数学和常识推理基准测试中为每个样本探索并确定最优CoLa配置。相较于固定深度的静态模型，CoLa支持快捷路径（快速思考）、单层循环（深度思考）及二者组合，为不同输入提供更灵活的动态架构。通过对MCTS优化的CoLa进行深入分析，我们获得两项关键发现：（1）在原始LLM预测正确的样本中，超过75%存在更短的CoLa路径，这表明推理效率存在巨大提升空间；（2）在原始预测错误的样本中，超过60%可通过特定CoLa实现正确预测，这揭示了性能改进的广阔潜力。这些结果凸显了固定架构预训练LLM在不同样本推理中的局限性，为解锁测试时深度自适应泛化能力开辟了新途径。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>