<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">Fairy$\pm i$：首个全参数为${\pm1, \pm i}$的2位复数大语言模型</h2><a id="user-content-fairypm-i首个全参数为pm1-pm-i的2位复数大语言模型" class="anchor" aria-label="Permalink: Fairy$\pm i$：首个全参数为${\pm1, \pm i}$的2位复数大语言模型" href="#fairypm-i首个全参数为pm1-pm-i的2位复数大语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：此翻译在保持数学符号规范性的同时，采用以下处理：</p>
<ol>
<li>保留技术术语"LLM"的标准译法"大语言模型"</li>
<li>复数单位"i"沿用数学领域通用符号不作翻译</li>
<li>"2-bit"译为"2位"符合中文计算机术语习惯</li>
<li>使用中文括号保持数学集合表示规范</li>
<li>标题结构采用中文技术文献常用的冒号分层表述）</li>
</ol>
<p>量化感知训练（QAT）将量化过程融入训练循环，使大语言模型能够学习鲁棒的低比特表征，被公认为最具前景的研究方向之一。现有QAT研究均致力于最小化全精度模型的量化误差——全精度模型的准确率在此被视为不可逾越的上限（精度天花板）。目前尚无任何方法尝试突破这一极限。为打破此桎梏，我们提出全新范式：先抬升天花板（全精度模型性能），再将其高效量化为2比特。我们推出首个面向复数值大语言模型的2比特量化框架Fairy$\pm i$。具体而言，该方法利用复数域的表示优势提升全精度准确率：将权重映射至四次单位根${\pm1, \pm i}$，形成完全对称且信息论最优的2比特表征。关键突破在于，每个量化权重的实部或虚部必为零，使得推理过程仅需加法与元素交换即可完成，彻底消除乘法运算。实验表明，Fairy$\pm i$在PPL和下游任务中均超越现有2比特量化方法的精度上限，同时严格保持存储与计算效率。此项研究为在极低比特约束下构建高精度实用化大语言模型开辟了新方向。</p>
<p>（翻译说明：</p>
<ol>
<li>专业术语处理："Quantization-Aware Training"采用业界通用译名"量化感知训练"，"LLMs"译为"大语言模型"</li>
<li>数学概念本地化："fourth roots of unity"译为"四次单位根"符合数学术语规范</li>
<li>技术亮点传达：通过"实部或虚部必为零"准确表达复数量化的核心创新</li>
<li>句式重构：将英语长句拆分为符合中文表达习惯的短句，如对实验结果的描述</li>
<li>文化适配："ceiling"译为"天花板"并保留括号英文注释，兼顾专业性与可读性</li>
<li>符号保留：框架名称Fairy$\pm i$保留原始数学符号，维持学术严谨性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">MoBE：基于专家混合的压缩技术——面向MoE架构的大语言模型</h2><a id="user-content-mobe基于专家混合的压缩技术面向moe架构的大语言模型" class="anchor" aria-label="Permalink: MoBE：基于专家混合的压缩技术——面向MoE架构的大语言模型" href="#mobe基于专家混合的压缩技术面向moe架构的大语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（翻译说明：</p>
<ol>
<li>采用"专家混合"对应"Mixture-of-Experts"的标准学术译法</li>
<li>"Basis-Experts"创造性译为"基础专家"，保留算法层级含义
3 使用破折号衔接副标题，符合中文技术文献标题规范</li>
<li>"LLMs"完整展开为"大语言模型"，避免术语缩写</li>
<li>整体结构保持原标题的技术精确性，同时通过"面向"一词增强中文动态表达）</li>
</ol>
<p>混合专家（Mixture-of-Experts，MoE）架构已成为扩展大语言模型（LLM）的主流范式。尽管DeepSeek-V3-0324、Kimi-K2-Instruct等基于MoE的大型语言模型展现出卓越性能和计算效率，但其部署时巨大的内存需求带来了严峻挑战。虽然近期研究尝试通过MoE压缩解决该问题，但现有方法即便在适度压缩率下也常伴随显著精度下降（例如相对下降7%-14%）。本文提出一种创新的混合基底专家（Mixture-of-Basis-Experts，MoBE）方法，能在几乎不损失精度的情况下实现模型压缩。具体而言，该方法通过秩分解将每个专家模块的上投影/门控矩阵表示为W = AB，其中矩阵A为专家独有；而规模较大的共享矩阵B则被进一步重构为MoE层内所有专家共用的基底矩阵{Bi}的线性组合。通过最小化权重矩阵重构误差学习分解参数，实验表明MoBE相比现有方法能显著降低精度损失。例如在Qwen3-235B-A22B-2507、DeepSeek-V3-0324（671B）和Kimi-K2-Instruct（1T）等模型上，MoBE可实现24%-30%的参数缩减，绝对精度损失仅1%-2%（相对精度损失约2%）。</p>
<div class="markdown-heading"><h2 class="heading-element">通过识别并保留功能网络来修剪大型语言模型</h2><a id="user-content-通过识别并保留功能网络来修剪大型语言模型" class="anchor" aria-label="Permalink: 通过识别并保留功能网络来修剪大型语言模型" href="#通过识别并保留功能网络来修剪大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>结构化剪枝是压缩大型语言模型（LLM）的代表性技术之一，旨在降低GPU内存消耗并加速推理速度。该技术对于提升LLM在实际应用中的效率具有重要实用价值。当前主流的结构化剪枝方法通常基于对结构单元重要性的评估，进而裁剪重要性较低的单元。然而这类方法大多忽视了人工神经元间的交互与协作——这些特性对LLM的功能实现至关重要，往往会导致LLM宏观功能架构的破坏，最终造成剪枝效果下降。</p>
<p>受人工神经网络与人脑功能神经网络内在相似性的启发，本研究通过识别并保留LLM内部的功能网络来解决这一难题。具体而言，我们将LLM视为数字大脑，仿照神经影像数据中功能脑网络的识别方法，将LLM分解为多个功能网络。随后通过保留这些功能网络中的关键神经元来实现模型剪枝。实验结果表明，所提方法能成功识别并定位LLM中的功能网络与关键神经元，实现高效模型压缩。项目代码已开源：<a href="https://github.com/WhatAboutMyStar/LLM_ACTIVATION%E3%80%82">https://github.com/WhatAboutMyStar/LLM_ACTIVATION。</a></p>
<p>（注：根据技术文本特点，翻译中进行了以下处理：</p>
<ol>
<li>专业术语统一："structured pruning"译为"结构化剪枝"，"functional networks"译为"功能网络"</li>
<li>长句拆分：将原文复合句按中文表达习惯分解为多个短句</li>
<li>概念显化："digital brain"译为"数字大脑"并补充说明其类比关系</li>
<li>被动语态转换："are crucial for"译为"对...至关重要"</li>
<li>学术规范：保留技术缩写LLM及项目URL原貌）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>