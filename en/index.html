<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h1 class="heading-element">Personalized Daily Arxiv Papers 10/01/2025</h1><a id="user-content-personalized-daily-arxiv-papers-10012025" class="anchor" aria-label="Permalink: Personalized Daily Arxiv Papers 10/01/2025" href="#personalized-daily-arxiv-papers-10012025"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>Total relevant papers: 43</p>
<p>Paper selection prompt and criteria at the bottom</p>
<p>Table of contents with paper titles:</p>
<ol start="0">
<li>
<p><a href="#link0">Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology</a>
<strong>Authors:</strong> Suvrankar Datta, Divya Buchireddygari, Lakshmi Vennela Chowdary Kaza, Mrudula Bhalke, Kautik Singh, Ayush Pandey, Sonit Sai Vasipalli, Upasana Karnwal, Hakikat Bir Singh Bhatti, Bhavya Ratan Maroo, Sanjana Hebbar, Rahul Joseph, Gurkawal Kaur, Devyani Singh, Akhil V, Dheeksha Devasya Shama Prasad, Nishtha Mahajan, Ayinaparthi Arisha, Rajesh Vanagundi, Reet Nandy, Kartik Vuthoo, Snigdhaa Rajvanshi, Nikhileswar Kondaveeti, Suyash Gunjal, Rishabh Jain, Rajat Jain, Anurag Agrawal</p>
</li>
<li>
<p><a href="#link1">Saliency Guided Longitudinal Medical Visual Question Answering</a>
<strong>Authors:</strong> Jialin Wu, Xiaofeng Liu</p>
</li>
<li>
<p><a href="#link2">Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline</a>
<strong>Authors:</strong> Haiyang Li, Yaxiong Wang, Lianwei Wu, Lechao Cheng, Zhun Zhong</p>
</li>
<li>
<p><a href="#link3">Automated Model Discovery via Multi-modal &amp; Multi-step Pipeline</a>
<strong>Authors:</strong> Lee Jung-Mok, Nam Hyeon-Woo, Moon Ye-Bin, Junhyun Nam, Tae-Hyun Oh</p>
</li>
<li>
<p><a href="#link4">Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models</a>
<strong>Authors:</strong> Max Hartman, Vidhata Jayaraman, Moulik Choraria, Akhil Bhimaraju, Lav R. Varshney</p>
</li>
<li>
<p><a href="#link5">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a>
<strong>Authors:</strong> Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos</p>
</li>
<li>
<p><a href="#link6">CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search</a>
<strong>Authors:</strong> Olga Krestinskaya, Mohammed E. Fouda, Ahmed Eltawil, Khaled N. Salama</p>
</li>
<li>
<p><a href="#link7">MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning</a>
<strong>Authors:</strong> Seong-Hyeon Hwang, Soyoung Choi, Steven Euijong Whang</p>
</li>
<li>
<p><a href="#link8">MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series</a>
<strong>Authors:</strong> Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, Qi Zhu</p>
</li>
<li>
<p><a href="#link9">VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps</a>
<strong>Authors:</strong> Zhuoning Xu, Xinyan Liu</p>
</li>
<li>
<p><a href="#link10">dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought</a>
<strong>Authors:</strong> Junjie Wen, Minjie Zhu, Jiaming Liu, Zhiyuan Liu, Yicun Yang, Linfeng Zhang, Shanghang Zhang, Yichen Zhu, Yi Xu</p>
</li>
<li>
<p><a href="#link11">InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</a>
<strong>Authors:</strong> Liangjian Wen, Qun Dai, Jianzhuang Liu, Jiangtao Zheng, Yong Dai, Dongkai Wang, Zhao Kang, Jun Wang, Zenglin Xu, Jiang Duan</p>
</li>
<li>
<p><a href="#link12">From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models</a>
<strong>Authors:</strong> Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao, Hehan Li, Andi Zhang, Wenxuan Wang, Lingling Li, Zhiwu Lu, Yang Lu, Yike Guo</p>
</li>
<li>
<p><a href="#link13">AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond</a>
<strong>Authors:</strong> Shangding Gu, Xiaohan Wang, Donghao Ying, Haoyu Zhao, Runing Yang, Ming Jin, Boyi Li, Marco Pavone, Serena Yeung-Levy, Jun Wang, Dawn Song, Costas Spanos</p>
</li>
<li>
<p><a href="#link14">Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition</a>
<strong>Authors:</strong> Jiacheng Shi, Hongfei Du, Y. Alicia Hong, Ye Gao</p>
</li>
<li>
<p><a href="#link15">HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided Controllable Speech Synthesis</a>
<strong>Authors:</strong> Ziyu Zhang, Hanzhao Li, Jingbin Hu, Wenhao Li, Lei Xie</p>
</li>
<li>
<p><a href="#link16">GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination</a>
<strong>Authors:</strong> Xinxi Chen, Tianyang Chen, Lijia Hong</p>
</li>
<li>
<p><a href="#link17">On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs</a>
<strong>Authors:</strong> Rongguang Ye, Ming Tang, Edith C. H. Ngai</p>
</li>
<li>
<p><a href="#link18">Guiding Mixture-of-Experts with Temporal Multimodal Interactions</a>
<strong>Authors:</strong> Xing Han, Hsing-Huan Chung, Joydeep Ghosh, Paul Pu Liang, Suchi Saria</p>
</li>
<li>
<p><a href="#link19">AMLA: MUL by ADD in FlashAttention Rescaling</a>
<strong>Authors:</strong> Qichen Liao, Chengqiu Hu, Fangzheng Miao, Bao Li, Yiyang Liu, Junlong Lyu, Lirui Jiang, Jun Wang, Lingchao Zheng, Jun Li, Yuwei Fan</p>
</li>
<li>
<p><a href="#link20">Effective Model Pruning</a>
<strong>Authors:</strong> Yixuan Wang, Dan Guralnik, Saiedeh Akbari, Warren Dixon</p>
</li>
<li>
<p><a href="#link21">Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks</a>
<strong>Authors:</strong> Hailong Zhang, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</p>
</li>
<li>
<p><a href="#link22">90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development</a>
<strong>Authors:</strong> Runxin Yang, Yuxuan Wan, Shuqing Li, Michael R. Lyu</p>
</li>
<li>
<p><a href="#link23">DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick</a>
<strong>Authors:</strong> Mohammad Hassan Vali, Tom B"ackstr"om, Arno Solin</p>
</li>
<li>
<p><a href="#link24">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a>
<strong>Authors:</strong> Hao Ban, Kaiyi Ji</p>
</li>
<li>
<p><a href="#link25">Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</a>
<strong>Authors:</strong> Xiang Zhang, Kun Wei, Xu Yang, Chenghao Xu, Su Yan, Cheng Deng</p>
</li>
<li>
<p><a href="#link26">NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language</a>
<strong>Authors:</strong> Danial Kamali, Parisa Kordjamshidi</p>
</li>
<li>
<p><a href="#link27">VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning</a>
<strong>Authors:</strong> Si-Cheng Wang, Tian-Yu Xiang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Ao-Qun Jin, Zeng-Guang Hou</p>
</li>
<li>
<p><a href="#link28">Collaborative Compression for Large-Scale MoE Deployment on Edge</a>
<strong>Authors:</strong> Yixiao Chen, Yanyue Xie, Ruining Yang, Wei Jiang, Wei Wang, Yong He, Yue Chen, Pu Zhao, Yanzhi Wang</p>
</li>
<li>
<p><a href="#link29">MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation</a>
<strong>Authors:</strong> Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang</p>
</li>
<li>
<p><a href="#link30">Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks</a>
<strong>Authors:</strong> Qihang Yao, Constantine Dovrolis</p>
</li>
<li>
<p><a href="#link31">FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers</a>
<strong>Authors:</strong> Liang Qiao, Yue Dai, Yeqi Huang, Hongyu Kan, Jun Shi, Hong An</p>
</li>
<li>
<p><a href="#link32">Commmunication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation</a>
<strong>Authors:</strong> Le-Tuan Nguyen, Minh-Duong Nguyen, Seon-Geun Jeong, Dung D. Le, Quoc-Viet Pham</p>
</li>
<li>
<p><a href="#link33">Distillation of Large Language Models via Concrete Score Matching</a>
<strong>Authors:</strong> Yeongmin Kim, Donghyeok Shin, Mina Kang, Byeonghu Na, Il-Chul Moon</p>
</li>
<li>
<p><a href="#link34">Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking</a>
<strong>Authors:</strong> Dengming Zhang, Xiaowen Ma, Zhenliang Ni, Zhenkai Wu, Han Shu, Xin Jiang, Xinghao Chen</p>
</li>
<li>
<p><a href="#link35">Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces</a>
<strong>Authors:</strong> John Gkountouras, Ivan Titov</p>
</li>
<li>
<p><a href="#link36">Layer-wise dynamic rank for compressing large language models</a>
<strong>Authors:</strong> Zhendong Mi, Bian Sun, Grace Li Zhang, Shaoyi Huang</p>
</li>
<li>
<p><a href="#link37">CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models</a>
<strong>Authors:</strong> Weiyu Huang, Yuezhou Hu, Jun Zhu, Jianfei Chen</p>
</li>
<li>
<p><a href="#link38">OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation</a>
<strong>Authors:</strong> Xinda Xue, Junjun Hu, Minghua Luo, Xie Shichao, Jintao Chen, Zixun Xie, Quan Kuichen, Guo Wei, Mu Xu, Zedong Chu</p>
</li>
<li>
<p><a href="#link39">TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning</a>
<strong>Authors:</strong> Seohyun Lee, Wenzhi Fang, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton</p>
</li>
<li>
<p><a href="#link40">STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models</a>
<strong>Authors:</strong> Shaoxiong Guo, Tianyi Du, Lijun Li, Yuyao Wu, Jie Li, Jing Shao</p>
</li>
<li>
<p><a href="#link41">Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?</a>
<strong>Authors:</strong> Takuya Fujimura, Kota Dohi, Natsuo Yamashita, Yohei Kawaguchi</p>
</li>
<li>
<p><a href="#link42">TASP: Topology-aware Sequence Parallelism</a>
<strong>Authors:</strong> Yida Wang (Capital Normal University, Infinigence-AI), Ke Hong (Tsinghua University, Infinigence-AI), Xiuhong Li (Infinigence-AI), Yuanchao Xu (Capital Normal University), Wenxun Wang (Tsinghua University), Guohao Dai (Infinigence-AI, Shanghai Jiao Tong University), Yu Wang (Tsinghua University)</p>
</li>
</ol>
<hr>
<div class="markdown-heading"><h2 class="heading-element">0. <a href="https://arxiv.org/abs/2509.25559" rel="nofollow">Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology</a> <a id="user-content-link0"></a>
</h2><a id="user-content-0-radiologys-last-exam-radle-benchmarking-frontier-multimodal-ai-against-human-experts-and-a-taxonomy-of-visual-reasoning-errors-in-radiology-" class="anchor" aria-label="Permalink: 0. Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology" href="#0-radiologys-last-exam-radle-benchmarking-frontier-multimodal-ai-against-human-experts-and-a-taxonomy-of-visual-reasoning-errors-in-radiology-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25559
<strong>Authors:</strong> Suvrankar Datta, Divya Buchireddygari, Lakshmi Vennela Chowdary Kaza, Mrudula Bhalke, Kautik Singh, Ayush Pandey, Sonit Sai Vasipalli, Upasana Karnwal, Hakikat Bir Singh Bhatti, Bhavya Ratan Maroo, Sanjana Hebbar, Rahul Joseph, Gurkawal Kaur, Devyani Singh, Akhil V, Dheeksha Devasya Shama Prasad, Nishtha Mahajan, Ayinaparthi Arisha, Rajesh Vanagundi, Reet Nandy, Kartik Vuthoo, Snigdhaa Rajvanshi, Nikhileswar Kondaveeti, Suyash Gunjal, Rishabh Jain, Rajat Jain, Anurag Agrawal</p>
<p><strong>Abstract:</strong> arXiv:2509.25559v1 Announce Type: new  Abstract: Generalist multimodal AI systems such as large language models (LLMs) and vision language models (VLMs) are increasingly accessed by clinicians and patients alike for medical image interpretation through widely available consumer-facing chatbots. Most evaluations claiming expert level performance are on public datasets containing common pathologies. Rigorous evaluation of frontier models on difficult diagnostic cases remains limited. We developed a pilot benchmark of 50 expert-level "spot diagnosis" cases across multiple imaging modalities to evaluate the performance of frontier AI models against board-certified radiologists and radiology trainees. To mirror real-world usage, the reasoning modes of five popular frontier AI models were tested through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5 Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and reproducibility was assessed across three independent runs. GPT-5 was additionally evaluated across various reasoning modes. Reasoning quality errors were assessed and a taxonomy of visual reasoning errors was defined. Board-certified radiologists achieved the highest diagnostic accuracy (83%), outperforming trainees (45%) and all AI models (best performance shown by GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate that advanced frontier models fall far short of radiologists in challenging diagnostic cases. Our benchmark highlights the present limitations of generalist AI in medical imaging and cautions against unsupervised clinical use. We also provide a qualitative analysis of reasoning traces and propose a practical taxonomy of visual reasoning errors by AI models for better understanding their failure modes, informing evaluation standards and guiding more robust model development.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">1. <a href="https://arxiv.org/abs/2509.25374" rel="nofollow">Saliency Guided Longitudinal Medical Visual Question Answering</a> <a id="user-content-link1"></a>
</h2><a id="user-content-1-saliency-guided-longitudinal-medical-visual-question-answering-" class="anchor" aria-label="Permalink: 1. Saliency Guided Longitudinal Medical Visual Question Answering" href="#1-saliency-guided-longitudinal-medical-visual-question-answering-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25374
<strong>Authors:</strong> Jialin Wu, Xiaofeng Liu</p>
<p><strong>Abstract:</strong> arXiv:2509.25374v1 Announce Type: new  Abstract: Longitudinal medical visual question answering (Diff-VQA) requires comparing paired studies from different time points and answering questions about clinically meaningful changes. In this setting, the difference signal and the consistency of visual focus across time are more informative than absolute single-image findings. We propose a saliency-guided encoder-decoder for chest X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The model first performs a lightweight near-identity affine pre-alignment to reduce nuisance motion between visits. It then executes a within-epoch two-step loop: step 1 extracts a medically relevant keyword from the answer and generates keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency; step 2 applies the shared saliency mask to both time points and generates the final answer. This closes the language-vision loop so that the terms that matter also guide where the model looks, enforcing spatially consistent attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing intrinsic interpretability. Notably, the backbone and decoder are general-domain pretrained without radiology-specific pretraining, highlighting practicality and transferability. These results support saliency-conditioned generation with mild pre-alignment as a principled framework for longitudinal reasoning in medical VQA.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">2. <a href="https://arxiv.org/abs/2509.25991" rel="nofollow">Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline</a> <a id="user-content-link2"></a>
</h2><a id="user-content-2-towards-unified-multimodal-misinformation-detection-in-social-media-a-benchmark-dataset-and-baseline-" class="anchor" aria-label="Permalink: 2. Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline" href="#2-towards-unified-multimodal-misinformation-detection-in-social-media-a-benchmark-dataset-and-baseline-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25991
<strong>Authors:</strong> Haiyang Li, Yaxiong Wang, Lianwei Wu, Lechao Cheng, Zhun Zhong</p>
<p><strong>Abstract:</strong> arXiv:2509.25991v1 Announce Type: new  Abstract: In recent years, detecting fake multimodal content on social media has drawn increasing attention. Two major forms of deception dominate: human-crafted misinformation (e.g., rumors and misleading posts) and AI-generated content produced by image synthesis models or vision-language models (VLMs). Although both share deceptive intent, they are typically studied in isolation. NLP research focuses on human-written misinformation, while the CV community targets AI-generated artifacts. As a result, existing models are often specialized for only one type of fake content. In real-world scenarios, however, the type of a multimodal post is usually unknown, limiting the effectiveness of such specialized systems. To bridge this gap, we construct the Omnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive benchmark of 127K samples that integrates human-curated misinformation from existing resources with newly synthesized AI-generated examples. Based on this dataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a framework designed to handle both forms of deception. UMFDet leverages a VLM backbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to capture category-specific cues, and an attribution chain-of-thought mechanism that provides implicit reasoning guidance for locating salient deceptive signals. Extensive experiments demonstrate that UMFDet achieves robust and consistent performance across both misinformation types, outperforming specialized baselines and offering a practical solution for real-world multimodal deception detection.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">3. <a href="https://arxiv.org/abs/2509.25946" rel="nofollow">Automated Model Discovery via Multi-modal &amp; Multi-step Pipeline</a> <a id="user-content-link3"></a>
</h2><a id="user-content-3-automated-model-discovery-via-multi-modal--multi-step-pipeline-" class="anchor" aria-label="Permalink: 3. Automated Model Discovery via Multi-modal &amp; Multi-step Pipeline" href="#3-automated-model-discovery-via-multi-modal--multi-step-pipeline-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25946
<strong>Authors:</strong> Lee Jung-Mok, Nam Hyeon-Woo, Moon Ye-Bin, Junhyun Nam, Tae-Hyun Oh</p>
<p><strong>Abstract:</strong> arXiv:2509.25946v1 Announce Type: new  Abstract: Automated model discovery is the process of automatically searching and identifying the most appropriate model for a given dataset over a large combinatorial search space. Existing approaches, however, often face challenges in balancing the capture of fine-grained details with ensuring generalizability beyond training data regimes with a reasonable model complexity. In this paper, we present a multi-modal &amp; multi-step pipeline for effective automated model discovery. Our approach leverages two vision-language-based modules (VLM), AnalyzerVLM and EvaluatorVLM, for effective model proposal and evaluation in an agentic way. AnalyzerVLM autonomously plans and executes multi-step analyses to propose effective candidate models. EvaluatorVLM assesses the candidate models both quantitatively and perceptually, regarding the fitness for local details and the generalibility for overall trends. Our results demonstrate that our pipeline effectively discovers models that capture fine details and ensure strong generalizability. Additionally, extensive ablation studies show that both multi-modality and multi-step reasoning play crucial roles in discovering favorable models.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">4. <a href="https://arxiv.org/abs/2509.25584" rel="nofollow">Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models</a> <a id="user-content-link4"></a>
</h2><a id="user-content-4-skip-it-theoretical-conditions-for-layer-skipping-in-vision-language-models-" class="anchor" aria-label="Permalink: 4. Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models" href="#4-skip-it-theoretical-conditions-for-layer-skipping-in-vision-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25584
<strong>Authors:</strong> Max Hartman, Vidhata Jayaraman, Moulik Choraria, Akhil Bhimaraju, Lav R. Varshney</p>
<p><strong>Abstract:</strong> arXiv:2509.25584v1 Announce Type: new  Abstract: Vision-language models (VLMs) achieve incredible performance across a wide range of tasks, but their large size makes inference costly. Recent work shows that selectively skipping VLM layers can improve efficiency with minimal performance loss or even performance improvements. However, this technique remains underused due to the limited understanding of when layer skipping is beneficial. In this paper, we develop a framework that uses information and learning theory to characterize the conditions under which layer skipping enhances efficiency without sacrificing performance. Motivated by these observations, we analyze the evolution of the VLM's hidden representations through the LLM backbone and show that layers with large redundancy as predicted by our framework coincide with those skipped by popular layer-skipping methods in practice, providing a unified theoretical scaffolding for multiple efficient inference techniques. Our experiments demonstrate that skipping such layers yields faster inference that preserves performance, and also show that applying skipping outside these conditions leads to model degradation.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">5. <a href="https://arxiv.org/abs/2509.26625" rel="nofollow">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a> <a id="user-content-link5"></a>
</h2><a id="user-content-5-learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training-" class="anchor" aria-label="Permalink: 5. Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training" href="#5-learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26625
<strong>Authors:</strong> Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos</p>
<p><strong>Abstract:</strong> arXiv:2509.26625v1 Announce Type: new  Abstract: Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">6. <a href="https://arxiv.org/abs/2509.25862" rel="nofollow">CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search</a> <a id="user-content-link6"></a>
</h2><a id="user-content-6-cimnas-a-joint-framework-for-compute-in-memory-aware-neural-architecture-search-" class="anchor" aria-label="Permalink: 6. CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search" href="#6-cimnas-a-joint-framework-for-compute-in-memory-aware-neural-architecture-search-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25862
<strong>Authors:</strong> Olga Krestinskaya, Mohammed E. Fouda, Ahmed Eltawil, Khaled N. Salama</p>
<p><strong>Abstract:</strong> arXiv:2509.25862v1 Announce Type: new  Abstract: To maximize hardware efficiency and performance accuracy in Compute-In-Memory (CIM)-based neural network accelerators for Artificial Intelligence (AI) applications, co-optimizing both software and hardware design parameters is essential. Manual tuning is impractical due to the vast number of parameters and their complex interdependencies. To effectively automate the design and optimization of CIM-based neural network accelerators, hardware-aware neural architecture search (HW-NAS) techniques can be applied. This work introduces CIMNAS, a joint model-quantization-hardware optimization framework for CIM architectures. CIMNAS simultaneously searches across software parameters, quantization policies, and a broad range of hardware parameters, incorporating device-, circuit-, and architecture-level co-optimizations. CIMNAS experiments were conducted over a search space of 9.9x10^85 potential parameter combinations with the MobileNet model as a baseline and RRAM-based CIM architecture. Evaluated on the ImageNet dataset, CIMNAS achieved a reduction in energy-delay-area product (EDAP) ranging from 90.1x to 104.5x, an improvement in TOPS/W between 4.68x and 4.82x, and an enhancement in TOPS/mm^2 from 11.3x to 12.78x relative to various baselines, all while maintaining an accuracy of 73.81%. The adaptability and robustness of CIMNAS are demonstrated by extending the framework to support the SRAM-based ResNet50 architecture, achieving up to an 819.5x reduction in EDAP. Unlike other state-of-the-art methods, CIMNAS achieves EDAP-focused optimization without any accuracy loss, generating diverse software-hardware parameter combinations for high-performance CIM-based neural network designs. The source code of CIMNAS is available at <a href="https://github.com/OlgaKrestinskaya/CIMNAS">https://github.com/OlgaKrestinskaya/CIMNAS</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">7. <a href="https://arxiv.org/abs/2509.25831" rel="nofollow">MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning</a> <a id="user-content-link7"></a>
</h2><a id="user-content-7-midas-misalignment-based-data-augmentation-strategy-for-imbalanced-multimodal-learning-" class="anchor" aria-label="Permalink: 7. MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning" href="#7-midas-misalignment-based-data-augmentation-strategy-for-imbalanced-multimodal-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25831
<strong>Authors:</strong> Seong-Hyeon Hwang, Soyoung Choi, Steven Euijong Whang</p>
<p><strong>Abstract:</strong> arXiv:2509.25831v1 Announce Type: new  Abstract: Multimodal models often over-rely on dominant modalities, failing to achieve optimal performance. While prior work focuses on modifying training objectives or optimization procedures, data-centric solutions remain underexplored. We propose MIDAS, a novel data augmentation strategy that generates misaligned samples with semantically inconsistent cross-modal information, labeled using unimodal confidence scores to compel learning from contradictory signals. However, this confidence-based labeling can still favor the more confident modality. To address this within our misaligned samples, we introduce weak-modality weighting, which dynamically increases the loss weight of the least confident modality, thereby helping the model fully utilize weaker modality. Furthermore, when misaligned features exhibit greater similarity to the aligned features, these misaligned samples pose a greater challenge, thereby enabling the model to better distinguish between classes. To leverage this, we propose hard-sample weighting, which prioritizes such semantically ambiguous misaligned samples. Experiments on multiple multimodal classification benchmarks demonstrate that MIDAS significantly outperforms related baselines in addressing modality imbalance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">8. <a href="https://arxiv.org/abs/2509.25278" rel="nofollow">MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series</a> <a id="user-content-link8"></a>
</h2><a id="user-content-8-maestro--adaptive-sparse-attention-and-robust-learning-for-multimodal-dynamic-time-series-" class="anchor" aria-label="Permalink: 8. MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series" href="#8-maestro--adaptive-sparse-attention-and-robust-learning-for-multimodal-dynamic-time-series-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25278
<strong>Authors:</strong> Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, Qi Zhu</p>
<p><strong>Abstract:</strong> arXiv:2509.25278v1 Announce Type: new  Abstract: From clinical healthcare to daily living, continuous sensor monitoring across multiple modalities has shown great promise for real-world intelligent decision-making but also faces various challenges. In this work, we introduce MAESTRO, a novel framework that overcomes key limitations of existing multimodal learning approaches: (1) reliance on a single primary modality for alignment, (2) pairwise modeling of modalities, and (3) assumption of complete modality observations. These limitations hinder the applicability of these approaches in real-world multimodal time-series settings, where primary modality priors are often unclear, the number of modalities can be large (making pairwise modeling impractical), and sensor failures often result in arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra- and cross-modal interactions based on task relevance, and leverages symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, which are processed via sparse cross-modal attention. The resulting cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE) mechanism, enabling black-box specialization under varying modality combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets spanning three applications, and observe average relative improvements of 4% and 8% over the best existing multimodal and multivariate approaches, respectively, under complete observations. Under partial observations -- with up to 40% of missing modalities -- MAESTRO achieves an average 9% improvement. Further analysis also demonstrates the robustness and efficiency of MAESTRO's sparse, modality-aware design for learning from dynamic time series.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">9. <a href="https://arxiv.org/abs/2509.25202" rel="nofollow">VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps</a> <a id="user-content-link9"></a>
</h2><a id="user-content-9-vlhsa-vision-language-hierarchical-semantic-alignment-for-jigsaw-puzzle-solving-with-eroded-gaps-" class="anchor" aria-label="Permalink: 9. VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps" href="#9-vlhsa-vision-language-hierarchical-semantic-alignment-for-jigsaw-puzzle-solving-with-eroded-gaps-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25202
<strong>Authors:</strong> Zhuoning Xu, Xinyan Liu</p>
<p><strong>Abstract:</strong> arXiv:2509.25202v1 Announce Type: new  Abstract: Jigsaw puzzle solving remains challenging in computer vision, requiring an understanding of both local fragment details and global spatial relationships. While most traditional approaches only focus on visual cues like edge matching and visual coherence, few methods explore natural language descriptions for semantic guidance in challenging scenarios, especially for eroded gap puzzles. We propose a vision-language framework that leverages textual context to enhance puzzle assembly performance. Our approach centers on the Vision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns visual patches with textual descriptions through multi-level semantic matching from local tokens to global context. Also, a multimodal architecture that combines dual visual encoders with language features for cross-modal reasoning is integrated into this module. Experiments demonstrate that our method significantly outperforms state-of-the-art models across various datasets, achieving substantial improvements, including a 14.2 percentage point gain in piece accuracy. Ablation studies confirm the critical role of the VLHSA module in driving improvements over vision-only approaches. Our work establishes a new paradigm for jigsaw puzzle solving by incorporating multimodal semantic insights.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">10. <a href="https://arxiv.org/abs/2509.25681" rel="nofollow">dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought</a> <a id="user-content-link10"></a>
</h2><a id="user-content-10-dvla-diffusion-vision-language-action-model-with-multimodal-chain-of-thought-" class="anchor" aria-label="Permalink: 10. dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought" href="#10-dvla-diffusion-vision-language-action-model-with-multimodal-chain-of-thought-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25681
<strong>Authors:</strong> Junjie Wen, Minjie Zhu, Jiaming Liu, Zhiyuan Liu, Yicun Yang, Linfeng Zhang, Shanghang Zhang, Yichen Zhu, Yi Xu</p>
<p><strong>Abstract:</strong> arXiv:2509.25681v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">11. <a href="https://arxiv.org/abs/2509.25270" rel="nofollow">InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</a> <a id="user-content-link11"></a>
</h2><a id="user-content-11-infmasking-unleashing-synergistic-information-by-contrastive-multimodal-interactions-" class="anchor" aria-label="Permalink: 11. InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions" href="#11-infmasking-unleashing-synergistic-information-by-contrastive-multimodal-interactions-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25270
<strong>Authors:</strong> Liangjian Wen, Qun Dai, Jianzhuang Liu, Jiangtao Zheng, Yong Dai, Dongkai Wang, Zhao Kang, Jun Wang, Zenglin Xu, Jiang Duan</p>
<p><strong>Abstract:</strong> arXiv:2509.25270v1 Announce Type: new  Abstract: In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an \textbf{Inf}inite \textbf{Masking} strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at <a href="https://github.com/brightest66/InfMasking">https://github.com/brightest66/InfMasking</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">12. <a href="https://arxiv.org/abs/2509.25373" rel="nofollow">From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models</a> <a id="user-content-link12"></a>
</h2><a id="user-content-12-from-perception-to-cognition-a-survey-of-vision-language-interactive-reasoning-in-multimodal-large-language-models-" class="anchor" aria-label="Permalink: 12. From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models" href="#12-from-perception-to-cognition-a-survey-of-vision-language-interactive-reasoning-in-multimodal-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25373
<strong>Authors:</strong> Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao, Hehan Li, Andi Zhang, Wenxuan Wang, Lingling Li, Zhiwu Lu, Yang Lu, Yike Guo</p>
<p><strong>Abstract:</strong> arXiv:2509.25373v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition." We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">13. <a href="https://arxiv.org/abs/2509.26636" rel="nofollow">AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond</a> <a id="user-content-link13"></a>
</h2><a id="user-content-13-accidentbench-benchmarking-multimodal-understanding-and-reasoning-in-vehicle-accidents-and-beyond-" class="anchor" aria-label="Permalink: 13. AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond" href="#13-accidentbench-benchmarking-multimodal-understanding-and-reasoning-in-vehicle-accidents-and-beyond-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26636
<strong>Authors:</strong> Shangding Gu, Xiaohan Wang, Donghao Ying, Haoyu Zhao, Runing Yang, Ming Jin, Boyi Li, Marco Pavone, Serena Yeung-Levy, Jun Wang, Dawn Song, Costas Spanos</p>
<p><strong>Abstract:</strong> arXiv:2509.26636v1 Announce Type: new  Abstract: Rapid advances in multimodal models demand benchmarks that rigorously evaluate understanding and reasoning in safety-critical, dynamic real-world settings. We present AccidentBench, a large-scale benchmark that combines vehicle accident scenarios with Beyond domains, safety-critical settings in air and water that emphasize spatial and temporal reasoning (e.g., navigation, orientation, multi-vehicle motion). The benchmark contains approximately 2000 videos and over 19000 human-annotated question--answer pairs spanning multiple video lengths (short/medium/long) and difficulty levels (easy/medium/hard). Tasks systematically probe core capabilities: temporal, spatial, and intent understanding and reasoning. By unifying accident-centric traffic scenes with broader safety-critical scenarios in air and water, AccidentBench offers a comprehensive, physically grounded testbed for evaluating models under real-world variability. Evaluations of state-of-the-art models (e.g., Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning. AccidentBench is designed to expose these critical gaps and drive the development of multimodal models that are safer, more robust, and better aligned with real-world safety-critical challenges. The code and dataset are available at: <a href="https://github.com/SafeRL-Lab/AccidentBench">https://github.com/SafeRL-Lab/AccidentBench</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">14. <a href="https://arxiv.org/abs/2509.25458" rel="nofollow">Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition</a> <a id="user-content-link14"></a>
</h2><a id="user-content-14-plug-and-play-emotion-graphs-for-compositional-prompting-in-zero-shot-speech-emotion-recognition-" class="anchor" aria-label="Permalink: 14. Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition" href="#14-plug-and-play-emotion-graphs-for-compositional-prompting-in-zero-shot-speech-emotion-recognition-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25458
<strong>Authors:</strong> Jiacheng Shi, Hongfei Du, Y. Alicia Hong, Ye Gao</p>
<p><strong>Abstract:</strong> arXiv:2509.25458v1 Announce Type: new  Abstract: Large audio-language models (LALMs) exhibit strong zero-shot performance across speech tasks but struggle with speech emotion recognition (SER) due to weak paralinguistic modeling and limited cross-modal reasoning. We propose Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a framework that introduces structured Emotion Graphs (EGs) to guide LALMs in emotion inference without fine-tuning. Each EG encodes seven acoustic features (e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and cross-modal associations. Embedded into prompts, EGs provide interpretable and compositional representations that enhance LALM reasoning. Experiments across SER benchmarks show that CCoT-Emo outperforms prior SOTA and improves accuracy over zero-shot baselines.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">15. <a href="https://arxiv.org/abs/2509.25842" rel="nofollow">HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided Controllable Speech Synthesis</a> <a id="user-content-link15"></a>
</h2><a id="user-content-15-histyle-hierarchical-style-embedding-predictor-for-text-prompt-guided-controllable-speech-synthesis-" class="anchor" aria-label="Permalink: 15. HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided Controllable Speech Synthesis" href="#15-histyle-hierarchical-style-embedding-predictor-for-text-prompt-guided-controllable-speech-synthesis-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25842
<strong>Authors:</strong> Ziyu Zhang, Hanzhao Li, Jingbin Hu, Wenhao Li, Lei Xie</p>
<p><strong>Abstract:</strong> arXiv:2509.25842v1 Announce Type: new  Abstract: Controllable speech synthesis refers to the precise control of speaking style by manipulating specific prosodic and paralinguistic attributes, such as gender, volume, speech rate, pitch, and pitch fluctuation. With the integration of advanced generative models, particularly large language models (LLMs) and diffusion models, controllable text-to-speech (TTS) systems have increasingly transitioned from label-based control to natural language description-based control, which is typically implemented by predicting global style embeddings from textual prompts. However, this straightforward prediction overlooks the underlying distribution of the style embeddings, which may hinder the full potential of controllable TTS systems. In this study, we use t-SNE analysis to visualize and analyze the global style embedding distribution of various mainstream TTS systems, revealing a clear hierarchical clustering pattern: embeddings first cluster by timbre and subsequently subdivide into finer clusters based on style attributes. Based on this observation, we propose HiStyle, a two-stage style embedding predictor that hierarchically predicts style embeddings conditioned on textual prompts, and further incorporate contrastive learning to help align the text and audio embedding spaces. Additionally, we propose a style annotation strategy that leverages the complementary strengths of statistical methodologies and human auditory preferences to generate more accurate and perceptually consistent textual prompts for style control. Comprehensive experiments demonstrate that when applied to the base TTS model, HiStyle achieves significantly better style controllability than alternative style embedding predicting approaches while preserving high speech quality in terms of naturalness and intelligibility. Audio samples are available at <a href="https://anonymous.4open.science/w/HiStyle-2517/" rel="nofollow">https://anonymous.4open.science/w/HiStyle-2517/</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">16. <a href="https://arxiv.org/abs/2509.25669" rel="nofollow">GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination</a> <a id="user-content-link16"></a>
</h2><a id="user-content-16-groundsight-augmenting-vision-language-models-with-grounding-information-and-de-hallucination-" class="anchor" aria-label="Permalink: 16. GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination" href="#16-groundsight-augmenting-vision-language-models-with-grounding-information-and-de-hallucination-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25669
<strong>Authors:</strong> Xinxi Chen, Tianyang Chen, Lijia Hong</p>
<p><strong>Abstract:</strong> arXiv:2509.25669v1 Announce Type: new  Abstract: We propose a method to improve Visual Question Answering (VQA) with Retrieval-Augmented Generation (RAG) by introducing text-grounded object localization. Rather than retrieving information based on the entire image, our approach enables the model to generate a bounding box around the object most relevant to the question, allowing for targeted image cropping and focused retrieval. This reduces background noise, improves alignment between visual and textual cues, and helps mitigate hallucinations. Our RAG method enhances context-aware VQA responses increased the accuracy from 22.19% to 25.64%, with an absolute increase of 3.45 percentage points, compared to the baseline Llama-3.2-Vision-11B agent. We also proposed a de-hallucination method based on question type which can effectively reduce the hallucination rate from 65.79% to 13.88% and improves the truthfulness score.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">17. <a href="https://arxiv.org/abs/2509.25214" rel="nofollow">On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs</a> <a id="user-content-link17"></a>
</h2><a id="user-content-17-on-the-fly-adaptation-to-quantization-configuration-aware-lora-for-efficient-fine-tuning-of-quantized-llms-" class="anchor" aria-label="Permalink: 17. On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs" href="#17-on-the-fly-adaptation-to-quantization-configuration-aware-lora-for-efficient-fine-tuning-of-quantized-llms-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25214
<strong>Authors:</strong> Rongguang Ye, Ming Tang, Edith C. H. Ngai</p>
<p><strong>Abstract:</strong> arXiv:2509.25214v1 Announce Type: new  Abstract: As increasingly large pre-trained models are released, deploying them on edge devices for privacy-preserving applications requires effective compression. Recent works combine quantization with the fine-tuning of high-precision LoRA adapters, which can substantially reduce model size while mitigating the accuracy loss from quantization. However, edge devices have inherently heterogeneous capabilities, while performing configuration-wise fine-tuning for every quantization setting is computationally prohibitive. In this paper, we propose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to arbitrary quantization configurations (i.e., the per-layer bit-width choices of a pre-trained model) without requiring repeated fine-tuning. This is accomplished via a configuration-aware model that maps each configuration to its low-rank adjustments. The effectiveness of this model critically depends on the training configuration set, a collection of configurations chosen to cover different total bit-width budgets. However, constructing a high-quality configuration set is non-trivial. We therefore design a Pareto-based configuration search that iteratively optimizes the training configuration set, yielding more precise low-rank adjustments. Our experiments demonstrate that, unlike the state-of-the-art methods that require fine-tuning a separate LoRA adapter for each configuration, CoA-LoRA incurs no additional time cost while achieving comparable or even superior performance to those methods.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">18. <a href="https://arxiv.org/abs/2509.25678" rel="nofollow">Guiding Mixture-of-Experts with Temporal Multimodal Interactions</a> <a id="user-content-link18"></a>
</h2><a id="user-content-18-guiding-mixture-of-experts-with-temporal-multimodal-interactions-" class="anchor" aria-label="Permalink: 18. Guiding Mixture-of-Experts with Temporal Multimodal Interactions" href="#18-guiding-mixture-of-experts-with-temporal-multimodal-interactions-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25678
<strong>Authors:</strong> Xing Han, Hsing-Huan Chung, Joydeep Ghosh, Paul Pu Liang, Suchi Saria</p>
<p><strong>Abstract:</strong> arXiv:2509.25678v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) architectures have become pivotal for large-scale multimodal models. However, their routing mechanisms typically overlook the informative, time-varying interaction dynamics between modalities. This limitation hinders expert specialization, as the model cannot explicitly leverage intrinsic modality relationships for effective reasoning. To address this, we propose a novel framework that guides MoE routing using quantified temporal interaction. A multimodal interaction-aware router learns to dispatch tokens to experts based on the nature of their interactions. This dynamic routing encourages experts to acquire generalizable interaction-processing skills rather than merely learning task-specific features. Our framework builds on a new formulation of temporal multimodal interaction dynamics, which are used to guide expert routing. We first demonstrate that these temporal multimodal interactions reveal meaningful patterns across applications, and then show how they can be leveraged to improve both the design and performance of MoE-based models. Comprehensive experiments on challenging multimodal benchmarks validate our approach, demonstrating both enhanced performance and improved interpretability.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">19. <a href="https://arxiv.org/abs/2509.25224" rel="nofollow">AMLA: MUL by ADD in FlashAttention Rescaling</a> <a id="user-content-link19"></a>
</h2><a id="user-content-19-amla-mul-by-add-in-flashattention-rescaling-" class="anchor" aria-label="Permalink: 19. AMLA: MUL by ADD in FlashAttention Rescaling" href="#19-amla-mul-by-add-in-flashattention-rescaling-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25224
<strong>Authors:</strong> Qichen Liao, Chengqiu Hu, Fangzheng Miao, Bao Li, Yiyang Liu, Junlong Lyu, Lirui Jiang, Jun Wang, Lingchao Zheng, Jun Li, Yuwei Fan</p>
<p><strong>Abstract:</strong> arXiv:2509.25224v1 Announce Type: new  Abstract: Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage in Large Language Models while introducing substantial computational overhead and intermediate variable expansion. This poses challenges for efficient hardware implementation -- especially during the decode phase. This paper introduces Ascend MLA (AMLA), a high-performance kernel specifically optimized for Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel FlashAttention-based algorithm that replaces floating-point multiplications with integer additions for output block rescaling, leveraging binary correspondence between FP32 and INT32 representations; (2) A Preload Pipeline strategy with hierarchical tiling that maximizes FLOPS utilization: the Preload Pipeline achieves Cube-bound performance, while hierarchical tiling overlaps data movement and computation within the Cube core. Experiments show that on Ascend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS, reaching 86.8% of the theoretical maximum FLOPS, outperforming the state-of-the-art open-source FlashMLA implementation, whose FLOPS utilization is up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into Huawei's CANN and will be released soon.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">20. <a href="https://arxiv.org/abs/2509.25606" rel="nofollow">Effective Model Pruning</a> <a id="user-content-link20"></a>
</h2><a id="user-content-20-effective-model-pruning-" class="anchor" aria-label="Permalink: 20. Effective Model Pruning" href="#20-effective-model-pruning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25606
<strong>Authors:</strong> Yixuan Wang, Dan Guralnik, Saiedeh Akbari, Warren Dixon</p>
<p><strong>Abstract:</strong> arXiv:2509.25606v1 Announce Type: new  Abstract: We introduce Effective Model Pruning (EMP), a context-agnostic, parameter-free rule addressing a fundamental question about pruning: how many entries to keep. EMP does not prescribe how to score the parameters or prune the models; instead, it supplies a universal adaptive threshold that can be applied to any pruning criterion: weight magnitude, attention score, KAN importance score, or even feature-level signals such as image pixel, and used on structural parts or weights of the models. Given any score vector s, EMP maps s to a built-in effective number N_eff which is inspired by the Inverse Simpson index of contributors. Retaining the N_eff highest scoring entries and zeroing the remainder yields sparse models with performance comparable to the original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our experiments. By leveraging the geometry of the simplex, we derive a tight lower bound on the preserved mass s_eff (the sum of retained scores) over the corresponding ordered probability simplex associated with the score vector s. We further verify the effectiveness of N_eff by pruning the model with a scaled threshold \b{eta}*N_eff across a variety of criteria and models. Experiments suggest that the default \b{eta} = 1 yields a robust threshold for model pruning while \b{eta} not equal to 1 still serves as an optional adjustment to meet specific sparsity requirements.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">21. <a href="https://arxiv.org/abs/2509.25652" rel="nofollow">Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks</a> <a id="user-content-link21"></a>
</h2><a id="user-content-21-iterative-residual-cross-attention-mechanism-an-integrated-approach-for-audio-visual-navigation-tasks-" class="anchor" aria-label="Permalink: 21. Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks" href="#21-iterative-residual-cross-attention-mechanism-an-integrated-approach-for-audio-visual-navigation-tasks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25652
<strong>Authors:</strong> Hailong Zhang, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</p>
<p><strong>Abstract:</strong> arXiv:2509.25652v1 Announce Type: new  Abstract: Audio-visual navigation represents a significant area of research in which intelligent agents utilize egocentric visual and auditory perceptions to identify audio targets. Conventional navigation methodologies typically adopt a staged modular design, which involves first executing feature fusion, then utilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally making decisions through reinforcement learning. While this modular approach has demonstrated effectiveness, it may also lead to redundant information processing and inconsistencies in information transmission between the various modules during the feature fusion and GRU sequence modeling phases. This paper presents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for Audiovisual Navigation), an end-to-end framework that integrates multimodal information fusion and sequence modeling within a unified IRCAM module, thereby replacing the traditional separate components for fusion and GRU. This innovative mechanism employs a multi-level residual design that concatenates initial multimodal sequences with processed information sequences. This methodological shift progressively optimizes the feature extraction process while reducing model bias and enhancing the model's stability and generalization capabilities. Empirical results indicate that intelligent agents employing the iterative residual cross-attention mechanism exhibit superior navigation performance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">22. <a href="https://arxiv.org/abs/2509.26161" rel="nofollow">90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development</a> <a id="user-content-link22"></a>
</h2><a id="user-content-22-90-faster-100-code-free-mllm-driven-zero-code-3d-game-development-" class="anchor" aria-label="Permalink: 22. 90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development" href="#22-90-faster-100-code-free-mllm-driven-zero-code-3d-game-development-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26161
<strong>Authors:</strong> Runxin Yang, Yuxuan Wan, Shuqing Li, Michael R. Lyu</p>
<p><strong>Abstract:</strong> arXiv:2509.26161v1 Announce Type: new  Abstract: Developing 3D games requires specialized expertise across multiple domains, including programming, 3D modeling, and engine configuration, which limits access to millions of potential creators. Recently, researchers have begun to explore automated game development. However, existing approaches face three primary challenges: (1) limited scope to 2D content generation or isolated code snippets; (2) requirement for manual integration of generated components into game engines; and (3) poor performance on handling interactive game logic and state management. While Multimodal Large Language Models (MLLMs) demonstrate potential capabilities to ease the game generation task, a critical gap still remains in translating these outputs into production-ready, executable game projects based on game engines such as Unity and Unreal Engine.   To bridge the gap, this paper introduces UniGen, the first end-to-end coordinated multi-agent framework that automates zero-coding development of runnable 3D games from natural language requirements. Specifically, UniGen uses a Planning Agent that interprets user requirements into structured blueprints and engineered logic descriptions; after which a Generation Agent produces executable C# scripts; then an Automation Agent handles engine-specific component binding and scene construction; and lastly a Debugging Agent provides real-time error correction through conversational interaction. We evaluated UniGen on three distinct game prototypes. Results demonstrate that UniGen not only democratizes game creation by requiring no coding from the user, but also reduces development time by 91.4%. We release UniGen at <a href="https://github.com/yxwan123/UniGen">https://github.com/yxwan123/UniGen</a>. A video demonstration is available at <a href="https://www.youtube.com/watch?v=xyJjFfnxUx0" rel="nofollow">https://www.youtube.com/watch?v=xyJjFfnxUx0</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">23. <a href="https://arxiv.org/abs/2509.26469" rel="nofollow">DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick</a> <a id="user-content-link23"></a>
</h2><a id="user-content-23-diveq-differentiable-vector-quantization-using-the-reparameterization-trick-" class="anchor" aria-label="Permalink: 23. DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick" href="#23-diveq-differentiable-vector-quantization-using-the-reparameterization-trick-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26469
<strong>Authors:</strong> Mohammad Hassan Vali, Tom B"ackstr"om, Arno Solin</p>
<p><strong>Abstract:</strong> arXiv:2509.26469v1 Announce Type: new  Abstract: Vector quantization is common in deep models, yet its hard assignments block gradients and hinder end-to-end training. We propose DiVeQ, which treats quantization as adding an error vector that mimics the quantization distortion, keeping the forward pass hard while letting gradients flow. We also present a space-filling variant (SF-DiVeQ) that assigns to a curve constructed by the lines connecting codewords, resulting in less quantization error and full codebook usage. Both methods train end-to-end without requiring auxiliary losses or temperature schedules. On VQ-VAE compression and VQGAN generation across various data sets, they improve reconstruction and sample quality over alternative quantization approaches.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">24. <a href="https://arxiv.org/abs/2509.25414" rel="nofollow">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a> <a id="user-content-link24"></a>
</h2><a id="user-content-24-rethinking-parameter-sharing-for-llm-fine-tuning-with-multiple-loras-" class="anchor" aria-label="Permalink: 24. Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs" href="#24-rethinking-parameter-sharing-for-llm-fine-tuning-with-multiple-loras-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25414
<strong>Authors:</strong> Hao Ban, Kaiyi Ji</p>
<p><strong>Abstract:</strong> arXiv:2509.25414v1 Announce Type: new  Abstract: Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at <a href="https://github.com/OptMN-Lab/ALoRA">https://github.com/OptMN-Lab/ALoRA</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">25. <a href="https://arxiv.org/abs/2509.25743" rel="nofollow">Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space</a> <a id="user-content-link25"></a>
</h2><a id="user-content-25-rotation-control-unlearning-quantifying-and-controlling-continuous-unlearning-for-llm-with-the-cognitive-rotation-space-" class="anchor" aria-label="Permalink: 25. Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space" href="#25-rotation-control-unlearning-quantifying-and-controlling-continuous-unlearning-for-llm-with-the-cognitive-rotation-space-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25743
<strong>Authors:</strong> Xiang Zhang, Kun Wei, Xu Yang, Chenghao Xu, Su Yan, Cheng Deng</p>
<p><strong>Abstract:</strong> arXiv:2509.25743v1 Announce Type: new  Abstract: As Large Language Models (LLMs) become increasingly prevalent, their security vulnerabilities have already drawn attention. Machine unlearning is introduced to seek to mitigate these risks by removing the influence of undesirable data. However, existing methods not only rely on the retained dataset to preserve model utility, but also suffer from cumulative catastrophic utility loss under continuous unlearning requests. To solve this dilemma, we propose a novel method, called Rotation Control Unlearning (RCU), which leverages the rotational salience weight of RCU to quantify and control the unlearning degree in the continuous unlearning process. The skew symmetric loss is designed to construct the existence of the cognitive rotation space, where the changes of rotational angle can simulate the continuous unlearning process. Furthermore, we design an orthogonal rotation axes regularization to enforce mutually perpendicular rotation directions for continuous unlearning requests, effectively minimizing interference and addressing cumulative catastrophic utility loss. Experiments on multiple datasets confirm that our method without retained dataset achieves SOTA performance.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">26. <a href="https://arxiv.org/abs/2509.25757" rel="nofollow">NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language</a> <a id="user-content-link26"></a>
</h2><a id="user-content-26-neptune-a-neuro-pythonic-framework-for-tunable-compositional-reasoning-on-vision-language-" class="anchor" aria-label="Permalink: 26. NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language" href="#26-neptune-a-neuro-pythonic-framework-for-tunable-compositional-reasoning-on-vision-language-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25757
<strong>Authors:</strong> Danial Kamali, Parisa Kordjamshidi</p>
<p><strong>Abstract:</strong> arXiv:2509.25757v1 Announce Type: new  Abstract: Modern Vision-Language Models (VLMs) have achieved impressive performance in various tasks, yet they often struggle with compositional reasoning, the ability to decompose and recombine concepts to solve novel problems. While neuro-symbolic approaches offer a promising direction, they are typically constrained by crisp logical execution or predefined predicates, which limit flexibility. In this work, we introduce NePTune, a neuro-symbolic framework that overcomes these limitations through a hybrid execution model that integrates the perception capabilities of foundation vision models with the compositional expressiveness of symbolic reasoning. NePTune dynamically translates natural language queries into executable Python programs that blend imperative control flow with soft logic operators capable of reasoning over VLM-generated uncertainty. Operating in a training-free manner, NePTune, with a modular design, decouples perception from reasoning, yet its differentiable operations support fine-tuning. We evaluate NePTune on multiple visual reasoning benchmarks and various domains, utilizing adversarial tests, and demonstrate a significant improvement over strong base models, as well as its effective compositional generalization and adaptation capabilities in novel environments.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">27. <a href="https://arxiv.org/abs/2509.25718" rel="nofollow">VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning</a> <a id="user-content-link27"></a>
</h2><a id="user-content-27-vla-model-post-training-via-action-chunked-ppo-and-self-behavior-cloning-" class="anchor" aria-label="Permalink: 27. VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning" href="#27-vla-model-post-training-via-action-chunked-ppo-and-self-behavior-cloning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25718
<strong>Authors:</strong> Si-Cheng Wang, Tian-Yu Xiang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Ao-Qun Jin, Zeng-Guang Hou</p>
<p><strong>Abstract:</strong> arXiv:2509.25718v1 Announce Type: new  Abstract: Reinforcement learning (RL) is a promising avenue for post-training vision-language-action (VLA) models, but practical deployment is hindered by sparse rewards and unstable training. This work mitigates these challenges by introducing an action chunk based on proximal policy optimization (PPO) with behavior cloning using self-collected demonstrations. Aggregating consecutive actions into chunks improves the temporal consistency of the policy and the density of informative feedback. In addition, an auxiliary behavior cloning loss is applied with a dynamically updated demonstration buffer that continually collects high-quality task trials during training. The relative weight between the action-chunked PPO objective and the self behavior clone auxiliary loss is adapted online to stabilize the post-training process. Experiments on the MetaWorld benchmark indicate improved performance over supervised fine-tuning, achieving a high success rate (0.93) and few steps to success (42.17). These results demonstrate the viability of RL for VLA post-training and help lay the groundwork for downstream VLA applications.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">28. <a href="https://arxiv.org/abs/2509.25689" rel="nofollow">Collaborative Compression for Large-Scale MoE Deployment on Edge</a> <a id="user-content-link28"></a>
</h2><a id="user-content-28-collaborative-compression-for-large-scale-moe-deployment-on-edge-" class="anchor" aria-label="Permalink: 28. Collaborative Compression for Large-Scale MoE Deployment on Edge" href="#28-collaborative-compression-for-large-scale-moe-deployment-on-edge-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25689
<strong>Authors:</strong> Yixiao Chen, Yanyue Xie, Ruining Yang, Wei Jiang, Wei Wang, Yong He, Yue Chen, Pu Zhao, Yanzhi Wang</p>
<p><strong>Abstract:</strong> arXiv:2509.25689v1 Announce Type: new  Abstract: The Mixture of Experts (MoE) architecture is an important method for scaling Large Language Models (LLMs). It increases model capacity while keeping computation cost low. However, the ultra-large MoE models still have hundreds of billions of parameters, requiring massive memory/storage and leading to difficulties for deployment on resource-constrained edge platforms. Pruning or quantization alone can hardly address the issue, because of the super-aggressive compression ratio with significantly degraded accuracy and output quality. To facilitate the deployment of ultra-large MoEs on edge platforms, we propose a collaborative compression framework by combining expert pruning, mixed-precision quantization, and activation optimization. It can effectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3 from 1.3TB to 103GB, while preserving high output quality with better accuracy than traditional uniform low-bit quantization methods. To the best of our knowledge, we are the first to deploy a compressed model from the ultra-large DeepSeek-V3 on the platform with a strict 128GB total memory limit. Our comprehensive experiments on multiple benchmarks under various memory constraints demonstrate the effectiveness of our method with smaller model sizes and higher accuracy than uniform low-bit quantization methods.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">29. <a href="https://arxiv.org/abs/2509.26642" rel="nofollow">MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation</a> <a id="user-content-link29"></a>
</h2><a id="user-content-29-mla-a-multisensory-language-action-model-for-multimodal-understanding-and-forecasting-in-robotic-manipulation-" class="anchor" aria-label="Permalink: 29. MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation" href="#29-mla-a-multisensory-language-action-model-for-multimodal-understanding-and-forecasting-in-robotic-manipulation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26642
<strong>Authors:</strong> Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang</p>
<p><strong>Abstract:</strong> arXiv:2509.26642v1 Announce Type: new  Abstract: Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: <a href="https://sites.google.com/view/open-mla" rel="nofollow">https://sites.google.com/view/open-mla</a></p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">30. <a href="https://arxiv.org/abs/2509.25665" rel="nofollow">Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks</a> <a id="user-content-link30"></a>
</h2><a id="user-content-30-growing-winning-subnetworks-not-pruning-them-a-paradigm-for-density-discovery-in-sparse-neural-networks-" class="anchor" aria-label="Permalink: 30. Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks" href="#30-growing-winning-subnetworks-not-pruning-them-a-paradigm-for-density-discovery-in-sparse-neural-networks-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25665
<strong>Authors:</strong> Qihang Yao, Constantine Dovrolis</p>
<p><strong>Abstract:</strong> arXiv:2509.25665v1 Announce Type: new  Abstract: The lottery ticket hypothesis suggests that dense networks contain sparse subnetworks that can be trained in isolation to match full-model performance. Existing approaches-iterative pruning, dynamic sparse training, and pruning at initialization-either incur heavy retraining costs or assume the target density is fixed in advance. We introduce Path Weight Magnitude Product-biased Random growth (PWMPR), a constructive sparse-to-dense training paradigm that grows networks rather than pruning them, while automatically discovering their operating density. Starting from a sparse seed, PWMPR adds edges guided by path-kernel-inspired scores, mitigates bottlenecks via randomization, and stops when a logistic-fit rule detects plateauing accuracy. Experiments on CIFAR, TinyImageNet, and ImageNet show that PWMPR approaches the performance of IMP-derived lottery tickets-though at higher density-at substantially lower cost (~1.5x dense vs. 3-4x for IMP). These results establish growth-based density discovery as a promising paradigm that complements pruning and dynamic sparsity.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">31. <a href="https://arxiv.org/abs/2509.25401" rel="nofollow">FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers</a> <a id="user-content-link31"></a>
</h2><a id="user-content-31-flashomni-a-unified-sparse-attention-engine-for-diffusion-transformers-" class="anchor" aria-label="Permalink: 31. FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers" href="#31-flashomni-a-unified-sparse-attention-engine-for-diffusion-transformers-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25401
<strong>Authors:</strong> Liang Qiao, Yue Dai, Yeqi Huang, Hongyu Kan, Jun Shi, Hong An</p>
<p><strong>Abstract:</strong> arXiv:2509.25401v1 Announce Type: new  Abstract: Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional capabilities in visual synthesis, yet their deployment remains constrained by substantial computational demands. To alleviate this bottleneck, many sparsity-based acceleration methods have been proposed. However, their diverse sparsity patterns often require customized kernels for high-performance inference, limiting universality. We propose FlashOmni, a unified sparse attention engine compatible with arbitrary DiT architectures. FlashOmni introduces flexible sparse symbols to standardize the representation of a wide range of sparsity strategies, such as feature caching and block-sparse skipping. This unified abstraction enables the execution of diverse sparse computations within a single attention kernel. In addition, FlashOmni designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and further improve efficiency. Experiments demonstrate that FlashOmni delivers near-linear, closely matching the sparsity ratio speedup (1:1) in attention and GEMM-$Q$, and achieves 2.5$\times$-3.8$\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of the theoretical limit). Applied with a multi-granularity sparsity strategy, it enables the Hunyuan model (33K) to achieve about 1.5$\times$ end-to-end acceleration without degrading visual quality.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">32. <a href="https://arxiv.org/abs/2509.26399" rel="nofollow">Commmunication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation</a> <a id="user-content-link32"></a>
</h2><a id="user-content-32-commmunication-efficient-and-accurate-approach-for-aggregation-in-federated-low-rank-adaptation-" class="anchor" aria-label="Permalink: 32. Commmunication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation" href="#32-commmunication-efficient-and-accurate-approach-for-aggregation-in-federated-low-rank-adaptation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26399
<strong>Authors:</strong> Le-Tuan Nguyen, Minh-Duong Nguyen, Seon-Geun Jeong, Dung D. Le, Quoc-Viet Pham</p>
<p><strong>Abstract:</strong> arXiv:2509.26399v1 Announce Type: new  Abstract: With the rapid emergence of foundation models and the increasing need for fine-tuning across distributed environments, Federated Low-Rank Adaptation (FedLoRA) has recently gained significant attention. Despite enormous potential, current FedLoRA methods face notable challenges due to inexact updates. Existing approaches have attempted to mitigate this issue, but they often introduce a \emph{local-global generalization gap} and incur \emph{substantial communication overhead}, limiting their scalability and effectiveness. To address these limitations, we propose \textbf{F}ederated \textbf{Lo}w-\textbf{R}ank \textbf{A}ggregation with \textbf{N}early \textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA matrices on the server to estimate the aggregated matrices $\hat{A}$ and $\hat{B}$, which are then distributed to clients for local updates. This surrogated aggregated matrices minimizes the divergence between ideal $\nabla \Bar{W} = \sum^{U}_{u=1}B_u A_u$ and practical updates $\nabla \hat{W} = \hat{B}\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By doing so, FLoRA-NA achieves communication efficiency and bridges the gap between local personalization and global generalization, addressing a key limitation of prior personalized FedLoRA approaches. We conduct extensive evaluations across diverse tasks, including natural language understanding, mathematical reasoning, and code-solving ability using various foundation models. Experimental results consistently demonstrate that FLoRA-NA achieves state-of-the-art global performance while maintaining low communication overhead.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">33. <a href="https://arxiv.org/abs/2509.25837" rel="nofollow">Distillation of Large Language Models via Concrete Score Matching</a> <a id="user-content-link33"></a>
</h2><a id="user-content-33-distillation-of-large-language-models-via-concrete-score-matching-" class="anchor" aria-label="Permalink: 33. Distillation of Large Language Models via Concrete Score Matching" href="#33-distillation-of-large-language-models-via-concrete-score-matching-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25837
<strong>Authors:</strong> Yeongmin Kim, Donghyeok Shin, Mina Kang, Byeonghu Na, Il-Chul Moon</p>
<p><strong>Abstract:</strong> arXiv:2509.25837v1 Announce Type: new  Abstract: Large language models (LLMs) deliver remarkable performance but are costly to deploy, motivating knowledge distillation (KD) for efficient inference. Existing KD objectives typically match student and teacher probabilities via softmax, which blurs valuable logit information. While direct logit distillation (DLD) mitigates softmax smoothing, it fails to account for logit shift invariance, thereby restricting the solution space. We propose Concrete Score Distillation (CSD), a discrete score-matching objective that overcomes both softmax-induced smoothing and restrictions on the optimal solution set. We resolve the training instability and quadratic complexity of discrete score-matching in autoregressive LLMs, and the resulting CSD objective aligns relative logit differences across all vocabulary pairs between student and teacher with flexible weighting. We provide both mode-seeking and mode-covering instances within our framework and evaluate CSD on task-agnostic instruction-following and task-specific distillation using GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques, demonstrating its scalability and effectiveness for LLM distillation.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">34. <a href="https://arxiv.org/abs/2509.25712" rel="nofollow">Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking</a> <a id="user-content-link34"></a>
</h2><a id="user-content-34-expert-merging-model-merging-with-unsupervised-expert-alignment-and-importance-guided-layer-chunking-" class="anchor" aria-label="Permalink: 34. Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking" href="#34-expert-merging-model-merging-with-unsupervised-expert-alignment-and-importance-guided-layer-chunking-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25712
<strong>Authors:</strong> Dengming Zhang, Xiaowen Ma, Zhenliang Ni, Zhenkai Wu, Han Shu, Xin Jiang, Xinghao Chen</p>
<p><strong>Abstract:</strong> arXiv:2509.25712v1 Announce Type: new  Abstract: Model merging, which combines multiple domain-specialized experts into a single model, offers a practical path to endow Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) with broad capabilities without the cost of joint training or serving many models. However, training-free methods rely on hand-tuned coefficients, whereas training-based methods primarily align parameters rather than downstream task behavior and typically treat all layers uniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a training-light method that learns a small set of layer-wise coefficients using only unlabeled calibration data. The coefficients are optimized to explicitly align the merged model's hidden states and logits with those of the corresponding experts, with a coefficient regularizer for stability and task-weighted losses for controllable trade-offs. To capture inter-layer variation, Expert Merging++ augments this design with importance-guided chunking: a normalized layer-importance metric, derived from learned coefficients, task-vector magnitudes, and parameter counts, allocates more chunk-wise coefficients to high-importance layers while keeping low-importance layers lightweight. The result is a label-free, parameter-efficient, and scalable approach to multi-expert model merging across LLMs and MLLMs. Across MLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our method surpasses strong training-free and training-based merging baselines, with Expert Merging++ delivering further gains and, in some cases, even exceeding supervised Mixture Training. The source code is available at <a href="https://github.com/Littleor/ExpertMerging">https://github.com/Littleor/ExpertMerging</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">35. <a href="https://arxiv.org/abs/2509.26594" rel="nofollow">Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces</a> <a id="user-content-link35"></a>
</h2><a id="user-content-35-clarification-as-supervision-reinforcement-learning-for-vision-language-interfaces-" class="anchor" aria-label="Permalink: 35. Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces" href="#35-clarification-as-supervision-reinforcement-learning-for-vision-language-interfaces-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26594
<strong>Authors:</strong> John Gkountouras, Ivan Titov</p>
<p><strong>Abstract:</strong> arXiv:2509.26594v1 Announce Type: new  Abstract: Recent text-only models demonstrate remarkable mathematical reasoning capabilities. Extending these to visual domains requires vision-language models to translate images into text descriptions. However, current models, trained to produce captions for human readers, often omit the precise details that reasoning systems require. This creates an interface mismatch: reasoners often fail not due to reasoning limitations but because they lack access to critical visual information. We propose Adaptive-Clarification Reinforcement Learning (AC-RL), which teaches vision models what information reasoners need through interaction. Our key insight is that clarification requests during training reveal information gaps; by penalizing success that requires clarification, we create pressure for comprehensive initial captions that enable the reasoner to solve the problem in a single pass. AC-RL improves average accuracy by 4.4 points over pretrained baselines across seven visual mathematical reasoning benchmarks, and analysis shows it would cut clarification requests by up to 39% if those were allowed. By treating clarification as a form of implicit supervision, AC-RL demonstrates that vision-language interfaces can be effectively learned through interaction alone, without requiring explicit annotations.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">36. <a href="https://arxiv.org/abs/2509.25622" rel="nofollow">Layer-wise dynamic rank for compressing large language models</a> <a id="user-content-link36"></a>
</h2><a id="user-content-36-layer-wise-dynamic-rank-for-compressing-large-language-models-" class="anchor" aria-label="Permalink: 36. Layer-wise dynamic rank for compressing large language models" href="#36-layer-wise-dynamic-rank-for-compressing-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25622
<strong>Authors:</strong> Zhendong Mi, Bian Sun, Grace Li Zhang, Shaoyi Huang</p>
<p><strong>Abstract:</strong> arXiv:2509.25622v1 Announce Type: new  Abstract: Large language models (LLMs) have rapidly scaled in size, bringing severe memory and computational challenges that hinder their deployment. Singular Value Decomposition (SVD)-based compression has emerged as an appealing post-training compression technique for LLMs, yet most existing methods apply a uniform compression ratio across all layers, implicitly assuming homogeneous information included in various layers. This overlooks the substantial intra-layer heterogeneity observed in LLMs, where middle layers tend to encode richer information while early and late layers are more redundant. In this work, we revisit the existing SVD-based compression method and propose D-Rank, a framework with layer-wise balanced Dynamic Rank allocation for LLMs compression. We first introduce effective rank as a principled metric to measure the information density of weight matrices, and then allocate ranks via a Lagrange multiplier-based optimization scheme to adaptively assign more capacity to groups with higher information density under a fixed compression ratio. Moreover, we rebalance the allocated ranks across attention layers to account for their varying importance and extend D-Rank to latest LLMs with grouped-query attention. Extensive experiments on various LLMs with different scales across multiple compression ratios demonstrate that D-Rank consistently outperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower perplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up to 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40% compression ratio while achieving even higher throughput.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">37. <a href="https://arxiv.org/abs/2509.25996" rel="nofollow">CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models</a> <a id="user-content-link37"></a>
</h2><a id="user-content-37-cast-continuous-and-differentiable-semi-structured-sparsity-aware-training-for-large-language-models-" class="anchor" aria-label="Permalink: 37. CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models" href="#37-cast-continuous-and-differentiable-semi-structured-sparsity-aware-training-for-large-language-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25996
<strong>Authors:</strong> Weiyu Huang, Yuezhou Hu, Jun Zhu, Jianfei Chen</p>
<p><strong>Abstract:</strong> arXiv:2509.25996v1 Announce Type: new  Abstract: Sparsity-aware training is an effective approach for transforming large language models (LLMs) into hardware-friendly sparse patterns, thereby reducing latency and memory consumption during inference. In this paper, we propose Continuous Adaptive Sparse Trainer (CAST), a fully continuous and differentiable sparsity-aware training framework for semi-structured (or "N:M") sparse models. Unlike previous approaches that optimize sparsity patterns and weights separately, CAST enables seamless joint optimization during training, while progressively transforming the model into the desired sparsity format. Specifically, CAST introduces three key components: 1) AdamS, a sparsity-aware optimizer that leverages adaptive L1 decay to promote uniform sparsification across all parameters; 2) Weight Scaling, a module designed to mitigate the magnitude reduction caused by decay while preserving desired sparsity patterns; 3) Knowledge Distillation, which employs the dense model as a self-teacher to enhance training efficiency. We evaluate CAST under 2:4 sparsity patterns across multiple model families, ranging from 125M to 13B parameters. Our results demonstrate significant improvements over previous state-of-the-art methods in both perplexity and zero-shot accuracy with minimal training resources. Notably, on LLaMA2-7B, our 2:4 sparse model achieves a negligible perplexity increase of 0.09 and a 0.36% gain in zero-shot accuracy compared to the dense model using only 2% of the original pretraining tokens. Additionally, we establish an accurate and robust empirical scaling law to predict sparse model performance given adequate training resources. Finally, we demonstrate the practical applicability of our sparse models by evaluating them under quantization and fine-tuning scenarios.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">38. <a href="https://arxiv.org/abs/2509.25687" rel="nofollow">OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation</a> <a id="user-content-link38"></a>
</h2><a id="user-content-38-omninav-a-unified-framework-for-prospective-exploration-and-visual-language-navigation-" class="anchor" aria-label="Permalink: 38. OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation" href="#38-omninav-a-unified-framework-for-prospective-exploration-and-visual-language-navigation-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25687
<strong>Authors:</strong> Xinda Xue, Junjun Hu, Minghua Luo, Xie Shichao, Jintao Chen, Zixun Xie, Quan Kuichen, Guo Wei, Mu Xu, Zedong Chu</p>
<p><strong>Abstract:</strong> arXiv:2509.25687v1 Announce Type: new  Abstract: Embodied navigation presents a core challenge for intelligent robots, requiring the comprehension of visual environments, natural language instructions, and autonomous exploration. Existing models often fall short in offering a unified solution across diverse navigation paradigms, resulting in low success rates and limited generalization. We introduce OmniNav, a unified framework addressing instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. Our approach features a lightweight, low-latency policy that accurately predicts continuous-space waypoints (coordinates and orientations). This policy surpasses action-chunk methods in precision and supports real-world deployment at control frequencies up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast module generates waypoints using short-horizon visual context and subtasks, while a slow module performs deliberative planning with long-horizon observations and candidate frontiers to select subsequent subgoals and subtasks. This collaboration enhances path efficiency and maintains trajectory coherence, particularly in exploration and memory-intensive scenarios. Crucially, we identify that the primary bottleneck isn't merely navigation policy learning, but a robust understanding of general instructions and objects. To boost generalization, OmniNav integrates large-scale, general-purpose training datasets, including those for image captioning and visual recognition, into a joint multi-task regimen. This significantly improves success rates and robustness. Extensive experiments confirm OmniNav's state-of-the-art performance across various navigation benchmarks, with real-world deployment further validating its efficacy. OmniNav provides practical insights for embodied navigation, charting a scalable path towards versatile, highly generalizable robotic intelligence.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">39. <a href="https://arxiv.org/abs/2509.26524" rel="nofollow">TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning</a> <a id="user-content-link39"></a>
</h2><a id="user-content-39-tap-two-stage-adaptive-personalization-of-multi-task-and-multi-modal-foundation-models-in-federated-learning-" class="anchor" aria-label="Permalink: 39. TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning" href="#39-tap-two-stage-adaptive-personalization-of-multi-task-and-multi-modal-foundation-models-in-federated-learning-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26524
<strong>Authors:</strong> Seohyun Lee, Wenzhi Fang, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton</p>
<p><strong>Abstract:</strong> arXiv:2509.26524v1 Announce Type: new  Abstract: Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at <a href="https://github.com/lee3296/TAP">https://github.com/lee3296/TAP</a>.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">40. <a href="https://arxiv.org/abs/2509.26473" rel="nofollow">STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models</a> <a id="user-content-link40"></a>
</h2><a id="user-content-40-star-attack-a-spatio-temporal-and-narrative-reasoning-attack-framework-for-unified-multimodal-understanding-and-generation-models-" class="anchor" aria-label="Permalink: 40. STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models" href="#40-star-attack-a-spatio-temporal-and-narrative-reasoning-attack-framework-for-unified-multimodal-understanding-and-generation-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26473
<strong>Authors:</strong> Shaoxiong Guo, Tianyi Du, Lijun Li, Yuyao Wu, Jie Li, Jing Shao</p>
<p><strong>Abstract:</strong> arXiv:2509.26473v1 Announce Type: new  Abstract: Unified Multimodal understanding and generation Models (UMMs) have demonstrated remarkable capabilities in both understanding and generation tasks. However, we identify a vulnerability arising from the generation-understanding coupling in UMMs. The attackers can use the generative function to craft an information-rich adversarial image and then leverage the understanding function to absorb it in a single pass, which we call Cross-Modal Generative Injection (CMGI). Current attack methods on malicious instructions are often limited to a single modality while also relying on prompt rewriting with semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We propose STaR-Attack, the first multi-turn jailbreak attack framework that exploits unique safety weaknesses of UMMs without semantic drift. Specifically, our method defines a malicious event that is strongly correlated with the target query within a spatio-temporal context. Using the three-act narrative theory, STaR-Attack generates the pre-event and the post-event scenes while concealing the malicious event as the hidden climax. When executing the attack strategy, the opening two rounds exploit the UMM's generative ability to produce images for these scenes. Subsequently, an image-based question guessing and answering game is introduced by exploiting the understanding capability. STaR-Attack embeds the original malicious question among benign candidates, forcing the model to select and answer the most relevant one given the narrative context. Extensive experiments show that STaR-Attack consistently surpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and surpasses the strongest prior baseline, FlipAttack. Our work uncovers a critical yet underdeveloped vulnerability and highlights the need for safety alignments in UMMs.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">41. <a href="https://arxiv.org/abs/2509.25696" rel="nofollow">Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?</a> <a id="user-content-link41"></a>
</h2><a id="user-content-41-can-vlm-pseudo-labels-train-a-time-series-qa-model-that-outperforms-the-vlm-" class="anchor" aria-label="Permalink: 41. Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?" href="#41-can-vlm-pseudo-labels-train-a-time-series-qa-model-that-outperforms-the-vlm-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.25696
<strong>Authors:</strong> Takuya Fujimura, Kota Dohi, Natsuo Yamashita, Yohei Kawaguchi</p>
<p><strong>Abstract:</strong> arXiv:2509.25696v1 Announce Type: new  Abstract: Time-series question answering (TSQA) tasks face significant challenges due to the lack of labeled data. Alternatively, with recent advancements in large-scale models, vision-language models (VLMs) have demonstrated the potential to analyze time-series signals in a zero-shot manner. In this paper, we propose a training approach that uses pseudo labels generated by a VLM. Although VLMs can produce incorrect labels, TSQA models can still be effectively trained based on the property that deep neural networks are inherently robust to such noisy labels. Our experimental results demonstrate that TSQA models are not only successfully trained with pseudo labels, but also surpass the performance of the VLM itself by leveraging a large amount of unlabeled data.</p>
<hr>
<div class="markdown-heading"><h2 class="heading-element">42. <a href="https://arxiv.org/abs/2509.26541" rel="nofollow">TASP: Topology-aware Sequence Parallelism</a> <a id="user-content-link42"></a>
</h2><a id="user-content-42-tasp-topology-aware-sequence-parallelism-" class="anchor" aria-label="Permalink: 42. TASP: Topology-aware Sequence Parallelism" href="#42-tasp-topology-aware-sequence-parallelism-"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p><strong>ArXiv ID:</strong> 2509.26541
<strong>Authors:</strong> Yida Wang (Capital Normal University, Infinigence-AI), Ke Hong (Tsinghua University, Infinigence-AI), Xiuhong Li (Infinigence-AI), Yuanchao Xu (Capital Normal University), Wenxun Wang (Tsinghua University), Guohao Dai (Infinigence-AI, Shanghai Jiao Tong University), Yu Wang (Tsinghua University)</p>
<p><strong>Abstract:</strong> arXiv:2509.26541v1 Announce Type: new  Abstract: Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at <a href="https://github.com/infinigence/HamiltonAttention">https://github.com/infinigence/HamiltonAttention</a>.</p>
<hr>
<hr>
<div class="markdown-heading"><h2 class="heading-element">Paper selection prompt</h2><a id="user-content-paper-selection-prompt" class="anchor" aria-label="Permalink: Paper selection prompt" href="#paper-selection-prompt"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<ol>
<li>Quantization, Pruning, and KVCache Compression for Efficient Large Language Models
<ul>
<li>Relevant: To optimize large language models (LLMs) for reduced memory and computational costs while maintaining performance, this research direction integrates quantization, pruning, and KVCache compression into a unified framework.</li>
</ul>
</li>
<li>Voice, Language, and Visual Multimodal Large Models
<ul>
<li>Relevant: This research aims to train a model that effectively integrates multiple modalities, such as text, language, and vision, to enhance the model's performance and generalization ability. The approach helps optimize model training and improves the synergy between different modalities during the fusion process.
In suggesting papers to your friend, remember that he enjoys papers on large language model (llm) compression, Multimodal Large Models.</li>
</ul>
</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>