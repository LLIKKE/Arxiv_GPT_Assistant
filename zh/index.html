<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">通过相对熵核心集选择与级联层校正增强边缘设备上的量化感知训练</h2><a id="user-content-通过相对熵核心集选择与级联层校正增强边缘设备上的量化感知训练" class="anchor" aria-label="Permalink: 通过相对熵核心集选择与级联层校正增强边缘设备上的量化感知训练" href="#通过相对熵核心集选择与级联层校正增强边缘设备上的量化感知训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.17768v1 公告类型：新研究<br>
摘要：随着移动与边缘计算的发展，边缘设备对低比特量化模型的部署需求日益增长。为提升模型性能，通常需利用边缘数据对量化模型进行重训练。然而受隐私限制，部分敏感数据仅能在边缘设备本地处理。因此，在边缘设备上实施量化感知训练（QAT）成为有效解决方案。但传统QAT依赖完整数据集进行训练，计算成本高昂。核心集选择技术可通过筛选最具代表性的数据子集来缓解此问题。现有方法在使用小规模数据集（如仅10%数据）时难以消除模型量化误差，导致性能显著下降。</p>
<p>针对这些问题，我们提出QuaRC框架——面向边缘设备的带核心集QAT方案，包含两大阶段：在核心集选择阶段，QuaRC创新性提出"相对熵评分"机制，精准识别最能反映模型量化误差的数据子集；在训练阶段，采用"级联层校正"策略，将量化模型中间层输出与全精度模型对齐，有效抑制中间层量化误差。实验结果表明：当使用1%数据子集将ResNet-18量化为2比特时，QuaRC在ImageNet-1K数据集上的Top-1准确率较现有最优技术提升5.72%。</p>
<p>（注：根据学术文献翻译规范，关键术语保持英文缩写如QAT/QuaRC，技术名词如"Coreset"译为"核心集"，"Cascaded Layer Correction"采用意译加注策略。长句按中文表达习惯拆分为短句，被动语态转为主动句式，同时保留技术细节的精确性。）</p>
<div class="markdown-heading"><h2 class="heading-element">大型语言模型量化的几何学：GPTQ即巴拜最近平面算法</h2><a id="user-content-大型语言模型量化的几何学gptq即巴拜最近平面算法" class="anchor" aria-label="Permalink: 大型语言模型量化的几何学：GPTQ即巴拜最近平面算法" href="#大型语言模型量化的几何学gptq即巴拜最近平面算法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2507.18553v1 公告类型：新研究<br>
摘要：将大型语言模型（LLM）的权重从16位量化至更低比特位宽，是将海量Transformer模型部署到经济型加速器上的实际解决方案。GPTQ作为LLM规模单次训练后量化的标准方法之一崭露头角。然而，其内部机制被描述为一系列临时性代数更新，掩盖了几何意义与最坏情况保证。本研究表明，当对线性层执行从后向前（从末维到首维）量化时，GPTQ在数学上等同于Babai经典最近向量问题（CVP）的最近平面算法，该算法的晶格由层输入的海森矩阵定义。这一等价性基于严密的数学论证，并产生两项分析性推论：(i) GPTQ误差传播步骤获得了直观的几何解释；(ii) 在无剪裁条件下，GPTQ继承了Babai算法的误差上界。这些发现使GPTQ的理论基础更为坚实，并为未来十亿参数模型的量化算法设计打开了借鉴数十年晶格算法进展的大门。</p>
<p>（注：根据学术文献翻译规范，对部分术语进行了专业处理：</p>
<ol>
<li>"one-shot post-training quantization"译为"单次训练后量化"以保持技术准确性</li>
<li>"ad-hoc"译为"临时性"以符合中文计算机论文表述习惯</li>
<li>"Hessian matrix"保留专业术语"海森矩阵"</li>
<li>"no-clipping condition"意译为"无剪裁条件"以平衡专业性与可读性</li>
<li>长句按中文习惯拆分为多个短句，如将原文最后复合句分解为因果逻辑链）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">《Squeeze10-LLM：通过分阶段混合精度量化方法将大语言模型权重压缩十倍》</h2><a id="user-content-squeeze10-llm通过分阶段混合精度量化方法将大语言模型权重压缩十倍" class="anchor" aria-label="Permalink: 《Squeeze10-LLM：通过分阶段混合精度量化方法将大语言模型权重压缩十倍》" href="#squeeze10-llm通过分阶段混合精度量化方法将大语言模型权重压缩十倍"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>注：译文采用技术文献常见的标题处理方式：</p>
<ol>
<li>保留原创新术语"Squeeze10-LLM"作为专有名词不译</li>
<li>使用破折号替代原标题中的冒号以符合中文标点规范</li>
<li>"Staged Mixed-Precision"译为"分阶段混合精度"，准确传达分步骤实施不同量化精度的技术特征</li>
<li>添加"方法"二字明确量化方法的属性，使中文表述更完整</li>
<li>"10 Times"译为"十倍"符合中文数量级表达习惯</li>
<li>整体采用主动语态("将...压缩")，比被动式("被压缩")更符合中文技术文献表述惯例</li>
</ol>
<p>arXiv:2507.18073v1 公告类型：新研究<br>
摘要：部署大型语言模型（LLM）面临巨大参数量和高计算成本的挑战。超低位量化能显著降低存储需求并加速推理，但极端压缩（即平均位宽≤2比特）通常会导致性能严重下降。为此，我们提出Squeeze10-LLM框架，将16比特LLM的权重有效"压缩"10倍。该框架采用分阶段混合精度训练后量化（PTQ）方案，通过将80%权重量化为1比特、20%量化为4比特，实现平均1.6比特/权重的压缩率。我们引入两大创新技术：后二值化激活鲁棒性（PBAR）和全信息激活监督（FIAS）。PBAR是一种改进的权重重要性度量方法，能评估量化对激活值的影响，从而提升低位设置下的精度；FIAS则通过在量化过程中保留完整的激活信息，有效缓解跨层累积误差传播。在LLaMA和LLaMA2上的实验表明，Squeeze10-LLM在2比特以下仅权重量化任务中达到最先进水平，将六个零样本分类任务的平均准确率从43%提升至56%，较现有PTQ方法实现显著突破。代码将在论文发表时开源。</p>
<p>（注：根据学术文献翻译规范，专业术语如"post-training quantization"统一译为"训练后量化"，"zero-shot"译为"零样本"；技术名词"weight significance metric"采用意译处理为"权重重要性度量方法"；长难句通过拆分和语序调整符合中文表达习惯；创新技术名称PBAR/FIAS保留英文缩写但补充完整译名便于理解）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>