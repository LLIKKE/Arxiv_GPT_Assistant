<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">FreeKV：提升KV缓存检索效率以优化大语言模型推理</h2><a id="user-content-freekv提升kv缓存检索效率以优化大语言模型推理" class="anchor" aria-label="Permalink: FreeKV：提升KV缓存检索效率以优化大语言模型推理" href="#freekv提升kv缓存检索效率以优化大语言模型推理"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.13109v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）的部署规模正随着快速扩展的上下文窗口而不断扩大，以支持日益苛刻的应用需求。然而，长上下文带来了显著的部署挑战，主要源于键值缓存（KV cache）的大小与上下文长度成比例增长。虽然已有KV缓存压缩方法试图解决这一问题，但KV丢弃方法会导致显著的准确率损失，而KV检索方法则面临严重的效率瓶颈。我们提出FreeKV——一个算法-系统协同优化框架，在保持准确性的同时提升KV检索效率。在算法层面，FreeKV引入推测式检索将KV选择与召回过程移出关键路径，并结合细粒度校正确保准确性；在系统层面，FreeKV采用跨CPU与GPU内存的混合KV布局以消除碎片化数据传输，并利用双缓冲流式召回进一步提升效率。实验表明，FreeKV在多种场景和模型下均能实现接近无损的准确率，相比最先进的KV检索方法可获得高达13倍的加速。</p>
<p>（注：SOTA保持英文缩写形式，因其在机器学习领域作为"State-Of-The-Art"的通用表述；公式符号"13$\times$"保留原格式以符合学术文献惯例）</p>
<div class="markdown-heading"><h2 class="heading-element">权重量化的最佳格式</h2><a id="user-content-权重量化的最佳格式" class="anchor" aria-label="Permalink: 权重量化的最佳格式" href="#权重量化的最佳格式"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.12988v1 公告类型：新研究<br>
摘要：权重量化是实现现代深度学习模型高效训练与部署的关键技术。然而，现有量化方法种类繁多，格式选择往往依赖经验性判断。本文提出一个系统性设计与分析量化格式的框架。通过将格式设计问题与经典量化理论相关联，我们揭示了流行格式优异实践表现的根源——其采用变长编码表示数值的能力。将优化问题构建为最小化原始模型与量化模型输出之间的KL散度，该目标函数等同于最小化模型参数的平方量化误差。基于此，我们针对已知分布开发并评估了平方误差最优的量化格式，观察到变长编码较定长编码实现了显著提升。研究证明：均匀量化配合变长编码的无损压缩是最优方案。有趣的是，常用的分块格式和稀疏离群值格式同样优于定长编码，这表明它们也隐含利用了变长编码原理。最后，通过建立费希尔信息与KL散度的关联，我们推导出模型各层参数张量的最优位宽分配方案，在语言模型的直接转换量化测试中，该方案实现了每参数节省0.25比特的效果。</p>
<p>（注：根据学术论文翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"variable-length codes"统一译为"变长编码"以保持计算机领域术语一致性</li>
<li>"Fisher information"采用学科通用译名"费希尔信息"</li>
<li>长难句按中文表达习惯进行了分拆重组，如将"Framing the optimization problem..."处理为因果句式</li>
<li>保留了专业缩写如"KL散度"并在首次出现时给出全称"KL divergence"）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">使用零阶优化微调量化神经网络</h2><a id="user-content-使用零阶优化微调量化神经网络" class="anchor" aria-label="Permalink: 使用零阶优化微调量化神经网络" href="#使用零阶优化微调量化神经网络"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.13430v1 公告类型：新成果<br>
摘要：随着大语言模型规模呈指数级增长，GPU内存已成为这些模型适应下游任务的主要瓶颈。本文旨在通过统一框架最小化模型权重、梯度和优化器状态的内存占用，突破高效内存训练的极限。我们的核心思路是结合零阶优化消除梯度与优化器状态——该方法通过前向传播时扰动权重来估算梯度方向。为降低权重内存消耗，我们采用模型量化技术（如将bfloat16转换为int4）。然而，由于离散权重与连续梯度之间的精度鸿沟（需反复解量化与再量化），直接将零阶优化应用于量化权重并不可行。为此，我们提出量化零阶优化（QZO）这一创新方法：通过扰动连续量化尺度进行梯度估计，并采用方向导数裁剪技术稳定训练。QZO与基于标量或码本的训练后量化方法均正交。实验表明，相较于bfloat16全参数微调，QZO能为4位量化大语言模型实现18倍以上的总内存节省，仅需单块24GB GPU即可完成Llama-2-13B和Stable Diffusion 3.5 Large模型的微调。</p>
<p>（注：根据学术论文翻译规范，对技术术语保持统一："zeroth-order optimization"译为"零阶优化"、"quantization scale"译为"量化尺度"、"directional derivative clipping"译为"方向导数裁剪"。关键数字18$\times$保留原文格式，$\times$符号在中文语境下表示倍数关系时无需调整。长句按中文表达习惯拆分为短句，如将"which approximates..."处理为破折号补充说明句式。）</p>
<div class="markdown-heading"><h2 class="heading-element">基于模型分割为顺序子图的自动混合精度优化，以受限损失均方误差为准则来提升时间效率</h2><a id="user-content-基于模型分割为顺序子图的自动混合精度优化以受限损失均方误差为准则来提升时间效率" class="anchor" aria-label="Permalink: 基于模型分割为顺序子图的自动混合精度优化，以受限损失均方误差为准则来提升时间效率" href="#基于模型分割为顺序子图的自动混合精度优化以受限损失均方误差为准则来提升时间效率"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.13060v1 公告类型：新研究<br>
摘要：量化技术对神经网络（NN）压缩至关重要，它通过采用低位宽数据类型来减小模型体积和计算需求，但过度压缩往往会损害精度。混合精度（MP）方法通过在不同网络层间动态调整数值精度，有效缓解了这一矛盾。本研究专注于在训练后量化（PTQ）推理过程中自动选择最优MP配置。首个核心贡献是提出了一种基于损失函数一阶泰勒展开的新型敏感度度量指标，该指标将权重和激活值的量化误差作为变量。通过在小规模校准数据集上执行高精度前向与反向传播，可高效计算出各层基于均方误差（MSE）的敏感度。该指标具有跨层累加特性，且因无需权重优化而显著降低校准内存开销。第二个贡献是提出精确的硬件感知方法，通过将MP时间增益建模为顺序子图的累加效应来预测加速效果：算法先将模型图划分为顺序子图，再通过少量样本测量各配置的时间增益。在完成逐层敏感度和时间增益校准后，构建整数规划（IP）问题以在保证损失MSE不超过设定阈值的前提下最大化时间增益。该方法同时考虑了内存增益和基于乘积累加运算（MAC）的理论时间增益。在英特尔Gaudi 2加速器上对多个大语言模型（LLM）的严格实验验证了该方法的有效性。</p>
<p>（注：根据学术文献翻译规范，对部分术语处理如下：</p>
<ol>
<li>"Post-Training Quantization"采用"训练后量化"这一通用译法</li>
<li>"Integer Programming"保留专业术语"整数规划"</li>
<li>"Multiply and Accumulate"使用"乘积累加运算"标准译名</li>
<li>长难句按中文表达习惯进行了分拆重组，如将"modeling it as additive for sequential sub-graphs"转化为"通过将...建模为顺序子图的累加效应"</li>
<li>专业设备名"Intel Gaudi 2 accelerator"保留品牌名直译）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">Qronos：通过塑造未来修正过去... 在后训练量化领域</h2><a id="user-content-qronos通过塑造未来修正过去-在后训练量化领域" class="anchor" aria-label="Permalink: Qronos：通过塑造未来修正过去... 在后训练量化领域" href="#qronos通过塑造未来修正过去-在后训练量化领域"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.11695v1 公告类型：新成果<br>
摘要：我们推出Qronos——一种全新的最先进训练后量化算法，通过顺序舍入和更新神经网络权重实现突破。Qronos不仅能显式修正权重和激活值量化导致的误差，还能纠正先前层级量化产生的累积误差。我们的迭代算法基于一个可解释、规范化的优化框架，该框架涵盖并超越了现有数据驱动方法。在每一步中，Qronos通过最优更新规则交替执行误差校正与扩散过程。值得注意的是，我们证明了Qronos可采用Cholesky分解高效求解最小二乘问题。实验还表明，Qronos与现有变换技术（如基于Hadamard变换的非相干处理、权重-激活缩放均衡等）完全兼容。我们在Llama3系列自回归语言生成模型上评估Qronos：当对权重、激活值和/或KV缓存进行量化时，Qronos在各项指标上持续超越先前最先进的自适应舍入方法。</p>
<div class="markdown-heading"><h2 class="heading-element">InfiJanice：大语言模型量化引发数学性能下降的联合分析与原位校正引擎</h2><a id="user-content-infijanice大语言模型量化引发数学性能下降的联合分析与原位校正引擎" class="anchor" aria-label="Permalink: InfiJanice：大语言模型量化引发数学性能下降的联合分析与原位校正引擎" href="#infijanice大语言模型量化引发数学性能下降的联合分析与原位校正引擎"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.11574v1 公告类型：新研究<br>
摘要：大型语言模型（LLMs）在GSM8K、MATH和AIME等复杂推理基准测试中展现了卓越性能。然而，这些任务巨大的计算需求给实际部署带来了严峻挑战。模型量化作为一种有前景的解决方案，通过用更低位宽表示权重和激活值，能有效减少内存占用和推理延迟。本研究对主流量化方法（如AWQ、GPTQ、SmoothQuant）在热门开源模型（如Qwen2.5、LLaMA3系列）上的表现进行了系统评估，发现量化可能导致数学推理准确率下降高达69.81%。为深入理解性能衰退机制，我们开发了自动化答案判定流程，将错误定性归类为四种类型，并定量识别受影响最严重的推理能力。基于这些发现，我们采用自动化数据筛选流程构建了精炼的"银弹"数据集。实验表明，在单块GPU上仅需使用332个精选样本进行3-5分钟的微调，即可使量化模型的推理准确率完全恢复至全精度基准水平。</p>
<p>（注：根据学术论文摘要的文体特征，译文在保持专业性的同时注重句式结构的平衡性，将英语长句合理切分为符合中文阅读习惯的短句。关键术语如"Silver Bullet"采用意译"银弹"保留隐喻色彩，技术概念如"data-curation pipeline"译为"数据筛选流程"以准确传达自动化处理的核心含义。量化比较数据"69.81%"等精确数值予以保留，确保科研严谨性。）</p>
<div class="markdown-heading"><h2 class="heading-element">SageAttention3：面向推理的微缩放FP4注意力机制及8位训练探索</h2><a id="user-content-sageattention3面向推理的微缩放fp4注意力机制及8位训练探索" class="anchor" aria-label="Permalink: SageAttention3：面向推理的微缩放FP4注意力机制及8位训练探索" href="#sageattention3面向推理的微缩放fp4注意力机制及8位训练探索"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.11594v1 公告类型：新研究<br>
摘要：注意力机制因其二次时间复杂度而亟需提升效率。我们通过两项关键创新显著优化了注意力计算效率：首先，利用Blackwell GPU新一代FP4张量核心加速注意力运算。在RTX5090上实现1038 TOPS算力，相较同平台最快的FlashAttention提速5倍。实验表明，我们的FP4注意力能以即插即用方式加速各类模型推理。其次，我们开创性地将低位宽注意力应用于训练任务。现有低位宽方案如FlashAttention3和SageAttention仅聚焦推理场景，但大模型训练效率同样关键。为验证低位宽注意力在训练中的适用性，我们设计了精准高效的8位注意力算法，同时支持前向与反向传播。实验显示8位注意力在微调任务中可实现无损性能，但在预训练任务中收敛速度较慢。代码将开源在<a href="https://github.com/thu-ml/SageAttention%E3%80%82">https://github.com/thu-ml/SageAttention。</a></p>
<p>（注：根据学术论文摘要的文体特点，翻译时进行了以下处理：</p>
<ol>
<li>将"quadratic time complexity"译为专业术语"二次时间复杂度"</li>
<li>"plug-and-play"采用计算机领域通用译法"即插即用"</li>
<li>"lossless performance"译为"无损性能"以准确传达技术含义</li>
<li>保持1038 TOPS等数值及型号RTX5090的原文形式</li>
<li>将长复合句合理切分为符合中文表达习惯的短句</li>
<li>使用"低位宽"统一翻译"low-bit"这一核心概念）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>