<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">FineScope：基于SAE引导自数据培育的领域专用大语言模型精准剪枝技术</h2><a id="user-content-finescope基于sae引导自数据培育的领域专用大语言模型精准剪枝技术" class="anchor" aria-label="Permalink: FineScope：基于SAE引导自数据培育的领域专用大语言模型精准剪枝技术" href="#finescope基于sae引导自数据培育的领域专用大语言模型精准剪枝技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明如下）</p>
<ol>
<li>"Precision Pruning" 译为"精准剪枝"以保持技术术语一致性，体现对模型参数的精细化裁剪</li>
<li>"Domain-Specialized" 采用"领域专用"的译法，强调针对特定领域的定制化特性</li>
<li>"SAE-Guided" 保留SAE缩写（假设指Sparse Autoencoder稀疏自编码器），添加"引导"明确指导关系</li>
<li>"Self-Data Cultivation" 创新译为"自数据培育"，既保留"自我生成数据"的核心概念，又体现系统性培育过程</li>
<li>整体采用技术文献常见的"技术+方法"命名结构，通过冒号分层保持原标题逻辑层次</li>
</ol>
<p>从头训练大型语言模型（LLMs）需要消耗大量计算资源，这促使研究者致力于开发更小巧、面向特定领域且能兼顾效率与任务性能的LLMs。虽然LLaMA等中等规模模型已成为领域适配的常见起点，但这些模型在专业数据集测试时往往出现准确率下降的问题。我们提出FineScope框架——通过结构化剪枝与自数据蒸馏技术，从预训练大模型中提取紧凑的领域优化模型。该框架创新性地采用稀疏自编码器（SAE），借鉴其生成可解释特征表示的能力，从海量数据中提取领域相关子集。在实施剪枝时，我们引入领域特定约束条件，确保修剪后的模型能保留目标领域的核心知识。为进一步提升性能，剪枝模型会通过SAE筛选的数据集进行自蒸馏学习，以恢复剪枝过程中丢失的关键领域信息。大量实验与消融研究表明，FineScope在领域特定任务中展现出极具竞争力的性能，优于多个最先进的大规模LLMs。值得注意的是，当使用SAE筛选的数据集微调时，剪枝模型能恢复其原始性能的显著部分。此外，即使不进行剪枝，用这些数据集微调预训练LLMs也能提升其领域准确率，充分证明了我们方法的鲁棒性。相关代码将开源发布。</p>
<p>（注：根据技术文本特点，在翻译时进行了以下处理：</p>
<ol>
<li>专业术语统一："structured pruning"译为"结构化剪枝"；"self-data distillation"译为"自数据蒸馏"</li>
<li>被动语态转化：将英文被动式调整为中文主动表述（如"are derived from"译为"从...提取"）</li>
<li>长句拆分：将原文复合长句按中文表达习惯分解为多个短句</li>
<li>概念显化：如"SAE-curated datasets"译为"SAE筛选的数据集"以明确动作主体</li>
<li>保持技术严谨性的同时增强可读性，如添加"值得注意的是"等衔接词）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">突破低比特优化器的极限：聚焦EMA动态机制</h2><a id="user-content-突破低比特优化器的极限聚焦ema动态机制" class="anchor" aria-label="Permalink: 突破低比特优化器的极限：聚焦EMA动态机制" href="#突破低比特优化器的极限聚焦ema动态机制"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>模型规模的爆炸式增长导致训练/微调成本持续攀升至令人望而却步的程度，尤其是对于需要维护相当于模型体积2倍的辅助信息以实现最优收敛的状态化优化器而言。为此，我们提出了一种具有极轻量级状态负载的新型优化器，其核心技术在于超低精度量化。虽然前人研究已通过8位或4位量化取得一定成果，但我们的方法首次实现了每个状态元素仅需3比特甚至2比特的超低精度运行。这一突破源于我们对两个关键挑战的识别与解决：无符号量化中导致状态动态停滞的信号淹没问题，以及有符号量化中因梯度方差激增引发的错误下降方向问题。理论分析表明，前者需采用定制化的对数量化方案，后者则需要根据精度动态调整动量值。最终实现的SOLO优化器在训练7B规模模型时可节省约45GB内存，且精度损失微乎其微。我们期待SOLO能助力突破算力资源瓶颈，从而推动基础研究实现更广泛的普惠性。</p>
<p>（翻译说明：1. 专业术语如"stateful optimizers"译为"状态化优化器"保持技术准确性；2. "prohibitive costs"译为"令人望而却步的程度"增强文学表现力；3. 将原文被动语态"is accomplished by"转化为主动式"源于...识别与解决"符合中文表达习惯；4. 技术概念"signal swamping problem"采用"信号淹没问题"的意象化翻译；5. 结尾"greater accessibility"译为"更广泛的普惠性"既准确传达开放共享理念，又呼应中文政策话语体系）</p>
<div class="markdown-heading"><h2 class="heading-element">利用安全引导自压缩优化深度神经网络</h2><a id="user-content-利用安全引导自压缩优化深度神经网络" class="anchor" aria-label="Permalink: 利用安全引导自压缩优化深度神经网络" href="#利用安全引导自压缩优化深度神经网络"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>在资源受限设备上部署深度神经网络，需要采用能审慎平衡模型规模压缩与性能保持的有效模型压缩策略。本研究提出了一种创新的安全驱动量化框架，通过利用保护集对神经网络权重进行系统性剪枝和量化，从而在不损失精度的前提下优化模型复杂度。该研究方法在卷积神经网络（CNN）和基于注意力的语言模型上进行了严格验证，证明了其在不同架构范式中的普适性。实验结果表明，本框架在保持初始模型体积60%的同时，相较于原始未量化模型实现了最高2.5%的测试准确率提升。与传统量化技术相比，我们的方法通过消除参数噪声、保留核心权重，不仅增强了模型的泛化能力，还降低了方差，从而确保关键模型特征得以保留。这些发现充分印证了安全驱动量化作为一种稳健可靠的深度学习模型优化策略的有效性。本框架的实现代码及完整实验评估已在GitHub平台开源。</p>
<p>（注：根据学术翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"preservation sets"译为"保护集"而非字面的"保存集"，更符合机器学习领域术语习惯</li>
<li>"parameter noise"译为"参数噪声"而非"参数噪音"，遵循信号处理领域术语标准</li>
<li>保持"CNN"和"GitHub"等专业缩写不变</li>
<li>"variance"在统计学语境下译为"方差"而非"变异"</li>
<li>采用"剪枝"而非"修剪"对应"prune"的机器学习术语）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">Pack-PTQ：通过分组重构推进神经网络的后训练量化</h2><a id="user-content-pack-ptq通过分组重构推进神经网络的后训练量化" class="anchor" aria-label="Permalink: Pack-PTQ：通过分组重构推进神经网络的后训练量化" href="#pack-ptq通过分组重构推进神经网络的后训练量化"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>训练后量化（PTQ）已成为压缩复杂模型的重要解决方案，其优势在于仅需少量校准数据且无需端到端重新训练。然而现有PTQ方法大多采用逐块重建策略，忽视了块间依赖性，在低比特量化场景中会出现显著精度下降。针对这些局限性，本文提出创新性方法Pack-PTQ：首先设计基于Hessian矩阵的自适应分组机制，将网络块划分成非重叠的量化组作为重建单元，既保持块间依赖关系又能精确估计量化参数；其次基于分组结构提出混合精度量化方案，根据各分组不同的灵敏度分配差异化比特位宽，从而进一步提升性能。通过在2D图像和3D点云分类任务上的大量实验（涵盖多种网络架构），本方法相较最先进的PTQ方案展现出显著优势。</p>
<p>（翻译说明：1. 专业术语如"Hessian-guided"译为"Hessian矩阵"符合数学领域惯例；2. "non-overlapping packs"译为"非重叠量化组"既准确体现技术特征又符合中文表达；3. 长难句拆分重组，如将原文第二个长句拆分为两个中文短句，符合汉语多用短句的特点；4. 被动语态转换，如"are partitioned"译为主动式"将...划分"；5. 技术概念如"mixed-precision quantization"采用行业通用译法"混合精度量化"）</p>
<div class="markdown-heading"><h2 class="heading-element">稀疏注意力混合：基于内容的可学习稀疏注意力通过专家选择路由</h2><a id="user-content-稀疏注意力混合基于内容的可学习稀疏注意力通过专家选择路由" class="anchor" aria-label="Permalink: 稀疏注意力混合：基于内容的可学习稀疏注意力通过专家选择路由" href="#稀疏注意力混合基于内容的可学习稀疏注意力通过专家选择路由"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>近期大型语言模型的研究进展凸显了自注意力机制中二次方计算成本过高的问题。尽管学界付出了大量研究努力，但次二次方注意力方法在实际应用中仍存在性能不足的缺陷。我们提出假设：基于动态学习的内容稀疏化可以催生更高效的注意力机制。为此，我们提出稀疏注意力混合机制（MoSA），这一创新方法受到专家混合（MoE）架构中专家选择路由机制的启发。MoSA为每个注意力头动态选择token，允许任意的稀疏注意力模式。通过从长度为T的序列中选择k个token，MoSA将每个注意力头的计算复杂度从O(T²)降至O(k²+T)。这使得在相同计算资源下可以使用更多注意力头，从而实现更高的专业化程度。实验表明，在测试的各类稀疏注意力变体中，MoSA是唯一能超越密集注意力基线的方案，在相同计算预算下有时能获得高达27%的困惑度提升。相较于密集自注意力，MoSA还能降低资源消耗。尽管当前使用的是未经内核优化的torch实现，但在困惑度匹配的情况下，MoSA模型不仅实际运行速度更快、训练所需内存更少，其KV缓存大小相比密集Transformer基线也实现了大幅缩减。</p>
<div class="markdown-heading"><h2 class="heading-element">FreqKV：面向上下文窗口高效扩展的频域键值压缩技术</h2><a id="user-content-freqkv面向上下文窗口高效扩展的频域键值压缩技术" class="anchor" aria-label="Permalink: FreqKV：面向上下文窗口高效扩展的频域键值压缩技术" href="#freqkv面向上下文窗口高效扩展的频域键值压缩技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译时做了以下技术性处理：</p>
<ol>
<li>保留技术术语"FreqKV"作为专有名词不译</li>
<li>"Frequency Domain"译为专业术语"频域"而非字面"频率域"</li>
<li>"Key-Value Compression"译为"键值压缩"，保持计算机领域术语规范</li>
<li>"Efficient Context Window Extension"采用动态对等译法处理为"高效扩展"，并增补"技术"二字体现技术方案特性</li>
<li>整体采用"技术"作为中心词，符合中文技术论文标题命名习惯</li>
<li>使用"面向..."结构替代原介词"for"，更符合中文标题表达逻辑）</li>
</ol>
<p>扩展大型语言模型（LLMs）的上下文窗口对于涉及长文本生成的应用至关重要。然而，键值（KV）缓存内存需求的线性增长，以及自注意力机制相对于序列长度的二次方复杂度，在微调和推理过程中带来了重大挑战。现有方法在扩展到更长上下文时会出现性能下降。本文提出了一种新颖的上下文扩展方法，可同时优化微调和推理效率。我们的方法基于一个关键发现：在频域中，KV缓存的能量分布主要集中在低频分量。通过滤除高频分量，KV缓存可以在信息损失最小的情况下实现高效压缩。基于这一洞见，我们提出了一种高效压缩技术FreqKV，该技术通过频域迭代将不断增长的KV缓存压缩至固定大小，适用于微调和推理全过程。FreqKV无需引入额外参数或修改模型架构。经过极少量微调后，LLMs即可学会利用频域压缩后的有限缓存，高效扩展上下文窗口。在多种长上下文语言建模与理解任务上的实验验证了该方法的效率与有效性。</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>