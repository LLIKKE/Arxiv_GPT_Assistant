<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">销售：长上下文LLM预填充中高效稀疏注意力的低位估计</h2><a id="user-content-销售长上下文llm预填充中高效稀疏注意力的低位估计" class="anchor" aria-label="Permalink: 销售：长上下文LLM预填充中高效稀疏注意力的低位估计" href="#销售长上下文llm预填充中高效稀疏注意力的低位估计"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.24179v1 公告类型：新论文<br>
摘要：许多先进的大语言模型（LLM）应用需要处理长上下文，但在推理的预填充阶段，自注意力模块因其随序列长度呈二次方增长的时间复杂度成为性能瓶颈。现有稀疏注意力方法通过跳过注意力图中较不重要的区域来加速计算，但这些方法通常对注意力图进行粗粒度检测，导致模型精度显著下降。本文提出SALE——一种细粒度稀疏注意力方法，能在模型精度损失可忽略的前提下加速LLM的长上下文预填充阶段。SALE通过4位量化的查询-键值乘积实现快速精确的注意力权重估计，再结合块稀疏注意力机制加速预填充计算。针对查询-键值对的重要性评估，我们采用相对注意力分数指标，该指标在我们的框架内具有显著更高的效率。我们为该方法实现了定制化的CUDA内核以提升硬件效率，将额外开销控制在完整注意力延迟的约11%。值得注意的是，SALE无需参数训练，仅需极少的代码修改即可无缝集成到现有系统中。长上下文基准测试表明，本方法在精度-效率权衡上优于现有方案，在Llama-3.1-8B模型上对超过64K长度的序列实现至少3.36倍加速的同时保持模型质量。</p>
<div class="markdown-heading"><h2 class="heading-element">ReCalKV：基于头部重排序与离线校准的低秩KV缓存压缩技术</h2><a id="user-content-recalkv基于头部重排序与离线校准的低秩kv缓存压缩技术" class="anchor" aria-label="Permalink: ReCalKV：基于头部重排序与离线校准的低秩KV缓存压缩技术" href="#recalkv基于头部重排序与离线校准的低秩kv缓存压缩技术"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明：</p>
<ol>
<li>"Low-Rank KV Cache Compression"译为"低秩KV缓存压缩技术"，其中：
<ul>
<li>"KV"作为技术术语保留不译</li>
<li>"Low-Rank"是线性代数概念，专业场景下译为"低秩"</li>
</ul>
</li>
<li>"Head Reordering"译为"头部重排序"，指注意力机制中多头（multi-head）的重新排列</li>
<li>"Offline Calibration"译为"离线校准"，强调非实时调校过程</li>
<li>采用"基于...的"结构保持技术方案描述的准确性</li>
<li>主副标题用中文破折号连接，符合中文技术文档标题规范）</li>
</ol>
<p>arXiv:2505.24357v1 公告类型：新研究<br>
摘要：尽管大语言模型（LLMs）已展现出卓越性能，但其长上下文推理能力常受限于存储键值（KV）缓存所需的高内存消耗。因此，KV缓存压缩成为实现高效长上下文推理的关键步骤。现有方法多尝试降低KV缓存的隐藏维度，但往往需要通过投影层引入额外计算，或在高压缩比下出现显著性能下降。为解决这些问题，我们提出ReCalKV——一种训练后KV缓存压缩方法，通过降低KV缓存的隐藏维度实现高效压缩。基于键和值在注意力机制中的不同作用及重要性差异，我们分别为其设计了专属压缩策略：对于键矩阵，提出<strong>头间相似度感知重排序（HSR）</strong>，通过聚类相似注意力头并对键投影矩阵实施分组奇异值分解（SVD），在减少额外计算的同时保持精度；对于值矩阵，采用**离线校准与矩阵融合（OCMF）**策略，无需增加计算开销即可维持准确性。实验表明，ReCalKV优于现有低秩压缩方法，能以极小的性能损失实现高压缩比。代码已开源：<a href="https://github.com/XIANGLONGYAN/ReCalKV">https://github.com/XIANGLONGYAN/ReCalKV</a></p>
<p>（翻译说明：</p>
<ol>
<li>专业术语保留英文缩写（LLMs/KV/HSR/SVD等）并首次出现时标注中文全称</li>
<li>技术概念如"post-training"译为"训练后"符合机器学习领域惯例</li>
<li>方法名称采用加粗处理（<strong>头间相似度感知重排序</strong>）突出关键技术</li>
<li>长句拆分重组（如摘要第二句）以符合中文表达习惯</li>
<li>被动语态转换为主动句式（如"experiments show"译为"实验表明"）</li>
<li>保持学术文本的严谨性，避免口语化表达）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">交由专家处理：通过稀疏进化进行稀疏微调以修复稀疏大型语言模型</h2><a id="user-content-交由专家处理通过稀疏进化进行稀疏微调以修复稀疏大型语言模型" class="anchor" aria-label="Permalink: 交由专家处理：通过稀疏进化进行稀疏微调以修复稀疏大型语言模型" href="#交由专家处理通过稀疏进化进行稀疏微调以修复稀疏大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.24037v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）虽在多类任务中表现卓越，但其庞大的计算需求给实际部署带来挑战。现有后训练剪枝方法（如SparseGPT和Wanda）虽能有效压缩模型规模，但在高稀疏度下难以保持性能，制约了其在下游任务中的应用。传统微调方法（如全参数微调和LoRA）因需更新完整稠密矩阵而无法保持稀疏性，并不适配稀疏化LLMs。本文提出稀疏演化微调（SEFT）——专为稀疏LLMs设计的新方法，通过在微调过程中动态演化剪枝模型的稀疏拓扑结构，同时全程保持整体稀疏度。SEFT的核心优势在于采用权重丢弃-重生策略实现任务自适应，使剪枝模型能根据目标数据集自主调整稀疏连接模式，并运用敏感度驱动的剪枝准则确保微调全程维持目标稀疏度。我们在LLaMA系列、DeepSeek和Mistral等多种LLMs上的实验表明，相较于现有基线方法，SEFT在保持更优内存和时间效率的同时实现了更强性能。代码已开源：<a href="https://github.com/QiaoXiao7282/SEFT%E3%80%82">https://github.com/QiaoXiao7282/SEFT。</a></p>
<p>（注：根据学术文献翻译规范，对部分术语进行了标准化处理：</p>
<ol>
<li>"sparse topology"译为"稀疏拓扑结构"以保持计算机领域术语一致性</li>
<li>"weight drop-and-grow strategy"采用"丢弃-重生策略"的意译以准确传达算法机制</li>
<li>"sensitivity-driven pruning criterion"译为"敏感度驱动的剪枝准则"突出技术特征</li>
<li>长复合句按中文习惯拆分为多个短句，如将原文最后一句拆分为实验对象说明和结论两个部分）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">HELM：基于混合曲率专家的双曲大型语言模型</h2><a id="user-content-helm基于混合曲率专家的双曲大型语言模型" class="anchor" aria-label="Permalink: HELM：基于混合曲率专家的双曲大型语言模型" href="#helm基于混合曲率专家的双曲大型语言模型"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>（注：翻译说明如下：</p>
<ol>
<li>"Hyperbolic" 译为"双曲"，这是数学/几何学中"hyperbolic space"的标准译法</li>
<li>"Mixture-of-Curvature Experts" 采用意译结合专业术语的处理方式：
<ul>
<li>"Mixture-of-Experts"是机器学习领域的专有名词，学界通用译法为"混合专家"</li>
<li>"Curvature"在此语境下译为"曲率"（几何概念）</li>
<li>整体采用"混合曲率专家"的译法既保留原术语结构，又准确传达"不同曲率空间的专家网络组合"的技术内涵</li>
</ul>
</li>
<li>标题结构采用主副标题形式，用破折号连接，符合中文技术文献命名惯例</li>
<li>增补"基于"二字使技术路径描述更完整，同时不改变原意）</li>
</ol>
<p>arXiv:2505.24722v1 公告类型：新研究<br>
摘要：大语言模型（LLMs）在跨领域的文本建模任务中展现出卓越成效。然而，自然语言具有内在的语义层级结构和微妙的几何特性，而当前LLMs因依赖欧几里得运算未能完全捕捉这些特征。近期研究还表明，忽视词嵌入的几何特性会导致训练不稳定和生成能力退化。这些发现提示，转向非欧几里得几何能更好地使语言模型与文本的底层几何结构对齐。为此，我们提出在双曲空间（以其可扩展性、无标度性和低失真特性著称）中构建完整运算体系，推出HELM系列双曲大语言模型，对基于Transformer的LLM进行几何重构，解决了现有双曲语言模型表征僵化、必要运算缺失和扩展性不足三大难题。</p>
<p>我们进一步提出混合曲率专家模型HELM-MICE——每个专家在独特曲率空间中运作以编码更细粒度的文本几何结构，以及稠密模型HELM-D。针对HELM-MICE，我们开发了双曲多头潜在注意力机制（HMLA），实现高效的低KV缓存训练推理。对于两类模型，我们均开发了关键的双曲等效模块：旋转位置编码与RMS归一化。我们率先实现了十亿参数规模的全双曲LLMs训练，并在MMLU、ARC等涵盖STEM问题求解、通识与常识推理的基准测试中评估。实验结果表明，相比LLaMA和DeepSeek采用的欧几里得架构，HELM系列模型性能持续领先（最高提升4%），印证了双曲几何在大规模语言模型预训练中提升推理效能的核心价值。</p>
<p>（注：根据学术文本翻译规范，关键技术术语保持英文缩写+中文全称的呈现方式；长句按中文表达习惯拆分为逻辑连贯的短句；专业概念如"Mixture-of-Curvature Experts"采用"混合曲率专家"的意译方案；数学符号"KV-cache"保留英文缩写但补充说明性翻译）</p>
<div class="markdown-heading"><h2 class="heading-element">TSENOR：高效寻找可转置N:M稀疏掩码的算法</h2><a id="user-content-tsenor高效寻找可转置nm稀疏掩码的算法" class="anchor" aria-label="Permalink: TSENOR：高效寻找可转置N:M稀疏掩码的算法" href="#tsenor高效寻找可转置nm稀疏掩码的算法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.23949v1 公告类型：新研究<br>
摘要：网络剪枝技术通过降低大型神经网络的计算需求来实现模型压缩，其中N:M稀疏模式（即每连续M个权重中仅保留N个）在压缩模型质量与硬件加速之间提供了理想的平衡。然而，N:M稀疏仅能加速前向传播计算，由于矩阵转置时N:M模式无法保持，在训练阶段（正反向传播均需密集计算）的效率受到限制。虽然可转置N:M稀疏方案已被提出以解决这一缺陷，但现有方法要么无法扩展至大型模型，要么仅限于M=4的配置，导致压缩精度权衡欠佳。</p>
<p>我们提出了一种高效的可转置N:M掩码求解器，可扩展至十亿参数级模型。该方法将掩码生成建模为最优传输问题，通过熵正则化与Dykstra算法求解，辅以舍入处理。基于张量的GPU并行实现相较现有方法提速达100倍，误差仅1-10%。本方案可与Wanda、SparseGPT、ALPS等分层N:M剪枝框架结合，生成任意N:M值的可转置稀疏模型。实验表明，采用可转置16:32稀疏的LLaMA3.2-8B模型性能接近标准N:M版本，且优于标准2:4稀疏模型，验证了本方法的实用价值。</p>
<p>（注：专业术语处理说明：</p>
<ol>
<li>"transposable N:M sparsity"译为"可转置N:M稀疏"，强调矩阵转置特性</li>
<li>"optimal transport problems"采用学界通用译法"最优传输问题"</li>
<li>"Dykstra's algorithm"保留算法原名并标注"算法"以符合中文习惯</li>
<li>"billion-parameter models"译为"十亿参数级模型"以准确反映数量级</li>
<li>技术名词如"Wanda/SparseGPT/ALPS"保留原名确保专业性）</li>
</ol>
<div class="markdown-heading"><h2 class="heading-element">AutoChemSchematic AI：一种用于自动生成化工工艺与仪表流程图的闭环物理感知智能体框架</h2><a id="user-content-autochemschematic-ai一种用于自动生成化工工艺与仪表流程图的闭环物理感知智能体框架" class="anchor" aria-label="Permalink: AutoChemSchematic AI：一种用于自动生成化工工艺与仪表流程图的闭环物理感知智能体框架" href="#autochemschematic-ai一种用于自动生成化工工艺与仪表流程图的闭环物理感知智能体框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2505.24584v1 公告类型：新成果<br>
摘要：尽管生成式AI的最新进展加速了新型化学品与材料的发现，但将这些发现转化为工业化规模生产仍存在关键瓶颈——这需要开发全新的化学制造工艺流程。当前AI方法尚无法自动生成在化工放大过程中至关重要的工艺流程图（PFD）和管道仪表流程图（PID），同时满足工程约束条件。我们提出一种闭环、物理感知的自动化框架，用于生成具备工业可行性的PFD与PID。该框架通过三个核心组件，将面向化工过程QA任务训练的领域专用小规模语言模型（SLM）与第一性原理模拟相结合：（1）包含1,020+种化学品的工艺流程与仪器描述的分层知识图谱；（2）利用监督微调（SFT）、直接偏好优化（DPO）和检索增强指令调优（RAIT）在合成数据集上微调领域专用SLM的多阶段训练管道；（3）基于DWSIM的模拟器闭环验证以确保可行性。为提升运行时效率与模型紧凑性，框架整合了FlashAttention、前瞻解码、支持KV缓存量化的分页注意力机制、测试时推理缩放等先进推理优化技术，并采用启发式重要性引导的结构化剪枝（宽度与深度）以最小精度损失压缩模型规模。实验表明，该框架生成的工艺描述经模拟器验证具有高保真度，在正确性上超越基线方法，并能泛化至未见化学品。通过连接AI驱动设计与工业级可行性，本工作显著缩短了从实验室发现到工厂部署的研发周期。</p>
<p>（注：专业术语处理说明：</p>
<ol>
<li>PFDs/PIDs保留英文缩写并添加中文全称首译注释</li>
<li>SLMs采用"小规模语言模型"译法突出参数规模特征</li>
<li>DPO/RAIT等算法名按行业惯例保留英文缩写</li>
<li>"simulator in the loop"译为"闭环验证"体现控制论概念</li>
<li>"Lookahead Decoding"译为"前瞻解码"准确传达算法特性）</li>
</ol>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>