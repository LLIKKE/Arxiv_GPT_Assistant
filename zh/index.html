<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/Arxiv_GPT_Assistant</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/Arxiv_GPT_Assistant/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/Arxiv_GPT_Assistant"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">HHNAS-AM：基于自适应变异策略的分层混合神经架构搜索</h2><a id="user-content-hhnas-am基于自适应变异策略的分层混合神经架构搜索" class="anchor" aria-label="Permalink: HHNAS-AM：基于自适应变异策略的分层混合神经架构搜索" href="#hhnas-am基于自适应变异策略的分层混合神经架构搜索"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14946v1 公告类型：新研究<br>
摘要：神经架构搜索（NAS）因其能够发现优于人工设计架构的方案而获得广泛研究关注。文本表示学习对文本分类及其他语言相关任务至关重要。现有用于文本分类的NAS模型缺乏混合层次结构，且对架构结构无限制约束，导致搜索空间过于庞大且存在大量冗余，使得现有强化学习模型难以有效遍历该空间。扁平化的架构搜索还会导致搜索空间无序化，增加遍历难度。为此，我们提出HHNAS-AM（基于自适应变异策略的层次化混合神经架构搜索），通过创新方法高效探索多样化架构配置。我们引入若干架构模板来组织搜索空间，这些空间基于领域特定线索设计。该方法采用通过Q学习动态调整的变异策略，根据历史迭代的性能反馈实现更高效、更快速的搜索空间遍历。所提出的全概率模型能有效探索搜索空间。我们在数据库ID预测任务上评估本方法，其在不同实验中持续发现高性能架构。在Spider数据集上，我们的方法相比现有基线实现了8%的测试准确率提升。</p>
<div class="markdown-heading"><h2 class="heading-element">重新思考层冻结在高效深度神经网络训练中的潜力</h2><a id="user-content-重新思考层冻结在高效深度神经网络训练中的潜力" class="anchor" aria-label="Permalink: 重新思考层冻结在高效深度神经网络训练中的潜力" href="#重新思考层冻结在高效深度神经网络训练中的潜力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15033v1 公告类型：新成果<br>
摘要：随着深度神经网络与数据集规模的不断增长，训练所需的计算成本显著增加。层冻结技术作为有效降低网络训练成本的重要方法，近期受到广泛关注。然而传统层冻结方法中，被冻结层仍需参与前向传播以生成未冻结层所需的特征图，这限制了计算成本的降低幅度。为突破此限制，已有研究提出一种假设性解决方案：将冻结层输出的特征图缓存为新的数据集，使后续层能直接基于存储的特征图进行训练。该方法虽看似直接，却存在几个被既往文献严重忽视的关键挑战——例如如何对特征图有效实施数据增强，以及由此产生的巨大存储开销。若这些被忽视的挑战得不到解决，缓存方法的性能将严重受损甚至不可行。本文首次系统性地探讨这些挑战并提出完整解决方案：为提升训练精度，我们提出"相似感知通道增强"技术，以最小额外存储成本缓存具有高增强敏感度的通道；为降低存储开销，我们将有损数据压缩融入层冻结过程，设计"渐进式压缩"策略，随着冻结层数增加逐步提高压缩率，有效控制存储成本。最终，我们的方案在保持模型精度的同时显著降低训练成本，且仅引入极小的时间开销。此外，我们全面评估了冻结与压缩策略的组合效果，为优化高效深度神经网络训练提供了重要参考依据。</p>
<div class="markdown-heading"><h2 class="heading-element">利用SparseLoCo实现通信高效的大型语言模型预训练</h2><a id="user-content-利用sparseloco实现通信高效的大型语言模型预训练" class="anchor" aria-label="Permalink: 利用SparseLoCo实现通信高效的大型语言模型预训练" href="#利用sparseloco实现通信高效的大型语言模型预训练"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15706v1 公告类型：新研究<br>
摘要：通信高效的分布式训练算法近年来备受关注，因其能在带宽受限场景（如跨数据中心和互联网）中为大规模语言模型（LLM）训练提供优势。尽管这些方法降低了通信频率，但仍通常需要传输完整的模型梯度副本——即使对于跨数据中心链路也会造成通信瓶颈。此外，与朴素的AdamW DDP基线相比，其性能可能略有下降。虽然量化和误差反馈常被用于压缩伪梯度大小，但在LLM预训练背景下，现有方法尚未能额外利用稀疏化技术，且量化程度有限。本研究提出SparseLoCo——一种面向LLM的通信高效训练算法，通过有效结合Top-k稀疏化与量化技术，实现了高达1-3%稀疏度和2比特量化的极端压缩比，同时性能优于全精度DiLoCo。我们的核心发现是：外部动量可通过误差反馈结合激进稀疏化进行本地近似，且稀疏聚合实际上能提升模型性能。通过在多种通信受限的LLM训练场景中进行实证验证，我们证明SparseLoCo在性能和通信成本方面均能带来显著收益。</p>
<div class="markdown-heading"><h2 class="heading-element">归纳性领域迁移在误设的基于模拟的推理中</h2><a id="user-content-归纳性领域迁移在误设的基于模拟的推理中" class="anchor" aria-label="Permalink: 归纳性领域迁移在误设的基于模拟的推理中" href="#归纳性领域迁移在误设的基于模拟的推理中"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15593v1 公告类型：新研究<br>
摘要：基于模拟的推断（SBI）是一种统计推断方法，用于在似然函数难以处理但可进行模拟的情况下估计物理系统的潜在参数。实践中，SBI常受模型失配问题阻碍——即由于建模简化导致的模拟观测与真实观测之间的不匹配。近期提出的RoPE方法通过结合半监督校准与基于最优传输（OT）的分布对齐，以两阶段域迁移过程应对这一挑战。然而，RoPE完全基于转导式设定，需要在推断时访问批量测试样本，这限制了其扩展性与泛化能力。本文提出一种完全归纳式且摊销化的SBI框架，将校准与分布对齐整合到单一端到端可训练模型中。我们的方法利用闭式耦合的小批量OT技术，通过配对校准数据与未配对样本，对齐对应相同潜在参数的真实与模拟观测。随后训练条件标准化流来近似OT诱导的后验分布，实现在测试阶段无需模拟访问的高效推断。在包括复杂医学生物标志物估计在内的多组合成与真实基准测试中，本方法性能匹配或超越RoPE及其他标准SBI与非SBI估计器，同时在具有挑战性的失配环境中展现出更优的扩展性与适用性。</p>
<div class="markdown-heading"><h2 class="heading-element">微控制器量化神经网络：方法、平台与应用全面综述</h2><a id="user-content-微控制器量化神经网络方法平台与应用全面综述" class="anchor" aria-label="Permalink: 微控制器量化神经网络：方法、平台与应用全面综述" href="#微控制器量化神经网络方法平台与应用全面综述"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15008v1 公告类型：新成果<br>
摘要：量化神经网络（QNN）在资源受限设备（如微控制器）上的部署，面临着平衡模型性能、计算复杂度与内存限制的重大挑战。微型机器学习（TinyML）通过整合机器学习算法、硬件加速和软件优化方面的进展，有效解决了这些问题，实现在嵌入式系统上高效运行深度神经网络。本综述从硬件中心视角介绍量化技术，系统回顾了用于加速嵌入式应用深度学习模型的关键量化方法，特别着重分析了模型性能与硬件能力之间的关键权衡。研究进一步评估了现有专门支持微控制器上QNN运行的软件框架与硬件平台，并对快速发展的QNN部署领域当前面临的挑战进行了剖析，同时展望了未来具有前景的研究方向。</p>
<div class="markdown-heading"><h2 class="heading-element">Hydra：一款拥有16亿参数的状态空间语言模型，具备稀疏注意力机制、专家混合架构与内存优化功能</h2><a id="user-content-hydra一款拥有16亿参数的状态空间语言模型具备稀疏注意力机制专家混合架构与内存优化功能" class="anchor" aria-label="Permalink: Hydra：一款拥有16亿参数的状态空间语言模型，具备稀疏注意力机制、专家混合架构与内存优化功能" href="#hydra一款拥有16亿参数的状态空间语言模型具备稀疏注意力机制专家混合架构与内存优化功能"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15099v1 公告类型：新研究<br>
摘要：本文提出九头蛇（Hydra）架构方案——一种融合条件计算、长上下文记忆机制和稀疏专家混合的混合式长上下文语言模型，其参数量控制在约16亿的设计框架内。该架构以Mamba风格的结构化状态空间模型（SSM）为骨干，集成间歇性稀疏全局注意力机制、分块级MoE前馈路由以及双内存系统（工作内存+事实PKM内存）。我们规范了组件接口标准，提供透明的参数量与计算复杂度核算，并制定了分阶段课程学习策略以稳定激活各组件。配套发布的玩具级原型测试结果（在合成数据上使用数千万参数）仅用于验证实现可行性及定性扩展行为（如长上下文吞吐量拐点与可控专家路由），并非宣称具备竞品级完整性能。我们明确阐述了基本假设与开放风险（训练复杂度、内存利用率、专业化动态），并将九头蛇定位为激发后续实证研究的蓝图而非成熟系统。通过结合SSM效率、选择性稀疏注意力、MoE容量与可学习内存，该架构为模块化、输入自适应的长上下文语言模型指明发展方向，但目标规模下的终端任务增益验证尚待后续研究。</p>
<div class="markdown-heading"><h2 class="heading-element">迈向无需源数据的机器遗忘</h2><a id="user-content-迈向无需源数据的机器遗忘" class="anchor" aria-label="Permalink: 迈向无需源数据的机器遗忘" href="#迈向无需源数据的机器遗忘"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15127v1 公告类型：新研究<br>
摘要：随着机器学习技术的普及和数据隐私法规的发展，从训练好的模型中移除私有或受版权保护信息的能力已成为日益关键的需求。现有遗忘方法通常依赖于在遗忘过程中能访问完整训练数据集的假设。然而，在实际场景中，原始训练数据可能无法获取（即源数据不可用设置），这一假设未必成立。为应对此挑战，我们聚焦于源数据不可用的遗忘场景，要求遗忘算法能在无需原始训练数据集的情况下，从已训练模型中移除特定数据。基于近期研究成果，我们提出一种能估算未知剩余训练数据海森矩阵的方法——这是实现高效遗忘的关键组件。借助该估算技术，我们的方法在保持剩余数据性能的同时，既能实现高效的零样本遗忘，又能为遗忘性能提供坚实的理论保证。跨多个数据集的广泛实验验证了本方法的有效性。</p>
<div class="markdown-heading"><h2 class="heading-element">生成式神经算子以对数复杂度同时求解无穷多凸规划问题</h2><a id="user-content-生成式神经算子以对数复杂度同时求解无穷多凸规划问题" class="anchor" aria-label="Permalink: 生成式神经算子以对数复杂度同时求解无穷多凸规划问题" href="#生成式神经算子以对数复杂度同时求解无穷多凸规划问题"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.14995v1 公告类型：新成果<br>
摘要：神经算子（NOs）是一类深度学习模型，其设计目标是通过将无限多个相关问题映射到无限维空间中进行统一求解。然而理论与实践之间存在显著差距：通用逼近定理给出的最坏情况参数边界表明，神经算子可能需要不切实际的大量参数才能解决大多数算子学习问题，这与大量实验证据直接相悖。本文针对特定类别的神经算子——生成式平衡算子（GEOs），在使用（现实可行的）有限维深度平衡层求解可分离希尔伯特空间$X$上凸优化问题族时，成功弥合了这一差距。该模型以$X$上的光滑凸损失函数作为输入，输出对应每个输入损失所定义优化问题的（近似）解。</p>
<p>我们证明当输入损失函数位于合适的无限维紧集中时，所提出的GEO能够以仅按逼近误差倒数对数级增长的秩、深度和宽度，一致地逼近对应优化问题的解至任意精度。随后通过三个应用场景验证理论结果及GEO的可训练性：（1）非线性偏微分方程；（2）随机最优控制问题；（3）流动性约束下数理金融领域的对冲问题。</p>
<div class="markdown-heading"><h2 class="heading-element">S3LoRA：智能体规划器自适应中的安全谱锐度导向剪枝方法</h2><a id="user-content-s3lora智能体规划器自适应中的安全谱锐度导向剪枝方法" class="anchor" aria-label="Permalink: S3LoRA：智能体规划器自适应中的安全谱锐度导向剪枝方法" href="#s3lora智能体规划器自适应中的安全谱锐度导向剪枝方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15068v1 公告类型：新研究<br>
摘要：采用LoRA等参数高效微调（PEFT）技术对大语言模型（LLM）进行适配，已在基于LLM的智能体中展现出强大能力。然而这些适配可能无意中破坏安全对齐机制，导致不安全或不稳定行为，尤其在智能体规划任务中。现有安全感知适配方法通常需要同时获取基础模型和指令微调模型的检查点，这在实际应用中往往不可行，限制了其适用性。我们提出S3LoRA（安全谱锐度引导剪枝LoRA），这是一个轻量级、无需数据且模型无关的框架，仅通过检查微调后的权重更新即可降低LoRA适配模型的安全风险。我们首先提出幅度感知球面归一化SVD（MAS-SVD），在保留全局幅度信息的同时鲁棒分析LoRA更新的结构特性；进而设计谱锐度指数（SSI），这种锐度感知指标可检测更新高度集中且存在潜在安全风险的层级。通过事后剪枝这些风险层级，可在保持任务性能的同时降低风险。在智能体规划和语言生成任务上的大量实验与消融研究表明，S3LoRA能持续提升安全指标，同时维持或改进效用指标，并显著降低推理成本。这些结果证明S3LoRA是面向现实世界、资源受限且安全关键场景中安全部署LLM智能体的实用可扩展解决方案。</p>
<div class="markdown-heading"><h2 class="heading-element">NiceWebRL：一个用于在强化学习环境中进行人体实验的Python库</h2><a id="user-content-nicewebrl一个用于在强化学习环境中进行人体实验的python库" class="anchor" aria-label="Permalink: NiceWebRL：一个用于在强化学习环境中进行人体实验的Python库" href="#nicewebrl一个用于在强化学习环境中进行人体实验的python库"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15693v1 公告类型：新成果<br>
摘要：我们推出NiceWebRL——一款研究工具，使研究人员能够利用机器强化学习（RL）环境进行在线人类受试者实验。NiceWebRL作为Python库，可将任何基于Jax的环境转化为在线交互界面，同时支持单智能体与多智能体环境。这一工具使得AI研究者能够将其算法与人类表现进行对比，认知科学家可测试机器学习算法作为人类认知理论的可行性，多智能体研究者则能开发人机协作算法。我们通过三个案例研究展示NiceWebRL的潜力：助力开发类人AI、人本兼容AI及人类辅助AI。在首个案例（类人AI）中，NiceWebRL支持开发新型认知RL模型，通过在网格世界和2D Minecraft领域Craftax中与人类参与者对比验证该模型。第二案例（人本兼容AI）中，该工具助力开发新型多智能体RL算法，能在《Overcooked》游戏中泛化至人类合作伙伴。最后在第三案例（人类辅助AI）中，我们演示了研究者如何利用NiceWebRL探索大语言模型（LLM）在包含数百万层级任务的XLand-Minigrid环境中辅助人类完成复杂任务。该库已开源：<a href="https://github.com/KempnerInstitute/nicewebrl%E3%80%82">https://github.com/KempnerInstitute/nicewebrl。</a></p>
<div class="markdown-heading"><h2 class="heading-element">DiagECG：一种基于大语言模型的诊断推理框架，通过离散化心电信号标记化实现</h2><a id="user-content-diagecg一种基于大语言模型的诊断推理框架通过离散化心电信号标记化实现" class="anchor" aria-label="Permalink: DiagECG：一种基于大语言模型的诊断推理框架，通过离散化心电信号标记化实现" href="#diagecg一种基于大语言模型的诊断推理框架通过离散化心电信号标记化实现"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15338v1 公告类型：新研究<br>
摘要：心电图在心血管诊断中占据核心地位，但现有自动化方法往往难以跨临床任务泛化，且对开放式推理的支持有限。我们提出DiagECG创新框架，通过使大语言模型处理12导联心电图信号以完成临床文本生成任务，实现了时间序列与语言建模的融合。该框架采用导联无关的编码器和量化模块，将连续的心电图嵌入离散化为符号标记。这些标记随后用于扩展大语言模型的词汇表，使其能以统一方式处理心电图和自然语言输入。为弥合模态差异，我们通过自回归心电图预测任务对模型进行预训练，使语言模型能利用其固有的语言建模能力捕捉时序动态特征。最后，我们在心电图问答和诊断报告生成任务上进行了指令微调。在不修改核心模型的前提下，DiagECG在保持对分布外场景泛化能力的同时，实现了跨任务的强劲性能。大量实验验证了各组件的有效性，并凸显了将符号化心电图表征融入大语言模型用于医疗推理的潜力。</p>
<div class="markdown-heading"><h2 class="heading-element">采用二维时间序列方法增强基于群体数据的预测能力</h2><a id="user-content-采用二维时间序列方法增强基于群体数据的预测能力" class="anchor" aria-label="Permalink: 采用二维时间序列方法增强基于群体数据的预测能力" href="#采用二维时间序列方法增强基于群体数据的预测能力"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15369v1 公告类型：新成果<br>
摘要：本文提出了一种新颖的二维时间序列预测模型，该模型整合了群体随时间变化的行为特征，有效应对小数据环境下的预测挑战。通过多个真实数据集的验证，我们证明该模型在预测精度与适应性方面均优于现有参考模型。这一方法为面临财务与营销预测难题的各行业提供了具有战略决策价值的见解。</p>
<div class="markdown-heading"><h2 class="heading-element">Lang2Lift：集成于户外自主叉车作业中的语言引导托盘检测与位姿估计框架</h2><a id="user-content-lang2lift集成于户外自主叉车作业中的语言引导托盘检测与位姿估计框架" class="anchor" aria-label="Permalink: Lang2Lift：集成于户外自主叉车作业中的语言引导托盘检测与位姿估计框架" href="#lang2lift集成于户外自主叉车作业中的语言引导托盘检测与位姿估计框架"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>arXiv:2508.15427v1 公告类型：新研究<br>
摘要：物流与建筑行业在实现托盘自动化搬运方面持续面临挑战，尤其是在户外环境中存在载荷多变、托盘质量与尺寸不一以及非结构化周边条件等问题。本文致力于解决托盘运输中的关键环节——托盘拾取操作的自动化难题。我们的研究动机源于劳动力短缺、安全隐患以及在此类条件下人工定位与提取托盘的效率低下问题。我们提出Lang2Lift框架，该框架利用基础模型实现自然语言引导的托盘检测与六维位姿估计，使操作人员能够通过"拾取起重机附近的钢梁托盘"等直观指令指定目标。感知系统集成Florence-2与SAM-2实现语言锚定分割，结合FoundationPose在多变光照条件下的杂乱多托盘户外场景中实现鲁棒位姿估计。生成的位姿数据输入运动规划模块，实现全自动叉车操作。我们在ADAPT自动叉车平台上验证Lang2Lift系统，在真实测试数据集上达到0.76 mIoU的托盘分割精度。时序与误差分析证明了系统的稳健性，并确认其在物流与建筑作业环境中部署的可行性。视频演示详见：<a href="https://eric-nguyen1402.github.io/lang2lift.github.io/" rel="nofollow">https://eric-nguyen1402.github.io/lang2lift.github.io/</a></p>
<p>（注：mIoU为平均交并比指标，ADAPT为自主适应平台技术缩写，均保留专业术语特征）</p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/Arxiv_GPT_Assistant" href="https://github.com/LLIKKE/Arxiv_GPT_Assistant" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>