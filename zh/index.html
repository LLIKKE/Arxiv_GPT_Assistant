<!DOCTYPE html><html data-color-mode="light" data-light-theme="light" data-dark-theme="dark" lang="en-US"><head><title>LLIKKE/gpt_paper_assistant_ori</title><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="description" content="Deepseek based personalized ArXiv paper assistant bot"><link rel="canonical" href="https://llikke.github.io/gpt_paper_assistant_ori/"><meta property="og:title" content="LLIKKE/gpt_paper_assistant_ori"><meta property="og:type" content="website"><meta property="og:url" content="https://llikke.github.io/gpt_paper_assistant_ori/"><meta property="og:description" content="Deepseek based personalized ArXiv paper assistant bot"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="LLIKKE/gpt_paper_assistant_ori"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LLIKKE/gpt_paper_assistant_ori"><meta name="twitter:description" content="Deepseek based personalized ArXiv paper assistant bot"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" media="(prefers-color-scheme: light)"><link class="js-site-favicon" rel="alternate icon" type="image/png" href="https://github.githubassets.com/favicons/favicon-dark.png" media="(prefers-color-scheme: dark)"><link class="js-site-favicon" rel="icon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon-dark.svg" media="(prefers-color-scheme: dark)"><link rel="mask-icon" href="https://github.githubassets.com/pinned-octocat.svg" color="#000000"><link href="index.css" rel="stylesheet"></head><body><div class="container-lg px-3 my-5 markdown-body"><div class="position-relative"><span class="profile-color-modes-toggle js-promo-color-modes-toggle" tabindex="0" aria-label="Toggle dark mode" aria-checked="true" role="checkbox"><div class="profile-color-modes-toggle-track" div></div><div class="profile-color-modes-toggle-thumb"><svg style="fill: var(--color-scale-yellow-0); margin: 7px 0 0 7px;" aria-hidden="true" width="14" height="13" viewBox="0 0 14 13" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z"></path></svg></div></span></div><script type="text/javascript">(function() {
  var MODE_KEY = 'markdown_to_pages_dark_mode';
  function toggleMode() {
    var mode = document.documentElement.getAttribute('data-color-mode') === 'light' ? 'dark' : 'light';
    document.documentElement.setAttribute('data-color-mode', mode);
    localStorage.setItem(MODE_KEY, mode);
  }
  var mode = localStorage.getItem(MODE_KEY);
  if (mode == null) {
    var query = window.matchMedia('(prefers-color-scheme: dark)');
    mode = query.matches ? 'dark' : 'light';
  }
  document.documentElement.setAttribute('data-color-mode', mode);
  document.querySelector('.profile-color-modes-toggle').onclick = toggleMode;
})();</script><div><div class="markdown-heading"><h2 class="heading-element">RoSTE：一种针对大型语言模型的高效量化感知监督微调方法</h2><a id="user-content-roste一种针对大型语言模型的高效量化感知监督微调方法" class="anchor" aria-label="Permalink: RoSTE：一种针对大型语言模型的高效量化感知监督微调方法" href="#roste一种针对大型语言模型的高效量化感知监督微调方法"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>监督微调是一种将预训练的大型语言模型（LLMs）适配到下游任务的标准方法。量化技术最近被研究作为一种训练后技术，用于高效部署LLMs。为了获得量化微调的LLMs，传统流程会先对预训练模型进行微调，随后进行训练后量化。这种做法往往导致性能欠佳，因为它未能充分利用微调与量化之间的协同效应。为了有效实现LLMs中权重、激活值和KV缓存的低位量化，我们提出了一种名为旋转直通估计器（Rotated Straight-Through-Estimator, RoSTE）的算法，该算法结合了量化感知的监督微调（Quantization-Aware Supervised Fine-Tuning, QA-SFT）与自适应旋转策略，后者能够识别出有效的旋转配置以减少激活值中的异常值。通过分析RoSTE在应用于过参数化最小二乘量化训练问题时的预测误差，我们提供了理论见解。我们的研究发现，预测误差与收敛权重的量化误差成正比，而通过优化的旋转配置可以有效控制这一误差。在不同规模的Pythia和Llama模型上的实验验证了RoSTE的有效性。与现有的训练后量化基线相比，我们的方法在各种任务和不同LLM架构上均能持续取得更优的性能。</p>
<div class="markdown-heading"><h2 class="heading-element">通过带有类别信息的结构化Lasso实现自动修剪</h2><a id="user-content-通过带有类别信息的结构化lasso实现自动修剪" class="anchor" aria-label="Permalink: 通过带有类别信息的结构化Lasso实现自动修剪" href="#通过带有类别信息的结构化lasso实现自动修剪"><span aria-hidden="true" class="octicon octicon-link"></span></a></div>
<p>大多数剪枝方法聚焦于神经网络中不重要的过滤器。然而，由于缺乏对类别数据的考虑，这些方法面临统计信息丢失的问题。本文从利用精确类别信息进行模型剪枝的角度出发，结合信息瓶颈理论的指导，采用结构化Lasso方法。我们的方法确保了在剪枝过程中统计信息得以保留。借助这些技术，我们提出了两种创新的自适应网络剪枝方案：基于信息瓶颈的稀疏图结构Lasso剪枝（\textbf{sGLP-IB}）和基于信息瓶颈的稀疏树引导Lasso剪枝（\textbf{sTLP-IB}）。关键在于使用sGLP-IB和sTLP-IB剪枝模型过滤器，以更好地捕捉类别间的关联性。与多种先进方法相比，我们的方法在三个数据集和六种模型架构上的大量实验中展现了卓越性能。例如，在CIFAR-10数据集上使用VGG16模型，我们实现了参数减少85%，FLOPs降低61%，同时保持了94.10%的准确率（比原模型高0.14%）；在ImageNet上使用ResNet架构，我们减少了55%的参数，准确率达到76.12%（仅下降0.03%）。总之，我们成功地在保持准确性的同时，减少了模型大小和计算资源的使用。我们的代码可在<a href="https://anonymous.4open.science/r/IJCAI-8104%E8%8E%B7%E5%8F%96%E3%80%82" rel="nofollow">https://anonymous.4open.science/r/IJCAI-8104获取。</a></p>
</div></div><div class="footer container-xl width-full p-responsive"><div class="position-relative flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-4 pb-4 mt-6 f6 color-text-secondary border-top color-border-secondary text-center"><div class="footer-octicon d-lg-block mx-lg-4"><a title="LLIKKE/gpt_paper_assistant_ori" href="https://github.com/LLIKKE/gpt_paper_assistant_ori" target="_blank" rel="noreferrer noopener"><svg class="octicon octicon-mark-github gh-logo" width="36" height="36" viewBox="0 0 98 98" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"></path></svg></a></div><span class="mt-2 d-block footprint"><span>powered by </span><a href="https://github.com/wranders/markdown-to-pages-action" target="_blank" rel="noreferrer noopener">markdown-to-pages-action</a></span></div></div></body></html>